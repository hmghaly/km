{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmghaly/km/blob/main/stock_trading_support.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Start"
      ],
      "metadata": {
        "id": "jalf_0IogEiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An8Nie2b7DaE",
        "outputId": "888ac767-178a-4eab-ca18-fec40f6f4399"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "cwd='/content/drive/MyDrive/stocks' #directory where we keep the data\n",
        "os.chdir(cwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHUpJP-Dh4no"
      },
      "source": [
        "#Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HNBM21yu6pXj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "#device = torch.device('cpu')\n",
        "device = torch.device('cuda')\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, apply_sigmoid=True, apply_softmax=False, batch_size=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.apply_softmax=apply_softmax\n",
        "    self.apply_sigmoid=apply_sigmoid\n",
        "    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector \n",
        "    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)\n",
        "    self.hidden2out = nn.Linear(hidden_size, output_size)\n",
        "    if self.apply_softmax: self.softmax =nn.Softmax(dim=2)\n",
        "    if self.apply_sigmoid: self.sigmoid =nn.Sigmoid() \n",
        "    \n",
        "    #self.sigmoid = torch.sigmoid(dim=1)\n",
        "    self.hidden = self.init_hidden()\n",
        "  def forward(self, feature_list):\n",
        "    self.hidden = self.init_hidden() ### check\n",
        "    feature_list=torch.tensor(feature_list)\n",
        "    feature_list=feature_list.to(device) #### <<<<<<<<<<<<<<<<< \n",
        "    if self.matching_in_out:\n",
        "      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))\n",
        "      output_scores = self.hidden2out(lstm_out.view(len( feature_list), -1))\n",
        "      if self.apply_sigmoid: output_scores=self.sigmoid(output_scores).to(device)\n",
        "      elif self.apply_softmax: output_scores=self.softmax(output_scores).to(device)\n",
        "      #output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid\n",
        "      return output_scores #output_scores\n",
        "    else:\n",
        "      outs=[]\n",
        "      for i in range(len(feature_list)):\n",
        "        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])\n",
        "        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])\n",
        "        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)\n",
        "        outs=self.hidden2out(lstm_out)\n",
        "        if self.apply_sigmoid: outs = self.sigmoid(outs).to(device) #self.sigmoid =nn.Sigmoid()\n",
        "        elif self.apply_softmax: outs = self.softmax(outs).to(device)\n",
        "        \n",
        "      return outs\n",
        "  def init_hidden(self):\n",
        "    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)\n",
        "    return (torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device),\n",
        "            torch.rand(self.num_layers, self.batch_size, self.hidden_size).to(device))\n",
        "\n",
        "n_input=1\n",
        "n_output=3\n",
        "n_hidden =64#64\n",
        "n_layers=2\n",
        "LR=0.01\n",
        "\n",
        "#rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "\n",
        "# actual_out=[1.,0.,1.]\n",
        "# print(rnn)\n",
        "# for a in range(10):\n",
        "#   #rnn.zero_grad()\n",
        "#   input_list=[-2.,5.2,-1.1,3.2,2,8.9,-7.3]\n",
        "#   input_tensor=torch.tensor(input_list)\n",
        "#   actual_out_tensor=torch.tensor(actual_out).to(device)\n",
        "#   #input_tensor=torch.rand((5, n_input)).to(device)\n",
        "#   rnn_output = rnn(input_tensor).to(device)\n",
        "#   print(\"Input:\",input_tensor)\n",
        "#   #print(\"Output:\", output.shape)\n",
        "#   #rnn_output_list=rnn_output.ravel().tolist()\n",
        "#   rnn_output_list=rnn_output.tolist()\n",
        "#   print(rnn_output_list)\n",
        "#   #print([round(v,4) for v in rnn_output_list])\n",
        "#   loss = loss_func(actual_out_tensor.ravel(), rnn_output.ravel()) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "#   loss.backward()\n",
        "#   optimizer.step()\n",
        "#   print(\"-------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wJ2YZ2nF0NI"
      },
      "source": [
        "#Main Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDoSxjfW7rq0",
        "outputId": "541d0112-6d5a-4a3d-b399-dff88969ab24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, -25.0, 25.0, -16.67, 8.33, 16.67, 0.0]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#define functions to extract features and process labels\n",
        "def get_diff_percent(val_list,ref_val0): #ref_val is the present val\n",
        "  out_vals=[]\n",
        "  for val0 in val_list:\n",
        "    try:\n",
        "      diff0=val0-ref_val0\n",
        "      precent0=100*(diff0/ref_val0)\n",
        "      out_vals.append(round(precent0,2))\n",
        "    except: pass\n",
        "  return out_vals\n",
        "\n",
        "class io_cls: #input to output: category >< onehot\n",
        "  def __init__(self,spacing=2,max_val=10): #for a general purpose, this can be where we define the labels\n",
        "    self.spacing=spacing\n",
        "    self.max_val=max_val\n",
        "    self.all_labels=[]\n",
        "    for mv_val0 in range(-max_val,max_val+1,spacing):self.all_labels.append(str(mv_val0))\n",
        "    self.n_labels=len(self.all_labels)\n",
        "  def one_hot(self,val_list,ref_val): #and this is when we convert from categorical to one hot\n",
        "    self.diff_list=[]\n",
        "    self.one_hot_list=[]\n",
        "    for val0 in val_list:\n",
        "      diff0=val0-ref_val\n",
        "      precent0=100*(diff0/ref_val)\n",
        "      precent0_norm=int(round(precent0/self.spacing)*self.spacing) #int(round(spacing*precent0)/spacing)\n",
        "      if precent0_norm<=-self.max_val: diff_str=str(-self.max_val)\n",
        "      elif precent0_norm>=self.max_val: diff_str=str(self.max_val)\n",
        "      else: diff_str=str(precent0_norm)\n",
        "      self.diff_list.append(diff_str)\n",
        "      tmp_one_hot_vals=[0.]*len(self.all_labels)\n",
        "      if diff_str in self.all_labels: \n",
        "        tmp_i=self.all_labels.index(diff_str)\n",
        "        tmp_one_hot_vals[tmp_i]=1.\n",
        "      self.one_hot_list.append(tmp_one_hot_vals)\n",
        "    return self.one_hot_list\n",
        "  def out2labels(self,rnn_flat_out): #a flat rnn output to split into slices, and get the label weights for each slice - and then from one hot to categorical\n",
        "    final_list=[]\n",
        "    n_slices=int(len(rnn_flat_out)/len(self.all_labels))\n",
        "    for i0 in range(n_slices):\n",
        "      i1=i0+1\n",
        "      cur_slice=rnn_flat_out[i0*len(self.all_labels):i1*len(self.all_labels)]\n",
        "      tmp_list=[]\n",
        "      for lb0,cs0 in zip(self.all_labels,cur_slice): tmp_list.append((lb0,cs0))\n",
        "      tmp_list.sort(key=lambda x:-x[-1])\n",
        "      final_list.append(tmp_list)\n",
        "    return final_list\n",
        "\n",
        "#Getting the input\n",
        "def get_norm_close(fpath,prev_n0=20,next_n0=10,train_ratio=0.75):\n",
        "  pd_df=pd.read_csv(fpath)\n",
        "  close_col=pd_df[\"Close\"].fillna(0)\n",
        "  #close_col=pd_df.dropna(subset=['Close'], how='all', inplace=True)\n",
        "  #close_col = pd_df[pd_df['Close'].notna()]\n",
        "  data_len=len(close_col)\n",
        "  all_data=[]\n",
        "  for test_i in range(prev_n0,len(close_col)-next_n0):\n",
        "    prev_items=close_col[test_i-prev_n0:test_i].to_list() #[0,1,2,3,4,5,6,7,8,9] predict the closing today and the following next_n-1 days\n",
        "    next_items=close_col[test_i:test_i+next_n0].to_list()\n",
        "    all_data.append((prev_items,next_items))    \n",
        "  train_size=int(train_ratio*data_len)\n",
        "  train_data=all_data[:train_size]\n",
        "  test_data=all_data[train_size:]\n",
        "  return train_data,test_data\n",
        "\n",
        "\n",
        "cur_test_list=[12,9,15,10,13,14,12]\n",
        "cur_ref_val=12\n",
        "test_out=get_diff_percent(cur_test_list,cur_ref_val)\n",
        "print(test_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyVdax4bIdEd"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51DlutEVHCS7",
        "outputId": "6064410c-e53b-4a6a-a13d-1df3f7a1df14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stock_market_data/sp500/csv/AAPL.csv\n",
            "AAPL.csv 7786 2566\n",
            "stock_market_data/sp500/csv/GOOG.csv\n",
            "GOOG.csv 3303 1071\n",
            "stock_market_data/sp500/csv/FB.csv\n",
            "FB.csv 1839 583\n",
            "stock_market_data/sp500/csv/AMZN.csv\n",
            "AMZN.csv 4672 1528\n",
            "stock_market_data/sp500/csv/EA.csv\n",
            "EA.csv 6123 2012\n",
            "stock_market_data/sp500/csv/IBM.csv\n",
            "IBM.csv 9861 3257\n",
            "stock_market_data/sp500/csv/MSFT.csv\n",
            "MSFT.csv 6792 2234\n",
            "stock_market_data/sp500/csv/GM.csv\n",
            "GM.csv 2121 678\n",
            "stock_market_data/sp500/csv/UPS.csv\n",
            "UPS.csv 4201 1371\n",
            "stock_market_data/sp500/csv/PG.csv\n",
            "PG.csv 9861 3257\n"
          ]
        }
      ],
      "source": [
        "prev_n,next_n=20,10\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "sample_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "for fname in sample_files:\n",
        "  cur_fname= fname+\".csv\"\n",
        "  cur_fpath=os.path.join(root_dir,cur_fname)\n",
        "  print(cur_fpath)\n",
        "  cur_train0,cur_test0=get_norm_close(cur_fpath) #stock_market_data/sp500/csv/AAPL.csv\n",
        "  print(cur_fname, len(cur_train0),len(cur_test0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDZnMVcWI0Tl",
        "outputId": "0350710c-cd65-4898-e3fc-d7cb13b4ddae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "prev_vals: [1.72, 1.73, 1.72, 1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72]\n",
            "next_vals: [1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81]\n",
            "Next percent differences: [0.58, 0.0, 1.74, 1.74, 2.33, 3.49, 4.65, 4.07, 4.65, 5.23]\n",
            "min val: 0.0 @ index: 1\n",
            "max val: 5.23 @ index: 9\n",
            "max rebound val: 5.23\n",
            "rebound_diff: 5.23\n",
            "---------\n",
            "1\n",
            "prev_vals: [1.73, 1.72, 1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73]\n",
            "next_vals: [1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79]\n",
            "Next percent differences: [-0.58, 1.16, 1.16, 1.73, 2.89, 4.05, 3.47, 4.05, 4.62, 3.47]\n",
            "min val: -0.58 @ index: 0\n",
            "max val: 4.62 @ index: 8\n",
            "max rebound val: 4.62\n",
            "rebound_diff: 5.2\n",
            "---------\n",
            "2\n",
            "prev_vals: [1.72, 1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72]\n",
            "next_vals: [1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79]\n",
            "Next percent differences: [1.74, 1.74, 2.33, 3.49, 4.65, 4.07, 4.65, 5.23, 4.07, 4.07]\n",
            "min val: 1.74 @ index: 0\n",
            "max val: 5.23 @ index: 7\n",
            "max rebound val: 5.23\n",
            "rebound_diff: 3.49\n",
            "---------\n",
            "3\n",
            "prev_vals: [1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75]\n",
            "next_vals: [1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73]\n",
            "Next percent differences: [0.0, 0.57, 1.71, 2.86, 2.29, 2.86, 3.43, 2.29, 2.29, -1.14]\n",
            "min val: -1.14 @ index: 9\n",
            "max val: 3.43 @ index: 6\n",
            "max rebound val: -1.14\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "4\n",
            "prev_vals: [1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75]\n",
            "next_vals: [1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69]\n",
            "Next percent differences: [0.57, 1.71, 2.86, 2.29, 2.86, 3.43, 2.29, 2.29, -1.14, -3.43]\n",
            "min val: -3.43 @ index: 9\n",
            "max val: 3.43 @ index: 5\n",
            "max rebound val: -3.43\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "5\n",
            "prev_vals: [1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76]\n",
            "next_vals: [1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7]\n",
            "Next percent differences: [1.14, 2.27, 1.7, 2.27, 2.84, 1.7, 1.7, -1.7, -3.98, -3.41]\n",
            "min val: -3.98 @ index: 8\n",
            "max val: 2.84 @ index: 4\n",
            "max rebound val: -3.41\n",
            "rebound_diff: 0.57\n",
            "---------\n",
            "6\n",
            "prev_vals: [1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78]\n",
            "next_vals: [1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69]\n",
            "Next percent differences: [1.12, 0.56, 1.12, 1.69, 0.56, 0.56, -2.81, -5.06, -4.49, -5.06]\n",
            "min val: -5.06 @ index: 7\n",
            "max val: 1.69 @ index: 3\n",
            "max rebound val: -4.49\n",
            "rebound_diff: 0.57\n",
            "---------\n",
            "7\n",
            "prev_vals: [1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8]\n",
            "next_vals: [1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7]\n",
            "Next percent differences: [-0.56, 0.0, 0.56, -0.56, -0.56, -3.89, -6.11, -5.56, -6.11, -5.56]\n",
            "min val: -6.11 @ index: 6\n",
            "max val: 0.56 @ index: 2\n",
            "max rebound val: -5.56\n",
            "rebound_diff: 0.55\n",
            "---------\n",
            "8\n",
            "prev_vals: [1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79]\n",
            "next_vals: [1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69]\n",
            "Next percent differences: [0.56, 1.12, 0.0, 0.0, -3.35, -5.59, -5.03, -5.59, -5.03, -5.59]\n",
            "min val: -5.59 @ index: 5\n",
            "max val: 1.12 @ index: 1\n",
            "max rebound val: -5.03\n",
            "rebound_diff: 0.56\n",
            "---------\n",
            "9\n",
            "prev_vals: [1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8]\n",
            "next_vals: [1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65]\n",
            "Next percent differences: [0.56, -0.56, -0.56, -3.89, -6.11, -5.56, -6.11, -5.56, -6.11, -8.33]\n",
            "min val: -8.33 @ index: 9\n",
            "max val: 0.56 @ index: 0\n",
            "max rebound val: -8.33\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "10\n",
            "prev_vals: [1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81]\n",
            "next_vals: [1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71]\n",
            "Next percent differences: [-1.1, -1.1, -4.42, -6.63, -6.08, -6.63, -6.08, -6.63, -8.84, -5.52]\n",
            "min val: -8.84 @ index: 8\n",
            "max val: -1.1 @ index: 0\n",
            "max rebound val: -5.52\n",
            "rebound_diff: 3.32\n",
            "---------\n",
            "11\n",
            "prev_vals: [1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79]\n",
            "next_vals: [1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71]\n",
            "Next percent differences: [0.0, -3.35, -5.59, -5.03, -5.59, -5.03, -5.59, -7.82, -4.47, -4.47]\n",
            "min val: -7.82 @ index: 7\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -4.47\n",
            "rebound_diff: 3.35\n",
            "---------\n",
            "12\n",
            "prev_vals: [1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79]\n",
            "next_vals: [1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73]\n",
            "Next percent differences: [-3.35, -5.59, -5.03, -5.59, -5.03, -5.59, -7.82, -4.47, -4.47, -3.35]\n",
            "min val: -7.82 @ index: 6\n",
            "max val: -3.35 @ index: 0\n",
            "max rebound val: -3.35\n",
            "rebound_diff: 4.47\n",
            "---------\n",
            "13\n",
            "prev_vals: [1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73]\n",
            "next_vals: [1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72]\n",
            "Next percent differences: [-2.31, -1.73, -2.31, -1.73, -2.31, -4.62, -1.16, -1.16, 0.0, -0.58]\n",
            "min val: -4.62 @ index: 5\n",
            "max val: 0.0 @ index: 8\n",
            "max rebound val: 0.0\n",
            "rebound_diff: 4.62\n",
            "---------\n",
            "14\n",
            "prev_vals: [1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69]\n",
            "next_vals: [1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71]\n",
            "Next percent differences: [0.59, 0.0, 0.59, 0.0, -2.37, 1.18, 1.18, 2.37, 1.78, 1.18]\n",
            "min val: -2.37 @ index: 4\n",
            "max val: 2.37 @ index: 7\n",
            "max rebound val: 2.37\n",
            "rebound_diff: 4.74\n",
            "---------\n",
            "15\n",
            "prev_vals: [1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7]\n",
            "next_vals: [1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7]\n",
            "Next percent differences: [-0.59, 0.0, -0.59, -2.94, 0.59, 0.59, 1.76, 1.18, 0.59, 0.0]\n",
            "min val: -2.94 @ index: 3\n",
            "max val: 1.76 @ index: 6\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 4.7\n",
            "---------\n",
            "16\n",
            "prev_vals: [1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69]\n",
            "next_vals: [1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71]\n",
            "Next percent differences: [0.59, 0.0, -2.37, 1.18, 1.18, 2.37, 1.78, 1.18, 0.59, 1.18]\n",
            "min val: -2.37 @ index: 2\n",
            "max val: 2.37 @ index: 5\n",
            "max rebound val: 2.37\n",
            "rebound_diff: 4.74\n",
            "---------\n",
            "17\n",
            "prev_vals: [1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7]\n",
            "next_vals: [1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72]\n",
            "Next percent differences: [-0.59, -2.94, 0.59, 0.59, 1.76, 1.18, 0.59, 0.0, 0.59, 1.18]\n",
            "min val: -2.94 @ index: 1\n",
            "max val: 1.76 @ index: 4\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 4.7\n",
            "---------\n",
            "18\n",
            "prev_vals: [1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69]\n",
            "next_vals: [1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72]\n",
            "Next percent differences: [-2.37, 1.18, 1.18, 2.37, 1.78, 1.18, 0.59, 1.18, 1.78, 1.78]\n",
            "min val: -2.37 @ index: 0\n",
            "max val: 2.37 @ index: 3\n",
            "max rebound val: 2.37\n",
            "rebound_diff: 4.74\n",
            "---------\n",
            "19\n",
            "prev_vals: [1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65]\n",
            "next_vals: [1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72]\n",
            "Next percent differences: [3.64, 3.64, 4.85, 4.24, 3.64, 3.03, 3.64, 4.24, 4.24, 4.24]\n",
            "min val: 3.03 @ index: 5\n",
            "max val: 4.85 @ index: 2\n",
            "max rebound val: 4.24\n",
            "rebound_diff: 1.21\n",
            "---------\n",
            "20\n",
            "prev_vals: [1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71]\n",
            "next_vals: [1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7]\n",
            "Next percent differences: [0.0, 1.17, 0.58, 0.0, -0.58, 0.0, 0.58, 0.58, 0.58, -0.58]\n",
            "min val: -0.58 @ index: 4\n",
            "max val: 1.17 @ index: 1\n",
            "max rebound val: 0.58\n",
            "rebound_diff: 1.16\n",
            "---------\n",
            "21\n",
            "prev_vals: [1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71]\n",
            "next_vals: [1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66]\n",
            "Next percent differences: [1.17, 0.58, 0.0, -0.58, 0.0, 0.58, 0.58, 0.58, -0.58, -2.92]\n",
            "min val: -2.92 @ index: 9\n",
            "max val: 1.17 @ index: 0\n",
            "max rebound val: -2.92\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "22\n",
            "prev_vals: [1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73]\n",
            "next_vals: [1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66]\n",
            "Next percent differences: [-0.58, -1.16, -1.73, -1.16, -0.58, -0.58, -0.58, -1.73, -4.05, -4.05]\n",
            "min val: -4.05 @ index: 8\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -4.05\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "23\n",
            "prev_vals: [1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72]\n",
            "next_vals: [1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68]\n",
            "Next percent differences: [-0.58, -1.16, -0.58, 0.0, 0.0, 0.0, -1.16, -3.49, -3.49, -2.33]\n",
            "min val: -3.49 @ index: 7\n",
            "max val: 0.0 @ index: 3\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 1.16\n",
            "---------\n",
            "24\n",
            "prev_vals: [1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71]\n",
            "next_vals: [1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66]\n",
            "Next percent differences: [-0.58, 0.0, 0.58, 0.58, 0.58, -0.58, -2.92, -2.92, -1.75, -2.92]\n",
            "min val: -2.92 @ index: 6\n",
            "max val: 0.58 @ index: 2\n",
            "max rebound val: -1.75\n",
            "rebound_diff: 1.17\n",
            "---------\n",
            "25\n",
            "prev_vals: [1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7]\n",
            "next_vals: [1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68]\n",
            "Next percent differences: [0.59, 1.18, 1.18, 1.18, 0.0, -2.35, -2.35, -1.18, -2.35, -1.18]\n",
            "min val: -2.35 @ index: 5\n",
            "max val: 1.18 @ index: 1\n",
            "max rebound val: -1.18\n",
            "rebound_diff: 1.17\n",
            "---------\n",
            "26\n",
            "prev_vals: [1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71]\n",
            "next_vals: [1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66]\n",
            "Next percent differences: [0.58, 0.58, 0.58, -0.58, -2.92, -2.92, -1.75, -2.92, -1.75, -2.92]\n",
            "min val: -2.92 @ index: 4\n",
            "max val: 0.58 @ index: 0\n",
            "max rebound val: -1.75\n",
            "rebound_diff: 1.17\n",
            "---------\n",
            "27\n",
            "prev_vals: [1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72]\n",
            "next_vals: [1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68]\n",
            "Next percent differences: [0.0, 0.0, -1.16, -3.49, -3.49, -2.33, -3.49, -2.33, -3.49, -2.33]\n",
            "min val: -3.49 @ index: 3\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 1.16\n",
            "---------\n",
            "28\n",
            "prev_vals: [1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72]\n",
            "next_vals: [1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7]\n",
            "Next percent differences: [0.0, -1.16, -3.49, -3.49, -2.33, -3.49, -2.33, -3.49, -2.33, -1.16]\n",
            "min val: -3.49 @ index: 2\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -1.16\n",
            "rebound_diff: 2.33\n",
            "---------\n",
            "29\n",
            "prev_vals: [1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72]\n",
            "next_vals: [1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7]\n",
            "Next percent differences: [-1.16, -3.49, -3.49, -2.33, -3.49, -2.33, -3.49, -2.33, -1.16, -1.16]\n",
            "min val: -3.49 @ index: 1\n",
            "max val: -1.16 @ index: 0\n",
            "max rebound val: -1.16\n",
            "rebound_diff: 2.33\n",
            "---------\n",
            "30\n",
            "prev_vals: [1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7]\n",
            "next_vals: [1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68]\n",
            "Next percent differences: [-2.35, -2.35, -1.18, -2.35, -1.18, -2.35, -1.18, 0.0, 0.0, -1.18]\n",
            "min val: -2.35 @ index: 0\n",
            "max val: 0.0 @ index: 7\n",
            "max rebound val: 0.0\n",
            "rebound_diff: 2.35\n",
            "---------\n",
            "31\n",
            "prev_vals: [1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66]\n",
            "next_vals: [1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7]\n",
            "Next percent differences: [0.0, 1.2, 0.0, 1.2, 0.0, 1.2, 2.41, 2.41, 1.2, 2.41]\n",
            "min val: 0.0 @ index: 0\n",
            "max val: 2.41 @ index: 6\n",
            "max rebound val: 2.41\n",
            "rebound_diff: 2.41\n",
            "---------\n",
            "32\n",
            "prev_vals: [1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66]\n",
            "next_vals: [1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73]\n",
            "Next percent differences: [1.2, 0.0, 1.2, 0.0, 1.2, 2.41, 2.41, 1.2, 2.41, 4.22]\n",
            "min val: 0.0 @ index: 1\n",
            "max val: 4.22 @ index: 9\n",
            "max rebound val: 4.22\n",
            "rebound_diff: 4.22\n",
            "---------\n",
            "33\n",
            "prev_vals: [1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68]\n",
            "next_vals: [1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72]\n",
            "Next percent differences: [-1.19, 0.0, -1.19, 0.0, 1.19, 1.19, 0.0, 1.19, 2.98, 2.38]\n",
            "min val: -1.19 @ index: 0\n",
            "max val: 2.98 @ index: 8\n",
            "max rebound val: 2.98\n",
            "rebound_diff: 4.17\n",
            "---------\n",
            "34\n",
            "prev_vals: [1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66]\n",
            "next_vals: [1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72]\n",
            "Next percent differences: [1.2, 0.0, 1.2, 2.41, 2.41, 1.2, 2.41, 4.22, 3.61, 3.61]\n",
            "min val: 0.0 @ index: 1\n",
            "max val: 4.22 @ index: 7\n",
            "max rebound val: 4.22\n",
            "rebound_diff: 4.22\n",
            "---------\n",
            "35\n",
            "prev_vals: [1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68]\n",
            "next_vals: [1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71]\n",
            "Next percent differences: [-1.19, 0.0, 1.19, 1.19, 0.0, 1.19, 2.98, 2.38, 2.38, 1.79]\n",
            "min val: -1.19 @ index: 0\n",
            "max val: 2.98 @ index: 6\n",
            "max rebound val: 2.98\n",
            "rebound_diff: 4.17\n",
            "---------\n",
            "36\n",
            "prev_vals: [1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66]\n",
            "next_vals: [1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72]\n",
            "Next percent differences: [1.2, 2.41, 2.41, 1.2, 2.41, 4.22, 3.61, 3.61, 3.01, 3.61]\n",
            "min val: 1.2 @ index: 0\n",
            "max val: 4.22 @ index: 5\n",
            "max rebound val: 4.22\n",
            "rebound_diff: 3.02\n",
            "---------\n",
            "37\n",
            "prev_vals: [1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68]\n",
            "next_vals: [1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71]\n",
            "Next percent differences: [1.19, 1.19, 0.0, 1.19, 2.98, 2.38, 2.38, 1.79, 2.38, 1.79]\n",
            "min val: 0.0 @ index: 2\n",
            "max val: 2.98 @ index: 4\n",
            "max rebound val: 2.98\n",
            "rebound_diff: 2.98\n",
            "---------\n",
            "38\n",
            "prev_vals: [1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7]\n",
            "next_vals: [1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7]\n",
            "Next percent differences: [0.0, -1.18, 0.0, 1.76, 1.18, 1.18, 0.59, 1.18, 0.59, 0.0]\n",
            "min val: -1.18 @ index: 1\n",
            "max val: 1.76 @ index: 3\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 2.94\n",
            "---------\n",
            "39\n",
            "prev_vals: [1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7]\n",
            "next_vals: [1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69]\n",
            "Next percent differences: [-1.18, 0.0, 1.76, 1.18, 1.18, 0.59, 1.18, 0.59, 0.0, -0.59]\n",
            "min val: -1.18 @ index: 0\n",
            "max val: 1.76 @ index: 2\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 2.94\n",
            "---------\n",
            "40\n",
            "prev_vals: [1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68]\n",
            "next_vals: [1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67]\n",
            "Next percent differences: [1.19, 2.98, 2.38, 2.38, 1.79, 2.38, 1.79, 1.19, 0.6, -0.6]\n",
            "min val: -0.6 @ index: 9\n",
            "max val: 2.98 @ index: 1\n",
            "max rebound val: -0.6\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "41\n",
            "prev_vals: [1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7]\n",
            "next_vals: [1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67]\n",
            "Next percent differences: [1.76, 1.18, 1.18, 0.59, 1.18, 0.59, 0.0, -0.59, -1.76, -1.76]\n",
            "min val: -1.76 @ index: 8\n",
            "max val: 1.76 @ index: 0\n",
            "max rebound val: -1.76\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "42\n",
            "prev_vals: [1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73]\n",
            "next_vals: [1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66]\n",
            "Next percent differences: [-0.58, -0.58, -1.16, -0.58, -1.16, -1.73, -2.31, -3.47, -3.47, -4.05]\n",
            "min val: -4.05 @ index: 9\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -4.05\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "43\n",
            "prev_vals: [1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72]\n",
            "next_vals: [1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63]\n",
            "Next percent differences: [0.0, -0.58, 0.0, -0.58, -1.16, -1.74, -2.91, -2.91, -3.49, -5.23]\n",
            "min val: -5.23 @ index: 9\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -5.23\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "44\n",
            "prev_vals: [1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72]\n",
            "next_vals: [1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68]\n",
            "Next percent differences: [-0.58, 0.0, -0.58, -1.16, -1.74, -2.91, -2.91, -3.49, -5.23, -2.33]\n",
            "min val: -5.23 @ index: 8\n",
            "max val: 0.0 @ index: 1\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 2.9\n",
            "---------\n",
            "45\n",
            "prev_vals: [1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71]\n",
            "next_vals: [1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66]\n",
            "Next percent differences: [0.58, 0.0, -0.58, -1.17, -2.34, -2.34, -2.92, -4.68, -1.75, -2.92]\n",
            "min val: -4.68 @ index: 7\n",
            "max val: 0.58 @ index: 0\n",
            "max rebound val: -1.75\n",
            "rebound_diff: 2.93\n",
            "---------\n",
            "46\n",
            "prev_vals: [1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72]\n",
            "next_vals: [1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64]\n",
            "Next percent differences: [-0.58, -1.16, -1.74, -2.91, -2.91, -3.49, -5.23, -2.33, -3.49, -4.65]\n",
            "min val: -5.23 @ index: 6\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 2.9\n",
            "---------\n",
            "47\n",
            "prev_vals: [1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71]\n",
            "next_vals: [1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62]\n",
            "Next percent differences: [-0.58, -1.17, -2.34, -2.34, -2.92, -4.68, -1.75, -2.92, -4.09, -5.26]\n",
            "min val: -5.26 @ index: 9\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -5.26\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "48\n",
            "prev_vals: [1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7]\n",
            "next_vals: [1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59]\n",
            "Next percent differences: [-0.59, -1.76, -1.76, -2.35, -4.12, -1.18, -2.35, -3.53, -4.71, -6.47]\n",
            "min val: -6.47 @ index: 9\n",
            "max val: -0.59 @ index: 0\n",
            "max rebound val: -6.47\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "49\n",
            "prev_vals: [1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69]\n",
            "next_vals: [1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6]\n",
            "Next percent differences: [-1.18, -1.18, -1.78, -3.55, -0.59, -1.78, -2.96, -4.14, -5.92, -5.33]\n",
            "min val: -5.92 @ index: 8\n",
            "max val: -0.59 @ index: 4\n",
            "max rebound val: -5.33\n",
            "rebound_diff: 0.59\n",
            "---------\n",
            "50\n",
            "prev_vals: [1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67]\n",
            "next_vals: [1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55]\n",
            "Next percent differences: [0.0, -0.6, -2.4, 0.6, -0.6, -1.8, -2.99, -4.79, -4.19, -7.19]\n",
            "min val: -7.19 @ index: 9\n",
            "max val: 0.6 @ index: 3\n",
            "max rebound val: -7.19\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "51\n",
            "prev_vals: [1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67]\n",
            "next_vals: [1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56]\n",
            "Next percent differences: [-0.6, -2.4, 0.6, -0.6, -1.8, -2.99, -4.79, -4.19, -7.19, -6.59]\n",
            "min val: -7.19 @ index: 8\n",
            "max val: 0.6 @ index: 2\n",
            "max rebound val: -6.59\n",
            "rebound_diff: 0.6\n",
            "---------\n",
            "52\n",
            "prev_vals: [1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66]\n",
            "next_vals: [1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58]\n",
            "Next percent differences: [-1.81, 1.2, 0.0, -1.2, -2.41, -4.22, -3.61, -6.63, -6.02, -4.82]\n",
            "min val: -6.63 @ index: 7\n",
            "max val: 1.2 @ index: 1\n",
            "max rebound val: -4.82\n",
            "rebound_diff: 1.81\n",
            "---------\n",
            "53\n",
            "prev_vals: [1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63]\n",
            "next_vals: [1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56]\n",
            "Next percent differences: [3.07, 1.84, 0.61, -0.61, -2.45, -1.84, -4.91, -4.29, -3.07, -4.29]\n",
            "min val: -4.91 @ index: 6\n",
            "max val: 3.07 @ index: 0\n",
            "max rebound val: -3.07\n",
            "rebound_diff: 1.84\n",
            "---------\n",
            "54\n",
            "prev_vals: [1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68]\n",
            "next_vals: [1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54]\n",
            "Next percent differences: [-1.19, -2.38, -3.57, -5.36, -4.76, -7.74, -7.14, -5.95, -7.14, -8.33]\n",
            "min val: -8.33 @ index: 9\n",
            "max val: -1.19 @ index: 0\n",
            "max rebound val: -8.33\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "55\n",
            "prev_vals: [1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66]\n",
            "next_vals: [1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5]\n",
            "Next percent differences: [-1.2, -2.41, -4.22, -3.61, -6.63, -6.02, -4.82, -6.02, -7.23, -9.64]\n",
            "min val: -9.64 @ index: 9\n",
            "max val: -1.2 @ index: 0\n",
            "max rebound val: -9.64\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "56\n",
            "prev_vals: [1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64]\n",
            "next_vals: [1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47]\n",
            "Next percent differences: [-1.22, -3.05, -2.44, -5.49, -4.88, -3.66, -4.88, -6.1, -8.54, -10.37]\n",
            "min val: -10.37 @ index: 9\n",
            "max val: -1.22 @ index: 0\n",
            "max rebound val: -10.37\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "57\n",
            "prev_vals: [1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62]\n",
            "next_vals: [1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48]\n",
            "Next percent differences: [-1.85, -1.23, -4.32, -3.7, -2.47, -3.7, -4.94, -7.41, -9.26, -8.64]\n",
            "min val: -9.26 @ index: 8\n",
            "max val: -1.23 @ index: 1\n",
            "max rebound val: -8.64\n",
            "rebound_diff: 0.62\n",
            "---------\n",
            "58\n",
            "prev_vals: [1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59]\n",
            "next_vals: [1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52]\n",
            "Next percent differences: [0.63, -2.52, -1.89, -0.63, -1.89, -3.14, -5.66, -7.55, -6.92, -4.4]\n",
            "min val: -7.55 @ index: 7\n",
            "max val: 0.63 @ index: 0\n",
            "max rebound val: -4.4\n",
            "rebound_diff: 3.15\n",
            "---------\n",
            "59\n",
            "prev_vals: [1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6]\n",
            "next_vals: [1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49]\n",
            "Next percent differences: [-3.13, -2.5, -1.25, -2.5, -3.75, -6.25, -8.13, -7.5, -5.0, -6.88]\n",
            "min val: -8.13 @ index: 6\n",
            "max val: -1.25 @ index: 2\n",
            "max rebound val: -5.0\n",
            "rebound_diff: 3.13\n",
            "---------\n",
            "60\n",
            "prev_vals: [1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55]\n",
            "next_vals: [1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47]\n",
            "Next percent differences: [0.65, 1.94, 0.65, -0.65, -3.23, -5.16, -4.52, -1.94, -3.87, -5.16]\n",
            "min val: -5.16 @ index: 5\n",
            "max val: 1.94 @ index: 1\n",
            "max rebound val: -1.94\n",
            "rebound_diff: 3.22\n",
            "---------\n",
            "61\n",
            "prev_vals: [1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56]\n",
            "next_vals: [1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45]\n",
            "Next percent differences: [1.28, 0.0, -1.28, -3.85, -5.77, -5.13, -2.56, -4.49, -5.77, -7.05]\n",
            "min val: -7.05 @ index: 9\n",
            "max val: 1.28 @ index: 0\n",
            "max rebound val: -7.05\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "62\n",
            "prev_vals: [1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58]\n",
            "next_vals: [1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44]\n",
            "Next percent differences: [-1.27, -2.53, -5.06, -6.96, -6.33, -3.8, -5.7, -6.96, -8.23, -8.86]\n",
            "min val: -8.86 @ index: 9\n",
            "max val: -1.27 @ index: 0\n",
            "max rebound val: -8.86\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "63\n",
            "prev_vals: [1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56]\n",
            "next_vals: [1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44]\n",
            "Next percent differences: [-1.28, -3.85, -5.77, -5.13, -2.56, -4.49, -5.77, -7.05, -7.69, -7.69]\n",
            "min val: -7.69 @ index: 8\n",
            "max val: -1.28 @ index: 0\n",
            "max rebound val: -7.69\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "64\n",
            "prev_vals: [1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54]\n",
            "next_vals: [1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51]\n",
            "Next percent differences: [-2.6, -4.55, -3.9, -1.3, -3.25, -4.55, -5.84, -6.49, -6.49, -1.95]\n",
            "min val: -6.49 @ index: 7\n",
            "max val: -1.3 @ index: 3\n",
            "max rebound val: -1.95\n",
            "rebound_diff: 4.54\n",
            "---------\n",
            "65\n",
            "prev_vals: [1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5]\n",
            "next_vals: [1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5]\n",
            "Next percent differences: [-2.0, -1.33, 1.33, -0.67, -2.0, -3.33, -4.0, -4.0, 0.67, 0.0]\n",
            "min val: -4.0 @ index: 6\n",
            "max val: 1.33 @ index: 2\n",
            "max rebound val: 0.67\n",
            "rebound_diff: 4.67\n",
            "---------\n",
            "66\n",
            "prev_vals: [1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47]\n",
            "next_vals: [1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45]\n",
            "Next percent differences: [0.68, 3.4, 1.36, 0.0, -1.36, -2.04, -2.04, 2.72, 2.04, -1.36]\n",
            "min val: -2.04 @ index: 5\n",
            "max val: 3.4 @ index: 1\n",
            "max rebound val: 2.72\n",
            "rebound_diff: 4.76\n",
            "---------\n",
            "67\n",
            "prev_vals: [1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48]\n",
            "next_vals: [1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41]\n",
            "Next percent differences: [2.7, 0.68, -0.68, -2.03, -2.7, -2.7, 2.03, 1.35, -2.03, -4.73]\n",
            "min val: -4.73 @ index: 9\n",
            "max val: 2.7 @ index: 0\n",
            "max rebound val: -4.73\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "68\n",
            "prev_vals: [1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52]\n",
            "next_vals: [1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39]\n",
            "Next percent differences: [-1.97, -3.29, -4.61, -5.26, -5.26, -0.66, -1.32, -4.61, -7.24, -8.55]\n",
            "min val: -8.55 @ index: 9\n",
            "max val: -0.66 @ index: 5\n",
            "max rebound val: -8.55\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "69\n",
            "prev_vals: [1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49]\n",
            "next_vals: [1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34]\n",
            "Next percent differences: [-1.34, -2.68, -3.36, -3.36, 1.34, 0.67, -2.68, -5.37, -6.71, -10.07]\n",
            "min val: -10.07 @ index: 9\n",
            "max val: 1.34 @ index: 4\n",
            "max rebound val: -10.07\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "70\n",
            "prev_vals: [1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47]\n",
            "next_vals: [1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3]\n",
            "Next percent differences: [-1.36, -2.04, -2.04, 2.72, 2.04, -1.36, -4.08, -5.44, -8.84, -11.56]\n",
            "min val: -11.56 @ index: 9\n",
            "max val: 2.72 @ index: 3\n",
            "max rebound val: -11.56\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "71\n",
            "prev_vals: [1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45]\n",
            "next_vals: [1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26]\n",
            "Next percent differences: [-0.69, -0.69, 4.14, 3.45, 0.0, -2.76, -4.14, -7.59, -10.34, -13.1]\n",
            "min val: -13.1 @ index: 9\n",
            "max val: 4.14 @ index: 2\n",
            "max rebound val: -13.1\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "72\n",
            "prev_vals: [1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44]\n",
            "next_vals: [1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38]\n",
            "Next percent differences: [0.0, 4.86, 4.17, 0.69, -2.08, -3.47, -6.94, -9.72, -12.5, -4.17]\n",
            "min val: -12.5 @ index: 8\n",
            "max val: 4.86 @ index: 1\n",
            "max rebound val: -4.17\n",
            "rebound_diff: 8.33\n",
            "---------\n",
            "73\n",
            "prev_vals: [1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44]\n",
            "next_vals: [1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46]\n",
            "Next percent differences: [4.86, 4.17, 0.69, -2.08, -3.47, -6.94, -9.72, -12.5, -4.17, 1.39]\n",
            "min val: -12.5 @ index: 7\n",
            "max val: 4.86 @ index: 0\n",
            "max rebound val: 1.39\n",
            "rebound_diff: 13.89\n",
            "---------\n",
            "74\n",
            "prev_vals: [1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51]\n",
            "next_vals: [1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53]\n",
            "Next percent differences: [-0.66, -3.97, -6.62, -7.95, -11.26, -13.91, -16.56, -8.61, -3.31, 1.32]\n",
            "min val: -16.56 @ index: 6\n",
            "max val: 1.32 @ index: 9\n",
            "max rebound val: 1.32\n",
            "rebound_diff: 17.88\n",
            "---------\n",
            "75\n",
            "prev_vals: [1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5]\n",
            "next_vals: [1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52]\n",
            "Next percent differences: [-3.33, -6.0, -7.33, -10.67, -13.33, -16.0, -8.0, -2.67, 2.0, 1.33]\n",
            "min val: -16.0 @ index: 5\n",
            "max val: 2.0 @ index: 8\n",
            "max rebound val: 2.0\n",
            "rebound_diff: 18.0\n",
            "---------\n",
            "76\n",
            "prev_vals: [1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45]\n",
            "next_vals: [1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51]\n",
            "Next percent differences: [-2.76, -4.14, -7.59, -10.34, -13.1, -4.83, 0.69, 5.52, 4.83, 4.14]\n",
            "min val: -13.1 @ index: 4\n",
            "max val: 5.52 @ index: 7\n",
            "max rebound val: 5.52\n",
            "rebound_diff: 18.62\n",
            "---------\n",
            "77\n",
            "prev_vals: [1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41]\n",
            "next_vals: [1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51]\n",
            "Next percent differences: [-1.42, -4.96, -7.8, -10.64, -2.13, 3.55, 8.51, 7.8, 7.09, 7.09]\n",
            "min val: -10.64 @ index: 3\n",
            "max val: 8.51 @ index: 6\n",
            "max rebound val: 8.51\n",
            "rebound_diff: 19.15\n",
            "---------\n",
            "78\n",
            "prev_vals: [1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39]\n",
            "next_vals: [1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52]\n",
            "Next percent differences: [-3.6, -6.47, -9.35, -0.72, 5.04, 10.07, 9.35, 8.63, 8.63, 9.35]\n",
            "min val: -9.35 @ index: 2\n",
            "max val: 10.07 @ index: 5\n",
            "max rebound val: 10.07\n",
            "rebound_diff: 19.42\n",
            "---------\n",
            "79\n",
            "prev_vals: [1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34]\n",
            "next_vals: [1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48]\n",
            "Next percent differences: [-2.99, -5.97, 2.99, 8.96, 14.18, 13.43, 12.69, 12.69, 13.43, 10.45]\n",
            "min val: -5.97 @ index: 1\n",
            "max val: 14.18 @ index: 4\n",
            "max rebound val: 14.18\n",
            "rebound_diff: 20.15\n",
            "---------\n",
            "80\n",
            "prev_vals: [1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3]\n",
            "next_vals: [1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49]\n",
            "Next percent differences: [-3.08, 6.15, 12.31, 17.69, 16.92, 16.15, 16.15, 16.92, 13.85, 14.62]\n",
            "min val: -3.08 @ index: 0\n",
            "max val: 17.69 @ index: 3\n",
            "max rebound val: 17.69\n",
            "rebound_diff: 20.77\n",
            "---------\n",
            "81\n",
            "prev_vals: [1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26]\n",
            "next_vals: [1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48]\n",
            "Next percent differences: [9.52, 15.87, 21.43, 20.63, 19.84, 19.84, 20.63, 17.46, 18.25, 17.46]\n",
            "min val: 9.52 @ index: 0\n",
            "max val: 21.43 @ index: 2\n",
            "max rebound val: 21.43\n",
            "rebound_diff: 11.91\n",
            "---------\n",
            "82\n",
            "prev_vals: [1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38]\n",
            "next_vals: [1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47]\n",
            "Next percent differences: [5.8, 10.87, 10.14, 9.42, 9.42, 10.14, 7.25, 7.97, 7.25, 6.52]\n",
            "min val: 5.8 @ index: 0\n",
            "max val: 10.87 @ index: 1\n",
            "max rebound val: 10.87\n",
            "rebound_diff: 5.07\n",
            "---------\n",
            "83\n",
            "prev_vals: [1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46]\n",
            "next_vals: [1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42]\n",
            "Next percent differences: [4.79, 4.11, 3.42, 3.42, 4.11, 1.37, 2.05, 1.37, 0.68, -2.74]\n",
            "min val: -2.74 @ index: 9\n",
            "max val: 4.79 @ index: 0\n",
            "max rebound val: -2.74\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "84\n",
            "prev_vals: [1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53]\n",
            "next_vals: [1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46]\n",
            "Next percent differences: [-0.65, -1.31, -1.31, -0.65, -3.27, -2.61, -3.27, -3.92, -7.19, -4.58]\n",
            "min val: -7.19 @ index: 8\n",
            "max val: -0.65 @ index: 0\n",
            "max rebound val: -4.58\n",
            "rebound_diff: 2.61\n",
            "---------\n",
            "85\n",
            "prev_vals: [1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52]\n",
            "next_vals: [1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48]\n",
            "Next percent differences: [-0.66, -0.66, 0.0, -2.63, -1.97, -2.63, -3.29, -6.58, -3.95, -2.63]\n",
            "min val: -6.58 @ index: 7\n",
            "max val: 0.0 @ index: 2\n",
            "max rebound val: -2.63\n",
            "rebound_diff: 3.95\n",
            "---------\n",
            "86\n",
            "prev_vals: [1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51]\n",
            "next_vals: [1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53]\n",
            "Next percent differences: [0.0, 0.66, -1.99, -1.32, -1.99, -2.65, -5.96, -3.31, -1.99, 1.32]\n",
            "min val: -5.96 @ index: 6\n",
            "max val: 1.32 @ index: 9\n",
            "max rebound val: 1.32\n",
            "rebound_diff: 7.28\n",
            "---------\n",
            "87\n",
            "prev_vals: [1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51]\n",
            "next_vals: [1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52]\n",
            "Next percent differences: [0.66, -1.99, -1.32, -1.99, -2.65, -5.96, -3.31, -1.99, 1.32, 0.66]\n",
            "min val: -5.96 @ index: 5\n",
            "max val: 1.32 @ index: 8\n",
            "max rebound val: 1.32\n",
            "rebound_diff: 7.28\n",
            "---------\n",
            "88\n",
            "prev_vals: [1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52]\n",
            "next_vals: [1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55]\n",
            "Next percent differences: [-2.63, -1.97, -2.63, -3.29, -6.58, -3.95, -2.63, 0.66, 0.0, 1.97]\n",
            "min val: -6.58 @ index: 4\n",
            "max val: 1.97 @ index: 9\n",
            "max rebound val: 1.97\n",
            "rebound_diff: 8.55\n",
            "---------\n",
            "89\n",
            "prev_vals: [1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48]\n",
            "next_vals: [1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59]\n",
            "Next percent differences: [0.68, 0.0, -0.68, -4.05, -1.35, 0.0, 3.38, 2.7, 4.73, 7.43]\n",
            "min val: -4.05 @ index: 3\n",
            "max val: 7.43 @ index: 9\n",
            "max rebound val: 7.43\n",
            "rebound_diff: 11.48\n",
            "---------\n",
            "90\n",
            "prev_vals: [1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49]\n",
            "next_vals: [1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55]\n",
            "Next percent differences: [-0.67, -1.34, -4.7, -2.01, -0.67, 2.68, 2.01, 4.03, 6.71, 4.03]\n",
            "min val: -4.7 @ index: 2\n",
            "max val: 6.71 @ index: 8\n",
            "max rebound val: 6.71\n",
            "rebound_diff: 11.41\n",
            "---------\n",
            "91\n",
            "prev_vals: [1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48]\n",
            "next_vals: [1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55]\n",
            "Next percent differences: [-0.68, -4.05, -1.35, 0.0, 3.38, 2.7, 4.73, 7.43, 4.73, 4.73]\n",
            "min val: -4.05 @ index: 1\n",
            "max val: 7.43 @ index: 7\n",
            "max rebound val: 7.43\n",
            "rebound_diff: 11.48\n",
            "---------\n",
            "92\n",
            "prev_vals: [1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47]\n",
            "next_vals: [1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55]\n",
            "Next percent differences: [-3.4, -0.68, 0.68, 4.08, 3.4, 5.44, 8.16, 5.44, 5.44, 5.44]\n",
            "min val: -3.4 @ index: 0\n",
            "max val: 8.16 @ index: 6\n",
            "max rebound val: 8.16\n",
            "rebound_diff: 11.56\n",
            "---------\n",
            "93\n",
            "prev_vals: [1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42]\n",
            "next_vals: [1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54]\n",
            "Next percent differences: [2.82, 4.23, 7.75, 7.04, 9.15, 11.97, 9.15, 9.15, 9.15, 8.45]\n",
            "min val: 2.82 @ index: 0\n",
            "max val: 11.97 @ index: 5\n",
            "max rebound val: 11.97\n",
            "rebound_diff: 9.15\n",
            "---------\n",
            "94\n",
            "prev_vals: [1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46]\n",
            "next_vals: [1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49]\n",
            "Next percent differences: [1.37, 4.79, 4.11, 6.16, 8.9, 6.16, 6.16, 6.16, 5.48, 2.05]\n",
            "min val: 1.37 @ index: 0\n",
            "max val: 8.9 @ index: 4\n",
            "max rebound val: 8.9\n",
            "rebound_diff: 7.53\n",
            "---------\n",
            "95\n",
            "prev_vals: [1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48]\n",
            "next_vals: [1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47]\n",
            "Next percent differences: [3.38, 2.7, 4.73, 7.43, 4.73, 4.73, 4.73, 4.05, 0.68, -0.68]\n",
            "min val: -0.68 @ index: 9\n",
            "max val: 7.43 @ index: 3\n",
            "max rebound val: -0.68\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "96\n",
            "prev_vals: [1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53]\n",
            "next_vals: [1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47]\n",
            "Next percent differences: [-0.65, 1.31, 3.92, 1.31, 1.31, 1.31, 0.65, -2.61, -3.92, -3.92]\n",
            "min val: -3.92 @ index: 8\n",
            "max val: 3.92 @ index: 2\n",
            "max rebound val: -3.92\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "97\n",
            "prev_vals: [1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52]\n",
            "next_vals: [1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47, 1.5]\n",
            "Next percent differences: [1.97, 4.61, 1.97, 1.97, 1.97, 1.32, -1.97, -3.29, -3.29, -1.32]\n",
            "min val: -3.29 @ index: 7\n",
            "max val: 4.61 @ index: 1\n",
            "max rebound val: -1.32\n",
            "rebound_diff: 1.97\n",
            "---------\n",
            "98\n",
            "prev_vals: [1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55]\n",
            "next_vals: [1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47, 1.5, 1.5]\n",
            "Next percent differences: [2.58, 0.0, 0.0, 0.0, -0.65, -3.87, -5.16, -5.16, -3.23, -3.23]\n",
            "min val: -5.16 @ index: 6\n",
            "max val: 2.58 @ index: 0\n",
            "max rebound val: -3.23\n",
            "rebound_diff: 1.93\n",
            "---------\n",
            "99\n",
            "prev_vals: [1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59]\n",
            "next_vals: [1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47, 1.5, 1.5, 1.52]\n",
            "Next percent differences: [-2.52, -2.52, -2.52, -3.14, -6.29, -7.55, -7.55, -5.66, -5.66, -4.4]\n",
            "min val: -7.55 @ index: 5\n",
            "max val: -2.52 @ index: 0\n",
            "max rebound val: -4.4\n",
            "rebound_diff: 3.15\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "train_i=130\n",
        "for train_i in range(100):\n",
        "  print(train_i)\n",
        "  prev_vals,next_vals=cur_train0[train_i]\n",
        "  prev_vals=[round(v,2) for v in prev_vals]\n",
        "  next_vals=[round(v,2) for v in next_vals]\n",
        "  prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "  min_val=min(next_percents)\n",
        "  min_val_index=next_percents.index(min_val)\n",
        "  max_val=max(next_percents)\n",
        "  max_val_index=next_percents.index(max_val)\n",
        "  rebound_max_val=max(next_percents[min_val_index:])\n",
        "  rebound_diff=round(rebound_max_val-min_val,2)\n",
        "\n",
        "  print(\"prev_vals:\", prev_vals)\n",
        "  print(\"next_vals:\", next_vals)\n",
        "  #print(prev_percents)\n",
        "  print(\"Next percent differences:\", next_percents)\n",
        "  print(\"min val:\",min_val, \"@ index:\",min_val_index)\n",
        "  print(\"max val:\",max_val, \"@ index:\",max_val_index)\n",
        "  print(\"max rebound val:\",rebound_max_val)\n",
        "  print(\"rebound_diff:\",rebound_diff)\n",
        "  print(\"---------\")\n",
        "\n",
        "#print(\"min val:\",min_next_diff_val, \"@ index:\",min_val_index, \"max val:\",max_next_diff_val, \"rebound_max_val:\",rebound_max_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP349k1sESqr"
      },
      "source": [
        "#Starting Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z0dOUrLJAAs",
        "outputId": "897df98f-8dda-44ee-aa57-760dab2bb1e5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading data\n",
            "all_training 632849 all_testing 155258\n",
            "loaded model for this epoch /content/drive/MyDrive/stocks/models/exp6-pred2-combined-stocks1-4layer-batches2/model-0.model\n",
            "train_loss 0.129435\n",
            "test_loss 0.130063\n",
            "loaded model for this epoch /content/drive/MyDrive/stocks/models/exp6-pred2-combined-stocks1-4layer-batches2/model-1.model\n",
            "train_loss 0.127988\n",
            "test_loss 0.12422\n",
            "loaded model for this epoch /content/drive/MyDrive/stocks/models/exp6-pred2-combined-stocks1-4layer-batches2/model-2.model\n",
            "train_loss 0.127113\n",
            "test_loss 0.120902\n",
            "epoch0 3\n",
            "batch_i0 0 cur_train_items 10000 cur_test_items 2464\n",
            "loaded model for this epoch /content/drive/MyDrive/stocks/models/exp6-pred2-combined-stocks1-4layer-batches2/tmp/model-batch-0.model\n",
            "batch_i0 1 cur_train_items 10000 cur_test_items 2464\n",
            "loaded model for this epoch /content/drive/MyDrive/stocks/models/exp6-pred2-combined-stocks1-4layer-batches2/tmp/model-batch-1.model\n",
            "batch_i0 2 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "40 rnn out: [0.6386612057685852, 0.3049921691417694] actual: [1.0, 0.0] loss: 0.11179296672344208\n",
            "92 rnn out: [0.675680935382843, 0.34003931283950806] actual: [0.0, 1.0] loss: 0.4460464119911194\n",
            "134 rnn out: [0.6416076421737671, 0.2823859751224518] actual: [0.0, 0.0] loss: 0.24570110440254211\n",
            "148 rnn out: [0.614881694316864, 0.2910372316837311] actual: [0.0, 0.0] loss: 0.2313910871744156\n",
            "171 rnn out: [0.6062631607055664, 0.24598395824432373] actual: [1.0, 0.0] loss: 0.10776840150356293\n",
            "176 rnn out: [0.6710989475250244, 0.3122907876968384] actual: [0.0, 0.0] loss: 0.27394968271255493\n",
            "281 rnn out: [0.6279201507568359, 0.29323819279670715] actual: [0.0, 0.0] loss: 0.24013617634773254\n",
            "353 rnn out: [0.6046468019485474, 0.23933196067810059] actual: [1.0, 0.0] loss: 0.10679197311401367\n",
            "383 rnn out: [0.6083676218986511, 0.2651468515396118] actual: [1.0, 0.0] loss: 0.11183938384056091\n",
            "418 rnn out: [0.6972544193267822, 0.3502110540866852] actual: [1.0, 0.0] loss: 0.1071513369679451\n",
            "511 rnn out: [0.5039871335029602, 0.5377707481384277] actual: [0.0, 0.0] loss: 0.2716001868247986\n",
            "586 rnn out: [0.6921193599700928, 0.3364867568016052] actual: [1.0, 1.0] loss: 0.26752015948295593\n",
            "636 rnn out: [0.5000810027122498, 0.46053752303123474] actual: [0.0, 0.0] loss: 0.23108790814876556\n",
            "745 rnn out: [0.5384066104888916, 0.49811550974845886] actual: [0.0, 1.0] loss: 0.27088484168052673\n",
            "851 rnn out: [0.6976637244224548, 0.338112473487854] actual: [1.0, 1.0] loss: 0.2647511661052704\n",
            "937 rnn out: [0.4907563328742981, 0.5308679938316345] actual: [0.0, 0.0] loss: 0.26133131980895996\n",
            "952 rnn out: [0.6313268542289734, 0.26365822553634644] actual: [1.0, 0.0] loss: 0.10271777212619781\n",
            "971 rnn out: [0.6569823622703552, 0.3049795925617218] actual: [1.0, 1.0] loss: 0.30035725235939026\n",
            "980 rnn out: [0.6160887479782104, 0.2526467740535736] actual: [0.0, 1.0] loss: 0.4690510928630829\n",
            "test_i 1000\n",
            "1233 rnn out: [0.6306211948394775, 0.2543863356113434] actual: [1.0, 0.0] loss: 0.10057654976844788\n",
            "1277 rnn out: [0.4677306115627289, 0.48402225971221924] actual: [1.0, 1.0] loss: 0.27477186918258667\n",
            "1355 rnn out: [0.6086755394935608, 0.2367607057094574] actual: [0.0, 0.0] loss: 0.21327076852321625\n",
            "1569 rnn out: [0.46829754114151, 0.5240905284881592] actual: [0.0, 0.0] loss: 0.24698671698570251\n",
            "1589 rnn out: [0.6179741621017456, 0.25350719690322876] actual: [1.0, 1.0] loss: 0.3515976369380951\n",
            "1613 rnn out: [0.4651145935058594, 0.5161423683166504] actual: [0.0, 0.0] loss: 0.24136725068092346\n",
            "1623 rnn out: [0.6107007265090942, 0.3056938648223877] actual: [1.0, 1.0] loss: 0.31680744886398315\n",
            "1769 rnn out: [0.680569589138031, 0.3176533579826355] actual: [0.0, 0.0] loss: 0.2820393145084381\n",
            "1899 rnn out: [0.6652365922927856, 0.3486143946647644] actual: [1.0, 1.0] loss: 0.2681848704814911\n",
            "1976 rnn out: [0.6020563840866089, 0.22649748623371124] actual: [0.0, 0.0] loss: 0.20688650012016296\n",
            "test_i 2000\n",
            "2008 rnn out: [0.6031286120414734, 0.24982404708862305] actual: [1.0, 0.0] loss: 0.10995947569608688\n",
            "2025 rnn out: [0.6975627541542053, 0.3381284475326538] actual: [1.0, 1.0] loss: 0.26477113366127014\n",
            "2029 rnn out: [0.5833418965339661, 0.43306663632392883] actual: [1.0, 0.0] loss: 0.18057534098625183\n",
            "2033 rnn out: [0.6937731504440308, 0.34891849756240845] actual: [0.0, 0.0] loss: 0.30153265595436096\n",
            "2160 rnn out: [0.6788179874420166, 0.31074073910713196] actual: [0.0, 0.0] loss: 0.278676837682724\n",
            "2162 rnn out: [0.48349201679229736, 0.4145036041736603] actual: [1.0, 0.0] loss: 0.2192968726158142\n",
            "2233 rnn out: [0.668106198310852, 0.29972001910209656] actual: [0.0, 1.0] loss: 0.4683789610862732\n",
            "2267 rnn out: [0.6298065781593323, 0.24983173608779907] actual: [1.0, 0.0] loss: 0.09972953051328659\n",
            "2271 rnn out: [0.5179922580718994, 0.43949630856513977] actual: [1.0, 1.0] loss: 0.27324795722961426\n",
            "2362 rnn out: [0.6136966943740845, 0.23403441905975342] actual: [0.0, 0.0] loss: 0.21569786965847015\n",
            "2423 rnn out: [0.6082031726837158, 0.2618410289287567] actual: [1.0, 0.0] loss: 0.11103273928165436\n",
            "pred_count 40 correct_count 25 correct_ratio 0.62\n",
            "Epoch # 3 - Batch: 2 -  train loss: 0.127724 - test loss: 0.104983 - elapsed: 206.14\n",
            "batch_i0 3 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "59 rnn out: [0.5080378651618958, 0.430843323469162] actual: [1.0, 0.0] loss: 0.21382635831832886\n",
            "354 rnn out: [0.4786943793296814, 0.5464511513710022] actual: [0.0, 0.0] loss: 0.26387858390808105\n",
            "393 rnn out: [0.6125789284706116, 0.24871520698070526] actual: [1.0, 1.0] loss: 0.3572619557380676\n",
            "416 rnn out: [0.5149489641189575, 0.4301077425479889] actual: [1.0, 1.0] loss: 0.2800258696079254\n",
            "445 rnn out: [0.6463900208473206, 0.28249263763427734] actual: [1.0, 0.0] loss: 0.10242106020450592\n",
            "501 rnn out: [0.4693911373615265, 0.5298107266426086] actual: [0.0, 1.0] loss: 0.22070299088954926\n",
            "509 rnn out: [0.6309051513671875, 0.24126723408699036] actual: [0.0, 1.0] loss: 0.4868583679199219\n",
            "645 rnn out: [0.5365573167800903, 0.4777090847492218] actual: [0.0, 0.0] loss: 0.2580498456954956\n",
            "816 rnn out: [0.6292493939399719, 0.2356891632080078] actual: [1.0, 1.0] loss: 0.36081352829933167\n",
            "885 rnn out: [0.640608012676239, 0.258859783411026] actual: [1.0, 1.0] loss: 0.33922573924064636\n",
            "899 rnn out: [0.6645506024360657, 0.2854139804840088] actual: [1.0, 1.0] loss: 0.31157973408699036\n",
            "935 rnn out: [0.6376240253448486, 0.298063725233078] actual: [0.0, 0.0] loss: 0.2477031797170639\n",
            "test_i 1000\n",
            "1103 rnn out: [0.6763175129890442, 0.30537858605384827] actual: [1.0, 0.0] loss: 0.09901321679353714\n",
            "1140 rnn out: [0.617074728012085, 0.3344685137271881] actual: [0.0, 1.0] loss: 0.4118567109107971\n",
            "1219 rnn out: [0.64628666639328, 0.26848241686820984] actual: [0.0, 0.0] loss: 0.2448846399784088\n",
            "1364 rnn out: [0.6660927534103394, 0.31715911626815796] actual: [1.0, 0.0] loss: 0.10604198276996613\n",
            "1458 rnn out: [0.6114702820777893, 0.22067484259605408] actual: [0.0, 0.0] loss: 0.21129664778709412\n",
            "1606 rnn out: [0.6509808897972107, 0.33195921778678894] actual: [1.0, 1.0] loss: 0.2840464115142822\n",
            "1846 rnn out: [0.6341660022735596, 0.2725015878677368] actual: [1.0, 0.0] loss: 0.1040458083152771\n",
            "1878 rnn out: [0.47163206338882446, 0.545120120048523] actual: [0.0, 1.0] loss: 0.21467626094818115\n",
            "1944 rnn out: [0.6615256667137146, 0.29450759291648865] actual: [0.0, 1.0] loss: 0.4676678478717804\n",
            "test_i 2000\n",
            "2038 rnn out: [0.48652490973472595, 0.5357860326766968] actual: [0.0, 0.0] loss: 0.2618865668773651\n",
            "2140 rnn out: [0.6647513508796692, 0.2875763475894928] actual: [1.0, 0.0] loss: 0.09754590690135956\n",
            "2353 rnn out: [0.624457597732544, 0.24225100874900818] actual: [0.0, 1.0] loss: 0.4820653796195984\n",
            "2420 rnn out: [0.6112481951713562, 0.22463586926460266] actual: [0.0, 1.0] loss: 0.48740696907043457\n",
            "2437 rnn out: [0.6729283332824707, 0.29957395792007446] actual: [0.0, 1.0] loss: 0.4717146158218384\n",
            "pred_count 26 correct_count 20 correct_ratio 0.77\n",
            "Epoch # 3 - Batch: 3 -  train loss: 0.127081 - test loss: 0.106781 - elapsed: 205.52\n",
            "batch_i0 4 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "183 rnn out: [0.6410976648330688, 0.3512265086174011] actual: [1.0, 0.0] loss: 0.12608547508716583\n",
            "192 rnn out: [0.6630366444587708, 0.29905274510383606] actual: [1.0, 0.0] loss: 0.10148842632770538\n",
            "256 rnn out: [0.6656621098518372, 0.2939724028110504] actual: [1.0, 0.0] loss: 0.09910079836845398\n",
            "269 rnn out: [0.6462098956108093, 0.29621651768684387] actual: [1.0, 0.0] loss: 0.10645583271980286\n",
            "318 rnn out: [0.5266528725624084, 0.4318426847457886] actual: [0.0, 0.0] loss: 0.23192566633224487\n",
            "359 rnn out: [0.6896105408668518, 0.3684271275997162] actual: [1.0, 0.0] loss: 0.11604008078575134\n",
            "596 rnn out: [0.6223078370094299, 0.3236338198184967] actual: [1.0, 0.0] loss: 0.12369510531425476\n",
            "618 rnn out: [0.6934234499931335, 0.33362826704978943] actual: [0.0, 0.0] loss: 0.29607194662094116\n",
            "711 rnn out: [0.6726699471473694, 0.30297142267227173] actual: [1.0, 0.0] loss: 0.09946832060813904\n",
            "725 rnn out: [0.7048056721687317, 0.3366811275482178] actual: [1.0, 1.0] loss: 0.2635658085346222\n",
            "727 rnn out: [0.46720772981643677, 0.5354534983634949] actual: [0.0, 1.0] loss: 0.21704325079917908\n",
            "763 rnn out: [0.6660030484199524, 0.3041539788246155] actual: [1.0, 0.0] loss: 0.10203180462121964\n",
            "764 rnn out: [0.6700351238250732, 0.2892201244831085] actual: [0.0, 0.0] loss: 0.26629766821861267\n",
            "782 rnn out: [0.6521217823028564, 0.27807050943374634] actual: [1.0, 0.0] loss: 0.0991712287068367\n",
            "940 rnn out: [0.6137166619300842, 0.23233048617839813] actual: [1.0, 0.0] loss: 0.10159613937139511\n",
            "test_i 1000\n",
            "1026 rnn out: [0.6214227676391602, 0.2831968665122986] actual: [1.0, 1.0] loss: 0.32856374979019165\n",
            "1029 rnn out: [0.6451973915100098, 0.27545005083084106] actual: [1.0, 0.0] loss: 0.10087881237268448\n",
            "1051 rnn out: [0.5422573089599609, 0.41929906606674194] actual: [0.0, 0.0] loss: 0.23492734134197235\n",
            "1053 rnn out: [0.6741683483123779, 0.3383769392967224] actual: [1.0, 1.0] loss: 0.271955668926239\n",
            "1299 rnn out: [0.6257524490356445, 0.274366170167923] actual: [0.0, 0.0] loss: 0.2334214597940445\n",
            "1353 rnn out: [0.6287816166877747, 0.2576417624950409] actual: [0.0, 0.0] loss: 0.23087279498577118\n",
            "1359 rnn out: [0.6141477823257446, 0.26206839084625244] actual: [0.0, 0.0] loss: 0.22292867302894592\n",
            "1391 rnn out: [0.682482123374939, 0.31605640053749084] actual: [1.0, 1.0] loss: 0.28429824113845825\n",
            "1414 rnn out: [0.6494190096855164, 0.30944451689720154] actual: [1.0, 1.0] loss: 0.29988694190979004\n",
            "1438 rnn out: [0.6148671507835388, 0.2741798162460327] actual: [0.0, 1.0] loss: 0.4524382948875427\n",
            "1499 rnn out: [0.6171713471412659, 0.23855477571487427] actual: [1.0, 0.0] loss: 0.10173308104276657\n",
            "1529 rnn out: [0.46891453862190247, 0.5342835783958435] actual: [0.0, 1.0] loss: 0.21838632225990295\n",
            "1534 rnn out: [0.6944409012794495, 0.3525729179382324] actual: [1.0, 0.0] loss: 0.10883700847625732\n",
            "1566 rnn out: [0.6325890421867371, 0.33473944664001465] actual: [1.0, 1.0] loss: 0.28878122568130493\n",
            "1619 rnn out: [0.522066593170166, 0.40298861265182495] actual: [1.0, 0.0] loss: 0.1954100877046585\n",
            "1675 rnn out: [0.6303977370262146, 0.25050655007362366] actual: [0.0, 0.0] loss: 0.23007741570472717\n",
            "1727 rnn out: [0.6710390448570251, 0.30161169171333313] actual: [1.0, 0.0] loss: 0.09959246218204498\n",
            "1739 rnn out: [0.6422677636146545, 0.26220014691352844] actual: [1.0, 0.0] loss: 0.09836062788963318\n",
            "1769 rnn out: [0.6089140176773071, 0.29399415850639343] actual: [1.0, 0.0] loss: 0.11969040334224701\n",
            "1873 rnn out: [0.6055307984352112, 0.2725057899951935] actual: [1.0, 1.0] loss: 0.34242692589759827\n",
            "1882 rnn out: [0.6469517350196838, 0.3210880756378174] actual: [1.0, 0.0] loss: 0.11387031525373459\n",
            "1948 rnn out: [0.4746716320514679, 0.5652123093605042] actual: [0.0, 0.0] loss: 0.2723890542984009\n",
            "1952 rnn out: [0.67332524061203, 0.32580727338790894] actual: [1.0, 0.0] loss: 0.10643339157104492\n",
            "test_i 2000\n",
            "2009 rnn out: [0.48017218708992004, 0.40314897894859314] actual: [1.0, 0.0] loss: 0.21637505292892456\n",
            "2028 rnn out: [0.6022003889083862, 0.31316903233528137] actual: [1.0, 0.0] loss: 0.12815968692302704\n",
            "2031 rnn out: [0.6497843265533447, 0.3129238784313202] actual: [1.0, 0.0] loss: 0.11028619110584259\n",
            "2064 rnn out: [0.5761502981185913, 0.40644025802612305] actual: [1.0, 1.0] loss: 0.26598086953163147\n",
            "2146 rnn out: [0.6727016568183899, 0.30110543966293335] actual: [1.0, 1.0] loss: 0.2977888882160187\n",
            "2232 rnn out: [0.6585354804992676, 0.2998972237110138] actual: [0.0, 0.0] loss: 0.26180365681648254\n",
            "2252 rnn out: [0.6966949105262756, 0.327677845954895] actual: [1.0, 0.0] loss: 0.09968337416648865\n",
            "2262 rnn out: [0.6152166724205017, 0.3019088804721832] actual: [1.0, 1.0] loss: 0.3176947236061096\n",
            "2286 rnn out: [0.6982353329658508, 0.34130334854125977] actual: [1.0, 0.0] loss: 0.10377494245767593\n",
            "2418 rnn out: [0.6883716583251953, 0.3163723051548004] actual: [1.0, 1.0] loss: 0.28222954273223877\n",
            "pred_count 48 correct_count 38 correct_ratio 0.79\n",
            "Epoch # 3 - Batch: 4 -  train loss: 0.126764 - test loss: 0.09428 - elapsed: 205.48\n",
            "batch_i0 5 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "63 rnn out: [0.6171053647994995, 0.23388329148292542] actual: [1.0, 1.0] loss: 0.3667715787887573\n",
            "85 rnn out: [0.6728314757347107, 0.31931304931640625] actual: [1.0, 0.0] loss: 0.10450003296136856\n",
            "153 rnn out: [0.647800087928772, 0.26411351561546326] actual: [0.0, 0.0] loss: 0.24470044672489166\n",
            "272 rnn out: [0.6722880005836487, 0.2912222743034363] actual: [0.0, 1.0] loss: 0.4771685004234314\n",
            "473 rnn out: [0.6673721671104431, 0.28513988852500916] actual: [1.0, 1.0] loss: 0.3108331263065338\n",
            "544 rnn out: [0.5011188983917236, 0.5381314158439636] actual: [0.0, 1.0] loss: 0.23222136497497559\n",
            "552 rnn out: [0.6619054675102234, 0.27250760793685913] actual: [1.0, 1.0] loss: 0.3217765688896179\n",
            "571 rnn out: [0.601112961769104, 0.23711398243904114] actual: [1.0, 1.0] loss: 0.37055301666259766\n",
            "575 rnn out: [0.6402193903923035, 0.2522069215774536] actual: [1.0, 1.0] loss: 0.34431830048561096\n",
            "663 rnn out: [0.6575692296028137, 0.30455470085144043] actual: [0.0, 0.0] loss: 0.2625754475593567\n",
            "682 rnn out: [0.635383129119873, 0.2673940360546112] actual: [1.0, 0.0] loss: 0.1022225171327591\n",
            "748 rnn out: [0.666527271270752, 0.28740328550338745] actual: [1.0, 0.0] loss: 0.09690235555171967\n",
            "test_i 1000\n",
            "1056 rnn out: [0.625730574131012, 0.2863580882549286] actual: [1.0, 0.0] loss: 0.11103928089141846\n",
            "1086 rnn out: [0.6082301735877991, 0.2550981640815735] actual: [0.0, 1.0] loss: 0.4624113440513611\n",
            "1095 rnn out: [0.5113415718078613, 0.47126153111457825] actual: [0.0, 1.0] loss: 0.2705172896385193\n",
            "1184 rnn out: [0.6721362471580505, 0.29386037588119507] actual: [1.0, 0.0] loss: 0.09692427515983582\n",
            "1421 rnn out: [0.6176939010620117, 0.23767933249473572] actual: [0.0, 0.0] loss: 0.2190186083316803\n",
            "1459 rnn out: [0.6179972290992737, 0.2293979525566101] actual: [1.0, 0.0] loss: 0.09927476942539215\n",
            "1484 rnn out: [0.5126819610595703, 0.4305115044116974] actual: [0.0, 1.0] loss: 0.29357999563217163\n",
            "1616 rnn out: [0.4773470163345337, 0.5314679741859436] actual: [0.0, 0.0] loss: 0.2551591992378235\n",
            "1850 rnn out: [0.6517305970191956, 0.325520396232605] actual: [0.0, 0.0] loss: 0.2653581500053406\n",
            "test_i 2000\n",
            "2118 rnn out: [0.6226016879081726, 0.23601055145263672] actual: [0.0, 1.0] loss: 0.48565638065338135\n",
            "2137 rnn out: [0.6567354798316956, 0.2742294371128082] actual: [1.0, 0.0] loss: 0.09651616215705872\n",
            "2158 rnn out: [0.4737701714038849, 0.43407171964645386] actual: [0.0, 0.0] loss: 0.20643821358680725\n",
            "2293 rnn out: [0.4788385033607483, 0.40693703293800354] actual: [1.0, 0.0] loss: 0.21860352158546448\n",
            "2352 rnn out: [0.47086432576179504, 0.5256291627883911] actual: [0.0, 1.0] loss: 0.22337046265602112\n",
            "2397 rnn out: [0.6538742780685425, 0.26165544986724854] actual: [1.0, 0.0] loss: 0.09413329511880875\n",
            "pred_count 27 correct_count 21 correct_ratio 0.78\n",
            "Epoch # 3 - Batch: 5 -  train loss: 0.126884 - test loss: 0.102584 - elapsed: 205.64\n",
            "batch_i0 6 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "10 rnn out: [0.6087706685066223, 0.32141628861427307] actual: [1.0, 0.0] loss: 0.12818440794944763\n",
            "64 rnn out: [0.665299654006958, 0.316647469997406] actual: [1.0, 0.0] loss: 0.1061449721455574\n",
            "191 rnn out: [0.6021372079849243, 0.24448879063129425] actual: [1.0, 1.0] loss: 0.364546000957489\n",
            "197 rnn out: [0.6664541959762573, 0.3004445731639862] actual: [0.0, 0.0] loss: 0.2672140598297119\n",
            "211 rnn out: [0.601284921169281, 0.27589887380599976] actual: [1.0, 0.0] loss: 0.117546945810318\n",
            "279 rnn out: [0.4704286754131317, 0.40284401178359985] actual: [0.0, 1.0] loss: 0.288949191570282\n",
            "382 rnn out: [0.6151082515716553, 0.28833189606666565] actual: [0.0, 0.0] loss: 0.23074671626091003\n",
            "463 rnn out: [0.5057575702667236, 0.4193859100341797] actual: [1.0, 0.0] loss: 0.21008005738258362\n",
            "479 rnn out: [0.48694169521331787, 0.4100632071495056] actual: [1.0, 1.0] loss: 0.30562710762023926\n",
            "493 rnn out: [0.6318495869636536, 0.2990456223487854] actual: [0.0, 0.0] loss: 0.24433109164237976\n",
            "505 rnn out: [0.6023223996162415, 0.326012521982193] actual: [1.0, 0.0] loss: 0.13221581280231476\n",
            "662 rnn out: [0.4811290502548218, 0.41449037194252014] actual: [0.0, 0.0] loss: 0.201643705368042\n",
            "739 rnn out: [0.4697146713733673, 0.41435864567756653] actual: [1.0, 1.0] loss: 0.312089204788208\n",
            "955 rnn out: [0.6610147953033447, 0.3273957371711731] actual: [1.0, 1.0] loss: 0.28365373611450195\n",
            "979 rnn out: [0.6268226504325867, 0.3730483949184418] actual: [1.0, 1.0] loss: 0.2661648094654083\n",
            "999 rnn out: [0.6699613332748413, 0.3141927719116211] actual: [1.0, 0.0] loss: 0.10382130742073059\n",
            "test_i 1000\n",
            "1000 rnn out: [0.4972042739391327, 0.5047168731689453] actual: [0.0, 0.0] loss: 0.2509756088256836\n",
            "1037 rnn out: [0.542431652545929, 0.4103288948535919] actual: [0.0, 0.0] loss: 0.231300950050354\n",
            "1114 rnn out: [0.6637068390846252, 0.3293178379535675] actual: [1.0, 1.0] loss: 0.28145384788513184\n",
            "1151 rnn out: [0.6765104532241821, 0.31390759348869324] actual: [1.0, 1.0] loss: 0.2876841127872467\n",
            "1209 rnn out: [0.4750196635723114, 0.44806286692619324] actual: [1.0, 1.0] loss: 0.2901194393634796\n",
            "1232 rnn out: [0.4682154953479767, 0.5525254607200623] actual: [1.0, 0.0] loss: 0.29403960704803467\n",
            "1246 rnn out: [0.6079341769218445, 0.31363531947135925] actual: [1.0, 0.0] loss: 0.12604135274887085\n",
            "1323 rnn out: [0.46182990074157715, 0.47980162501335144] actual: [1.0, 1.0] loss: 0.2801166772842407\n",
            "1518 rnn out: [0.46501174569129944, 0.4181993901729584] actual: [0.0, 1.0] loss: 0.27736392617225647\n",
            "1615 rnn out: [0.651820719242096, 0.33059248328208923] actual: [0.0, 1.0] loss: 0.4364883303642273\n",
            "1656 rnn out: [0.524432361125946, 0.44307753443717957] actual: [1.0, 0.0] loss: 0.21124114096164703\n",
            "1876 rnn out: [0.6242222785949707, 0.3532413840293884] actual: [1.0, 0.0] loss: 0.13299418985843658\n",
            "1934 rnn out: [0.6010077595710754, 0.23709838092327118] actual: [0.0, 1.0] loss: 0.4716145992279053\n",
            "1989 rnn out: [0.464147686958313, 0.536750078201294] actual: [0.0, 0.0] loss: 0.2517668604850769\n",
            "test_i 2000\n",
            "2188 rnn out: [0.5342227220535278, 0.47972536087036133] actual: [1.0, 1.0] loss: 0.24381709098815918\n",
            "2215 rnn out: [0.6111013293266296, 0.2950301766395569] actual: [0.0, 0.0] loss: 0.23024381697177887\n",
            "2339 rnn out: [0.6399682760238647, 0.26931700110435486] actual: [0.0, 0.0] loss: 0.2410455197095871\n",
            "2383 rnn out: [0.47723838686943054, 0.48492494225502014] actual: [0.0, 0.0] loss: 0.23145434260368347\n",
            "2416 rnn out: [0.6700252294540405, 0.3349466621875763] actual: [0.0, 1.0] loss: 0.44561487436294556\n",
            "pred_count 35 correct_count 25 correct_ratio 0.71\n",
            "Epoch # 3 - Batch: 6 -  train loss: 0.127931 - test loss: 0.10004 - elapsed: 207.34\n",
            "batch_i0 7 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "121 rnn out: [0.6105424165725708, 0.22065088152885437] actual: [1.0, 0.0] loss: 0.10018201172351837\n",
            "135 rnn out: [0.67242431640625, 0.29880011081695557] actual: [0.0, 0.0] loss: 0.270717978477478\n",
            "293 rnn out: [0.6317471265792847, 0.33666500449180603] actual: [1.0, 0.0] loss: 0.12447675317525864\n",
            "303 rnn out: [0.6497392654418945, 0.32734808325767517] actual: [1.0, 0.0] loss: 0.11491967737674713\n",
            "342 rnn out: [0.609805703163147, 0.31846797466278076] actual: [1.0, 0.0] loss: 0.12683671712875366\n",
            "508 rnn out: [0.6724601984024048, 0.29888635873794556] actual: [1.0, 0.0] loss: 0.09830768406391144\n",
            "547 rnn out: [0.619028627872467, 0.37097132205963135] actual: [0.0, 0.0] loss: 0.26040807366371155\n",
            "571 rnn out: [0.5829613208770752, 0.4215274155139923] actual: [1.0, 1.0] loss: 0.254275918006897\n",
            "636 rnn out: [0.6515589952468872, 0.3225531280040741] actual: [1.0, 0.0] loss: 0.11272582411766052\n",
            "688 rnn out: [0.477299302816391, 0.5495504140853882] actual: [0.0, 0.0] loss: 0.26491013169288635\n",
            "712 rnn out: [0.6146366000175476, 0.288697212934494] actual: [1.0, 1.0] loss: 0.32722824811935425\n",
            "952 rnn out: [0.6352543234825134, 0.39654460549354553] actual: [1.0, 0.0] loss: 0.1451435089111328\n",
            "956 rnn out: [0.49058449268341064, 0.40555545687675476] actual: [1.0, 0.0] loss: 0.21198970079421997\n",
            "999 rnn out: [0.6381758451461792, 0.29603683948516846] actual: [1.0, 0.0] loss: 0.10927726328372955\n",
            "test_i 1000\n",
            "1072 rnn out: [0.6138179302215576, 0.26823922991752625] actual: [1.0, 0.0] loss: 0.11054443567991257\n",
            "1174 rnn out: [0.49891239404678345, 0.5114070773124695] actual: [1.0, 0.0] loss: 0.2563129961490631\n",
            "1231 rnn out: [0.6258441805839539, 0.23225653171539307] actual: [1.0, 0.0] loss: 0.09696783870458603\n",
            "1235 rnn out: [0.6372564435005188, 0.3247103989124298] actual: [1.0, 0.0] loss: 0.11850986629724503\n",
            "1293 rnn out: [0.5704357028007507, 0.45936572551727295] actual: [1.0, 1.0] loss: 0.23840545117855072\n",
            "1451 rnn out: [0.6278208494186401, 0.31960710883140564] actual: [1.0, 0.0] loss: 0.12033301591873169\n",
            "1459 rnn out: [0.6295941472053528, 0.2533550560474396] actual: [1.0, 0.0] loss: 0.10069464147090912\n",
            "1466 rnn out: [0.6299421191215515, 0.2775924503803253] actual: [0.0, 0.0] loss: 0.236942321062088\n",
            "1536 rnn out: [0.6569724678993225, 0.2720882296562195] actual: [0.0, 0.0] loss: 0.2528223991394043\n",
            "1593 rnn out: [0.5140844583511353, 0.4044303894042969] actual: [0.0, 0.0] loss: 0.213923379778862\n",
            "1702 rnn out: [0.47565263509750366, 0.44420814514160156] actual: [1.0, 0.0] loss: 0.2361305207014084\n",
            "1838 rnn out: [0.64825040102005, 0.30092403292655945] actual: [1.0, 0.0] loss: 0.10714152455329895\n",
            "1885 rnn out: [0.6470069289207458, 0.2730744779109955] actual: [1.0, 0.0] loss: 0.09958688914775848\n",
            "test_i 2000\n",
            "2060 rnn out: [0.6908788084983826, 0.3164364695549011] actual: [0.0, 1.0] loss: 0.47228631377220154\n",
            "2102 rnn out: [0.6853800415992737, 0.3036959171295166] actual: [1.0, 1.0] loss: 0.2919125556945801\n",
            "2183 rnn out: [0.6398274302482605, 0.24828551709651947] actual: [1.0, 0.0] loss: 0.09568499028682709\n",
            "2253 rnn out: [0.4685612916946411, 0.42912617325782776] actual: [1.0, 0.0] loss: 0.23328818380832672\n",
            "2316 rnn out: [0.4717331528663635, 0.5506188869476318] actual: [0.0, 1.0] loss: 0.21223777532577515\n",
            "pred_count 32 correct_count 26 correct_ratio 0.81\n",
            "Epoch # 3 - Batch: 7 -  train loss: 0.126445 - test loss: 0.101957 - elapsed: 206.57\n",
            "batch_i0 8 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "71 rnn out: [0.6106821298599243, 0.3134702146053314] actual: [0.0, 1.0] loss: 0.422127902507782\n",
            "87 rnn out: [0.6792314648628235, 0.3003116250038147] actual: [1.0, 0.0] loss: 0.09653976559638977\n",
            "209 rnn out: [0.6242910623550415, 0.26310276985168457] actual: [0.0, 0.0] loss: 0.22948120534420013\n",
            "264 rnn out: [0.6487455368041992, 0.313183069229126] actual: [1.0, 0.0] loss: 0.11073166877031326\n",
            "523 rnn out: [0.6376410126686096, 0.2501046657562256] actual: [0.0, 0.0] loss: 0.23456919193267822\n",
            "590 rnn out: [0.6017209887504578, 0.3584262430667877] actual: [1.0, 0.0] loss: 0.14354777336120605\n",
            "673 rnn out: [0.6160568594932556, 0.30164358019828796] actual: [0.0, 1.0] loss: 0.4336138367652893\n",
            "717 rnn out: [0.5203771591186523, 0.4268573820590973] actual: [0.0, 1.0] loss: 0.2996424436569214\n",
            "864 rnn out: [0.6065397262573242, 0.2671360671520233] actual: [0.0, 0.0] loss: 0.2196260690689087\n",
            "916 rnn out: [0.4778599739074707, 0.4114148020744324] actual: [0.0, 1.0] loss: 0.28739133477211\n",
            "975 rnn out: [0.6575057506561279, 0.294552206993103] actual: [0.0, 0.0] loss: 0.25953739881515503\n",
            "test_i 1000\n",
            "1048 rnn out: [0.6147513389587402, 0.3844130039215088] actual: [0.0, 0.0] loss: 0.26284629106521606\n",
            "1078 rnn out: [0.6896820068359375, 0.33615145087242126] actual: [1.0, 0.0] loss: 0.10464753210544586\n",
            "1083 rnn out: [0.5290506482124329, 0.43884968757629395] actual: [0.0, 1.0] loss: 0.2973921298980713\n",
            "1182 rnn out: [0.6003561019897461, 0.27492982149124146] actual: [1.0, 0.0] loss: 0.1176508292555809\n",
            "1305 rnn out: [0.5150425434112549, 0.43222424387931824] actual: [1.0, 0.0] loss: 0.21100077033042908\n",
            "1313 rnn out: [0.6289566159248352, 0.2264007031917572] actual: [1.0, 0.0] loss: 0.0944652408361435\n",
            "1438 rnn out: [0.6462998390197754, 0.3227318525314331] actual: [0.0, 1.0] loss: 0.4381977915763855\n",
            "1463 rnn out: [0.631839394569397, 0.2861466109752655] actual: [1.0, 0.0] loss: 0.10871105641126633\n",
            "1464 rnn out: [0.6414737701416016, 0.2836519181728363] actual: [1.0, 0.0] loss: 0.10449972748756409\n",
            "1468 rnn out: [0.6056880950927734, 0.3429155647754669] actual: [1.0, 0.0] loss: 0.13653647899627686\n",
            "1470 rnn out: [0.505600094795227, 0.44368976354599] actual: [1.0, 1.0] loss: 0.276956170797348\n",
            "1501 rnn out: [0.6439229846000671, 0.24835307896137238] actual: [1.0, 0.0] loss: 0.09423504769802094\n",
            "1516 rnn out: [0.5291323065757751, 0.48187315464019775] actual: [0.0, 1.0] loss: 0.27421820163726807\n",
            "1525 rnn out: [0.646196722984314, 0.2922413945198059] actual: [1.0, 0.0] loss: 0.10529088973999023\n",
            "1549 rnn out: [0.6163409948348999, 0.33868318796157837] actual: [1.0, 0.0] loss: 0.13095027208328247\n",
            "1604 rnn out: [0.6685189008712769, 0.2912972569465637] actual: [1.0, 0.0] loss: 0.09736689925193787\n",
            "1726 rnn out: [0.6326211094856262, 0.2505822777748108] actual: [0.0, 1.0] loss: 0.48091816902160645\n",
            "1727 rnn out: [0.6900120377540588, 0.34051331877708435] actual: [1.0, 0.0] loss: 0.10602092742919922\n",
            "1754 rnn out: [0.6220822334289551, 0.260549932718277] actual: [0.0, 1.0] loss: 0.46688637137413025\n",
            "1778 rnn out: [0.6347676515579224, 0.2828391194343567] actual: [1.0, 0.0] loss: 0.10669632256031036\n",
            "1808 rnn out: [0.46713459491729736, 0.47452986240386963] actual: [1.0, 0.0] loss: 0.25456205010414124\n",
            "1824 rnn out: [0.6144016981124878, 0.3494829833507538] actual: [0.0, 0.0] loss: 0.24981389939785004\n",
            "1900 rnn out: [0.6357740163803101, 0.24253195524215698] actual: [0.0, 0.0] loss: 0.23151516914367676\n",
            "test_i 2000\n",
            "2379 rnn out: [0.6087034344673157, 0.25335806608200073] actual: [0.0, 0.0] loss: 0.21735510230064392\n",
            "2411 rnn out: [0.6171619892120361, 0.24596987664699554] actual: [1.0, 0.0] loss: 0.1035330593585968\n",
            "2418 rnn out: [0.6601824760437012, 0.3265592157840729] actual: [0.0, 1.0] loss: 0.44468170404434204\n",
            "pred_count 37 correct_count 29 correct_ratio 0.78\n",
            "Epoch # 3 - Batch: 8 -  train loss: 0.129926 - test loss: 0.104317 - elapsed: 205.91\n",
            "batch_i0 9 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "88 rnn out: [0.49387598037719727, 0.4318370819091797] actual: [1.0, 1.0] loss: 0.28948530554771423\n",
            "226 rnn out: [0.6458488702774048, 0.2887473702430725] actual: [0.0, 0.0] loss: 0.25024789571762085\n",
            "278 rnn out: [0.6533017754554749, 0.30748963356018066] actual: [1.0, 0.0] loss: 0.10737476497888565\n",
            "353 rnn out: [0.6065518260002136, 0.23758332431316376] actual: [1.0, 0.0] loss: 0.10562365502119064\n",
            "360 rnn out: [0.6119870543479919, 0.2653617560863495] actual: [1.0, 0.0] loss: 0.11048544943332672\n",
            "450 rnn out: [0.6171467304229736, 0.3215812146663666] actual: [0.0, 0.0] loss: 0.24214227497577667\n",
            "720 rnn out: [0.4824119210243225, 0.4817104637622833] actual: [0.0, 1.0] loss: 0.25067266821861267\n",
            "801 rnn out: [0.6509823203086853, 0.27484965324401855] actual: [0.0, 0.0] loss: 0.24966014921665192\n",
            "986 rnn out: [0.638062596321106, 0.32602620124816895] actual: [1.0, 0.0] loss: 0.11864588409662247\n",
            "test_i 1000\n",
            "1003 rnn out: [0.6714200973510742, 0.30797499418258667] actual: [1.0, 1.0] loss: 0.29343169927597046\n",
            "1233 rnn out: [0.488473504781723, 0.4622240364551544] actual: [0.0, 0.0] loss: 0.2261287122964859\n",
            "1289 rnn out: [0.55577152967453, 0.44083282351493835] actual: [1.0, 1.0] loss: 0.255003422498703\n",
            "1534 rnn out: [0.6590275168418884, 0.3183025121688843] actual: [0.0, 0.0] loss: 0.2678168714046478\n",
            "1589 rnn out: [0.6398639678955078, 0.2990542948246002] actual: [1.0, 1.0] loss: 0.31051144003868103\n",
            "1592 rnn out: [0.6165279150009155, 0.3995971977710724] actual: [1.0, 1.0] loss: 0.253767192363739\n",
            "1910 rnn out: [0.4804450571537018, 0.44326165318489075] actual: [1.0, 1.0] loss: 0.289947509765625\n",
            "test_i 2000\n",
            "2112 rnn out: [0.6137201189994812, 0.3894110918045044] actual: [1.0, 1.0] loss: 0.26101547479629517\n",
            "2140 rnn out: [0.4803759455680847, 0.44462186098098755] actual: [1.0, 0.0] loss: 0.23384886980056763\n",
            "2188 rnn out: [0.630420446395874, 0.270942747592926] actual: [0.0, 1.0] loss: 0.46447721123695374\n",
            "2229 rnn out: [0.5276766419410706, 0.4514096677303314] actual: [1.0, 1.0] loss: 0.2620203495025635\n",
            "2290 rnn out: [0.4856913387775421, 0.46050697565078735] actual: [0.0, 1.0] loss: 0.26347440481185913\n",
            "2404 rnn out: [0.630202054977417, 0.36200571060180664] actual: [0.0, 0.0] loss: 0.26410138607025146\n",
            "2408 rnn out: [0.6518670916557312, 0.28965073823928833] actual: [1.0, 1.0] loss: 0.3128962814807892\n",
            "pred_count 23 correct_count 17 correct_ratio 0.74\n",
            "Epoch # 3 - Batch: 9 -  train loss: 0.127078 - test loss: 0.100434 - elapsed: 205.06\n",
            "batch_i0 10 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "314 rnn out: [0.6431421041488647, 0.38119441270828247] actual: [0.0, 1.0] loss: 0.39827606081962585\n",
            "442 rnn out: [0.6203228235244751, 0.2943791151046753] actual: [1.0, 0.0] loss: 0.11540691554546356\n",
            "502 rnn out: [0.6745448112487793, 0.2995932996273041] actual: [1.0, 1.0] loss: 0.29824528098106384\n",
            "533 rnn out: [0.631618857383728, 0.2414834052324295] actual: [1.0, 0.0] loss: 0.09700945019721985\n",
            "737 rnn out: [0.6795020699501038, 0.29087400436401367] actual: [1.0, 0.0] loss: 0.0936633050441742\n",
            "828 rnn out: [0.6180177330970764, 0.2967257499694824] actual: [1.0, 1.0] loss: 0.32025256752967834\n",
            "890 rnn out: [0.680345356464386, 0.3076459765434265] actual: [1.0, 0.0] loss: 0.09841256588697433\n",
            "901 rnn out: [0.6683972477912903, 0.27542808651924133] actual: [1.0, 1.0] loss: 0.31748244166374207\n",
            "965 rnn out: [0.5596514344215393, 0.4261886477470398] actual: [1.0, 1.0] loss: 0.261583149433136\n",
            "test_i 1000\n",
            "1003 rnn out: [0.6322800517082214, 0.2938385307788849] actual: [1.0, 1.0] loss: 0.3169410228729248\n",
            "1011 rnn out: [0.6771867871284485, 0.28850358724594116] actual: [1.0, 0.0] loss: 0.09372134506702423\n",
            "1047 rnn out: [0.6343054175376892, 0.28271040320396423] actual: [1.0, 1.0] loss: 0.324118435382843\n",
            "1061 rnn out: [0.6643462181091309, 0.2744556665420532] actual: [0.0, 0.0] loss: 0.25834089517593384\n",
            "1141 rnn out: [0.5344541668891907, 0.5022298097610474] actual: [0.0, 0.0] loss: 0.26893800497055054\n",
            "1224 rnn out: [0.6372560262680054, 0.3381268382072449] actual: [0.0, 0.0] loss: 0.2602124810218811\n",
            "1275 rnn out: [0.6636616587638855, 0.3245719075202942] actual: [0.0, 0.0] loss: 0.2728968560695648\n",
            "1287 rnn out: [0.636398434638977, 0.38848286867141724] actual: [1.0, 1.0] loss: 0.2530796527862549\n",
            "1359 rnn out: [0.6806364059448242, 0.30923768877983093] actual: [1.0, 1.0] loss: 0.2895728051662445\n",
            "1445 rnn out: [0.6164563894271851, 0.2182411551475525] actual: [1.0, 0.0] loss: 0.09736745059490204\n",
            "1460 rnn out: [0.6670981049537659, 0.327320396900177] actual: [1.0, 0.0] loss: 0.10898115485906601\n",
            "1493 rnn out: [0.6303361654281616, 0.3053034842014313] actual: [1.0, 0.0] loss: 0.11493078619241714\n",
            "1500 rnn out: [0.6398677229881287, 0.2925306558609009] actual: [1.0, 0.0] loss: 0.10763472318649292\n",
            "1713 rnn out: [0.48154088854789734, 0.4781867265701294] actual: [0.0, 0.0] loss: 0.2302720844745636\n",
            "1853 rnn out: [0.6574790477752686, 0.3039494454860687] actual: [1.0, 1.0] loss: 0.30090346932411194\n",
            "1881 rnn out: [0.6277256608009338, 0.297115296125412] actual: [1.0, 0.0] loss: 0.11343283951282501\n",
            "1955 rnn out: [0.5333695411682129, 0.44419440627098083] actual: [0.0, 0.0] loss: 0.24089586734771729\n",
            "test_i 2000\n",
            "2000 rnn out: [0.6027267575263977, 0.2256479114294052] actual: [0.0, 0.0] loss: 0.20709826052188873\n",
            "2040 rnn out: [0.6381010413169861, 0.26774466037750244] actual: [1.0, 0.0] loss: 0.1013290286064148\n",
            "2134 rnn out: [0.6805946230888367, 0.30722180008888245] actual: [1.0, 0.0] loss: 0.09820251166820526\n",
            "2174 rnn out: [0.6273072361946106, 0.29736512899398804] actual: [1.0, 1.0] loss: 0.31629782915115356\n",
            "2175 rnn out: [0.6644381880760193, 0.3010375499725342] actual: [1.0, 0.0] loss: 0.10161267220973969\n",
            "2244 rnn out: [0.6394101977348328, 0.2507512867450714] actual: [0.0, 1.0] loss: 0.48510950803756714\n",
            "2334 rnn out: [0.6084155440330505, 0.2748234272003174] actual: [0.0, 1.0] loss: 0.44802525639533997\n",
            "pred_count 33 correct_count 26 correct_ratio 0.79\n",
            "Epoch # 3 - Batch: 10 -  train loss: 0.124821 - test loss: 0.097084 - elapsed: 205.22\n",
            "batch_i0 11 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "115 rnn out: [0.6624738574028015, 0.31455719470977783] actual: [0.0, 0.0] loss: 0.26890891790390015\n",
            "133 rnn out: [0.636184811592102, 0.25431230664253235] actual: [1.0, 1.0] loss: 0.3442058265209198\n",
            "365 rnn out: [0.6221213936805725, 0.23141618072986603] actual: [0.0, 0.0] loss: 0.22029423713684082\n",
            "475 rnn out: [0.48859718441963196, 0.43200138211250305] actual: [0.0, 0.0] loss: 0.21267619729042053\n",
            "494 rnn out: [0.6507681012153625, 0.2654665410518646] actual: [1.0, 0.0] loss: 0.09621770679950714\n",
            "511 rnn out: [0.688949704170227, 0.31254008412361145] actual: [1.0, 1.0] loss: 0.284676730632782\n",
            "584 rnn out: [0.679324746131897, 0.3018169403076172] actual: [0.0, 0.0] loss: 0.2762877941131592\n",
            "975 rnn out: [0.614614725112915, 0.3056112825870514] actual: [0.0, 1.0] loss: 0.42996349930763245\n",
            "995 rnn out: [0.6014943718910217, 0.22971390187740326] actual: [1.0, 0.0] loss: 0.10578760504722595\n",
            "test_i 1000\n",
            "1339 rnn out: [0.6061610579490662, 0.28632381558418274] actual: [0.0, 1.0] loss: 0.43838247656822205\n",
            "1362 rnn out: [0.5106083750724792, 0.47996217012405396] actual: [0.0, 1.0] loss: 0.26558011770248413\n",
            "1421 rnn out: [0.5127206444740295, 0.43215689063072205] actual: [0.0, 1.0] loss: 0.292664110660553\n",
            "1442 rnn out: [0.49822142720222473, 0.5263019800186157] actual: [1.0, 0.0] loss: 0.2643877863883972\n",
            "1443 rnn out: [0.6733249425888062, 0.31152161955833435] actual: [0.0, 1.0] loss: 0.46368446946144104\n",
            "1445 rnn out: [0.6222404837608337, 0.2743595242500305] actual: [1.0, 0.0] loss: 0.1089877039194107\n",
            "1545 rnn out: [0.4734390377998352, 0.5337206721305847] actual: [0.0, 1.0] loss: 0.22078046202659607\n",
            "1602 rnn out: [0.5188906192779541, 0.401294469833374] actual: [0.0, 1.0] loss: 0.3138478994369507\n",
            "1635 rnn out: [0.6820827722549438, 0.3048017919063568] actual: [0.0, 0.0] loss: 0.27907052636146545\n",
            "1654 rnn out: [0.6582579612731934, 0.291151762008667] actual: [1.0, 0.0] loss: 0.1007784828543663\n",
            "1930 rnn out: [0.5046244859695435, 0.49866342544555664] actual: [0.0, 0.0] loss: 0.25165554881095886\n",
            "1971 rnn out: [0.6151056885719299, 0.2931472957134247] actual: [1.0, 0.0] loss: 0.11703948676586151\n",
            "test_i 2000\n",
            "2001 rnn out: [0.6686584949493408, 0.2883494794368744] actual: [0.0, 1.0] loss: 0.47677528858184814\n",
            "2003 rnn out: [0.6778854727745056, 0.30993250012397766] actual: [1.0, 0.0] loss: 0.09990796446800232\n",
            "2019 rnn out: [0.6932863593101501, 0.33005452156066895] actual: [1.0, 1.0] loss: 0.27145010232925415\n",
            "2128 rnn out: [0.5033668875694275, 0.4222147762775421] actual: [1.0, 1.0] loss: 0.2902401387691498\n",
            "2216 rnn out: [0.6829121708869934, 0.30493268370628357] actual: [0.0, 1.0] loss: 0.4747437834739685\n",
            "2226 rnn out: [0.6174402236938477, 0.2873685956001282] actual: [0.0, 1.0] loss: 0.4445379674434662\n",
            "2318 rnn out: [0.6093453764915466, 0.22632437944412231] actual: [1.0, 0.0] loss: 0.10191687941551208\n",
            "2423 rnn out: [0.6926091909408569, 0.3308775722980499] actual: [1.0, 1.0] loss: 0.2711069881916046\n",
            "2447 rnn out: [0.6591318845748901, 0.33293449878692627] actual: [0.0, 0.0] loss: 0.2726500928401947\n",
            "pred_count 30 correct_count 23 correct_ratio 0.77\n",
            "Epoch # 3 - Batch: 11 -  train loss: 0.128103 - test loss: 0.101273 - elapsed: 205.21\n",
            "batch_i0 12 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "30 rnn out: [0.6211413145065308, 0.30524367094039917] actual: [1.0, 0.0] loss: 0.11835379898548126\n",
            "62 rnn out: [0.6336793303489685, 0.23207171261310577] actual: [0.0, 0.0] loss: 0.22770337760448456\n",
            "155 rnn out: [0.6500977873802185, 0.2491549849510193] actual: [0.0, 0.0] loss: 0.2423526644706726\n",
            "187 rnn out: [0.6217833161354065, 0.3144828677177429] actual: [1.0, 0.0] loss: 0.12097366154193878\n",
            "199 rnn out: [0.6152090430259705, 0.24057412147521973] actual: [1.0, 0.0] loss: 0.10296998918056488\n",
            "208 rnn out: [0.5827307105064392, 0.43282896280288696] actual: [0.0, 0.0] loss: 0.2634580135345459\n",
            "329 rnn out: [0.6003435850143433, 0.24029400944709778] actual: [1.0, 0.0] loss: 0.10873322933912277\n",
            "620 rnn out: [0.6716743111610413, 0.27936941385269165] actual: [0.0, 1.0] loss: 0.48522740602493286\n",
            "633 rnn out: [0.6449505090713501, 0.33644023537635803] actual: [1.0, 0.0] loss: 0.11962608993053436\n",
            "666 rnn out: [0.6322011947631836, 0.2553213834762573] actual: [1.0, 0.0] loss: 0.10023248195648193\n",
            "678 rnn out: [0.6612409949302673, 0.29966363310813904] actual: [1.0, 0.0] loss: 0.1022779792547226\n",
            "698 rnn out: [0.6334032416343689, 0.2809547483921051] actual: [1.0, 0.0] loss: 0.10666437447071075\n",
            "732 rnn out: [0.6113820672035217, 0.2986786663532257] actual: [0.0, 0.0] loss: 0.23149847984313965\n",
            "765 rnn out: [0.6011924743652344, 0.2312212735414505] actual: [0.0, 0.0] loss: 0.2074478417634964\n",
            "803 rnn out: [0.6420656442642212, 0.28487199544906616] actual: [1.0, 0.0] loss: 0.10463453084230423\n",
            "823 rnn out: [0.6375754475593567, 0.2327236384153366] actual: [0.0, 0.0] loss: 0.23033137619495392\n",
            "849 rnn out: [0.612716019153595, 0.2553940713405609] actual: [1.0, 0.0] loss: 0.10760749876499176\n",
            "932 rnn out: [0.6328219771385193, 0.21944570541381836] actual: [0.0, 0.0] loss: 0.2243100255727768\n",
            "935 rnn out: [0.6332064867019653, 0.24539008736610413] actual: [0.0, 0.0] loss: 0.23058338463306427\n",
            "942 rnn out: [0.6201717257499695, 0.2152729034423828] actual: [0.0, 0.0] loss: 0.21547770500183105\n",
            "947 rnn out: [0.6126402020454407, 0.27584007382392883] actual: [1.0, 0.0] loss: 0.11306767910718918\n",
            "958 rnn out: [0.5213096141815186, 0.5450637340545654] actual: [0.0, 0.0] loss: 0.28442907333374023\n",
            "test_i 1000\n",
            "1034 rnn out: [0.6776180863380432, 0.28943923115730286] actual: [0.0, 1.0] loss: 0.4820314645767212\n",
            "1191 rnn out: [0.6854656338691711, 0.3018544316291809] actual: [1.0, 1.0] loss: 0.2931695580482483\n",
            "1213 rnn out: [0.6084595322608948, 0.2044946551322937] actual: [1.0, 0.0] loss: 0.09756100177764893\n",
            "1225 rnn out: [0.6455444097518921, 0.28903013467788696] actual: [1.0, 1.0] loss: 0.31555846333503723\n",
            "1255 rnn out: [0.604663610458374, 0.29842138290405273] actual: [1.0, 0.0] loss: 0.12267309427261353\n",
            "1283 rnn out: [0.659146785736084, 0.3176938593387604] actual: [1.0, 0.0] loss: 0.1085551530122757\n",
            "1365 rnn out: [0.6399828791618347, 0.2831157445907593] actual: [0.0, 1.0] loss: 0.4617505669593811\n",
            "1379 rnn out: [0.636867344379425, 0.33894747495651245] actual: [1.0, 0.0] loss: 0.12337535619735718\n",
            "1415 rnn out: [0.4835301637649536, 0.4796605110168457] actual: [0.0, 0.0] loss: 0.23193782567977905\n",
            "1501 rnn out: [0.6674636006355286, 0.2790050506591797] actual: [1.0, 0.0] loss: 0.0942121371626854\n",
            "1710 rnn out: [0.501045286655426, 0.44628337025642395] actual: [0.0, 0.0] loss: 0.2251076102256775\n",
            "1784 rnn out: [0.6976425051689148, 0.3113293945789337] actual: [0.0, 0.0] loss: 0.29181551933288574\n",
            "1812 rnn out: [0.4707402288913727, 0.5499908328056335] actual: [0.0, 0.0] loss: 0.2620431184768677\n",
            "1838 rnn out: [0.6415687799453735, 0.27386146783828735] actual: [0.0, 0.0] loss: 0.24330531060695648\n",
            "1888 rnn out: [0.6996857523918152, 0.3226070702075958] actual: [1.0, 0.0] loss: 0.09713198244571686\n",
            "1960 rnn out: [0.48242464661598206, 0.41665729880332947] actual: [0.0, 0.0] loss: 0.20316842198371887\n",
            "test_i 2000\n",
            "2025 rnn out: [0.6520522832870483, 0.33363842964172363] actual: [1.0, 0.0] loss: 0.11619110405445099\n",
            "2114 rnn out: [0.6334102749824524, 0.3046649098396301] actual: [0.0, 1.0] loss: 0.44234973192214966\n",
            "2167 rnn out: [0.685720682144165, 0.2967148721218109] actual: [0.0, 1.0] loss: 0.48241138458251953\n",
            "2204 rnn out: [0.5439260601997375, 0.41880127787590027] actual: [1.0, 1.0] loss: 0.2728976607322693\n",
            "2291 rnn out: [0.6040864586830139, 0.23317386209964752] actual: [1.0, 0.0] loss: 0.10555879026651382\n",
            "2319 rnn out: [0.6108839511871338, 0.21981839835643768] actual: [0.0, 1.0] loss: 0.4909312427043915\n",
            "2345 rnn out: [0.49331021308898926, 0.49644583463668823] actual: [0.0, 0.0] loss: 0.2449067234992981\n",
            "2376 rnn out: [0.5054113268852234, 0.4080255925655365] actual: [1.0, 0.0] loss: 0.205551415681839\n",
            "2399 rnn out: [0.687296986579895, 0.34025248885154724] actual: [0.0, 1.0] loss: 0.45382195711135864\n",
            "pred_count 47 correct_count 30 correct_ratio 0.64\n",
            "Epoch # 3 - Batch: 12 -  train loss: 0.126611 - test loss: 0.103207 - elapsed: 205.12\n",
            "batch_i0 13 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "146 rnn out: [0.6775872111320496, 0.30181482434272766] actual: [1.0, 1.0] loss: 0.29570630192756653\n",
            "275 rnn out: [0.6446036696434021, 0.3568817973136902] actual: [1.0, 0.0] loss: 0.12683558464050293\n",
            "276 rnn out: [0.6150729060173035, 0.23606054484844208] actual: [0.0, 0.0] loss: 0.21701963245868683\n",
            "287 rnn out: [0.6566667556762695, 0.27072393894195557] actual: [0.0, 1.0] loss: 0.4815274178981781\n",
            "406 rnn out: [0.5167378783226013, 0.50626540184021] actual: [0.0, 0.0] loss: 0.2616613507270813\n",
            "465 rnn out: [0.6199281811714172, 0.33978819847106934] actual: [1.0, 1.0] loss: 0.29016709327697754\n",
            "471 rnn out: [0.6859031319618225, 0.319254070520401] actual: [1.0, 1.0] loss: 0.2810359597206116\n",
            "568 rnn out: [0.47245997190475464, 0.4710024893283844] actual: [0.0, 0.0] loss: 0.22253088653087616\n",
            "717 rnn out: [0.6487513780593872, 0.3345813751220703] actual: [1.0, 1.0] loss: 0.28307878971099854\n",
            "test_i 1000\n",
            "1164 rnn out: [0.5106843709945679, 0.4524309039115906] actual: [1.0, 1.0] loss: 0.2696308493614197\n",
            "1181 rnn out: [0.604938805103302, 0.26591992378234863] actual: [1.0, 1.0] loss: 0.3474734425544739\n",
            "1259 rnn out: [0.6058326959609985, 0.2604771554470062] actual: [1.0, 0.0] loss: 0.1116081029176712\n",
            "1279 rnn out: [0.6036620736122131, 0.23586715757846832] actual: [0.0, 0.0] loss: 0.21002060174942017\n",
            "1362 rnn out: [0.6257599592208862, 0.3978383243083954] actual: [1.0, 1.0] loss: 0.25132712721824646\n",
            "1366 rnn out: [0.6252902746200562, 0.29465970396995544] actual: [0.0, 0.0] loss: 0.2389061450958252\n",
            "1392 rnn out: [0.6192653179168701, 0.31570711731910706] actual: [0.0, 0.0] loss: 0.24158024787902832\n",
            "1441 rnn out: [0.6776852011680603, 0.30649057030677795] actual: [1.0, 1.0] loss: 0.2924211025238037\n",
            "1647 rnn out: [0.6360647082328796, 0.2568655014038086] actual: [0.0, 1.0] loss: 0.47841358184814453\n",
            "1751 rnn out: [0.6615014672279358, 0.2795949876308441] actual: [1.0, 0.0] loss: 0.09637730568647385\n",
            "1846 rnn out: [0.6543738842010498, 0.2833814024925232] actual: [1.0, 0.0] loss: 0.09988121688365936\n",
            "1956 rnn out: [0.6525267362594604, 0.2938099205493927] actual: [1.0, 0.0] loss: 0.10353097319602966\n",
            "1976 rnn out: [0.6829552054405212, 0.3284793198108673] actual: [1.0, 1.0] loss: 0.2757287323474884\n",
            "test_i 2000\n",
            "2023 rnn out: [0.6736611127853394, 0.31901976466178894] actual: [1.0, 0.0] loss: 0.10413534194231033\n",
            "2133 rnn out: [0.6341696381568909, 0.2803794741630554] actual: [1.0, 0.0] loss: 0.1062222570180893\n",
            "2140 rnn out: [0.6542406678199768, 0.3038771152496338] actual: [0.0, 0.0] loss: 0.2601860761642456\n",
            "2281 rnn out: [0.607424259185791, 0.2536803185939789] actual: [1.0, 0.0] loss: 0.10923470556735992\n",
            "2343 rnn out: [0.6751894950866699, 0.3043898344039917] actual: [1.0, 1.0] loss: 0.2946876883506775\n",
            "2367 rnn out: [0.684150218963623, 0.32675886154174805] actual: [1.0, 0.0] loss: 0.10326621681451797\n",
            "2368 rnn out: [0.6416193842887878, 0.32464587688446045] actual: [1.0, 0.0] loss: 0.11691580712795258\n",
            "2444 rnn out: [0.6249150037765503, 0.32782694697380066] actual: [1.0, 0.0] loss: 0.124079629778862\n",
            "pred_count 30 correct_count 23 correct_ratio 0.77\n",
            "Epoch # 3 - Batch: 13 -  train loss: 0.126808 - test loss: 0.099038 - elapsed: 205.69\n",
            "batch_i0 14 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "155 rnn out: [0.6161302328109741, 0.26916101574897766] actual: [0.0, 0.0] loss: 0.2260320633649826\n",
            "260 rnn out: [0.6309018135070801, 0.3208347260951996] actual: [0.0, 0.0] loss: 0.25048601627349854\n",
            "301 rnn out: [0.6463375091552734, 0.4165659546852112] actual: [0.0, 1.0] loss: 0.37907373905181885\n",
            "327 rnn out: [0.5504650473594666, 0.4059915840625763] actual: [0.0, 1.0] loss: 0.32792890071868896\n",
            "447 rnn out: [0.6501721143722534, 0.3238547742366791] actual: [1.0, 1.0] loss: 0.28977593779563904\n",
            "641 rnn out: [0.6040775179862976, 0.31970250606536865] actual: [0.0, 0.0] loss: 0.23355966806411743\n",
            "654 rnn out: [0.4807935655117035, 0.4282795190811157] actual: [1.0, 0.0] loss: 0.22649931907653809\n",
            "707 rnn out: [0.4817069172859192, 0.4697461426258087] actual: [0.0, 0.0] loss: 0.22635149955749512\n",
            "781 rnn out: [0.6461700201034546, 0.3317042589187622] actual: [0.0, 1.0] loss: 0.43207743763923645\n",
            "796 rnn out: [0.6154744029045105, 0.2964352071285248] actual: [1.0, 1.0] loss: 0.32143163681030273\n",
            "801 rnn out: [0.6359738111495972, 0.2873132526874542] actual: [1.0, 1.0] loss: 0.3202187716960907\n",
            "803 rnn out: [0.637893795967102, 0.3166242837905884] actual: [1.0, 0.0] loss: 0.11568592488765717\n",
            "910 rnn out: [0.6098400354385376, 0.28177154064178467] actual: [0.0, 1.0] loss: 0.4438784718513489\n",
            "981 rnn out: [0.556641161441803, 0.43346449732780457] actual: [0.0, 0.0] loss: 0.24887043237686157\n",
            "test_i 1000\n",
            "1161 rnn out: [0.6570299863815308, 0.3337743580341339] actual: [1.0, 1.0] loss: 0.2807425558567047\n",
            "1168 rnn out: [0.6655110120773315, 0.34862789511680603] actual: [1.0, 0.0] loss: 0.11671214550733566\n",
            "1604 rnn out: [0.6547945141792297, 0.30322572588920593] actual: [1.0, 0.0] loss: 0.10555633902549744\n",
            "1666 rnn out: [0.6406006217002869, 0.31061309576034546] actual: [1.0, 0.0] loss: 0.1128242015838623\n",
            "1718 rnn out: [0.681181013584137, 0.3602747321128845] actual: [1.0, 0.0] loss: 0.11572171002626419\n",
            "1726 rnn out: [0.6200677752494812, 0.31072381138801575] actual: [0.0, 1.0] loss: 0.4297928810119629\n",
            "1731 rnn out: [0.6720974445343018, 0.32857877016067505] actual: [1.0, 0.0] loss: 0.10774204879999161\n",
            "1925 rnn out: [0.5553439259529114, 0.4207758605480194] actual: [0.0, 0.0] loss: 0.24272960424423218\n",
            "1936 rnn out: [0.6530237793922424, 0.353509783744812] actual: [1.0, 0.0] loss: 0.12268083542585373\n",
            "1953 rnn out: [0.4877525269985199, 0.44977086782455444] actual: [0.0, 0.0] loss: 0.22009816765785217\n",
            "test_i 2000\n",
            "2122 rnn out: [0.6140049695968628, 0.3163588047027588] actual: [1.0, 0.0] loss: 0.12453752756118774\n",
            "2151 rnn out: [0.6777385473251343, 0.339630663394928] actual: [0.0, 1.0] loss: 0.4477086067199707\n",
            "2213 rnn out: [0.653023898601532, 0.3214550018310547] actual: [0.0, 0.0] loss: 0.2648867666721344\n",
            "2304 rnn out: [0.638582706451416, 0.32197335362434387] actual: [1.0, 0.0] loss: 0.11714465171098709\n",
            "2463 rnn out: [0.6790327429771423, 0.34487685561180115] actual: [1.0, 1.0] loss: 0.2661031484603882\n",
            "pred_count 29 correct_count 21 correct_ratio 0.72\n",
            "Epoch # 3 - Batch: 14 -  train loss: 0.127966 - test loss: 0.097457 - elapsed: 206.38\n",
            "batch_i0 15 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "29 rnn out: [0.632167637348175, 0.3153298497200012] actual: [1.0, 0.0] loss: 0.11736678332090378\n",
            "228 rnn out: [0.6162049174308777, 0.35351693630218506] actual: [1.0, 0.0] loss: 0.13613644242286682\n",
            "237 rnn out: [0.6374894976615906, 0.2478644698858261] actual: [0.0, 0.0] loss: 0.2339148372411728\n",
            "249 rnn out: [0.6757762432098389, 0.2970866560935974] actual: [1.0, 0.0] loss: 0.09669075906276703\n",
            "393 rnn out: [0.634507954120636, 0.29167431592941284] actual: [1.0, 1.0] loss: 0.3176548480987549\n",
            "417 rnn out: [0.6559954881668091, 0.3587210774421692] actual: [1.0, 0.0] loss: 0.1235099583864212\n",
            "672 rnn out: [0.6547638773918152, 0.3159233629703522] actual: [1.0, 1.0] loss: 0.2935744524002075\n",
            "940 rnn out: [0.6311511397361755, 0.24242715537548065] actual: [1.0, 0.0] loss: 0.09741020202636719\n",
            "test_i 1000\n",
            "1024 rnn out: [0.6363559365272522, 0.2488064467906952] actual: [0.0, 1.0] loss: 0.4846203029155731\n",
            "1074 rnn out: [0.636539876461029, 0.2679313123226166] actual: [0.0, 0.0] loss: 0.23848509788513184\n",
            "1133 rnn out: [0.6458240151405334, 0.3368362486362457] actual: [0.0, 0.0] loss: 0.26527366042137146\n",
            "1188 rnn out: [0.6644505262374878, 0.302484929561615] actual: [1.0, 1.0] loss: 0.2995603680610657\n",
            "1335 rnn out: [0.6369428038597107, 0.26340457797050476] actual: [0.0, 0.0] loss: 0.23753905296325684\n",
            "1441 rnn out: [0.6780335307121277, 0.3103792071342468] actual: [1.0, 1.0] loss: 0.2896196246147156\n",
            "1491 rnn out: [0.6113032102584839, 0.2334994524717331] actual: [1.0, 1.0] loss: 0.3693041205406189\n",
            "1603 rnn out: [0.6040154099464417, 0.24011319875717163] actual: [1.0, 0.0] loss: 0.1072290763258934\n",
            "1628 rnn out: [0.600727379322052, 0.20929303765296936] actual: [0.0, 0.0] loss: 0.20233847200870514\n",
            "1644 rnn out: [0.653020977973938, 0.27729323506355286] actual: [1.0, 1.0] loss: 0.3213497996330261\n",
            "1675 rnn out: [0.49427708983421326, 0.45793643593788147] actual: [0.0, 1.0] loss: 0.26907140016555786\n",
            "1783 rnn out: [0.4909798204898834, 0.4520473778247833] actual: [0.0, 0.0] loss: 0.22270400822162628\n",
            "1852 rnn out: [0.6624303460121155, 0.27820250391960144] actual: [0.0, 1.0] loss: 0.4799027442932129\n",
            "1868 rnn out: [0.6418495774269104, 0.367012619972229] actual: [1.0, 0.0] loss: 0.1314849853515625\n",
            "1886 rnn out: [0.6740793585777283, 0.29311108589172363] actual: [1.0, 0.0] loss: 0.09606918692588806\n",
            "1898 rnn out: [0.6426030993461609, 0.2482917457818985] actual: [0.0, 0.0] loss: 0.2372937649488449\n",
            "test_i 2000\n",
            "2110 rnn out: [0.6840676665306091, 0.3221951425075531] actual: [1.0, 0.0] loss: 0.1018114686012268\n",
            "2127 rnn out: [0.6521083116531372, 0.27105578780174255] actual: [1.0, 0.0] loss: 0.09724993258714676\n",
            "2132 rnn out: [0.6784451603889465, 0.2988668382167816] actual: [1.0, 1.0] loss: 0.2974925935268402\n",
            "2372 rnn out: [0.6198168992996216, 0.2581067979335785] actual: [0.0, 0.0] loss: 0.2253960520029068\n",
            "pred_count 28 correct_count 20 correct_ratio 0.71\n",
            "Epoch # 3 - Batch: 15 -  train loss: 0.124422 - test loss: 0.101455 - elapsed: 205.66\n",
            "batch_i0 16 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "12 rnn out: [0.4710530638694763, 0.5396222472190857] actual: [0.0, 0.0] loss: 0.25654157996177673\n",
            "31 rnn out: [0.6276519894599915, 0.260358601808548] actual: [1.0, 0.0] loss: 0.1032148227095604\n",
            "48 rnn out: [0.6233656406402588, 0.2515532970428467] actual: [1.0, 0.0] loss: 0.10256624966859818\n",
            "63 rnn out: [0.6433911323547363, 0.260373592376709] actual: [1.0, 0.0] loss: 0.09748214483261108\n",
            "77 rnn out: [0.4812176823616028, 0.4700138568878174] actual: [0.0, 0.0] loss: 0.22624173760414124\n",
            "118 rnn out: [0.6351813673973083, 0.26678141951560974] actual: [1.0, 0.0] loss: 0.10213248431682587\n",
            "133 rnn out: [0.4838874936103821, 0.4017795920372009] actual: [0.0, 1.0] loss: 0.2960073947906494\n",
            "360 rnn out: [0.6803685426712036, 0.29226091504096985] actual: [1.0, 1.0] loss: 0.3015294671058655\n",
            "366 rnn out: [0.47831466794013977, 0.5609029531478882] actual: [0.0, 0.0] loss: 0.271698534488678\n",
            "508 rnn out: [0.6626281142234802, 0.30442070960998535] actual: [1.0, 1.0] loss: 0.2988251745700836\n",
            "525 rnn out: [0.47653892636299133, 0.5702369809150696] actual: [0.0, 0.0] loss: 0.2761297821998596\n",
            "537 rnn out: [0.5232242345809937, 0.4709247350692749] actual: [0.0, 1.0] loss: 0.2768421173095703\n",
            "614 rnn out: [0.5470904111862183, 0.51072096824646] actual: [0.0, 1.0] loss: 0.26935094594955444\n",
            "668 rnn out: [0.640220582485199, 0.2643599212169647] actual: [1.0, 0.0] loss: 0.09966370463371277\n",
            "866 rnn out: [0.48610737919807434, 0.4059048295021057] actual: [1.0, 0.0] loss: 0.21442219614982605\n",
            "test_i 1000\n",
            "1105 rnn out: [0.6440001130104065, 0.2888122498989105] actual: [1.0, 0.0] loss: 0.10507421940565109\n",
            "1356 rnn out: [0.6147031784057617, 0.316750705242157] actual: [0.0, 0.0] loss: 0.2390955090522766\n",
            "1365 rnn out: [0.6595668792724609, 0.2993731200695038] actual: [1.0, 1.0] loss: 0.3033863604068756\n",
            "1434 rnn out: [0.6020532846450806, 0.25605371594429016] actual: [0.0, 1.0] loss: 0.45796215534210205\n",
            "1443 rnn out: [0.674635112285614, 0.29257896542549133] actual: [1.0, 0.0] loss: 0.09573237597942352\n",
            "1446 rnn out: [0.47435253858566284, 0.403566837310791] actual: [0.0, 1.0] loss: 0.2903714179992676\n",
            "1448 rnn out: [0.638564944267273, 0.28522711992263794] actual: [1.0, 0.0] loss: 0.10599491000175476\n",
            "1492 rnn out: [0.6011204123497009, 0.2857086658477783] actual: [0.0, 1.0] loss: 0.4357789158821106\n",
            "1741 rnn out: [0.6102883815765381, 0.26300638914108276] actual: [1.0, 0.0] loss: 0.11052374541759491\n",
            "1910 rnn out: [0.5099607110023499, 0.4043453335762024] actual: [1.0, 0.0] loss: 0.20181682705879211\n",
            "test_i 2000\n",
            "2138 rnn out: [0.6054513454437256, 0.2329626828432083] actual: [1.0, 1.0] loss: 0.37200742959976196\n",
            "2148 rnn out: [0.6088991761207581, 0.237726628780365] actual: [0.0, 0.0] loss: 0.21363607048988342\n",
            "2217 rnn out: [0.6338071227073669, 0.2604583203792572] actual: [1.0, 0.0] loss: 0.1009678766131401\n",
            "2331 rnn out: [0.5733879208564758, 0.4066099226474762] actual: [1.0, 0.0] loss: 0.17366474866867065\n",
            "2407 rnn out: [0.6427518725395203, 0.2496366947889328] actual: [1.0, 0.0] loss: 0.09497234970331192\n",
            "2412 rnn out: [0.47961190342903137, 0.45969197154045105] actual: [0.0, 0.0] loss: 0.2206721305847168\n",
            "pred_count 31 correct_count 24 correct_ratio 0.77\n",
            "Epoch # 3 - Batch: 16 -  train loss: 0.127312 - test loss: 0.098765 - elapsed: 205.83\n",
            "batch_i0 17 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "27 rnn out: [0.6222062110900879, 0.23541094362735748] actual: [0.0, 1.0] loss: 0.48586851358413696\n",
            "30 rnn out: [0.6291322708129883, 0.34474366903305054] actual: [0.0, 0.0] loss: 0.2573277950286865\n",
            "185 rnn out: [0.6232171654701233, 0.2351619303226471] actual: [1.0, 0.0] loss: 0.09863321483135223\n",
            "211 rnn out: [0.6478940844535828, 0.28947219252586365] actual: [1.0, 0.0] loss: 0.10388636589050293\n",
            "224 rnn out: [0.6909922957420349, 0.3420522212982178] actual: [1.0, 1.0] loss: 0.26419052481651306\n",
            "295 rnn out: [0.6168056726455688, 0.2349804937839508] actual: [1.0, 0.0] loss: 0.10102686285972595\n",
            "362 rnn out: [0.5282629728317261, 0.45565348863601685] actual: [0.0, 1.0] loss: 0.2876874506473541\n",
            "634 rnn out: [0.6659440398216248, 0.29866960644721985] actual: [0.0, 1.0] loss: 0.4676729142665863\n",
            "728 rnn out: [0.695371687412262, 0.332793265581131] actual: [0.0, 1.0] loss: 0.4643533229827881\n",
            "856 rnn out: [0.47518640756607056, 0.535512387752533] actual: [1.0, 0.0] loss: 0.28110140562057495\n",
            "881 rnn out: [0.4950017035007477, 0.41742560267448425] actual: [1.0, 0.0] loss: 0.21463371813297272\n",
            "932 rnn out: [0.6486660838127136, 0.2914274036884308] actual: [1.0, 0.0] loss: 0.10418272018432617\n",
            "977 rnn out: [0.6619170904159546, 0.30874738097190857] actual: [0.0, 0.0] loss: 0.26672959327697754\n",
            "test_i 1000\n",
            "1068 rnn out: [0.5081763863563538, 0.4436131715774536] actual: [0.0, 1.0] loss: 0.2839047610759735\n",
            "1172 rnn out: [0.6949644088745117, 0.33678099513053894] actual: [0.0, 0.0] loss: 0.29819849133491516\n",
            "1206 rnn out: [0.6355970501899719, 0.2768160402774811] actual: [1.0, 0.0] loss: 0.10470831394195557\n",
            "1317 rnn out: [0.6266499161720276, 0.28362515568733215] actual: [0.0, 1.0] loss: 0.45294153690338135\n",
            "1325 rnn out: [0.6076849102973938, 0.26920822262763977] actual: [0.0, 0.0] loss: 0.2208770215511322\n",
            "1439 rnn out: [0.6280062198638916, 0.2739109694957733] actual: [1.0, 0.0] loss: 0.10670329630374908\n",
            "1453 rnn out: [0.6799156665802002, 0.30550262331962585] actual: [1.0, 0.0] loss: 0.09789291024208069\n",
            "1496 rnn out: [0.6161896586418152, 0.26994457840919495] actual: [1.0, 0.0] loss: 0.1100902259349823\n",
            "1709 rnn out: [0.6194775104522705, 0.2435738444328308] actual: [1.0, 1.0] loss: 0.35848894715309143\n",
            "1859 rnn out: [0.6160513162612915, 0.34272122383117676] actual: [1.0, 0.0] loss: 0.13243721425533295\n",
            "1892 rnn out: [0.621950089931488, 0.23660430312156677] actual: [0.0, 0.0] loss: 0.22140176594257355\n",
            "1947 rnn out: [0.6568602323532104, 0.28996023535728455] actual: [1.0, 0.0] loss: 0.10091091692447662\n",
            "test_i 2000\n",
            "2090 rnn out: [0.46819600462913513, 0.5575178861618042] actual: [1.0, 0.0] loss: 0.29682081937789917\n",
            "2177 rnn out: [0.48530256748199463, 0.46507230401039124] actual: [1.0, 1.0] loss: 0.2755305767059326\n",
            "2211 rnn out: [0.5502994060516357, 0.4478261172771454] actual: [0.0, 0.0] loss: 0.2516888380050659\n",
            "2414 rnn out: [0.6061960458755493, 0.2584655284881592] actual: [1.0, 0.0] loss: 0.11094298958778381\n",
            "2422 rnn out: [0.6716182231903076, 0.3695210814476013] actual: [1.0, 1.0] loss: 0.2526691257953644\n",
            "pred_count 30 correct_count 24 correct_ratio 0.8\n",
            "Epoch # 3 - Batch: 17 -  train loss: 0.127742 - test loss: 0.100344 - elapsed: 204.97\n",
            "batch_i0 18 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "16 rnn out: [0.67425137758255, 0.3180258274078369] actual: [1.0, 1.0] loss: 0.285600483417511\n",
            "34 rnn out: [0.641555666923523, 0.2696010172367096] actual: [1.0, 0.0] loss: 0.1005835235118866\n",
            "189 rnn out: [0.6168502569198608, 0.2738227844238281] actual: [0.0, 0.0] loss: 0.22774158418178558\n",
            "225 rnn out: [0.6835519075393677, 0.329574316740036] actual: [1.0, 1.0] loss: 0.2748049795627594\n",
            "361 rnn out: [0.6880291104316711, 0.3619544804096222] actual: [1.0, 1.0] loss: 0.2522139847278595\n",
            "609 rnn out: [0.6083496809005737, 0.2923921048641205] actual: [1.0, 0.0] loss: 0.11944155395030975\n",
            "661 rnn out: [0.668018639087677, 0.30769243836402893] actual: [0.0, 1.0] loss: 0.4627693295478821\n",
            "713 rnn out: [0.6261770725250244, 0.26502853631973267] actual: [0.0, 0.0] loss: 0.2311689257621765\n",
            "828 rnn out: [0.5155597925186157, 0.4078977108001709] actual: [1.0, 1.0] loss: 0.29263371229171753\n",
            "834 rnn out: [0.6380596160888672, 0.2712358236312866] actual: [1.0, 0.0] loss: 0.10228486359119415\n",
            "838 rnn out: [0.6915781497955322, 0.33960816264152527] actual: [1.0, 1.0] loss: 0.26562070846557617\n",
            "866 rnn out: [0.6761648058891296, 0.3466402590274811] actual: [1.0, 0.0] loss: 0.11251434683799744\n",
            "897 rnn out: [0.6183711886405945, 0.31026363372802734] actual: [1.0, 0.0] loss: 0.12095203995704651\n",
            "971 rnn out: [0.5091089606285095, 0.5164831876754761] actual: [0.0, 0.0] loss: 0.262973427772522\n",
            "test_i 1000\n",
            "1109 rnn out: [0.6107997298240662, 0.349058598279953] actual: [1.0, 0.0] loss: 0.1366593837738037\n",
            "1237 rnn out: [0.7006722688674927, 0.36023038625717163] actual: [1.0, 1.0] loss: 0.24945113062858582\n",
            "1361 rnn out: [0.6317492723464966, 0.2866963744163513] actual: [0.0, 1.0] loss: 0.4539545774459839\n",
            "1385 rnn out: [0.6310946941375732, 0.2929660677909851] actual: [0.0, 1.0] loss: 0.4490887522697449\n",
            "1453 rnn out: [0.6037644743919373, 0.25847044587135315] actual: [1.0, 0.0] loss: 0.11190478503704071\n",
            "1459 rnn out: [0.6137893795967102, 0.26796260476112366] actual: [0.0, 0.0] loss: 0.22427068650722504\n",
            "1560 rnn out: [0.6255068182945251, 0.28869014978408813] actual: [1.0, 0.0] loss: 0.11179357022047043\n",
            "1997 rnn out: [0.6288143396377563, 0.3113296926021576] actual: [1.0, 1.0] loss: 0.3060227632522583\n",
            "test_i 2000\n",
            "2061 rnn out: [0.6901161670684814, 0.34583577513694763] actual: [1.0, 1.0] loss: 0.2619794011116028\n",
            "2064 rnn out: [0.6010274887084961, 0.361863374710083] actual: [1.0, 0.0] loss: 0.14506208896636963\n",
            "2091 rnn out: [0.6721210479736328, 0.3327678143978119] actual: [1.0, 1.0] loss: 0.2763516902923584\n",
            "2209 rnn out: [0.4814259707927704, 0.4287722408771515] actual: [0.0, 0.0] loss: 0.20780830085277557\n",
            "2253 rnn out: [0.6205126047134399, 0.29026997089385986] actual: [0.0, 1.0] loss: 0.44437628984451294\n",
            "2285 rnn out: [0.6999828815460205, 0.3568892776966095] actual: [1.0, 0.0] loss: 0.10869011282920837\n",
            "2387 rnn out: [0.6834072470664978, 0.32398563623428345] actual: [0.0, 0.0] loss: 0.28600606322288513\n",
            "pred_count 29 correct_count 23 correct_ratio 0.79\n",
            "Epoch # 3 - Batch: 18 -  train loss: 0.129042 - test loss: 0.101831 - elapsed: 204.95\n",
            "batch_i0 19 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "20 rnn out: [0.6584440469741821, 0.2888767123222351] actual: [1.0, 0.0] loss: 0.10005511343479156\n",
            "85 rnn out: [0.6297438740730286, 0.34627223014831543] actual: [1.0, 0.0] loss: 0.12849703431129456\n",
            "207 rnn out: [0.676209568977356, 0.3164360523223877] actual: [1.0, 1.0] loss: 0.2860499620437622\n",
            "238 rnn out: [0.4781462848186493, 0.4632815718650818] actual: [0.0, 0.0] loss: 0.22162684798240662\n",
            "269 rnn out: [0.6676161289215088, 0.3318347930908203] actual: [1.0, 0.0] loss: 0.11029668152332306\n",
            "286 rnn out: [0.6754182577133179, 0.30146703124046326] actual: [1.0, 0.0] loss: 0.09811784327030182\n",
            "294 rnn out: [0.6110472083091736, 0.32124295830726624] actual: [0.0, 0.0] loss: 0.23828786611557007\n",
            "349 rnn out: [0.6037181615829468, 0.2818972170352936] actual: [1.0, 0.0] loss: 0.11825266480445862\n",
            "635 rnn out: [0.4709206819534302, 0.5386862754821777] actual: [0.0, 0.0] loss: 0.25597459077835083\n",
            "695 rnn out: [0.6322841048240662, 0.24825988709926605] actual: [1.0, 0.0] loss: 0.09842398017644882\n",
            "896 rnn out: [0.6016219854354858, 0.3069760799407959] actual: [1.0, 0.0] loss: 0.1264696717262268\n",
            "test_i 1000\n",
            "1338 rnn out: [0.6489182710647583, 0.2810004949569702] actual: [1.0, 0.0] loss: 0.1011098325252533\n",
            "1413 rnn out: [0.6810675859451294, 0.33081671595573425] actual: [1.0, 0.0] loss: 0.10557879507541656\n",
            "1422 rnn out: [0.47712165117263794, 0.5009734630584717] actual: [0.0, 0.0] loss: 0.23930974304676056\n",
            "1529 rnn out: [0.6537265181541443, 0.3199367821216583] actual: [1.0, 0.0] loss: 0.1111324355006218\n",
            "1678 rnn out: [0.6487377882003784, 0.26746150851249695] actual: [1.0, 0.0] loss: 0.09746040403842926\n",
            "1691 rnn out: [0.6887471079826355, 0.32041135430336] actual: [1.0, 0.0] loss: 0.09977090358734131\n",
            "1762 rnn out: [0.6983800530433655, 0.3358759582042694] actual: [1.0, 0.0] loss: 0.10189362615346909\n",
            "1816 rnn out: [0.6105459928512573, 0.22574834525585175] actual: [0.0, 0.0] loss: 0.21186435222625732\n",
            "1822 rnn out: [0.6514689922332764, 0.2935936748981476] actual: [1.0, 0.0] loss: 0.10383555293083191\n",
            "1830 rnn out: [0.6186010837554932, 0.30531999468803406] actual: [0.0, 0.0] loss: 0.23794379830360413\n",
            "test_i 2000\n",
            "2103 rnn out: [0.686408281326294, 0.3330264389514923] actual: [1.0, 0.0] loss: 0.10462318360805511\n",
            "2222 rnn out: [0.6991686820983887, 0.3372837007045746] actual: [1.0, 1.0] loss: 0.26484617590904236\n",
            "2333 rnn out: [0.6081073880195618, 0.26539888978004456] actual: [1.0, 1.0] loss: 0.34660932421684265\n",
            "pred_count 24 correct_count 18 correct_ratio 0.75\n",
            "Epoch # 3 - Batch: 19 -  train loss: 0.129145 - test loss: 0.097858 - elapsed: 205.14\n",
            "batch_i0 20 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "90 rnn out: [0.6033496260643005, 0.34565144777297974] actual: [1.0, 0.0] loss: 0.13840322196483612\n",
            "102 rnn out: [0.6647001504898071, 0.314951628446579] actual: [1.0, 0.0] loss: 0.1058102548122406\n",
            "135 rnn out: [0.6946463584899902, 0.371095210313797] actual: [0.0, 0.0] loss: 0.31012260913848877\n",
            "142 rnn out: [0.47971922159194946, 0.46154773235321045] actual: [0.0, 0.0] loss: 0.2215784192085266\n",
            "185 rnn out: [0.6885871291160583, 0.32224467396736145] actual: [0.0, 0.0] loss: 0.28899693489074707\n",
            "208 rnn out: [0.539686381816864, 0.4372144341468811] actual: [1.0, 1.0] loss: 0.2643080949783325\n",
            "214 rnn out: [0.6167994141578674, 0.3177240490913391] actual: [1.0, 0.0] loss: 0.12389563024044037\n",
            "232 rnn out: [0.4759572744369507, 0.5331209897994995] actual: [0.0, 0.0] loss: 0.2553766369819641\n",
            "282 rnn out: [0.47977638244628906, 0.4929157495498657] actual: [1.0, 0.0] loss: 0.25679928064346313\n",
            "283 rnn out: [0.4792942404747009, 0.4915710687637329] actual: [0.0, 0.0] loss: 0.23568254709243774\n",
            "286 rnn out: [0.6877230405807495, 0.3221340775489807] actual: [1.0, 0.0] loss: 0.10064363479614258\n",
            "296 rnn out: [0.5013111233711243, 0.4299107789993286] actual: [1.0, 1.0] loss: 0.2868461608886719\n",
            "326 rnn out: [0.6027910709381104, 0.24332037568092346] actual: [0.0, 1.0] loss: 0.4679605960845947\n",
            "522 rnn out: [0.646206259727478, 0.2720986306667328] actual: [1.0, 0.0] loss: 0.09960383176803589\n",
            "545 rnn out: [0.6232478618621826, 0.3210240304470062] actual: [1.0, 0.0] loss: 0.12249930202960968\n",
            "634 rnn out: [0.609157145023346, 0.24452485144138336] actual: [0.0, 0.0] loss: 0.21543242037296295\n",
            "664 rnn out: [0.6282873153686523, 0.2632431089878082] actual: [1.0, 0.0] loss: 0.10373362898826599\n",
            "675 rnn out: [0.6466943025588989, 0.27705177664756775] actual: [0.0, 0.0] loss: 0.24748560786247253\n",
            "691 rnn out: [0.6061795353889465, 0.31115037202835083] actual: [0.0, 0.0] loss: 0.23213408887386322\n",
            "733 rnn out: [0.47542715072631836, 0.5375955700874329] actual: [0.0, 0.0] loss: 0.25751999020576477\n",
            "749 rnn out: [0.4948439300060272, 0.4709964096546173] actual: [0.0, 1.0] loss: 0.2623576521873474\n",
            "826 rnn out: [0.6795492172241211, 0.32526180148124695] actual: [0.0, 0.0] loss: 0.283791184425354\n",
            "test_i 1000\n",
            "1015 rnn out: [0.706828773021698, 0.35198357701301575] actual: [0.0, 1.0] loss: 0.45976611971855164\n",
            "1057 rnn out: [0.6025972366333008, 0.24644945561885834] actual: [0.0, 1.0] loss: 0.46548089385032654\n",
            "1059 rnn out: [0.5085719227790833, 0.40379372239112854] actual: [0.0, 0.0] loss: 0.2108473926782608\n",
            "1064 rnn out: [0.6014035940170288, 0.3641263544559479] actual: [0.0, 1.0] loss: 0.3830108046531677\n",
            "1071 rnn out: [0.6078430414199829, 0.39045801758766174] actual: [0.0, 1.0] loss: 0.37050729990005493\n",
            "1094 rnn out: [0.6279938817024231, 0.3821498155593872] actual: [1.0, 1.0] loss: 0.26006370782852173\n",
            "1148 rnn out: [0.5264569520950317, 0.45045265555381775] actual: [0.0, 1.0] loss: 0.2895795702934265\n",
            "1152 rnn out: [0.5233624577522278, 0.4214453399181366] actual: [1.0, 1.0] loss: 0.28095442056655884\n",
            "1236 rnn out: [0.6247492432594299, 0.2604576647281647] actual: [1.0, 0.0] loss: 0.10432565957307816\n",
            "1516 rnn out: [0.6639592051506042, 0.35058286786079407] actual: [1.0, 0.0] loss: 0.11791588366031647\n",
            "1685 rnn out: [0.6676222681999207, 0.3235834836959839] actual: [0.0, 1.0] loss: 0.4516294002532959\n",
            "1757 rnn out: [0.6086772084236145, 0.3688756823539734] actual: [0.0, 1.0] loss: 0.38440293073654175\n",
            "1795 rnn out: [0.6723622679710388, 0.33079206943511963] actual: [1.0, 0.0] loss: 0.10838493704795837\n",
            "1952 rnn out: [0.6489586234092712, 0.3037185072898865] actual: [1.0, 0.0] loss: 0.10773748904466629\n",
            "1958 rnn out: [0.6100457310676575, 0.24961046874523163] actual: [1.0, 1.0] loss: 0.35757437348365784\n",
            "test_i 2000\n",
            "2081 rnn out: [0.5233458876609802, 0.47237902879714966] actual: [0.0, 1.0] loss: 0.27613741159439087\n",
            "2119 rnn out: [0.6275696754455566, 0.3136957287788391] actual: [0.0, 1.0] loss: 0.43242862820625305\n",
            "2278 rnn out: [0.6576629281044006, 0.32491233944892883] actual: [1.0, 1.0] loss: 0.286469042301178\n",
            "2321 rnn out: [0.618313729763031, 0.2626325190067291] actual: [0.0, 0.0] loss: 0.22564385831356049\n",
            "2413 rnn out: [0.6199033856391907, 0.24992084503173828] actual: [0.0, 1.0] loss: 0.4734494686126709\n",
            "2418 rnn out: [0.6910043358802795, 0.3307112753391266] actual: [1.0, 1.0] loss: 0.27171286940574646\n",
            "2442 rnn out: [0.6476317048072815, 0.27515527606010437] actual: [0.0, 1.0] loss: 0.4724133014678955\n",
            "pred_count 44 correct_count 32 correct_ratio 0.73\n",
            "Epoch # 3 - Batch: 20 -  train loss: 0.127376 - test loss: 0.105293 - elapsed: 205.16\n",
            "batch_i0 21 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "54 rnn out: [0.63083815574646, 0.24365989863872528] actual: [1.0, 0.0] loss: 0.0978253036737442\n",
            "80 rnn out: [0.4936908781528473, 0.5316910147666931] actual: [0.0, 1.0] loss: 0.23152199387550354\n",
            "317 rnn out: [0.6265666484832764, 0.2756713628768921] actual: [0.0, 1.0] loss: 0.4586188793182373\n",
            "412 rnn out: [0.6626712083816528, 0.2735256850719452] actual: [0.0, 1.0] loss: 0.4834490418434143\n",
            "579 rnn out: [0.6222695708274841, 0.2192458212375641] actual: [1.0, 1.0] loss: 0.3761287033557892\n",
            "580 rnn out: [0.563465416431427, 0.4268321394920349] actual: [0.0, 0.0] loss: 0.24983948469161987\n",
            "664 rnn out: [0.6845675110816956, 0.32221922278404236] actual: [1.0, 1.0] loss: 0.2794421911239624\n",
            "871 rnn out: [0.6909801959991455, 0.31979212164878845] actual: [1.0, 0.0] loss: 0.09888012707233429\n",
            "test_i 1000\n",
            "1095 rnn out: [0.6540600657463074, 0.2583995759487152] actual: [0.0, 0.0] loss: 0.2472824603319168\n",
            "1137 rnn out: [0.6098424196243286, 0.2869303822517395] actual: [1.0, 0.0] loss: 0.11727598309516907\n",
            "1250 rnn out: [0.6056287288665771, 0.2352064549922943] actual: [0.0, 0.0] loss: 0.21105411648750305\n",
            "1274 rnn out: [0.6036593914031982, 0.2107355147600174] actual: [0.0, 0.0] loss: 0.20440705120563507\n",
            "1498 rnn out: [0.6553348898887634, 0.28170379996299744] actual: [0.0, 0.0] loss: 0.25441041588783264\n",
            "1607 rnn out: [0.6304053068161011, 0.27331048250198364] actual: [1.0, 1.0] loss: 0.33233895897865295\n",
            "1690 rnn out: [0.653313398361206, 0.3019148111343384] actual: [1.0, 0.0] loss: 0.10567207634449005\n",
            "1701 rnn out: [0.6845557689666748, 0.2951407730579376] actual: [1.0, 0.0] loss: 0.09330657124519348\n",
            "1726 rnn out: [0.6430466771125793, 0.262490838766098] actual: [1.0, 0.0] loss: 0.09815855324268341\n",
            "1794 rnn out: [0.6806402802467346, 0.2839408218860626] actual: [1.0, 0.0] loss: 0.09130650758743286\n",
            "1861 rnn out: [0.6848998665809631, 0.3140971064567566] actual: [0.0, 0.0] loss: 0.28387242555618286\n",
            "1911 rnn out: [0.6167012453079224, 0.26238566637039185] actual: [1.0, 0.0] loss: 0.10788208991289139\n",
            "test_i 2000\n",
            "2017 rnn out: [0.5426378846168518, 0.49471214413642883] actual: [1.0, 0.0] loss: 0.22696009278297424\n",
            "2121 rnn out: [0.6864469051361084, 0.3213161826133728] actual: [0.0, 0.0] loss: 0.28722670674324036\n",
            "2122 rnn out: [0.651494026184082, 0.3035847842693329] actual: [0.0, 0.0] loss: 0.25830408930778503\n",
            "2144 rnn out: [0.5395329594612122, 0.4717945456504822] actual: [0.0, 1.0] loss: 0.2850484251976013\n",
            "2203 rnn out: [0.5071516036987305, 0.40397125482559204] actual: [1.0, 0.0] loss: 0.20304615795612335\n",
            "2246 rnn out: [0.612583339214325, 0.3013102114200592] actual: [1.0, 0.0] loss: 0.12043975293636322\n",
            "2391 rnn out: [0.6316112279891968, 0.2393786758184433] actual: [1.0, 0.0] loss: 0.09650621563196182\n",
            "pred_count 27 correct_count 19 correct_ratio 0.7\n",
            "Epoch # 3 - Batch: 21 -  train loss: 0.12434 - test loss: 0.100199 - elapsed: 205.08\n",
            "batch_i0 22 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "2 rnn out: [0.6649779677391052, 0.29205644130706787] actual: [0.0, 0.0] loss: 0.26374632120132446\n",
            "29 rnn out: [0.6448514461517334, 0.34740471839904785] actual: [0.0, 0.0] loss: 0.26826170086860657\n",
            "51 rnn out: [0.6247249245643616, 0.31162410974502563] actual: [0.0, 0.0] loss: 0.24369540810585022\n",
            "79 rnn out: [0.6706224679946899, 0.3466714322566986] actual: [1.0, 0.0] loss: 0.1143353208899498\n",
            "82 rnn out: [0.6262289881706238, 0.26023155450820923] actual: [1.0, 0.0] loss: 0.10371261835098267\n",
            "180 rnn out: [0.7192592024803162, 0.37539011240005493] actual: [1.0, 1.0] loss: 0.2344764620065689\n",
            "191 rnn out: [0.6208886504173279, 0.2511700391769409] actual: [0.0, 1.0] loss: 0.47312450408935547\n",
            "217 rnn out: [0.6734813451766968, 0.32126638293266296] actual: [1.0, 0.0] loss: 0.10491326451301575\n",
            "300 rnn out: [0.5123738646507263, 0.40294381976127625] actual: [1.0, 0.0] loss: 0.20007148385047913\n",
            "318 rnn out: [0.6146146655082703, 0.23119892179965973] actual: [1.0, 0.0] loss: 0.10098739713430405\n",
            "355 rnn out: [0.6843464374542236, 0.3609427511692047] actual: [1.0, 0.0] loss: 0.11495842039585114\n",
            "366 rnn out: [0.6153336763381958, 0.4305993318557739] actual: [0.0, 0.0] loss: 0.28202566504478455\n",
            "376 rnn out: [0.6150503754615784, 0.2818928062915802] actual: [0.0, 0.0] loss: 0.22887524962425232\n",
            "381 rnn out: [0.7060648202896118, 0.3421725630760193] actual: [0.0, 0.0] loss: 0.30780479311943054\n",
            "444 rnn out: [0.6795945167541504, 0.3056013286113739] actual: [0.0, 0.0] loss: 0.27762043476104736\n",
            "450 rnn out: [0.6093758940696716, 0.25432249903678894] actual: [1.0, 1.0] loss: 0.3543110191822052\n",
            "543 rnn out: [0.6108092665672302, 0.2524397373199463] actual: [1.0, 1.0] loss: 0.35515788197517395\n",
            "551 rnn out: [0.6799163818359375, 0.32848605513572693] actual: [0.0, 0.0] loss: 0.285094678401947\n",
            "615 rnn out: [0.6324927806854248, 0.2917882204055786] actual: [1.0, 0.0] loss: 0.11010096222162247\n",
            "658 rnn out: [0.6123097538948059, 0.276183545589447] actual: [1.0, 0.0] loss: 0.11329053342342377\n",
            "665 rnn out: [0.6068433523178101, 0.3413850665092468] actual: [0.0, 1.0] loss: 0.4010162353515625\n",
            "702 rnn out: [0.6002262830734253, 0.2328798472881317] actual: [1.0, 0.0] loss: 0.10702602565288544\n",
            "862 rnn out: [0.6589935421943665, 0.2830750346183777] actual: [0.0, 0.0] loss: 0.25720199942588806\n",
            "903 rnn out: [0.6038411855697632, 0.2392193078994751] actual: [1.0, 0.0] loss: 0.10708384215831757\n",
            "915 rnn out: [0.6705594062805176, 0.30261969566345215] actual: [1.0, 1.0] loss: 0.2974351942539215\n",
            "982 rnn out: [0.7190494537353516, 0.3689233958721161] actual: [1.0, 0.0] loss: 0.10751884430646896\n",
            "test_i 1000\n",
            "1078 rnn out: [0.6321659684181213, 0.2797216475009918] actual: [1.0, 0.0] loss: 0.10677303373813629\n",
            "1084 rnn out: [0.6195105910301208, 0.2404659390449524] actual: [1.0, 0.0] loss: 0.101298026740551\n",
            "1092 rnn out: [0.6180419325828552, 0.266118586063385] actual: [0.0, 1.0] loss: 0.46027886867523193\n",
            "1123 rnn out: [0.7109460234642029, 0.3634108603000641] actual: [1.0, 0.0] loss: 0.10780982673168182\n",
            "1228 rnn out: [0.6012572050094604, 0.3363632261753082] actual: [1.0, 0.0] loss: 0.13606801629066467\n",
            "1298 rnn out: [0.60911625623703, 0.24725544452667236] actual: [1.0, 0.0] loss: 0.10696268081665039\n",
            "1301 rnn out: [0.633332371711731, 0.26565784215927124] actual: [0.0, 1.0] loss: 0.4701841473579407\n",
            "1307 rnn out: [0.6958258152008057, 0.329010009765625] actual: [0.0, 0.0] loss: 0.2962105870246887\n",
            "1311 rnn out: [0.6094586253166199, 0.320939302444458] actual: [0.0, 1.0] loss: 0.41628164052963257\n",
            "1407 rnn out: [0.717116117477417, 0.3655872941017151] actual: [1.0, 0.0] loss: 0.10683868080377579\n",
            "1508 rnn out: [0.6790321469306946, 0.3148523271083832] actual: [0.0, 0.0] loss: 0.28010833263397217\n",
            "1623 rnn out: [0.7075632810592651, 0.3461120128631592] actual: [1.0, 1.0] loss: 0.2565443813800812\n",
            "1624 rnn out: [0.6647999286651611, 0.30345895886421204] actual: [1.0, 0.0] loss: 0.1022232174873352\n",
            "1733 rnn out: [0.6294528245925903, 0.26013877987861633] actual: [0.0, 0.0] loss: 0.23194152116775513\n",
            "1822 rnn out: [0.694049060344696, 0.34420761466026306] actual: [1.0, 0.0] loss: 0.10604242980480194\n",
            "1958 rnn out: [0.6223478317260742, 0.3263031840324402] actual: [1.0, 1.0] loss: 0.29824429750442505\n",
            "test_i 2000\n",
            "2051 rnn out: [0.6367132067680359, 0.26624593138694763] actual: [1.0, 0.0] loss: 0.10143209248781204\n",
            "2057 rnn out: [0.690608561038971, 0.3330906927585602] actual: [1.0, 0.0] loss: 0.10333623737096786\n",
            "2159 rnn out: [0.6077207922935486, 0.37893837690353394] actual: [0.0, 0.0] loss: 0.25645941495895386\n",
            "2234 rnn out: [0.7049753069877625, 0.37540701031684875] actual: [1.0, 0.0] loss: 0.11398500204086304\n",
            "2247 rnn out: [0.6144846677780151, 0.3140348196029663] actual: [1.0, 0.0] loss: 0.12361996620893478\n",
            "2260 rnn out: [0.7084861397743225, 0.35382285714149475] actual: [1.0, 0.0] loss: 0.10508547723293304\n",
            "2281 rnn out: [0.6891108155250549, 0.34459567070007324] actual: [1.0, 0.0] loss: 0.10769912600517273\n",
            "2326 rnn out: [0.7135854363441467, 0.3528405725955963] actual: [1.0, 0.0] loss: 0.10326488316059113\n",
            "2413 rnn out: [0.6485700011253357, 0.3438708782196045] actual: [1.0, 0.0] loss: 0.12087511271238327\n",
            "2427 rnn out: [0.6131247282028198, 0.4131321907043457] actual: [1.0, 1.0] loss: 0.24704314768314362\n",
            "2430 rnn out: [0.6425926685333252, 0.3346106708049774] actual: [1.0, 0.0] loss: 0.11985214799642563\n",
            "pred_count 53 correct_count 40 correct_ratio 0.75\n",
            "Epoch # 3 - Batch: 22 -  train loss: 0.124317 - test loss: 0.099515 - elapsed: 204.94\n",
            "batch_i0 23 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "73 rnn out: [0.6255285739898682, 0.22483180463314056] actual: [1.0, 1.0] loss: 0.3705572783946991\n",
            "129 rnn out: [0.6144140362739563, 0.3941817581653595] actual: [1.0, 1.0] loss: 0.2578461468219757\n",
            "288 rnn out: [0.700303316116333, 0.3135228157043457] actual: [0.0, 1.0] loss: 0.4808378219604492\n",
            "346 rnn out: [0.6850828528404236, 0.3441862463951111] actual: [0.0, 0.0] loss: 0.29390132427215576\n",
            "410 rnn out: [0.7004536986351013, 0.3264029622077942] actual: [0.0, 1.0] loss: 0.4721841812133789\n",
            "515 rnn out: [0.7008217573165894, 0.31846171617507935] actual: [1.0, 0.0] loss: 0.09546273946762085\n",
            "521 rnn out: [0.4713388681411743, 0.5457935333251953] actual: [1.0, 0.0] loss: 0.2886865735054016\n",
            "539 rnn out: [0.6767618060112, 0.2843271493911743] actual: [0.0, 0.0] loss: 0.2694242298603058\n",
            "553 rnn out: [0.6167647242546082, 0.2140713334083557] actual: [0.0, 0.0] loss: 0.21311262249946594\n",
            "556 rnn out: [0.6256294846534729, 0.30769485235214233] actual: [0.0, 0.0] loss: 0.2430441975593567\n",
            "631 rnn out: [0.6583272218704224, 0.28960615396499634] actual: [1.0, 0.0] loss: 0.10030600428581238\n",
            "634 rnn out: [0.5426755547523499, 0.4684634506702423] actual: [0.0, 1.0] loss: 0.2885139584541321\n",
            "644 rnn out: [0.6451737284660339, 0.24395200610160828] actual: [1.0, 0.0] loss: 0.09270713478326797\n",
            "777 rnn out: [0.6324654221534729, 0.27612221240997314] actual: [0.0, 0.0] loss: 0.23812800645828247\n",
            "818 rnn out: [0.6848208904266357, 0.28808876872062683] actual: [1.0, 0.0] loss: 0.09116650372743607\n",
            "829 rnn out: [0.5165491104125977, 0.43764445185661316] actual: [1.0, 0.0] loss: 0.21262872219085693\n",
            "870 rnn out: [0.6230837106704712, 0.2289484739303589] actual: [1.0, 0.0] loss: 0.09724164009094238\n",
            "test_i 1000\n",
            "1015 rnn out: [0.6261666417121887, 0.3617345988750458] actual: [0.0, 0.0] loss: 0.2614682912826538\n",
            "1030 rnn out: [0.6413945555686951, 0.2424612045288086] actual: [1.0, 1.0] loss: 0.3512314558029175\n",
            "1070 rnn out: [0.6191007494926453, 0.23488374054431915] actual: [0.0, 0.0] loss: 0.21922805905342102\n",
            "1150 rnn out: [0.6499315500259399, 0.27176716923713684] actual: [1.0, 1.0] loss: 0.32643550634384155\n",
            "1162 rnn out: [0.6793484091758728, 0.32284286618232727] actual: [0.0, 0.0] loss: 0.282870888710022\n",
            "1250 rnn out: [0.6773669123649597, 0.3270363509654999] actual: [1.0, 1.0] loss: 0.27848607301712036\n",
            "1298 rnn out: [0.6540178060531616, 0.32227614521980286] actual: [0.0, 1.0] loss: 0.44352447986602783\n",
            "1327 rnn out: [0.6758238077163696, 0.2931983768939972] actual: [1.0, 0.0] loss: 0.09552774578332901\n",
            "1377 rnn out: [0.6302893757820129, 0.2206091433763504] actual: [1.0, 0.0] loss: 0.09267717599868774\n",
            "1668 rnn out: [0.6289496421813965, 0.23455147445201874] actual: [1.0, 0.0] loss: 0.09634637832641602\n",
            "1678 rnn out: [0.6214761734008789, 0.23911501467227936] actual: [1.0, 0.0] loss: 0.10022813826799393\n",
            "1692 rnn out: [0.6650282144546509, 0.34226828813552856] actual: [0.0, 0.0] loss: 0.2797050476074219\n",
            "1701 rnn out: [0.6699101328849792, 0.3321334719657898] actual: [1.0, 0.0] loss: 0.10963597893714905\n",
            "1726 rnn out: [0.6758409738540649, 0.2833331525325775] actual: [1.0, 0.0] loss: 0.09267837554216385\n",
            "1790 rnn out: [0.6225343346595764, 0.3099362850189209] actual: [0.0, 1.0] loss: 0.43186846375465393\n",
            "1927 rnn out: [0.6113112568855286, 0.242808997631073] actual: [1.0, 0.0] loss: 0.10501757264137268\n",
            "1935 rnn out: [0.508104681968689, 0.44975629448890686] actual: [0.0, 1.0] loss: 0.28046926856040955\n",
            "1968 rnn out: [0.6161056756973267, 0.32822710275650024] actual: [1.0, 1.0] loss: 0.2993268370628357\n",
            "test_i 2000\n",
            "2173 rnn out: [0.6812339425086975, 0.286018967628479] actual: [0.0, 0.0] loss: 0.27294325828552246\n",
            "2217 rnn out: [0.5219134092330933, 0.4294401705265045] actual: [1.0, 1.0] loss: 0.2770526707172394\n",
            "2317 rnn out: [0.6575185656547546, 0.3186855912208557] actual: [1.0, 0.0] loss: 0.10942701995372772\n",
            "2322 rnn out: [0.6864379048347473, 0.31430697441101074] actual: [1.0, 0.0] loss: 0.09855502843856812\n",
            "2326 rnn out: [0.5062490701675415, 0.49907687306404114] actual: [0.0, 1.0] loss: 0.25360608100891113\n",
            "2371 rnn out: [0.4870753288269043, 0.44596153497695923] actual: [0.0, 0.0] loss: 0.21806202828884125\n",
            "2443 rnn out: [0.6080104112625122, 0.22064024209976196] actual: [1.0, 1.0] loss: 0.3805287182331085\n",
            "pred_count 42 correct_count 31 correct_ratio 0.74\n",
            "Epoch # 3 - Batch: 23 -  train loss: 0.12682 - test loss: 0.101519 - elapsed: 204.7\n",
            "batch_i0 24 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "176 rnn out: [0.48368239402770996, 0.5317026376724243] actual: [0.0, 0.0] loss: 0.2583281695842743\n",
            "182 rnn out: [0.6038145422935486, 0.3717777132987976] actual: [0.0, 1.0] loss: 0.37962764501571655\n",
            "222 rnn out: [0.6231459975242615, 0.2544781565666199] actual: [0.0, 0.0] loss: 0.22653503715991974\n",
            "386 rnn out: [0.6366847157478333, 0.27162861824035645] actual: [0.0, 0.0] loss: 0.23957477509975433\n",
            "417 rnn out: [0.6586836576461792, 0.3172416388988495] actual: [1.0, 0.0] loss: 0.10856954753398895\n",
            "594 rnn out: [0.5299860239028931, 0.4165201783180237] actual: [1.0, 0.0] loss: 0.19720110297203064\n",
            "646 rnn out: [0.6381208896636963, 0.256159245967865] actual: [1.0, 0.0] loss: 0.09828702360391617\n",
            "678 rnn out: [0.6568824052810669, 0.29467201232910156] actual: [0.0, 0.0] loss: 0.2591630518436432\n",
            "754 rnn out: [0.6605411767959595, 0.29130443930625916] actual: [1.0, 1.0] loss: 0.3087408244609833\n",
            "test_i 1000\n",
            "1012 rnn out: [0.613188624382019, 0.3794157803058624] actual: [1.0, 0.0] loss: 0.14678968489170074\n",
            "1167 rnn out: [0.607485294342041, 0.2711259126663208] actual: [0.0, 0.0] loss: 0.22127380967140198\n",
            "1387 rnn out: [0.6425971388816833, 0.3229914903640747] actual: [1.0, 1.0] loss: 0.29303866624832153\n",
            "1817 rnn out: [0.6512571573257446, 0.28107205033302307] actual: [1.0, 0.0] loss: 0.1003115326166153\n",
            "1955 rnn out: [0.4735415279865265, 0.5397652983665466] actual: [0.0, 0.0] loss: 0.2577940821647644\n",
            "test_i 2000\n",
            "2307 rnn out: [0.5389567017555237, 0.4953818619251251] actual: [0.0, 1.0] loss: 0.2725569009780884\n",
            "2335 rnn out: [0.48546484112739563, 0.5491686463356018] actual: [0.0, 0.0] loss: 0.2686311602592468\n",
            "2435 rnn out: [0.6084203720092773, 0.2649945020675659] actual: [1.0, 0.0] loss: 0.11177834868431091\n",
            "pred_count 17 correct_count 10 correct_ratio 0.59\n",
            "Epoch # 3 - Batch: 24 -  train loss: 0.126979 - test loss: 0.09526 - elapsed: 204.59\n",
            "batch_i0 25 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "58 rnn out: [0.6515454053878784, 0.28904083371162415] actual: [1.0, 1.0] loss: 0.31344181299209595\n",
            "110 rnn out: [0.6293119192123413, 0.27710387110710144] actual: [0.0, 1.0] loss: 0.45930612087249756\n",
            "166 rnn out: [0.6965250968933105, 0.33632880449295044] actual: [1.0, 1.0] loss: 0.2662782371044159\n",
            "198 rnn out: [0.622986376285553, 0.2590368390083313] actual: [1.0, 0.0] loss: 0.10461968183517456\n",
            "382 rnn out: [0.6692607402801514, 0.3115057349205017] actual: [0.0, 0.0] loss: 0.27247288823127747\n",
            "387 rnn out: [0.4850066006183624, 0.40857523679733276] actual: [0.0, 0.0] loss: 0.20108255743980408\n",
            "671 rnn out: [0.6795672178268433, 0.3437478542327881] actual: [1.0, 0.0] loss: 0.1104198768734932\n",
            "test_i 1000\n",
            "1147 rnn out: [0.6943982243537903, 0.3357934057712555] actual: [1.0, 0.0] loss: 0.10307483375072479\n",
            "1263 rnn out: [0.4701760411262512, 0.5583329200744629] actual: [0.0, 0.0] loss: 0.2664005756378174\n",
            "1342 rnn out: [0.6926015019416809, 0.3314855098724365] actual: [1.0, 1.0] loss: 0.2707027196884155\n",
            "1355 rnn out: [0.6118543744087219, 0.3063119351863861] actual: [1.0, 1.0] loss: 0.3159300684928894\n",
            "1425 rnn out: [0.6957321763038635, 0.32970085740089417] actual: [1.0, 1.0] loss: 0.2709399461746216\n",
            "1526 rnn out: [0.6023945212364197, 0.227881520986557] actual: [1.0, 0.0] loss: 0.10501004755496979\n",
            "1557 rnn out: [0.6076806783676147, 0.2268744856119156] actual: [0.0, 1.0] loss: 0.4834994375705719\n",
            "1562 rnn out: [0.6276382803916931, 0.35162100195884705] actual: [1.0, 0.0] loss: 0.13114528357982635\n",
            "1568 rnn out: [0.6065533757209778, 0.2987515926361084] actual: [0.0, 1.0] loss: 0.4298281669616699\n",
            "1617 rnn out: [0.6222579479217529, 0.28734490275382996] actual: [0.0, 1.0] loss: 0.44754108786582947\n",
            "1638 rnn out: [0.6806892156600952, 0.30372899770736694] actual: [0.0, 1.0] loss: 0.47406554222106934\n",
            "test_i 2000\n",
            "2016 rnn out: [0.5099367499351501, 0.4594634473323822] actual: [1.0, 0.0] loss: 0.22563432157039642\n",
            "2060 rnn out: [0.6732910871505737, 0.30492663383483887] actual: [1.0, 1.0] loss: 0.29493284225463867\n",
            "2252 rnn out: [0.6655901074409485, 0.29023033380508423] actual: [0.0, 1.0] loss: 0.473391592502594\n",
            "2293 rnn out: [0.6410937309265137, 0.2634868025779724] actual: [0.0, 0.0] loss: 0.24021323025226593\n",
            "2399 rnn out: [0.6137629747390747, 0.24174055457115173] actual: [1.0, 0.0] loss: 0.10380876809358597\n",
            "pred_count 23 correct_count 19 correct_ratio 0.83\n",
            "Epoch # 3 - Batch: 25 -  train loss: 0.125721 - test loss: 0.100091 - elapsed: 204.72\n",
            "batch_i0 26 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "16 rnn out: [0.6931150555610657, 0.35116711258888245] actual: [1.0, 0.0] loss: 0.10874836146831512\n",
            "183 rnn out: [0.6723054647445679, 0.31119003891944885] actual: [0.0, 0.0] loss: 0.2744169235229492\n",
            "316 rnn out: [0.4818679690361023, 0.42351460456848145] actual: [1.0, 1.0] loss: 0.3003981113433838\n",
            "361 rnn out: [0.4687809348106384, 0.5444084405899048] actual: [0.0, 0.0] loss: 0.2580680549144745\n",
            "421 rnn out: [0.6459990739822388, 0.282781720161438] actual: [1.0, 1.0] loss: 0.3198593258857727\n",
            "438 rnn out: [0.6667696833610535, 0.33753877878189087] actual: [1.0, 0.0] loss: 0.11248743534088135\n",
            "456 rnn out: [0.6879027485847473, 0.33351993560791016] actual: [1.0, 0.0] loss: 0.10432012379169464\n",
            "514 rnn out: [0.6891642808914185, 0.36377325654029846] actual: [1.0, 1.0] loss: 0.2507016658782959\n",
            "653 rnn out: [0.6399127840995789, 0.27412304282188416] actual: [0.0, 0.0] loss: 0.24231591820716858\n",
            "669 rnn out: [0.6136906743049622, 0.3127758502960205] actual: [1.0, 0.0] loss: 0.12353181093931198\n",
            "682 rnn out: [0.6819071173667908, 0.3604629337787628] actual: [0.0, 0.0] loss: 0.29746541380882263\n",
            "690 rnn out: [0.6101754307746887, 0.24479781091213226] actual: [0.0, 1.0] loss: 0.4713221788406372\n",
            "712 rnn out: [0.6435626745223999, 0.3331041932106018] actual: [1.0, 1.0] loss: 0.2858988046646118\n",
            "724 rnn out: [0.6262857913970947, 0.29901322722435] actual: [1.0, 0.0] loss: 0.1145356148481369\n",
            "796 rnn out: [0.636409342288971, 0.3535354733467102] actual: [0.0, 1.0] loss: 0.4114665985107422\n",
            "800 rnn out: [0.534419059753418, 0.42546066641807556] actual: [1.0, 1.0] loss: 0.2734305262565613\n",
            "825 rnn out: [0.5973706841468811, 0.43548381328582764] actual: [1.0, 1.0] loss: 0.2403944432735443\n",
            "879 rnn out: [0.6124295592308044, 0.3618743419647217] actual: [1.0, 0.0] loss: 0.14058193564414978\n",
            "934 rnn out: [0.48175936937332153, 0.5373485088348389] actual: [0.0, 0.0] loss: 0.26041775941848755\n",
            "968 rnn out: [0.6191383600234985, 0.32108837366104126] actual: [1.0, 1.0] loss: 0.30298829078674316\n",
            "test_i 1000\n",
            "1067 rnn out: [0.6498977541923523, 0.28780972957611084] actual: [1.0, 0.0] loss: 0.10270300507545471\n",
            "1133 rnn out: [0.6948640942573547, 0.3443560302257538] actual: [0.0, 1.0] loss: 0.4563525319099426\n",
            "1464 rnn out: [0.5496324896812439, 0.4227931797504425] actual: [1.0, 1.0] loss: 0.2679993212223053\n",
            "1473 rnn out: [0.6665269136428833, 0.3636792302131653] actual: [1.0, 0.0] loss: 0.12173344194889069\n",
            "1640 rnn out: [0.6972152590751648, 0.34796983003616333] actual: [0.0, 1.0] loss: 0.4556262195110321\n",
            "1677 rnn out: [0.516278088092804, 0.46992242336273193] actual: [1.0, 0.0] loss: 0.22740697860717773\n",
            "1691 rnn out: [0.6836577653884888, 0.33959144353866577] actual: [1.0, 0.0] loss: 0.10769738256931305\n",
            "1695 rnn out: [0.6219659447669983, 0.25238439440727234] actual: [1.0, 1.0] loss: 0.3509193956851959\n",
            "1711 rnn out: [0.6407825350761414, 0.35378244519233704] actual: [1.0, 0.0] loss: 0.12709960341453552\n",
            "1720 rnn out: [0.6159917116165161, 0.31730133295059204] actual: [1.0, 0.0] loss: 0.12407125532627106\n",
            "1786 rnn out: [0.6426792740821838, 0.2816268801689148] actual: [1.0, 0.0] loss: 0.10349589586257935\n",
            "1849 rnn out: [0.6846382021903992, 0.3501277267932892] actual: [1.0, 0.0] loss: 0.1110212430357933\n",
            "test_i 2000\n",
            "2088 rnn out: [0.6405807733535767, 0.34028127789497375] actual: [0.0, 0.0] loss: 0.2630675435066223\n",
            "2125 rnn out: [0.4772648215293884, 0.5551018714904785] actual: [0.0, 0.0] loss: 0.2679598927497864\n",
            "2290 rnn out: [0.6927462220191956, 0.34956225752830505] actual: [1.0, 0.0] loss: 0.10829932987689972\n",
            "2339 rnn out: [0.6443585753440857, 0.34653088450431824] actual: [1.0, 0.0] loss: 0.12328223884105682\n",
            "pred_count 36 correct_count 29 correct_ratio 0.81\n",
            "Epoch # 3 - Batch: 26 -  train loss: 0.126734 - test loss: 0.100676 - elapsed: 204.71\n",
            "batch_i0 27 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "20 rnn out: [0.6131763458251953, 0.22095532715320587] actual: [1.0, 0.0] loss: 0.09922689944505692\n",
            "207 rnn out: [0.6042203307151794, 0.20215894281864166] actual: [1.0, 0.0] loss: 0.0987548902630806\n",
            "212 rnn out: [0.6244277954101562, 0.3056376874446869] actual: [0.0, 1.0] loss: 0.43602454662323\n",
            "226 rnn out: [0.49157071113586426, 0.46081915497779846] actual: [1.0, 1.0] loss: 0.27460819482803345\n",
            "307 rnn out: [0.6307971477508545, 0.31288328766822815] actual: [1.0, 0.0] loss: 0.11710334569215775\n",
            "415 rnn out: [0.4837581515312195, 0.41638728976249695] actual: [0.0, 1.0] loss: 0.2873128652572632\n",
            "550 rnn out: [0.5072417855262756, 0.4406396150588989] actual: [1.0, 1.0] loss: 0.2778473496437073\n",
            "577 rnn out: [0.6003895401954651, 0.2419537454843521] actual: [1.0, 0.0] loss: 0.10911506414413452\n",
            "673 rnn out: [0.6055040955543518, 0.25257840752601624] actual: [1.0, 1.0] loss: 0.35713306069374084\n",
            "681 rnn out: [0.6631968021392822, 0.31725525856018066] actual: [1.0, 0.0] loss: 0.10704364627599716\n",
            "920 rnn out: [0.6264508366584778, 0.27953460812568665] actual: [1.0, 0.0] loss: 0.10883928835391998\n",
            "test_i 1000\n",
            "1095 rnn out: [0.6064813733100891, 0.2253112643957138] actual: [0.0, 1.0] loss: 0.4839811325073242\n",
            "1113 rnn out: [0.6508423686027527, 0.29031962156295776] actual: [1.0, 0.0] loss: 0.10309826582670212\n",
            "1116 rnn out: [0.604231059551239, 0.3159237504005432] actual: [1.0, 0.0] loss: 0.12822043895721436\n",
            "1170 rnn out: [0.6109557151794434, 0.252554714679718] actual: [1.0, 0.0] loss: 0.10756967216730118\n",
            "1203 rnn out: [0.47140634059906006, 0.5324475765228271] actual: [0.0, 0.0] loss: 0.25286218523979187\n",
            "1473 rnn out: [0.6701881289482117, 0.3130604922771454] actual: [1.0, 1.0] loss: 0.2903308570384979\n",
            "1475 rnn out: [0.5169527530670166, 0.41853606700897217] actual: [1.0, 1.0] loss: 0.2857174873352051\n",
            "1506 rnn out: [0.49411824345588684, 0.42421069741249084] actual: [0.0, 0.0] loss: 0.21205377578735352\n",
            "1582 rnn out: [0.5007712244987488, 0.5295067429542542] actual: [0.0, 0.0] loss: 0.2655746042728424\n",
            "1665 rnn out: [0.6110707521438599, 0.20404571294784546] actual: [1.0, 0.0] loss: 0.0964503064751625\n",
            "1826 rnn out: [0.6847520470619202, 0.29452311992645264] actual: [1.0, 0.0] loss: 0.09306257218122482\n",
            "1903 rnn out: [0.6327223181724548, 0.3457992374897003] actual: [0.0, 1.0] loss: 0.4141581058502197\n",
            "test_i 2000\n",
            "2028 rnn out: [0.6599623560905457, 0.2791130840778351] actual: [1.0, 0.0] loss: 0.09676485508680344\n",
            "2081 rnn out: [0.48266178369522095, 0.41880202293395996] actual: [1.0, 1.0] loss: 0.3027149438858032\n",
            "2244 rnn out: [0.472600519657135, 0.5179219841957092] actual: [1.0, 0.0] loss: 0.2731966972351074\n",
            "2250 rnn out: [0.67746502161026, 0.28623726963996887] actual: [0.0, 0.0] loss: 0.270445317029953\n",
            "2337 rnn out: [0.6607877016067505, 0.27520501613616943] actual: [1.0, 0.0] loss: 0.09540139138698578\n",
            "2441 rnn out: [0.48311689496040344, 0.49368125200271606] actual: [0.0, 0.0] loss: 0.23856155574321747\n",
            "pred_count 29 correct_count 24 correct_ratio 0.83\n",
            "Epoch # 3 - Batch: 27 -  train loss: 0.127209 - test loss: 0.101863 - elapsed: 204.7\n",
            "batch_i0 28 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "29 rnn out: [0.47663700580596924, 0.41233623027801514] actual: [0.0, 1.0] loss: 0.2862657904624939\n",
            "109 rnn out: [0.491651326417923, 0.41081222891807556] actual: [0.0, 0.0] loss: 0.20524385571479797\n",
            "145 rnn out: [0.5645305514335632, 0.43625128269195557] actual: [1.0, 0.0] loss: 0.18997441232204437\n",
            "197 rnn out: [0.6147388219833374, 0.3143636882305145] actual: [1.0, 0.0] loss: 0.12362535297870636\n",
            "239 rnn out: [0.6875389218330383, 0.34254398941993713] actual: [1.0, 1.0] loss: 0.26494017243385315\n",
            "320 rnn out: [0.6101087331771851, 0.2900545001029968] actual: [1.0, 0.0] loss: 0.11807340383529663\n",
            "380 rnn out: [0.49123430252075195, 0.4861755967140198] actual: [0.0, 0.0] loss: 0.23883891105651855\n",
            "537 rnn out: [0.6025623679161072, 0.36689770221710205] actual: [1.0, 0.0] loss: 0.1462852954864502\n",
            "562 rnn out: [0.511542558670044, 0.5220562219619751] actual: [0.0, 0.0] loss: 0.267109215259552\n",
            "683 rnn out: [0.5149352550506592, 0.5013057589530945] actual: [0.0, 0.0] loss: 0.25823289155960083\n",
            "707 rnn out: [0.6366440653800964, 0.2684778571128845] actual: [1.0, 0.0] loss: 0.10205394774675369\n",
            "771 rnn out: [0.6172065138816833, 0.30675122141838074] actual: [1.0, 0.0] loss: 0.12031358480453491\n",
            "843 rnn out: [0.6703448295593262, 0.32099953293800354] actual: [1.0, 0.0] loss: 0.10585661232471466\n",
            "893 rnn out: [0.47874915599823, 0.5223200917243958] actual: [0.0, 0.0] loss: 0.2510095238685608\n",
            "919 rnn out: [0.6399560570716858, 0.3212435245513916] actual: [1.0, 0.0] loss: 0.11641451716423035\n",
            "924 rnn out: [0.6694279313087463, 0.3102644383907318] actual: [0.0, 1.0] loss: 0.4619344472885132\n",
            "943 rnn out: [0.6033137440681458, 0.2986695468425751] actual: [1.0, 0.0] loss: 0.12328174710273743\n",
            "944 rnn out: [0.6533820033073425, 0.2996029555797577] actual: [0.0, 1.0] loss: 0.458732008934021\n",
            "test_i 1000\n",
            "1004 rnn out: [0.5204524397850037, 0.4142085611820221] actual: [0.0, 1.0] loss: 0.3070111870765686\n",
            "1142 rnn out: [0.6310227513313293, 0.298921674489975] actual: [1.0, 0.0] loss: 0.11274918913841248\n",
            "1145 rnn out: [0.6854845881462097, 0.34406349062919617] actual: [0.0, 0.0] loss: 0.2941344082355499\n",
            "1232 rnn out: [0.5231165885925293, 0.4459367096424103] actual: [1.0, 1.0] loss: 0.2672019600868225\n",
            "1368 rnn out: [0.6719194054603577, 0.3356183171272278] actual: [1.0, 0.0] loss: 0.11013826727867126\n",
            "1417 rnn out: [0.5359728336334229, 0.5246220827102661] actual: [0.0, 0.0] loss: 0.281247615814209\n",
            "1516 rnn out: [0.6631997227668762, 0.356388658285141] actual: [0.0, 0.0] loss: 0.28342336416244507\n",
            "1534 rnn out: [0.6868170499801636, 0.3400440514087677] actual: [1.0, 1.0] loss: 0.2668127417564392\n",
            "test_i 2000\n",
            "2242 rnn out: [0.6459193229675293, 0.36792999505996704] actual: [1.0, 0.0] loss: 0.1303727924823761\n",
            "2346 rnn out: [0.6599288582801819, 0.34292879700660706] actual: [0.0, 1.0] loss: 0.43362435698509216\n",
            "2430 rnn out: [0.6135942935943604, 0.30014172196388245] actual: [1.0, 0.0] loss: 0.1196972131729126\n",
            "pred_count 29 correct_count 21 correct_ratio 0.72\n",
            "Epoch # 3 - Batch: 28 -  train loss: 0.129082 - test loss: 0.100395 - elapsed: 205.02\n",
            "batch_i0 29 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "205 rnn out: [0.6382972002029419, 0.26675155758857727] actual: [1.0, 0.0] loss: 0.10099264979362488\n",
            "221 rnn out: [0.606252133846283, 0.27809005975723267] actual: [1.0, 0.0] loss: 0.1161857396364212\n",
            "270 rnn out: [0.6175482273101807, 0.3368397653102875] actual: [1.0, 0.0] loss: 0.12986518442630768\n",
            "296 rnn out: [0.6103836894035339, 0.27940186858177185] actual: [1.0, 0.0] loss: 0.11493313312530518\n",
            "312 rnn out: [0.6677718758583069, 0.328220009803772] actual: [1.0, 0.0] loss: 0.10905195027589798\n",
            "513 rnn out: [0.641865074634552, 0.2956005036830902] actual: [1.0, 0.0] loss: 0.10782013833522797\n",
            "595 rnn out: [0.6777401566505432, 0.3193110525608063] actual: [1.0, 1.0] loss: 0.28359442949295044\n",
            "619 rnn out: [0.5528592467308044, 0.4245392978191376] actual: [1.0, 1.0] loss: 0.26554492115974426\n",
            "666 rnn out: [0.6350573301315308, 0.2546144127845764] actual: [0.0, 0.0] loss: 0.23406314849853516\n",
            "799 rnn out: [0.6355640292167664, 0.3136959969997406] actual: [1.0, 1.0] loss: 0.3019133508205414\n",
            "816 rnn out: [0.6371460556983948, 0.43581536412239075] actual: [1.0, 0.0] loss: 0.16079899668693542\n",
            "913 rnn out: [0.6472174525260925, 0.3382719159126282] actual: [0.0, 0.0] loss: 0.266659140586853\n",
            "test_i 1000\n",
            "1065 rnn out: [0.628533661365509, 0.25073081254959106] actual: [1.0, 0.0] loss: 0.10042659193277359\n",
            "1074 rnn out: [0.5121654868125916, 0.4169512093067169] actual: [0.0, 1.0] loss: 0.30112969875335693\n",
            "1115 rnn out: [0.6058422327041626, 0.3249368667602539] actual: [0.0, 0.0] loss: 0.23631438612937927\n",
            "1135 rnn out: [0.5065218806266785, 0.4338361918926239] actual: [1.0, 1.0] loss: 0.28203102946281433\n",
            "1220 rnn out: [0.6557609438896179, 0.29975903034210205] actual: [1.0, 0.0] loss: 0.10417800396680832\n",
            "1288 rnn out: [0.6152588725090027, 0.237546905875206] actual: [1.0, 1.0] loss: 0.3646802306175232\n",
            "1386 rnn out: [0.6003714203834534, 0.2460128217935562] actual: [1.0, 0.0] loss: 0.11011265218257904\n",
            "1418 rnn out: [0.6388720273971558, 0.3440239429473877] actual: [0.0, 0.0] loss: 0.26325497031211853\n",
            "1438 rnn out: [0.7042937278747559, 0.3509398102760315] actual: [1.0, 1.0] loss: 0.2543606758117676\n",
            "1443 rnn out: [0.6655367612838745, 0.3514203131198883] actual: [1.0, 0.0] loss: 0.11768094450235367\n",
            "1672 rnn out: [0.633010983467102, 0.2915704548358917] actual: [0.0, 0.0] loss: 0.24285811185836792\n",
            "1684 rnn out: [0.6858240962028503, 0.3429390788078308] actual: [1.0, 0.0] loss: 0.10815685987472534\n",
            "1770 rnn out: [0.635380744934082, 0.2823478877544403] actual: [1.0, 0.0] loss: 0.10633376985788345\n",
            "1890 rnn out: [0.6032447218894958, 0.23103974759578705] actual: [0.0, 1.0] loss: 0.4776020050048828\n",
            "1977 rnn out: [0.6696374416351318, 0.33443784713745117] actual: [0.0, 1.0] loss: 0.44569364190101624\n",
            "test_i 2000\n",
            "2171 rnn out: [0.46881312131881714, 0.5313907861709595] actual: [0.0, 0.0] loss: 0.2510809600353241\n",
            "2199 rnn out: [0.6879935264587402, 0.3241260051727295] actual: [1.0, 0.0] loss: 0.10120285302400589\n",
            "2264 rnn out: [0.5651654005050659, 0.4098348021507263] actual: [1.0, 1.0] loss: 0.26868805289268494\n",
            "2330 rnn out: [0.6302744746208191, 0.2555842697620392] actual: [1.0, 0.0] loss: 0.10101014375686646\n",
            "2374 rnn out: [0.6322981715202332, 0.3174930810928345] actual: [0.0, 0.0] loss: 0.25030142068862915\n",
            "2379 rnn out: [0.6970375180244446, 0.3639329969882965] actual: [0.0, 1.0] loss: 0.44522130489349365\n",
            "pred_count 33 correct_count 26 correct_ratio 0.79\n",
            "Epoch # 3 - Batch: 29 -  train loss: 0.126928 - test loss: 0.097289 - elapsed: 204.68\n",
            "batch_i0 30 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n",
            "test_i 0\n",
            "137 rnn out: [0.6787518262863159, 0.3396652042865753] actual: [1.0, 1.0] loss: 0.26962122321128845\n",
            "150 rnn out: [0.606533408164978, 0.3283155858516693] actual: [0.0, 0.0] loss: 0.23783695697784424\n",
            "274 rnn out: [0.6343134045600891, 0.2826123833656311] actual: [1.0, 0.0] loss: 0.10679822415113449\n",
            "286 rnn out: [0.6114290356636047, 0.27403128147125244] actual: [0.0, 0.0] loss: 0.22446930408477783\n",
            "390 rnn out: [0.6221488118171692, 0.2666688859462738] actual: [1.0, 0.0] loss: 0.10694190859794617\n",
            "619 rnn out: [0.651488721370697, 0.37929731607437134] actual: [0.0, 0.0] loss: 0.28415200114250183\n",
            "770 rnn out: [0.6393008828163147, 0.35224202275276184] actual: [1.0, 0.0] loss: 0.12708914279937744\n",
            "887 rnn out: [0.6739413738250732, 0.34112808108329773] actual: [1.0, 1.0] loss: 0.27021318674087524\n",
            "985 rnn out: [0.6130884289741516, 0.27649351954460144] actual: [0.0, 1.0] loss: 0.4496694803237915\n",
            "test_i 1000\n",
            "1105 rnn out: [0.6750736236572266, 0.3625105321407318] actual: [1.0, 0.0] loss: 0.11849552392959595\n",
            "1128 rnn out: [0.6603763699531555, 0.3370189666748047] actual: [1.0, 0.0] loss: 0.11446300148963928\n",
            "1212 rnn out: [0.6035799384117126, 0.32546764612197876] actual: [0.0, 1.0] loss: 0.40965133905410767\n",
            "1323 rnn out: [0.6607082486152649, 0.3188071846961975] actual: [1.0, 0.0] loss: 0.10837845504283905\n",
            "1430 rnn out: [0.6436468362808228, 0.3318704664707184] actual: [1.0, 0.0] loss: 0.11856278777122498\n",
            "1466 rnn out: [0.6763458251953125, 0.3278862535953522] actual: [1.0, 0.0] loss: 0.10613071173429489\n",
            "1499 rnn out: [0.6388585567474365, 0.28146982192993164] actual: [1.0, 0.0] loss: 0.10482420027256012\n",
            "1549 rnn out: [0.6348323225975037, 0.2764313817024231] actual: [0.0, 1.0] loss: 0.4632818102836609\n",
            "1767 rnn out: [0.6076809763908386, 0.3597707748413086] actual: [0.0, 1.0] loss: 0.38958480954170227\n",
            "1799 rnn out: [0.6153847575187683, 0.35917699337005615] actual: [1.0, 0.0] loss: 0.13846850395202637\n",
            "1972 rnn out: [0.6374019980430603, 0.2896607220172882] actual: [1.0, 0.0] loss: 0.10769031941890717\n",
            "1993 rnn out: [0.6849366426467896, 0.3410504162311554] actual: [1.0, 1.0] loss: 0.26673975586891174\n",
            "test_i 2000\n",
            "2208 rnn out: [0.6573375463485718, 0.3240559995174408] actual: [1.0, 0.0] loss: 0.11121492087841034\n",
            "2237 rnn out: [0.48995473980903625, 0.5245801210403442] actual: [0.0, 0.0] loss: 0.2576199769973755\n",
            "2343 rnn out: [0.6036079525947571, 0.2944623827934265] actual: [1.0, 1.0] loss: 0.32745498418807983\n",
            "pred_count 24 correct_count 20 correct_ratio 0.83\n",
            "Epoch # 3 - Batch: 30 -  train loss: 0.127773 - test loss: 0.098871 - elapsed: 204.14\n",
            "batch_i0 31 cur_train_items 10000 cur_test_items 2464\n",
            "train_i 0\n",
            "train_i 2000\n",
            "train_i 4000\n",
            "train_i 6000\n",
            "train_i 8000\n"
          ]
        }
      ],
      "source": [
        "import time, math, random\n",
        "from random import shuffle\n",
        "\n",
        "model_name=\"exp6-pred1-combined-stocks\"\n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-full\"\n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-4layer-batches2\"\n",
        "n_input=1\n",
        "n_output=1\n",
        "n_output=2 #>5, min <5 & rebound >5\n",
        "n_hidden =64#64\n",
        "n_layers=4#3\n",
        "n_epochs=100\n",
        "LR=0.0000001\n",
        "prev_n,next_n=20,10\n",
        "n_train,n_test=None,None\n",
        "#n_train,n_test=1000,50\n",
        "train_batch_size=10000\n",
        "\n",
        "test_cutoff_val=0.5\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "def extract_labels(next_percents0):\n",
        "  if next_percents0==[]: return [0.,0.]\n",
        "  min_val0=min(next_percents0)\n",
        "  #print(\"min_val0\",min_val0)\n",
        "  min_val_index=next_percents0.index(min_val0)\n",
        "  max_val=max(next_percents0[1:])\n",
        "  max_val_index=next_percents0.index(max_val)\n",
        "  rebound_max_val=max(next_percents0[min_val_index:])\n",
        "  rebound_diff=round(rebound_max_val-min_val0,2)\n",
        "  max_greater_than_5=0.\n",
        "  found_minus_5=False\n",
        "  found_rebound_greater_than_5=False\n",
        "  cur_min_val=None\n",
        "  for i0, percent_val in enumerate(next_percents0):\n",
        "    if percent_val<-5 and found_minus_5==False: \n",
        "      found_minus_5=True\n",
        "      if cur_min_val==None or percent_val<cur_min_val: cur_min_val=percent_val\n",
        "      #print(\"percent_val\",percent_val,\"found_minus_5\",found_minus_5)\n",
        "      continue\n",
        "    if cur_min_val!=None and percent_val-cur_min_val>5: \n",
        "      found_rebound_greater_than_5=True\n",
        "      break\n",
        "  min_5_rebound_greater_than_5=0.\n",
        "  if max_val>5: max_greater_than_5=1.\n",
        "  if found_rebound_greater_than_5: min_5_rebound_greater_than_5=1.\n",
        "  actual_out0=[max_greater_than_5,min_5_rebound_greater_than_5]\n",
        "  return actual_out0\n",
        "\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "initial_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "#cur_path=os.path.join(root_dir,\"AAPL.csv\")\n",
        "all_files=[v.split(\".\")[0] for v in os.listdir(root_dir) if v.endswith(\".csv\")]\n",
        "additional_files=[v for v in all_files if not v in initial_files]\n",
        "sample_files=initial_files+additional_files[:90]\n",
        "\n",
        "#cur_train0,cur_test0=get_norm_close(\"stock_market_data/sp500/csv/AAPL.csv\",prev_n,next_n) #stock_market_data/sp500/csv/AAPL.csv\n",
        "\n",
        "\n",
        "\n",
        "#rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "\n",
        "\n",
        "model_dir=os.path.join(cwd,\"models\", model_name) \n",
        "tmp_model_dir=os.path.join(cwd,\"models\", model_name,\"tmp\") \n",
        "if not os.path.exists(tmp_model_dir): os.makedirs(tmp_model_dir)\n",
        "log_fpath=os.path.join(model_dir,\"log.txt\")\n",
        "log_fopen=open(log_fpath,\"a\")\n",
        "log_fopen.write(str(rnn)+\"\\n\")\n",
        "\n",
        "print(\"loading data\")\n",
        "all_training,all_testing=[],[]\n",
        "for fname in sample_files:\n",
        "  cur_path=os.path.join(root_dir,fname+\".csv\")\n",
        "  cur_train0,cur_test0=get_norm_close(cur_path,prev_n,next_n,train_ratio=0.8)\n",
        "  if n_train!=None: cur_train0=cur_train0[:n_train]\n",
        "  if n_test!=None: cur_test0=cur_test0[:n_test]\n",
        "  all_training.extend(cur_train0)\n",
        "  all_testing.extend(cur_test0)\n",
        "shuffle(all_training)\n",
        "shuffle(all_testing)\n",
        "print(\"all_training\", len(all_training),\"all_testing\",len(all_testing))\n",
        "n_batches=math.floor(len(all_training)/train_batch_size)\n",
        "test_batch_size=math.floor(len(all_testing)/n_batches)\n",
        "\n",
        "\n",
        "for epoch0 in range(n_epochs):\n",
        "  PATH=os.path.join(model_dir, \"model-%s.model\"%epoch0)\n",
        "  if os.path.exists(PATH):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print(\"loaded model for this epoch\",PATH)\n",
        "    for a,b in  checkpoint.items():\n",
        "      if \"loss\" in a.lower(): print(a,round(b,6))\n",
        "    continue  \n",
        "  print(\"epoch0\",epoch0)\n",
        "  for batch_i0 in range(n_batches+1):\n",
        "    t0=time.time()\n",
        "    pred_count,correct_count=0,0\n",
        "    batch_i1=batch_i0+1\n",
        "    cur_train_items=all_training[batch_i0*train_batch_size:batch_i1*train_batch_size]\n",
        "    cur_test_items=all_testing[batch_i0*test_batch_size:batch_i1*test_batch_size]\n",
        "    print(\"batch_i0\",batch_i0, \"cur_train_items\",len(cur_train_items),\"cur_test_items\",len(cur_test_items))\n",
        "    tmp_path=os.path.join(tmp_model_dir, \"model-batch-%s.model\"%batch_i0)\n",
        "    if os.path.exists(tmp_path):\n",
        "      checkpoint = torch.load(tmp_path)\n",
        "      rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      print(\"loaded model for this epoch\",tmp_path)\n",
        "      continue  \n",
        "\n",
        "    total_train_loss,total_test_loss=0,0\n",
        "    train_counter,test_counter=0,0\n",
        "    test_pred_counter,test_correct_counter=0,0 #how many test items reach the cutoff val for prediction, how many are correctly predicted\n",
        "    #for train_i in range(1500):\n",
        "    for train_i, train_item in enumerate(cur_train_items):\n",
        "      if train_i%2000==0: print(\"train_i\",train_i)\n",
        "      #print(train_i)\n",
        "      prev_vals,next_vals=train_item# cur_train0[train_i]\n",
        "      prev_vals=[round(v,2) for v in prev_vals]\n",
        "      next_vals=[round(v,2) for v in next_vals]\n",
        "      prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "      if prev_percents==[] or next_percents==[]: continue\n",
        "      actual_out=extract_labels(next_percents)\n",
        "      # print(\"cur_actual\", cur_actual, next_percents)\n",
        "      # print(\"--------\")\n",
        "      # continue\n",
        "      # min_val=min(next_percents)\n",
        "      # min_val_index=next_percents.index(min_val)\n",
        "      # max_val=max(next_percents)\n",
        "      # max_val_index=next_percents.index(max_val)\n",
        "      # rebound_max_val=max(next_percents[min_val_index:])\n",
        "      # rebound_diff=round(rebound_max_val-min_val,2)\n",
        "      \n",
        "      # max_gr_5=0.\n",
        "      # if max_val>5: max_gr_5=1.\n",
        "      # actual_out=[max_gr_5]\n",
        "      #prev_percents=[math.log(v) for v in prev_percents] #testing\n",
        "      #if prev_percents==[]: continue\n",
        "      input_tensor=torch.tensor(prev_percents)\n",
        "      actual_out_tensor=torch.tensor(actual_out).to(device)\n",
        "      rnn_output = rnn(input_tensor).to(device)\n",
        "      rnn_output_list=rnn_output.tolist()\n",
        "      loss = loss_func(actual_out_tensor.ravel(), rnn_output.ravel()) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      # if epoch0>3 and train_i<50:\n",
        "      #   print(\"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss+=loss.item()\n",
        "      train_counter+=1\n",
        "\n",
        "    for test_i, test_item in enumerate(cur_test_items):\n",
        "      #print(train_i)\n",
        "      if test_i%1000==0: print(\"test_i\",test_i)\n",
        "      rnn.zero_grad()\n",
        "      prev_vals,next_vals=test_item# cur_train0[train_i]\n",
        "      prev_vals=[round(v,2) for v in prev_vals]\n",
        "      next_vals=[round(v,2) for v in next_vals]\n",
        "      prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "      if prev_percents==[] or next_percents==[]: continue\n",
        "      actual_out=extract_labels(next_percents)\n",
        "\n",
        "      # continue\n",
        "      # min_val=min(next_percents)\n",
        "      # min_val_index=next_percents.index(min_val)\n",
        "      # max_val=max(next_percents)\n",
        "      # max_val_index=next_percents.index(max_val)\n",
        "      # rebound_max_val=max(next_percents[min_val_index:])\n",
        "      # rebound_diff=round(rebound_max_val-min_val,2)\n",
        "      \n",
        "      # max_gr_5=0.\n",
        "      # if max_val>5: max_gr_5=1.\n",
        "      # actual_out=[max_gr_5]\n",
        "      input_tensor=torch.tensor(prev_percents)\n",
        "      actual_out_tensor=torch.tensor(actual_out).to(device)\n",
        "      rnn_output = rnn(input_tensor).to(device)\n",
        "      rnn_output_list=rnn_output.ravel().tolist()\n",
        "      loss = loss_func(actual_out_tensor.ravel(), rnn_output.ravel()) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      predicted_increase,predicted_rebound=rnn_output_list\n",
        "      actual_increase,actual_rebound=actual_out\n",
        "      if predicted_increase>0.6 or predicted_rebound>0.4:\n",
        "        pred_count+=1\n",
        "        if actual_increase>0.5 or actual_rebound>0.5: correct_count+=1\n",
        "        print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "\n",
        "\n",
        "      # if test_i<50:\n",
        "      #   print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "      #print(loss)\n",
        "      pred_val=sum(rnn_output_list)/len(rnn_output_list)\n",
        "      if pred_val>=test_cutoff_val and False:\n",
        "        test_pred_counter+=1\n",
        "        #if pred_val>0.5\n",
        "        print(\"pred:\", round(pred_val,2), \"actual:\", sum(actual_out))\n",
        "        print(\"prev_percents\",prev_percents)\n",
        "        print(\"next_percents\",next_percents)\n",
        "        print(\"-------------\")\n",
        "      total_test_loss+=loss.item()\n",
        "      test_counter+=1\n",
        "\n",
        "\n",
        "    avg_train_loss=round(total_train_loss/train_counter,6)\n",
        "    avg_test_loss=round(total_test_loss/test_counter,6)\n",
        "    correct_ratio=0\n",
        "    if pred_count>0: correct_ratio=round(correct_count/pred_count,2)\n",
        "    print(\"pred_count\",pred_count,\"correct_count\",correct_count,\"correct_ratio\",correct_ratio)\n",
        "    # print(\"epoch0\",epoch0, fname, \"avg_train_loss\",avg_train_loss, \"avg_test_loss\",avg_test_loss)\n",
        "    # print(\"-------\")\n",
        "    \n",
        "    t1=time.time()\n",
        "    elapsed=round(t1-t0,2) \n",
        "    t0=time.time()    \n",
        "    line=\"Epoch # %s - Batch: %s -  train loss: %s - test loss: %s - elapsed: %s\"%(epoch0, batch_i0, avg_train_loss,avg_test_loss, elapsed)\n",
        "    #line=\"Epoch # %s  -  train loss: %s - test loss: %s - elapsed: %s\"%(epoch0, avg_train_loss,avg_test_loss, elapsed)\n",
        "    print(line)\n",
        "    log_fopen=open(log_fpath,\"a\")\n",
        "    log_fopen.write(line+\"\\n\")\n",
        "    log_fopen.close() \n",
        "    cur_checkpoint={\n",
        "            'epoch': epoch0,\n",
        "            'n_input': n_input,\n",
        "            'n_hidden': n_hidden,\n",
        "            'n_layers': n_layers,\n",
        "            'n_output': n_output,\n",
        "            'LR': LR,\n",
        "            'model_state_dict': rnn.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'test_loss': avg_test_loss\n",
        "            }\n",
        "    torch.save(cur_checkpoint, tmp_path)\n",
        "  \n",
        "  torch.save(cur_checkpoint, PATH)  \n",
        "  print(\"model saved\")\n",
        "  for f in os.listdir(tmp_model_dir):\n",
        "    tmp_fpath=os.path.join(tmp_model_dir,f)\n",
        "    os.remove(tmp_fpath)\n",
        "  print(\"deleted temporary files\")\n",
        "  print(\"-----------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1byMZUiRCP"
      },
      "source": [
        "#Testing on actual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoFyl9DsfLl_",
        "outputId": "63bd8f81-08ae-4831-f38b-bfee7262fa98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "621 rnn out: [0.600794792175293, 0.21132026612758636] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2771 rnn out: [0.6441137790679932, 0.2492138147354126] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2777 rnn out: [0.6164774298667908, 0.23347288370132446] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2778 rnn out: [0.6829203367233276, 0.2958845794200897] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2779 rnn out: [0.6592207551002502, 0.28471821546554565] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2780 rnn out: [0.6596035361289978, 0.28418242931365967] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2781 rnn out: [0.6845536231994629, 0.30300453305244446] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2782 rnn out: [0.6667603254318237, 0.29412606358528137] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2783 rnn out: [0.6823538541793823, 0.30344557762145996] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2784 rnn out: [0.6484724283218384, 0.30321961641311646] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2785 rnn out: [0.6778029203414917, 0.30166390538215637] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2786 rnn out: [0.6488338112831116, 0.30195197463035583] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2787 rnn out: [0.6601043939590454, 0.29467740654945374] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2788 rnn out: [0.6615582704544067, 0.2840612530708313] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2790 rnn out: [0.5150359869003296, 0.3703283369541168] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2791 rnn out: [0.485639363527298, 0.3776531219482422] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2792 rnn out: [0.5199421048164368, 0.3583081364631653] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> XOM pred_count 17 correct_count 13 correct_ratio 0.76\n",
            "=========\n",
            "XEL\n",
            "2585 rnn out: [0.6331388354301453, 0.24070481956005096] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2588 rnn out: [0.6278783679008484, 0.256404310464859] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2589 rnn out: [0.6825563311576843, 0.2935536503791809] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2590 rnn out: [0.6756163239479065, 0.2933007478713989] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2591 rnn out: [0.6249553561210632, 0.2927921414375305] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2595 rnn out: [0.47652342915534973, 0.3942772448062897] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            ">>>> XEL pred_count 6 correct_count 6 correct_ratio 1.0\n",
            "=========\n",
            "WDC\n",
            "62 rnn out: [0.6415956616401672, 0.24820511043071747] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "64 rnn out: [0.6059674024581909, 0.2304941564798355] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "70 rnn out: [0.6158304214477539, 0.23423686623573303] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "71 rnn out: [0.6439609527587891, 0.25291791558265686] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "72 rnn out: [0.6185141801834106, 0.23908981680870056] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "113 rnn out: [0.6109286546707153, 0.22240406274795532] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "259 rnn out: [0.6044942140579224, 0.20706672966480255] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "268 rnn out: [0.6196458339691162, 0.22671371698379517] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "269 rnn out: [0.6040604710578918, 0.2201247662305832] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "308 rnn out: [0.4783897399902344, 0.3567270040512085] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1120 rnn out: [0.612511396408081, 0.21785640716552734] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1121 rnn out: [0.6088899970054626, 0.21572478115558624] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1123 rnn out: [0.6141831278800964, 0.2229251116514206] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1177 rnn out: [0.6087697744369507, 0.2110222727060318] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1178 rnn out: [0.6151663064956665, 0.2240595817565918] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1179 rnn out: [0.6422178745269775, 0.25177305936813354] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "1180 rnn out: [0.6376082301139832, 0.25264593958854675] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "1181 rnn out: [0.6288648247718811, 0.24882924556732178] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "1182 rnn out: [0.6423290371894836, 0.2549252510070801] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1184 rnn out: [0.6391240358352661, 0.257167249917984] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1194 rnn out: [0.633514940738678, 0.2530199885368347] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1195 rnn out: [0.638785183429718, 0.2493891716003418] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1196 rnn out: [0.6173914670944214, 0.23768103122711182] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1880 rnn out: [0.6821877956390381, 0.29291394352912903] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "1881 rnn out: [0.6828398704528809, 0.2951398491859436] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1882 rnn out: [0.6733174324035645, 0.292927622795105] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1883 rnn out: [0.6453027725219727, 0.296392023563385] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1886 rnn out: [0.5283599495887756, 0.35134240984916687] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1887 rnn out: [0.5112653374671936, 0.3569555878639221] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1891 rnn out: [0.6381921172142029, 0.2921788692474365] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2014 rnn out: [0.6077836751937866, 0.21143276989459991] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2134 rnn out: [0.6265750527381897, 0.2255423218011856] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2214 rnn out: [0.6587940454483032, 0.2647961378097534] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2215 rnn out: [0.6334881782531738, 0.24785298109054565] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2220 rnn out: [0.6135760545730591, 0.25238245725631714] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2221 rnn out: [0.681681752204895, 0.2933306097984314] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2222 rnn out: [0.6494184136390686, 0.2804694175720215] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2223 rnn out: [0.6789426207542419, 0.29788738489151] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2224 rnn out: [0.6892484426498413, 0.31015947461128235] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2225 rnn out: [0.6698482632637024, 0.3069342374801636] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2226 rnn out: [0.6904674172401428, 0.3140219449996948] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2227 rnn out: [0.6811314225196838, 0.3161892592906952] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2228 rnn out: [0.6892914772033691, 0.3195200264453888] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2229 rnn out: [0.670993447303772, 0.3163532316684723] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2230 rnn out: [0.6605793833732605, 0.3184099793434143] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2231 rnn out: [0.5951856374740601, 0.3657587766647339] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2232 rnn out: [0.5252538919448853, 0.4522339999675751] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2233 rnn out: [0.5021086931228638, 0.47079962491989136] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2234 rnn out: [0.47920292615890503, 0.47128811478614807] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2235 rnn out: [0.4957951009273529, 0.4624632000923157] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2236 rnn out: [0.4818853437900543, 0.4443669021129608] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2237 rnn out: [0.5252820253372192, 0.41761842370033264] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2238 rnn out: [0.6180037260055542, 0.3643524646759033] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2239 rnn out: [0.6203474998474121, 0.3441317677497864] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2240 rnn out: [0.6131816506385803, 0.32545894384384155] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2241 rnn out: [0.4813411235809326, 0.4414421617984772] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2242 rnn out: [0.4913943111896515, 0.4175276756286621] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2248 rnn out: [0.6058674454689026, 0.29324766993522644] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2250 rnn out: [0.6191898584365845, 0.2610958516597748] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2251 rnn out: [0.6272621154785156, 0.24673409759998322] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2259 rnn out: [0.6146562099456787, 0.27757859230041504] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2260 rnn out: [0.6220938563346863, 0.2698632478713989] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2261 rnn out: [0.6024170517921448, 0.2776700258255005] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2264 rnn out: [0.5076740980148315, 0.37018072605133057] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2267 rnn out: [0.6039078235626221, 0.2504340410232544] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2326 rnn out: [0.6345365047454834, 0.23277555406093597] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2327 rnn out: [0.6342834830284119, 0.24171240627765656] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2328 rnn out: [0.6082475781440735, 0.23282606899738312] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2329 rnn out: [0.6036720871925354, 0.2353219985961914] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2331 rnn out: [0.6008551120758057, 0.22670120000839233] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2699 rnn out: [0.6595479249954224, 0.2703048884868622] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> WDC pred_count 71 correct_count 52 correct_ratio 0.73\n",
            "=========\n",
            "WRB\n",
            "2538 rnn out: [0.6543266177177429, 0.2614596486091614] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2540 rnn out: [0.6526360511779785, 0.2648543417453766] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2541 rnn out: [0.685454785823822, 0.3021450936794281] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2542 rnn out: [0.644662618637085, 0.2900455594062805] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2543 rnn out: [0.6860906481742859, 0.3067012429237366] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2544 rnn out: [0.6407098770141602, 0.3107416331768036] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2545 rnn out: [0.6518514752388, 0.313082218170166] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2546 rnn out: [0.6613195538520813, 0.29962098598480225] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2547 rnn out: [0.6570943593978882, 0.3013480603694916] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2548 rnn out: [0.6777212023735046, 0.2964499592781067] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2549 rnn out: [0.6089388132095337, 0.3117176294326782] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2550 rnn out: [0.6015543341636658, 0.3121322989463806] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2551 rnn out: [0.5220197439193726, 0.36294516921043396] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2552 rnn out: [0.4935477375984192, 0.370415598154068] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2553 rnn out: [0.46509987115859985, 0.3788555860519409] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            ">>>> WRB pred_count 15 correct_count 15 correct_ratio 1.0\n",
            "=========\n",
            "WY\n",
            "143 rnn out: [0.6872524619102478, 0.3194885551929474] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "144 rnn out: [0.6845772862434387, 0.31501883268356323] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "145 rnn out: [0.6720530390739441, 0.32379963994026184] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "146 rnn out: [0.6540563106536865, 0.33773624897003174] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "147 rnn out: [0.629951000213623, 0.3561357855796814] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "148 rnn out: [0.6126328110694885, 0.35919684171676636] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "149 rnn out: [0.6191521286964417, 0.34362664818763733] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "150 rnn out: [0.6086594462394714, 0.3332710564136505] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "151 rnn out: [0.6143468618392944, 0.3120475709438324] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "409 rnn out: [0.653548538684845, 0.25923994183540344] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "411 rnn out: [0.6426630020141602, 0.2583213746547699] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2265 rnn out: [0.6084322929382324, 0.21380239725112915] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2568 rnn out: [0.6405714154243469, 0.2481062114238739] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2569 rnn out: [0.6091145277023315, 0.23741944134235382] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2570 rnn out: [0.6634559035301208, 0.27457016706466675] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2571 rnn out: [0.6901529431343079, 0.3113940954208374] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2572 rnn out: [0.6465545296669006, 0.30050694942474365] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2573 rnn out: [0.6891509890556335, 0.3165057599544525] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2574 rnn out: [0.6658798456192017, 0.3274495005607605] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2575 rnn out: [0.685828447341919, 0.31402263045310974] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2576 rnn out: [0.6782099008560181, 0.31162506341934204] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2577 rnn out: [0.6849691867828369, 0.3107425272464752] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2578 rnn out: [0.6882470846176147, 0.31372934579849243] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2579 rnn out: [0.6018674969673157, 0.36710578203201294] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2580 rnn out: [0.5556285977363586, 0.41264453530311584] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2581 rnn out: [0.5197092890739441, 0.44386377930641174] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2582 rnn out: [0.5580295920372009, 0.4061904549598694] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2583 rnn out: [0.570529043674469, 0.3858451247215271] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2584 rnn out: [0.5612385869026184, 0.37712937593460083] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2585 rnn out: [0.6222691535949707, 0.33582803606987] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2586 rnn out: [0.6279253363609314, 0.3195003867149353] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2587 rnn out: [0.6412277221679688, 0.3082602620124817] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2588 rnn out: [0.4999310374259949, 0.3842146098613739] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2589 rnn out: [0.4840560555458069, 0.3863765001296997] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2590 rnn out: [0.46038150787353516, 0.4016864597797394] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2591 rnn out: [0.46423831582069397, 0.39305511116981506] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2606 rnn out: [0.6459193825721741, 0.26791876554489136] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2607 rnn out: [0.6366627812385559, 0.26004981994628906] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2608 rnn out: [0.6245001554489136, 0.24825194478034973] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2611 rnn out: [0.527728796005249, 0.3793468773365021] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2612 rnn out: [0.529310405254364, 0.3522983193397522] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2614 rnn out: [0.6300725936889648, 0.2561095952987671] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2615 rnn out: [0.6077514290809631, 0.2626756429672241] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2617 rnn out: [0.5004634261131287, 0.39432162046432495] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2618 rnn out: [0.5248203277587891, 0.35071197152137756] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2619 rnn out: [0.47518596053123474, 0.3885234296321869] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2620 rnn out: [0.4728236198425293, 0.3611813187599182] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2634 rnn out: [0.6279404759407043, 0.2587708830833435] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> WY pred_count 48 correct_count 41 correct_ratio 0.85\n",
            "=========\n",
            "ZION\n",
            "14 rnn out: [0.6029197573661804, 0.21555358171463013] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1204 rnn out: [0.6169857978820801, 0.21821218729019165] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2132 rnn out: [0.6379095911979675, 0.24498704075813293] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2133 rnn out: [0.6390330195426941, 0.2499571442604065] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2134 rnn out: [0.6852337718009949, 0.3000137507915497] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2135 rnn out: [0.6641073226928711, 0.29330337047576904] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2136 rnn out: [0.675652265548706, 0.2952134311199188] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2137 rnn out: [0.6831438541412354, 0.3051053583621979] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2138 rnn out: [0.6514697074890137, 0.301559716463089] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2139 rnn out: [0.6774372458457947, 0.3024728000164032] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2140 rnn out: [0.6402249932289124, 0.30030691623687744] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2141 rnn out: [0.6349503397941589, 0.293904572725296] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2142 rnn out: [0.6263336539268494, 0.29318517446517944] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2143 rnn out: [0.6563460826873779, 0.28882086277008057] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2144 rnn out: [0.6689462065696716, 0.2868462800979614] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2145 rnn out: [0.6258065104484558, 0.27825742959976196] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2151 rnn out: [0.6061023473739624, 0.2732822000980377] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2161 rnn out: [0.6167500019073486, 0.28336167335510254] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2200 rnn out: [0.639535665512085, 0.28265032172203064] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2201 rnn out: [0.6073580980300903, 0.2790980339050293] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2309 rnn out: [0.5289937257766724, 0.35305896401405334] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2310 rnn out: [0.4804997146129608, 0.3582109212875366] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> ZION pred_count 22 correct_count 19 correct_ratio 0.86\n",
            "=========\n",
            "WHR\n",
            "114 rnn out: [0.6162372827529907, 0.2232305109500885] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "421 rnn out: [0.6512971520423889, 0.25795871019363403] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "422 rnn out: [0.6049448847770691, 0.22289511561393738] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "423 rnn out: [0.6353731751441956, 0.24920766055583954] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "429 rnn out: [0.6106755137443542, 0.24803772568702698] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "545 rnn out: [0.4625251591205597, 0.3540799617767334] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1046 rnn out: [0.6020940542221069, 0.20568186044692993] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2172 rnn out: [0.6401539444923401, 0.23847299814224243] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2173 rnn out: [0.6630250811576843, 0.27020522952079773] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2174 rnn out: [0.6218932867050171, 0.244500532746315] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2175 rnn out: [0.6002606749534607, 0.23973578214645386] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2580 rnn out: [0.6573779582977295, 0.26587367057800293] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2582 rnn out: [0.6402695775032043, 0.2575373351573944] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2583 rnn out: [0.6707619428634644, 0.28399112820625305] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2585 rnn out: [0.6843546032905579, 0.29841071367263794] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2586 rnn out: [0.6632278561592102, 0.29916027188301086] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2587 rnn out: [0.6789143085479736, 0.3009914457798004] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2588 rnn out: [0.682913064956665, 0.30320146679878235] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2589 rnn out: [0.6878246068954468, 0.3093118667602539] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2590 rnn out: [0.6901021003723145, 0.31581202149391174] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2591 rnn out: [0.6297557950019836, 0.3386109471321106] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2592 rnn out: [0.6139642596244812, 0.3471013009548187] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2593 rnn out: [0.5878711342811584, 0.3620976209640503] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2594 rnn out: [0.5593371391296387, 0.3650732636451721] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2595 rnn out: [0.5129998922348022, 0.3675036132335663] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2597 rnn out: [0.6039705872535706, 0.33263903856277466] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2600 rnn out: [0.4623485505580902, 0.3529033958911896] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2602 rnn out: [0.45659250020980835, 0.35981330275535583] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2603 rnn out: [0.45729488134384155, 0.3750557005405426] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2646 rnn out: [0.6058088541030884, 0.23009537160396576] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> WHR pred_count 30 correct_count 25 correct_ratio 0.83\n",
            "=========\n",
            "XLEFF\n",
            "26 rnn out: [0.6885229349136353, 0.30113768577575684] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "27 rnn out: [0.6801006197929382, 0.2969031035900116] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "28 rnn out: [0.6700443625450134, 0.2966281473636627] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "29 rnn out: [0.656572699546814, 0.2957073450088501] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "30 rnn out: [0.6402286887168884, 0.29339325428009033] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "31 rnn out: [0.6187486052513123, 0.2833878993988037] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "184 rnn out: [0.6886736750602722, 0.3170323073863983] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "185 rnn out: [0.683940052986145, 0.3119809329509735] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "186 rnn out: [0.6754179000854492, 0.3166150152683258] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "187 rnn out: [0.6650941967964172, 0.315103679895401] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "188 rnn out: [0.6512425541877747, 0.31342771649360657] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "189 rnn out: [0.6366979479789734, 0.3123864531517029] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "190 rnn out: [0.6155826449394226, 0.3013414442539215] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "274 rnn out: [0.46430233120918274, 0.5028321146965027] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "304 rnn out: [0.4639403820037842, 0.5357256531715393] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "305 rnn out: [0.46134090423583984, 0.5229092240333557] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "306 rnn out: [0.46380770206451416, 0.5076070427894592] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "307 rnn out: [0.4691997170448303, 0.5000600218772888] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "308 rnn out: [0.4760395884513855, 0.47537165880203247] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "309 rnn out: [0.4798057973384857, 0.45624324679374695] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "310 rnn out: [0.4824858009815216, 0.43188342452049255] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "311 rnn out: [0.6592448353767395, 0.36694103479385376] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "312 rnn out: [0.6563573479652405, 0.35935163497924805] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "313 rnn out: [0.6487780809402466, 0.3490367829799652] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "314 rnn out: [0.6830151677131653, 0.34868964552879333] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "315 rnn out: [0.6775310635566711, 0.34798964858055115] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "316 rnn out: [0.4857420027256012, 0.49488094449043274] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "317 rnn out: [0.48395460844039917, 0.49608200788497925] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "318 rnn out: [0.4840405583381653, 0.4904172122478485] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "319 rnn out: [0.4856029748916626, 0.4747065603733063] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "320 rnn out: [0.48774945735931396, 0.44978052377700806] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "321 rnn out: [0.4897454082965851, 0.4140104353427887] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "322 rnn out: [0.5763595700263977, 0.38016965985298157] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "334 rnn out: [0.6895048022270203, 0.31133541464805603] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "335 rnn out: [0.683591365814209, 0.3139665424823761] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "336 rnn out: [0.6760239601135254, 0.31333696842193604] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "337 rnn out: [0.6637759804725647, 0.3112492263317108] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "338 rnn out: [0.6503382921218872, 0.31260111927986145] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "339 rnn out: [0.5490534901618958, 0.43579351902008057] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "340 rnn out: [0.5298953652381897, 0.4583492875099182] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "341 rnn out: [0.5166618824005127, 0.47175103425979614] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "342 rnn out: [0.4772394597530365, 0.4144579768180847] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "343 rnn out: [0.4922313094139099, 0.43418872356414795] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "344 rnn out: [0.49774274230003357, 0.44109946489334106] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "345 rnn out: [0.6824143528938293, 0.33088186383247375] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "346 rnn out: [0.6774890422821045, 0.33118706941604614] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "347 rnn out: [0.669045627117157, 0.33211252093315125] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "348 rnn out: [0.6566776633262634, 0.33136454224586487] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "349 rnn out: [0.6408997774124146, 0.3286731243133545] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "350 rnn out: [0.6224273443222046, 0.32203805446624756] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "351 rnn out: [0.6015599370002747, 0.309386670589447] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "373 rnn out: [0.46275103092193604, 0.5375802516937256] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "374 rnn out: [0.4637492597103119, 0.5285246968269348] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "375 rnn out: [0.468215674161911, 0.5411158204078674] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "376 rnn out: [0.4715869724750519, 0.514337420463562] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "377 rnn out: [0.4756983816623688, 0.49820592999458313] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "378 rnn out: [0.4783947467803955, 0.47689175605773926] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "379 rnn out: [0.47940370440483093, 0.4508860111236572] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "380 rnn out: [0.4811164438724518, 0.40775665640830994] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "381 rnn out: [0.47980502247810364, 0.3511162996292114] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "393 rnn out: [0.6878761053085327, 0.3167864978313446] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "394 rnn out: [0.6825041174888611, 0.32053810358047485] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "395 rnn out: [0.6756269335746765, 0.3151743412017822] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "396 rnn out: [0.6649149656295776, 0.3159320652484894] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "397 rnn out: [0.6524800062179565, 0.32008615136146545] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "398 rnn out: [0.636786937713623, 0.3137587606906891] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "399 rnn out: [0.6910648345947266, 0.3140082359313965] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "400 rnn out: [0.6851776838302612, 0.31693264842033386] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "401 rnn out: [0.6772447228431702, 0.3198334872722626] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "402 rnn out: [0.6668182015419006, 0.3170318007469177] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "403 rnn out: [0.6529762744903564, 0.318420946598053] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "404 rnn out: [0.6361438632011414, 0.3119059205055237] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "405 rnn out: [0.6183412671089172, 0.3036627471446991] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "431 rnn out: [0.465975284576416, 0.48189327120780945] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "467 rnn out: [0.46500086784362793, 0.4787473976612091] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "474 rnn out: [0.47484445571899414, 0.5322949290275574] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "475 rnn out: [0.47449061274528503, 0.5326704978942871] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "476 rnn out: [0.4641398787498474, 0.5417801737785339] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "477 rnn out: [0.4795018136501312, 0.5295193791389465] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "478 rnn out: [0.49168768525123596, 0.5137067437171936] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "479 rnn out: [0.46713194251060486, 0.5340738892555237] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "480 rnn out: [0.4940606355667114, 0.5277647972106934] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "481 rnn out: [0.6264426708221436, 0.4012264013290405] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "482 rnn out: [0.6364384889602661, 0.38704347610473633] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "483 rnn out: [0.633995532989502, 0.3800886273384094] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "484 rnn out: [0.6810676455497742, 0.34098297357559204] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "485 rnn out: [0.6826592087745667, 0.3351844549179077] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "486 rnn out: [0.673654317855835, 0.32963451743125916] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "487 rnn out: [0.6606332063674927, 0.32220157980918884] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "488 rnn out: [0.4938089847564697, 0.5156551003456116] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "489 rnn out: [0.47833430767059326, 0.5291866064071655] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "490 rnn out: [0.46716660261154175, 0.5466497540473938] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "491 rnn out: [0.468195378780365, 0.5376673936843872] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "492 rnn out: [0.6245717406272888, 0.4006178379058838] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "493 rnn out: [0.609778642654419, 0.4173387885093689] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "494 rnn out: [0.608316957950592, 0.4120745360851288] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "495 rnn out: [0.4922018051147461, 0.5434859991073608] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "496 rnn out: [0.5700114369392395, 0.45417413115501404] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "497 rnn out: [0.565744161605835, 0.45265570282936096] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "498 rnn out: [0.48722538352012634, 0.5383111238479614] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "499 rnn out: [0.637776792049408, 0.37395137548446655] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "500 rnn out: [0.6347460746765137, 0.3645890951156616] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "501 rnn out: [0.5100651979446411, 0.5030130743980408] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "502 rnn out: [0.502882719039917, 0.49895569682121277] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "503 rnn out: [0.5568903684616089, 0.4267445504665375] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "504 rnn out: [0.5516387820243835, 0.41103050112724304] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "505 rnn out: [0.4822756350040436, 0.45401129126548767] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "506 rnn out: [0.4783048629760742, 0.4221133887767792] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "507 rnn out: [0.4763779640197754, 0.37930652499198914] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "508 rnn out: [0.4435155987739563, 0.35749930143356323] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "514 rnn out: [0.4692327082157135, 0.41383519768714905] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "515 rnn out: [0.47804972529411316, 0.4360319674015045] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "516 rnn out: [0.6033170223236084, 0.2857678532600403] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "517 rnn out: [0.5788989663124084, 0.3603380024433136] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "518 rnn out: [0.5542187094688416, 0.40819087624549866] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "519 rnn out: [0.5385120511054993, 0.43564602732658386] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "520 rnn out: [0.5325068235397339, 0.4263187646865845] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "521 rnn out: [0.517270565032959, 0.42616018652915955] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "522 rnn out: [0.5145550966262817, 0.3893662393093109] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "523 rnn out: [0.6672273874282837, 0.3001857399940491] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "524 rnn out: [0.6550462245941162, 0.2932084798812866] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "525 rnn out: [0.640661895275116, 0.2901812493801117] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "526 rnn out: [0.5343979001045227, 0.4114428162574768] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "527 rnn out: [0.522185206413269, 0.41700583696365356] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "528 rnn out: [0.5124775767326355, 0.4124678671360016] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "529 rnn out: [0.5067353844642639, 0.39293208718299866] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "530 rnn out: [0.5007040500640869, 0.3653666079044342] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "540 rnn out: [0.6750856041908264, 0.2836812138557434] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "541 rnn out: [0.6627684831619263, 0.2761102318763733] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "542 rnn out: [0.6455731391906738, 0.2669294476509094] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "543 rnn out: [0.6258437633514404, 0.2588461935520172] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "544 rnn out: [0.6075860857963562, 0.25546231865882874] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "545 rnn out: [0.6381709575653076, 0.2636283338069916] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "546 rnn out: [0.6135514378547668, 0.24742622673511505] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "593 rnn out: [0.47224611043930054, 0.37409621477127075] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "601 rnn out: [0.4808790981769562, 0.5049474835395813] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "602 rnn out: [0.48114433884620667, 0.4933149814605713] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "603 rnn out: [0.47912660241127014, 0.48706763982772827] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "604 rnn out: [0.4812382757663727, 0.4652343988418579] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "605 rnn out: [0.5252299308776855, 0.4341655373573303] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "606 rnn out: [0.5309149622917175, 0.4016628563404083] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "607 rnn out: [0.52890545129776, 0.3697519302368164] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "608 rnn out: [0.672813355922699, 0.29503825306892395] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "609 rnn out: [0.664186954498291, 0.2930923104286194] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "610 rnn out: [0.6511226892471313, 0.29073482751846313] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "611 rnn out: [0.6347398161888123, 0.2876587510108948] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "612 rnn out: [0.6146517395973206, 0.2808982729911804] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "713 rnn out: [0.46724051237106323, 0.3727724254131317] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "725 rnn out: [0.6792131662368774, 0.2930653393268585] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "734 rnn out: [0.47066712379455566, 0.41900309920310974] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "735 rnn out: [0.47889444231987, 0.39723581075668335] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "736 rnn out: [0.489119291305542, 0.3833810091018677] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "738 rnn out: [0.501487672328949, 0.36587563157081604] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "741 rnn out: [0.687423050403595, 0.30607935786247253] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "742 rnn out: [0.6806156039237976, 0.30461063981056213] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "743 rnn out: [0.6708855032920837, 0.3045848608016968] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "744 rnn out: [0.657473087310791, 0.3014548718929291] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "745 rnn out: [0.6401334404945374, 0.29457640647888184] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "746 rnn out: [0.5548621416091919, 0.4551039934158325] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "747 rnn out: [0.5339682698249817, 0.4743881821632385] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "748 rnn out: [0.5420137643814087, 0.4575077295303345] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "749 rnn out: [0.5336952209472656, 0.45602771639823914] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "750 rnn out: [0.6800824999809265, 0.3133075535297394] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "751 rnn out: [0.6731422543525696, 0.31651967763900757] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "752 rnn out: [0.5070435404777527, 0.49814143776893616] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            ">>>> XLEFF pred_count 165 correct_count 83 correct_ratio 0.5\n",
            "=========\n",
            "WMB\n",
            "705 rnn out: [0.6225947141647339, 0.23319411277770996] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "706 rnn out: [0.6137838959693909, 0.22808967530727386] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "835 rnn out: [0.47342008352279663, 0.37367627024650574] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "880 rnn out: [0.6062430143356323, 0.21118079125881195] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "903 rnn out: [0.670454204082489, 0.2772899270057678] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "904 rnn out: [0.6758139133453369, 0.288692444562912] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "905 rnn out: [0.631871223449707, 0.27126139402389526] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "951 rnn out: [0.6285930871963501, 0.22901222109794617] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "952 rnn out: [0.6873135566711426, 0.3013383150100708] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "953 rnn out: [0.6750447750091553, 0.29337063431739807] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "954 rnn out: [0.6404303312301636, 0.2998986542224884] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "955 rnn out: [0.6054456233978271, 0.3060777485370636] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "956 rnn out: [0.6687031388282776, 0.29806050658226013] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "957 rnn out: [0.6687890887260437, 0.2999005913734436] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "958 rnn out: [0.6490469574928284, 0.2944965362548828] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "959 rnn out: [0.6295032501220703, 0.2875254452228546] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "960 rnn out: [0.6648289561271667, 0.28261029720306396] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "961 rnn out: [0.6827257871627808, 0.29743248224258423] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "962 rnn out: [0.6644866466522217, 0.2906317412853241] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "963 rnn out: [0.6351352334022522, 0.2933463752269745] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "972 rnn out: [0.6437692642211914, 0.2917007505893707] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "973 rnn out: [0.6759297251701355, 0.29244619607925415] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "974 rnn out: [0.6683191061019897, 0.2884054481983185] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "975 rnn out: [0.6788280606269836, 0.29013872146606445] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "976 rnn out: [0.6867733001708984, 0.30003952980041504] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "977 rnn out: [0.6900750994682312, 0.3210904598236084] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "978 rnn out: [0.6365072727203369, 0.32813283801078796] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "979 rnn out: [0.6640804409980774, 0.33226072788238525] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "980 rnn out: [0.6589244604110718, 0.3387434780597687] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "981 rnn out: [0.6553666591644287, 0.3391192853450775] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "982 rnn out: [0.6146503686904907, 0.3535892069339752] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "983 rnn out: [0.5050210356712341, 0.4459054172039032] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "984 rnn out: [0.5220585465431213, 0.4546140134334564] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "985 rnn out: [0.481764554977417, 0.4484865963459015] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "986 rnn out: [0.5057789087295532, 0.4579646587371826] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "987 rnn out: [0.5071359872817993, 0.44971126317977905] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "988 rnn out: [0.5084372758865356, 0.43022623658180237] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "989 rnn out: [0.49549221992492676, 0.40891340374946594] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "990 rnn out: [0.5566713213920593, 0.36222025752067566] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "993 rnn out: [0.6433169841766357, 0.2920781075954437] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "994 rnn out: [0.6887702345848083, 0.31300246715545654] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "995 rnn out: [0.680460512638092, 0.3175937235355377] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "996 rnn out: [0.6613349318504333, 0.33046627044677734] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "997 rnn out: [0.6380695104598999, 0.34802329540252686] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "998 rnn out: [0.6134418845176697, 0.3617508113384247] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "999 rnn out: [0.5670024752616882, 0.4092233180999756] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1000 rnn out: [0.5360842943191528, 0.43467849493026733] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1001 rnn out: [0.5159972310066223, 0.440751314163208] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1002 rnn out: [0.5201265215873718, 0.43174150586128235] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1003 rnn out: [0.488552451133728, 0.4271966516971588] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1004 rnn out: [0.5133023262023926, 0.3963567912578583] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1005 rnn out: [0.46492859721183777, 0.39227399230003357] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "1006 rnn out: [0.4922366142272949, 0.35420000553131104] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1016 rnn out: [0.6366788148880005, 0.2666810154914856] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1017 rnn out: [0.618003249168396, 0.2573338747024536] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1021 rnn out: [0.5007579326629639, 0.36312195658683777] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1022 rnn out: [0.47998228669166565, 0.3600945770740509] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1025 rnn out: [0.611539900302887, 0.26962417364120483] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1026 rnn out: [0.6216523051261902, 0.2389734834432602] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1027 rnn out: [0.6165704727172852, 0.23692263662815094] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2020 rnn out: [0.60468590259552, 0.21448881924152374] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2021 rnn out: [0.6867207288742065, 0.301450252532959] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2022 rnn out: [0.645828902721405, 0.2796173691749573] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2023 rnn out: [0.6428612470626831, 0.29075098037719727] actual: [0.0, 1.0] loss: 0.07818968594074249\n",
            "2024 rnn out: [0.6878777146339417, 0.30579301714897156] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2025 rnn out: [0.6189226508140564, 0.32004863023757935] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "2026 rnn out: [0.6783890724182129, 0.3136606216430664] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2027 rnn out: [0.6864341497421265, 0.3104236423969269] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2028 rnn out: [0.6895732879638672, 0.3224737346172333] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2029 rnn out: [0.675546407699585, 0.3256548345088959] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2030 rnn out: [0.6479021310806274, 0.34007588028907776] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2031 rnn out: [0.6424890160560608, 0.34402161836624146] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2032 rnn out: [0.5616831183433533, 0.40365201234817505] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2033 rnn out: [0.5295009613037109, 0.43365493416786194] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2034 rnn out: [0.49038419127464294, 0.4816584587097168] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2035 rnn out: [0.49474260210990906, 0.4572456181049347] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2036 rnn out: [0.4981306493282318, 0.43592962622642517] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2037 rnn out: [0.477845162153244, 0.4255712330341339] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2038 rnn out: [0.5578235387802124, 0.36309489607810974] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "2039 rnn out: [0.4668324291706085, 0.39313656091690063] actual: [1.0, 1.0] loss: 0.07818968594074249\n",
            "2042 rnn out: [0.4565959870815277, 0.3568260967731476] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> WMB pred_count 81 correct_count 67 correct_ratio 0.83\n",
            "=========\n",
            "XLNX\n",
            "72 rnn out: [0.6279516816139221, 0.2247328907251358] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "73 rnn out: [0.6115984916687012, 0.2178235948085785] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1269 rnn out: [0.6120812296867371, 0.20947887003421783] actual: [0.0, 0.0] loss: 0.07818968594074249\n",
            "1463 rnn out: [0.610625684261322, 0.21607057750225067] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1491 rnn out: [0.6433704495429993, 0.2514725923538208] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1493 rnn out: [0.6104671955108643, 0.23766697943210602] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            "1739 rnn out: [0.6217787265777588, 0.2283991426229477] actual: [1.0, 0.0] loss: 0.07818968594074249\n",
            ">>>> XLNX pred_count 7 correct_count 4 correct_ratio 0.57\n",
            "=========\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "e0=2\n",
        "model_name=\"exp5-pred1-3-test\"\n",
        "model_name=\"exp6-pred1-combined-stocks\"\n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-full\" \n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-4layer-batches2\"\n",
        "\n",
        "pred_cutoff_val=0.6\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "model_dir=os.path.join(cwd,\"models\", model_name) \n",
        "PATH=os.path.join(model_dir, \"model-%s.model\"%e0)\n",
        "checkpoint = torch.load(PATH)\n",
        "rnn = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , matching_in_out=False).to(device)\n",
        "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn.eval()\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "sample_test_files=['XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "#sample_test_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "for fname in sample_test_files:\n",
        "  print(fname)\n",
        "  pred_count,correct_count=0,0\n",
        "  cur_fpath=os.path.join(root_dir,fname+\".csv\")\n",
        "  #cur_train0,cur_test0=get_norm_close(\"stock_market_data/sp500/csv/AAPL.csv\") #stock_market_data/sp500/csv/AAPL.csv\n",
        "  cur_train0,cur_test0=get_norm_close(cur_fpath) #stock_market_data/sp500/csv/AAPL.csv\n",
        "  for test_i, test_item in enumerate(cur_test0):\n",
        "    #print(train_i)\n",
        "    rnn.zero_grad()\n",
        "    prev_vals,next_vals=test_item# cur_train0[train_i]\n",
        "    prev_vals=[round(v,2) for v in prev_vals]\n",
        "    next_vals=[round(v,2) for v in next_vals]\n",
        "    prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "    actual_out=extract_labels(next_percents)\n",
        "    # min_val=min(next_percents)\n",
        "    # min_val_index=next_percents.index(min_val)\n",
        "    # max_val=max(next_percents)\n",
        "    # max_val_index=next_percents.index(max_val)\n",
        "    # rebound_max_val=max(next_percents[min_val_index:])\n",
        "    # rebound_diff=round(rebound_max_val-min_val,2)\n",
        "    \n",
        "    # max_gr_5=0.\n",
        "    # if max_val>5: max_gr_5=1.\n",
        "    # actual_out=[max_gr_5]\n",
        "    \n",
        "    input_tensor=torch.tensor(prev_percents)\n",
        "    actual_out_tensor=torch.tensor(actual_out)\n",
        "    rnn_output = rnn(input_tensor)\n",
        "    rnn_output_list=rnn_output.ravel().tolist()\n",
        "    predicted_increase,predicted_rebound=rnn_output_list\n",
        "    actual_increase,actual_rebound=actual_out\n",
        "    # predicted_val=sum(rnn_output_list)\n",
        "    # actual_val=sum(actual_out)\n",
        "    # if test_i<50:\n",
        "    #   print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "    #   #print(loss)\n",
        "    #   print(\"--------\")\n",
        "    # continue\n",
        "\n",
        "    if predicted_increase>pred_cutoff_val or predicted_rebound>0.35:\n",
        "      pred_count+=1\n",
        "      if actual_increase>0.5 or actual_rebound>0.5: correct_count+=1\n",
        "      print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "      #if actual_increase>0.5: correct_count+=1\n",
        "      # print(fname, \"predicted_val:\",round(predicted_val,2), \"actual_out:\",round(actual_val,2))\n",
        "      # print(\"next_percents\",next_percents)\n",
        "      # #print(rnn_output_list)\n",
        "      # #print(sum(rnn_output_list))\n",
        "      # print(\"------\")\n",
        "  correct_ratio=0    \n",
        "  if pred_count>0: correct_ratio=round(correct_count/pred_count,2)\n",
        "  print(\">>>>\", fname, \"pred_count\",pred_count,\"correct_count\",correct_count,\"correct_ratio\",correct_ratio)\n",
        "  print(\"=========\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viF5xes1g1K2",
        "outputId": "b9c79c52-1800-423f-f301-8359896980cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_batch_size 3000 test_batch_size 736 n_batches 101\n"
          ]
        }
      ],
      "source": [
        "train_batch_size=3000\n",
        "n_batches=math.floor(len(all_training)/train_batch_size)\n",
        "test_batch_size=math.floor(len(all_testing)/n_batches)\n",
        "print(\"train_batch_size\",train_batch_size, \"test_batch_size\",test_batch_size, \"n_batches\",n_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GStrVn9_4oPE"
      },
      "outputs": [],
      "source": [
        "all_files=[v.split(\".\")[0] for v in os.listdir(root_dir) if v.endswith(\".csv\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpOpfx4z4pz-",
        "outputId": "255360fc-6039-400d-f113-000fd37d6552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_files[-10:]\n",
        "#print(test_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6YdclmF7ZM9"
      },
      "source": [
        "#Dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imMJWNGz5MRE"
      },
      "outputs": [],
      "source": [
        "  # for fname in sample_files:\n",
        "  #   t0=time.time()\n",
        "  #   #print(\"epoch0\",epoch0, \"fname\",fname)\n",
        "  #   tmp_path=os.path.join(tmp_model_dir, \"model-%s.model\"%fname)\n",
        "  #   if os.path.exists(tmp_path):\n",
        "  #     checkpoint = torch.load(tmp_path)\n",
        "  #     rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "  #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  #     print(\"loaded model for this epoch\",tmp_path)\n",
        "  #     continue  \n",
        "  #   #\n",
        "  #   cur_path=os.path.join(root_dir,fname+\".csv\")\n",
        "  #   cur_train0,cur_test0=get_norm_close(cur_path,prev_n,next_n,train_ratio=0.8)\n",
        "  #   if n_train!=None: cur_train0=cur_train0[:n_train]\n",
        "  #   if n_test!=None: cur_test0=cur_test0[:n_test]\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "stock-trading-support.ipynb",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMc9y8IYIVN3ANVPzqEZw4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}