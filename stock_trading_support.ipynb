{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmghaly/km/blob/main/stock_trading_support.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jalf_0IogEiZ"
      },
      "source": [
        "#Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An8Nie2b7DaE",
        "outputId": "fa85f0d8-d531-41fd-e497-4b1d1be6c179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/13dgt1IEtslxXGld9lXtheEMLLQEWiZh4/stocks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "cwd='/content/drive/MyDrive/stocks' #directory where we keep the data\n",
        "os.chdir(cwd)\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/stocks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCUJ7qdH2gXC",
        "outputId": "240bbdf8-a431-416e-ceee-3af1d2808740"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AAPL.csv\t\t\t  results\n",
            " archive.zip\t\t\t 'Robinhood transcactions.gsheet'\n",
            " daily\t\t\t\t  robin.ipynb\n",
            " daily-script.py\t\t  robin_stocks\n",
            " data\t\t\t\t  stock_info-aug28.txt\n",
            " models\t\t\t\t  stock_info_sp500.txt\n",
            " prediction-2022-09-01.gsheet\t  stock_info.txt\n",
            " prediction-2022-09-01.tsv\t  stock_market_data\n",
            " prediction-2022-09-06.gsheet\t  stock_symbols.txt\n",
            " prediction-2022-09-06.tsv\t  stock-trading-support.ipynb\n",
            " pypy3.9-v7.3.8-linux64\t\t  test-numpy-pypy.py\n",
            " pypy3.9-v7.3.8-linux64.tar.bz2   valid_stock_25_less_aug_28.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHUpJP-Dh4no"
      },
      "source": [
        "#Network Definition - Aug 2022\n",
        "https://github.com/hmghaly/word_align/blob/master/rnn_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNBM21yu6pXj",
        "outputId": "56eea47b-f540-46b9-8497-38363e5b122d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1])\n",
            "tensor([[[ 0.1250, -0.0037, -0.0528]]], grad_fn=<ViewBackward0>)\n",
            "RNN(\n",
            "  (lstm): LSTM(1, 64, num_layers=2)\n",
            "  (hidden2out): Linear(in_features=64, out_features=3, bias=True)\n",
            ")\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.10608354955911636, 0.013464339077472687, -0.05862850695848465]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.3377465307712555, -0.05398379638791084, 0.17546875774860382]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.70696622133255, -0.0033850669860839844, 0.5571586489677429]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.5057740211486816, 0.22393426299095154, 1.3384796380996704]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.672014832496643, 0.06658178567886353, 1.6787805557250977]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2303682565689087, -0.12230534106492996, 1.3965998888015747]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7009804248809814, -0.2365240454673767, 0.9133829474449158]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.346429705619812, -0.24315953254699707, 0.5261783599853516]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.20360167324543, -0.1833760142326355, 0.32594364881515503]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.19432374835014343, -0.1244024783372879, 0.272468626499176]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.2308298945426941, -0.07646417617797852, 0.27593427896499634]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.28298142552375793, -0.03757026046514511, 0.3025377690792084]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.33755892515182495, -0.008276678621768951, 0.34007972478866577]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.38406533002853394, 0.022359002381563187, 0.3842792212963104]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.4332730174064636, 0.05006253346800804, 0.4331395626068115]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.46516871452331543, 0.07804541289806366, 0.46413880586624146]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.5327234268188477, 0.11536826193332672, 0.5459355115890503]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.5972508192062378, 0.1442420482635498, 0.6189490556716919]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6710292100906372, 0.17484259605407715, 0.7055449485778809]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7971217632293701, 0.22305145859718323, 0.858417272567749]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9349514245986938, 0.2381977140903473, 1.020379662513733]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1052217483520508, 0.21430902183055878, 1.2166181802749634]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.3057974576950073, 0.1316896677017212, 1.4396934509277344]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.4748899936676025, -0.013577908277511597, 1.6059640645980835]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.627909779548645, -0.23331014811992645, 1.7375224828720093]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.6851720809936523, -0.4638086259365082, 1.735011339187622]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.6696854829788208, -0.6162533760070801, 1.6468820571899414]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.5734126567840576, -0.6238131523132324, 1.4797943830490112]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.4131011962890625, -0.4645155072212219, 1.2710474729537964]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.225582480430603, -0.24701502919197083, 1.0645580291748047]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0479300022125244, -0.0640784278512001, 0.8879882097244263]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8987424969673157, 0.055390629917383194, 0.7485979199409485]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.794927716255188, 0.11580698192119598, 0.6507202982902527]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7265641689300537, 0.14800786972045898, 0.5852403044700623]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6818574666976929, 0.16625505685806274, 0.5443997383117676]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6642287969589233, 0.18422992527484894, 0.52809739112854]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6627364158630371, 0.20457573235034943, 0.5278937816619873]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6720851063728333, 0.2284766286611557, 0.5383559465408325]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6928917169570923, 0.25964727997779846, 0.5601165294647217]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7182554006576538, 0.29450273513793945, 0.5867023468017578]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7488783597946167, 0.33568909764289856, 0.6185312867164612]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.781181812286377, 0.38030731678009033, 0.6523000597953796]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.812004804611206, 0.425398051738739, 0.6850258111953735]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.841242790222168, 0.46900540590286255, 0.7167761325836182]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8673750758171082, 0.5075579285621643, 0.7463522553443909]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8913363218307495, 0.5394501090049744, 0.7747352123260498]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9135423898696899, 0.5628662109375, 0.8024578094482422]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9342735409736633, 0.5762312412261963, 0.8300061821937561]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9542669653892517, 0.5800108909606934, 0.8577755093574524]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9736493825912476, 0.5737705230712891, 0.8864743709564209]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9926630258560181, 0.5591265559196472, 0.9158645868301392]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0112080574035645, 0.5363131761550903, 0.9462764263153076]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0292956829071045, 0.506283700466156, 0.9776523113250732]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0469716787338257, 0.47006478905677795, 1.0098892450332642]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0646724700927734, 0.42892396450042725, 1.043238639831543]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0824732780456543, 0.3837727904319763, 1.0775783061981201]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1003642082214355, 0.33529984951019287, 1.1127711534500122]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.11854887008667, 0.28510040044784546, 1.1485990285873413]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1369152069091797, 0.23376119136810303, 1.1848771572113037]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.155259609222412, 0.18187016248703003, 1.2213236093521118]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1732158660888672, 0.12977278232574463, 1.2575562000274658]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1903603076934814, 0.07767186313867569, 1.2931621074676514]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2063194513320923, 0.025526931509375572, 1.327835202217102]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.220562219619751, -0.026504546403884888, 1.3609710931777954]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2328684329986572, -0.07823975384235382, 1.3922909498214722]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2427290678024292, -0.12988720834255219, 1.4211636781692505]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2498055696487427, -0.18123918771743774, 1.4469681978225708]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.253993272781372, -0.23228240013122559, 1.4693565368652344]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2550674676895142, -0.2829553782939911, 1.4877405166625977]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2529977560043335, -0.333151638507843, 1.5017313957214355]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2478004693984985, -0.3827230930328369, 1.511059284210205]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2395837306976318, -0.43152910470962524, 1.5156235694885254]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.228579044342041, -0.4794720411300659, 1.5155696868896484]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.2149453163146973, -0.5263347029685974, 1.511049509048462]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1989421844482422, -0.5719482898712158, 1.5023937225341797]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1807861328125, -0.6160895228385925, 1.489946722984314]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1607369184494019, -0.658540666103363, 1.4741065502166748]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1390011310577393, -0.6990481019020081, 1.4552392959594727]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.1156580448150635, -0.7372092604637146, 1.4335228204727173]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0913015604019165, -0.7730360627174377, 1.409714698791504]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0657122135162354, -0.8058808445930481, 1.3836373090744019]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.0391730070114136, -0.8354864120483398, 1.3556605577468872]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[1.011698603630066, -0.861377477645874, 1.3258377313613892]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9837349057197571, -0.8834002017974854, 1.2946581840515137]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.955596387386322, -0.901345431804657, 1.2624680995941162]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.9267623424530029, -0.9143404960632324, 1.228698492050171]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8978182673454285, -0.9224891662597656, 1.1939715147018433]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8686673641204834, -0.9253679513931274, 1.1581470966339111]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8397569060325623, -0.9230879545211792, 1.1216574907302856]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.8110572099685669, -0.9154819250106812, 1.084431767463684]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7835025787353516, -0.9032254219055176, 1.0473741292953491]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7556934356689453, -0.8852368593215942, 1.0089854001998901]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7273471355438232, -0.8613829612731934, 0.9689196348190308]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.7007821798324585, -0.8336535096168518, 0.9294548034667969]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6749600768089294, -0.8014050722122192, 0.8894534111022949]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6511248350143433, -0.7658547163009644, 0.8500745296478271]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6297021508216858, -0.7275282740592957, 0.8116246461868286]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.6079351902008057, -0.6843620538711548, 0.7711914777755737]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.5906723737716675, -0.640468955039978, 0.733547031879425]]]\n",
            "-------\n",
            "Input: tensor([-2.0000,  5.2000, -1.1000,  3.2000,  2.0000,  8.9000, -7.3000])\n",
            "[[[0.5506818890571594, -0.5734642148017883, 0.6708508133888245]]]\n",
            "-------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import dill as pickle\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "device = torch.device('cpu')\n",
        "#device = torch.device('cuda')\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size,num_layers, matching_in_out=False, init_val=None, apply_sigmoid=False, apply_softmax=False, batch_size=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.apply_softmax=apply_softmax\n",
        "    self.apply_sigmoid=apply_sigmoid\n",
        "    self.init_val=init_val\n",
        "    self.matching_in_out = matching_in_out #length of input vector matches the length of output vector \n",
        "    self.lstm = nn.LSTM(input_size, hidden_size,num_layers)\n",
        "    self.hidden2out = nn.Linear(hidden_size, output_size)\n",
        "    if self.apply_softmax: self.softmax =nn.Softmax(dim=2)\n",
        "    if self.apply_sigmoid: self.sigmoid =nn.Sigmoid() \n",
        "    \n",
        "    #self.sigmoid = torch.sigmoid(dim=1)\n",
        "    self.hidden = self.init_hidden()\n",
        "  def forward(self, feature_list):\n",
        "    self.hidden = self.init_hidden() ### check\n",
        "    feature_list=torch.tensor(feature_list)\n",
        "    feature_list=feature_list.to(device) #### <<<<<<<<<<<<<<<<< \n",
        "    if self.matching_in_out:\n",
        "      lstm_out, _ = self.lstm( feature_list.view(len( feature_list), 1, -1))\n",
        "      output_scores = self.hidden2out(lstm_out.view(len( feature_list), -1))\n",
        "      if self.apply_sigmoid: output_scores=self.sigmoid(output_scores).to(device)\n",
        "      elif self.apply_softmax: output_scores=self.softmax(output_scores).to(device)\n",
        "      #output_scores = torch.sigmoid(output_space) #we'll need to check if we need this sigmoid\n",
        "      return output_scores #output_scores\n",
        "    else:\n",
        "      outs=[]\n",
        "      for i in range(len(feature_list)):\n",
        "        cur_ft_tensor=feature_list[i]#.view([1,1,self.input_size])\n",
        "        cur_ft_tensor=cur_ft_tensor.view([1,1,self.input_size])\n",
        "        lstm_out, self.hidden = self.lstm(cur_ft_tensor, self.hidden)\n",
        "        outs=self.hidden2out(lstm_out)\n",
        "        if self.apply_sigmoid: outs = self.sigmoid(outs).to(device) #self.sigmoid =nn.Sigmoid()\n",
        "        elif self.apply_softmax: outs = self.softmax(outs).to(device)\n",
        "        \n",
        "      return outs\n",
        "  def init_hidden(self):\n",
        "    #return torch.rand(self.num_layers, self.batch_size, self.hidden_size)\n",
        "    if self.init_val!=None:\n",
        "      h1=torch.ones(self.num_layers, self.batch_size, self.hidden_size)*self.init_val\n",
        "      h2=torch.ones(self.num_layers, self.batch_size, self.hidden_size)*self.init_val\n",
        "    else:\n",
        "      h1=torch.rand(self.num_layers, self.batch_size, self.hidden_size)\n",
        "      h2=torch.rand(self.num_layers, self.batch_size, self.hidden_size)\n",
        "    return (h1.to(device),h2.to(device))\n",
        "\n",
        "def to_tensor(list1):\n",
        "  return torch.tensor(list1,dtype=torch.float32)\n",
        "\n",
        "def split_train_test(all_data0,train_ratio=0.8):\n",
        "  train_size=int(len(all_data0)*train_ratio)\n",
        "  train_set0,tes_set0=all_data0[:train_size],all_data0[train_size:]\n",
        "  return train_set0,tes_set0  \n",
        "\n",
        "def out2labels(rnn_flat_out,label_list): #a flat rnn output to split into slices, and get the label weights for each slice\n",
        "  final_list=[]\n",
        "  n_slices=int(len(rnn_flat_out)/len(label_list))\n",
        "  for i0 in range(n_slices):\n",
        "    i1=i0+1\n",
        "    cur_slice=rnn_flat_out[i0*len(label_list):i1*len(label_list)]\n",
        "    tmp_list=[]\n",
        "    for lb0,cs0 in zip(label_list,cur_slice): tmp_list.append((lb0,cs0))\n",
        "    tmp_list.sort(key=lambda x:-x[-1])\n",
        "    final_list.append(tmp_list)\n",
        "  return final_list\n",
        "\n",
        "def dill_pickle(obj0,fpath0):\n",
        "  pickle_fopen=open(fpath0,\"wb\")\n",
        "  pickle.settings['recurse'] = True\n",
        "  pickle.dump(obj0,pickle_fopen)\n",
        "  pickle_fopen.close()\n",
        "\n",
        "def dill_unpickle(fpath0):\n",
        "  pickle_fopen0=open(fpath0,\"rb\")\n",
        "  tmp_dict=pickle.load(pickle_fopen0)\n",
        "  pickle_fopen0.close()\n",
        "  return tmp_dict\n",
        "\n",
        "def log_something(text0,fpath0):\n",
        "  fopen0=open(fpath0,\"a\")\n",
        "  fopen0.write(text0+\"\\n\")\n",
        "  fopen0.close()\n",
        "  \n",
        "n_input=1\n",
        "n_output=3\n",
        "n_hidden =64#64\n",
        "n_layers=2\n",
        "LR=0.01\n",
        "\n",
        "#rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False,init_val=None, apply_sigmoid=False).to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "\n",
        "\n",
        "#print(rnn.init_hidden())\n",
        "input_tensor=torch.rand(10,n_input)\n",
        "print(input_tensor.shape)\n",
        "rnn_out=rnn(input_tensor)\n",
        "print(rnn_out)\n",
        "actual_out=[1.,0.,1.]\n",
        "print(rnn)\n",
        "for a in range(100):\n",
        "  #rnn.zero_grad()\n",
        "  input_list=[-2.,5.2,-1.1,3.2,2,8.9,-7.3]\n",
        "  input_tensor=torch.tensor(input_list)\n",
        "  actual_out_tensor=torch.tensor(actual_out).to(device)\n",
        "#   #input_tensor=torch.rand((5, n_input)).to(device)\n",
        "  rnn_output = rnn(input_tensor).to(device)\n",
        "  print(\"Input:\",input_tensor)\n",
        "  #print(\"Output:\", output.shape)\n",
        "#   #rnn_output_list=rnn_output.ravel().tolist()\n",
        "  rnn_output_list=rnn_output.tolist()\n",
        "  print(rnn_output_list)\n",
        "  #print([round(v,4) for v in rnn_output_list])\n",
        "  loss = loss_func(actual_out_tensor.ravel(), rnn_output.ravel()) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(\"-------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lem9NgnPv9if"
      },
      "source": [
        "#Main functions - Aug 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DtEUJHArvhw-"
      },
      "outputs": [],
      "source": [
        "from types import prepare_class\n",
        "import pandas as pd\n",
        "from random import shuffle, seed\n",
        "import os, re, random\n",
        "import numpy.polynomial as poly\n",
        "\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "\n",
        "def get_pairs(list1): #turn the flat list of high-lows into a lit of pairs\n",
        "  out=[]\n",
        "  for i0 in range(0,len(list1),2): out.append((list1[i0],list1[i0+1]))\n",
        "  return out\n",
        "\n",
        "\n",
        "def gen_labels(max_val0=5):\n",
        "  str_vals=[]\n",
        "  for val_i in range(-max_val0-1,max_val0+2): str_vals.append(str(int(val_i)))\n",
        "  return str_vals\n",
        "\n",
        "\n",
        "def transpose(list1):\n",
        "  return list(map(list, zip(*list1)))\n",
        "\n",
        "def get_poly(list1,deg0=2):\n",
        "  indexes0=list(range(1,len(list1)+1))\n",
        "  c = poly.Polynomial.fit(indexes0, list1, deg = deg0)\n",
        "  return c.convert().coef.tolist()\n",
        "\n",
        "def get_f0(list1):\n",
        "  avg0=sum(list1)/len(list1)\n",
        "  counter=0\n",
        "  for i0 in range(1,len(list1)):\n",
        "    prev_val,cur_val=list1[i0-1],list1[i0]\n",
        "    if prev_val<avg0 and cur_val>avg0: counter+=1\n",
        "  avg_freq=counter/len(list1)\n",
        "  return avg_freq\n",
        "\n",
        "# def get_pd_col_data(pd_frame0,col_names0):\n",
        "#   all_col_data=[]\n",
        "#   for index0,row_dict0 in pd_frame0.iterrows():\n",
        "#     cur_list=[row_dict0.get(v,0.) for v in col_names0]\n",
        "#     for cl in cur_list: print(cl,type(cl))\n",
        "    \n",
        "#     all_col_data.append(cur_list)\n",
        "#   return all_col_data\n",
        "\n",
        "def get_pd_col_data(pd_frame0,col_names0):\n",
        "  all_col_data=[]\n",
        "  for index0,row_dict0 in pd_frame0.iterrows():\n",
        "    cur_list=[row_dict0.get(v,0.) for v in col_names0]\n",
        "    valid_row=True\n",
        "    for cl in cur_list: \n",
        "      if not type(cl) is float: valid_row=False\n",
        "    if not valid_row: continue\n",
        "    all_col_data.append(cur_list)\n",
        "  return all_col_data  \n",
        "\n",
        "def normalize_ft_labels_new(prev_2d_data0,next_2d_data0,params0={}): #get a chunk of data and normalize both the features and labels to last closing value of the features chunk\n",
        "  last_val=prev_2d_data0[-1][-1]\n",
        "  normalized_prev_list,normalized_next_list=[],[]\n",
        "  normalized_prev_list=normalize_analyze_features(prev_2d_data0,params0)\n",
        "  # for item0 in prev_2d_data0:\n",
        "  #   normalized_item=[(v-last_val)/last_val for v in item0]\n",
        "  #   normalized_prev_list.append(normalized_item)\n",
        "  for item0 in next_2d_data0:\n",
        "    normalized_item=[(v-last_val)/last_val for v in item0]\n",
        "    normalized_next_list.append(normalized_item)\n",
        "  return normalized_prev_list,normalized_next_list\n",
        "\n",
        "def normalize_analyze_features(prev_2d_data0,params0={}): #get summary of features based on min/max/avg/std-dev/polynomial weights/F0 .. etc\n",
        "  cur_summary=params0.get(\"summary\",{})\n",
        "  last_val=prev_2d_data0[-1][-1]\n",
        "  normalized_prev_list_items=[]\n",
        "  final_list=[]\n",
        "  for item0 in prev_2d_data0:\n",
        "    normalized_item=[(v-last_val)/last_val for v in item0]\n",
        "    normalized_prev_list_items.append(normalized_item)\n",
        "  if cur_summary=={} or cur_summary==None: return normalized_prev_list_items\n",
        "  normalized_prev_list_items_transpose=transpose(normalized_prev_list_items)\n",
        "  for ti in normalized_prev_list_items_transpose:\n",
        "    cur_tmp_list=[]\n",
        "    mean_val=sum(ti)/len(ti)\n",
        "    max_val,min_val=max(ti),min(ti)\n",
        "    if cur_summary.get(\"mean\",False): cur_tmp_list.append(mean_val)\n",
        "    if cur_summary.get(\"max\",False): cur_tmp_list.append(max_val)\n",
        "    if cur_summary.get(\"min\",False): cur_tmp_list.append(min_val)\n",
        "    poly_deg=cur_summary.get(\"poly\",0)\n",
        "    if poly_deg>0: cur_tmp_list.extend(get_poly(ti,poly_deg))\n",
        "    if cur_summary.get(\"f0\",False): cur_tmp_list.append(get_f0(ti))\n",
        "    final_list.append(cur_tmp_list)\n",
        "  final_list_transpose=transpose(final_list)\n",
        "  if cur_summary.get(\"last-item-norm\",False): final_list_transpose.append(normalized_prev_list_items[-1])\n",
        "  if cur_summary.get(\"last-item-raw\",False): final_list_transpose.append(prev_2d_data0[-1])\n",
        "  return final_list_transpose\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_csv_data_new(csv_fpath0,params0={}):\n",
        "  columns0 =params0.get(\"columns\",[\"High\",\"Low\",\"Open\",\"Close\"])  \n",
        "  pd_df=pd.read_csv(csv_fpath0)\n",
        "  data_2d = get_pd_col_data(pd_df,columns0) #pd_df.filter(columns)\n",
        "  return data_2d\n",
        "\n",
        "def get_data_full_new(csv_fpath0,params0={}):\n",
        "  prev_n0=params0.get(\"prev_n\",30)\n",
        "  next_n0=params0.get(\"next_n\",5)\n",
        "  cur_csv_data0=get_csv_data_new(csv_fpath0,params0)\n",
        "  slice_size0=prev_n0+next_n0\n",
        "  all_data_items=[]\n",
        "  for i0 in range(len(cur_csv_data0)-slice_size0):\n",
        "    prev_slice=cur_csv_data0[i0:i0+prev_n0]\n",
        "    next_slice=cur_csv_data0[i0+prev_n0:i0+prev_n0+next_n0]\n",
        "    normalized_prev0,normalized_next0=normalize_ft_labels_new(prev_slice,next_slice,params0)\n",
        "    all_data_items.append((normalized_prev0,normalized_next0))\n",
        "  return all_data_items\n",
        "\n",
        "def get_data_full_from_list_new(cur_csv_data0,params0={}):\n",
        "  prev_n0=params0.get(\"prev_n\",30)\n",
        "  next_n0=params0.get(\"next_n\",5)\n",
        "  #cur_csv_data0=get_csv_data_new(csv_fpath0,params0)\n",
        "  slice_size0=prev_n0+next_n0\n",
        "  all_data_items=[]\n",
        "  for i0 in range(len(cur_csv_data0)-slice_size0):\n",
        "    prev_slice=cur_csv_data0[i0:i0+prev_n0]\n",
        "    next_slice=cur_csv_data0[i0+prev_n0:i0+prev_n0+next_n0]\n",
        "    normalized_prev0,normalized_next0=normalize_ft_labels_new(prev_slice,next_slice,params0)\n",
        "    all_data_items.append((normalized_prev0,normalized_next0))\n",
        "  return all_data_items\n",
        "\n",
        "\n",
        "def get_label_spec_dict(label0): # e.g. \"day:0;low:<0\"\n",
        "  tmp_dict0={}\n",
        "  lb_split=label0.split(\";\")\n",
        "  for sp0 in lb_split:\n",
        "    colon_split=sp0.split(\":\")\n",
        "    tmp_dict0[colon_split[0]]=colon_split[1]\n",
        "  return tmp_dict0\n",
        "\n",
        "def eval_pred2(rnn_out0,actual_outcome0):\n",
        "  eval_list=[]\n",
        "  rnn_out_flat=rnn_out0.ravel()\n",
        "  for ac0,pred0 in zip(actual_outcome0,rnn_out_flat):\n",
        "    cur_pred0=pred0.item()\n",
        "    if ac0==1: dist0=1-cur_pred0\n",
        "    else: dist0=cur_pred0\n",
        "    eval_list.append(dist0)\n",
        "    #print(\"ac0,pred0 >>>\",ac0,cur_pred0, \"dist0\",dist0)\n",
        "  return sum(eval_list)/len(eval_list)\n",
        "\n",
        "#===================================\n",
        "\n",
        "\n",
        "def normalize_ft_vals(prev_2d_data,flatten=False):\n",
        "  last_val=prev_2d_data[-1][-1]\n",
        "  #print(\"last_val\", last_val)\n",
        "  final_list=[]\n",
        "  for item0 in prev_2d_data:\n",
        "    normalized_item=[(v-last_val)/last_val for v in item0]\n",
        "    if flatten: final_list.extend(normalized_item)\n",
        "    else: final_list.append(normalized_item)\n",
        "    \n",
        "  return final_list\n",
        "\n",
        "def percent_bin_vals(ratio0,max_val0=5):\n",
        "  percent=ratio0*100\n",
        "  if abs(percent)<=max_val0: return str(int(percent))\n",
        "  elif percent<-max_val0: return str(-max_val0-1)\n",
        "  else: return str(max_val0+1)\n",
        "\n",
        "\n",
        "def one_hot_labels(label_list0,standard_labels0):\n",
        "  oh_list=[]\n",
        "  for lb0 in label_list0: \n",
        "    tmp_oh=[1. if v==lb0 else 0. for v in standard_labels0]\n",
        "    oh_list.extend(tmp_oh)\n",
        "  return oh_list\n",
        "\n",
        "def normalize_next_labels(next_2d_data0,last_closing_val0,max_percent_val0=5):\n",
        "  normalized_high_low_labels=[]\n",
        "  for item0 in next_2d_data0:\n",
        "    high0,low0,open0,close0=item0\n",
        "    high_ratio0=(high0-last_closing_val0)/last_closing_val0\n",
        "    low_ratio0=(low0-last_closing_val0)/last_closing_val0\n",
        "    high_label0=percent_bin_vals(high_ratio0,max_val0=max_percent_val0)\n",
        "    low_label0=percent_bin_vals(low_ratio0,max_val0=max_percent_val0)\n",
        "    normalized_high_low_labels.extend((high_label0,low_label0))\n",
        "  return normalized_high_low_labels\n",
        "\n",
        "def prep_ft_labels(data0,params0={}):\n",
        "  #prev_n0=20,next_n0=10,max_percent0=5\n",
        "  prev_n0=params0.get(\"prev_n\",30)\n",
        "  next_n0=params0.get(\"next_n\",10)\n",
        "  max_percent0=params0.get(\"max_percent\",5)\n",
        "  flatten0=params0.get(\"flatten\",True)\n",
        "\n",
        "  slice_size0=prev_n0+next_n0\n",
        "  all_data_items=[]\n",
        "  for i0 in range(len(data0)-slice_size0):\n",
        "    prev_slice=data0[i0:i0+prev_n0]\n",
        "    next_slice=data0[i0+prev_n0:i0+prev_n0+next_n0]\n",
        "    ref_val=prev_slice[-1][-1]\n",
        "    cur_ft_list=normalize_ft_vals(prev_slice,flatten0)\n",
        "    normalized_next_list=normalize_next_labels(next_slice,ref_val,max_percent_val0=max_percent0)\n",
        "    all_data_items.append((cur_ft_list,normalized_next_list,ref_val))\n",
        "  return all_data_items\n",
        "\n",
        "\n",
        "def get_data_full(csv_fpath0,params0={}):\n",
        "  columns0 =params0.get(\"columns\",[\"High\",\"Low\",\"Open\",\"Close\"])  \n",
        "  prev_n0=params0.get(\"prev_n\",30)\n",
        "  next_n0=params0.get(\"next_n\",10)\n",
        "  max_percent0=params0.get(\"max_percent\",5)\n",
        "  standard_labels0=params0.get(\"standard_labels\",gen_labels(max_percent0))\n",
        "\n",
        "  cur_data0=[]\n",
        "  fname0=os.path.split(csv_fpath0)[-1]\n",
        "  standard_labels0=gen_labels(max_percent0)\n",
        "  pd_df=pd.read_csv(csv_fpath0)\n",
        "  data_2d = get_pd_col_data(pd_df,columns0) #pd_df.filter(columns)\n",
        "  tmp_data_items=prep_ft_labels(data_2d,params0)\n",
        "  for cur_ft_vals0,cur_labels0,ref0 in tmp_data_items:\n",
        "    oh_labels0=one_hot_labels(cur_labels0,standard_labels0)\n",
        "    cur_data0.append((cur_ft_vals0,oh_labels0,cur_labels0,ref0,fname0))\n",
        "  return cur_data0\n",
        "\n",
        "def eval_pred(rnn_out0,actual_outcome0,standard_labels0,gain_threshold0=5):\n",
        "  gain_loss=0\n",
        "  rnn_out_flat=rnn_out0.ravel()\n",
        "  preds=out2labels(rnn_out_flat,standard_labels0)\n",
        "  eval_list=[]\n",
        "  for ac0,pred0 in zip(actual_outcome0,preds):\n",
        "    cur_pred=[(v[0],round(v[1].item(),4)) for v in pred0]\n",
        "    eval_list.append((ac0,cur_pred[0]))\n",
        "  for i0 in range(0,len(eval_list),2):\n",
        "    cur_high=eval_list[i0]\n",
        "    ac_high,pred_high_wt=cur_high\n",
        "    pred_high,pred_wt=pred_high_wt\n",
        "    if i0>0 and int(pred_high)>=gain_threshold0: \n",
        "      #print(i0/2, \"Sell Decision - predicted: %s - actual: %s\"%(pred_high,ac_high))\n",
        "      gain_loss=int(ac_high)\n",
        "      break\n",
        "  return gain_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyVdax4bIdEd"
      },
      "source": [
        "#Loading Data - Aug 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51DlutEVHCS7",
        "outputId": "1c25b8d1-0f5e-4302-9610-19a6c65488d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_files 58 ['ADM', 'ANET', 'AEP', 'ALGN', 'ALTR', 'AON', 'AIZ', 'BIO', 'AME', 'ATVI', 'AMAT', 'GOOG', 'AMZN', 'AMD', 'AMP', 'APD', 'AAPL', 'AMT', 'PG', 'AAL', 'AOS', 'BK', 'AKAM', 'ADSK', 'CAH', 'APA', 'AJG', 'ARE', 'AXP', 'ALK', 'GM', 'ADI', 'AMGN', 'ACN', 'AEE', 'AWK', 'ANTM', 'BXP', 'AVB', 'ABC', 'CDNS', 'FB', 'ALB', 'ABT', 'EA', 'AAP', 'AVY', 'APH', 'MSFT', 'ABBV', 'ALLE', 'AZO', 'CDE', 'A', 'IBM', 'ABMD', 'ADP', 'UPS']\n",
            "ADM.csv\n",
            "ANET.csv\n",
            "AEP.csv\n",
            "ALGN.csv\n",
            "ALTR.csv\n",
            "AON.csv\n",
            "AIZ.csv\n",
            "BIO.csv\n",
            "AME.csv\n",
            "ATVI.csv\n",
            "AMAT.csv\n",
            "GOOG.csv\n",
            "AMZN.csv\n",
            "AMD.csv\n",
            "AMP.csv\n",
            "APD.csv\n",
            "AAPL.csv\n",
            "AMT.csv\n",
            "PG.csv\n",
            "AAL.csv\n",
            "AOS.csv\n",
            "BK.csv\n",
            "AKAM.csv\n",
            "ADSK.csv\n",
            "CAH.csv\n",
            "APA.csv\n",
            "AJG.csv\n",
            "ARE.csv\n",
            "AXP.csv\n",
            "ALK.csv\n",
            "GM.csv\n",
            "ADI.csv\n",
            "AMGN.csv\n",
            "ACN.csv\n",
            "AEE.csv\n",
            "AWK.csv\n",
            "ANTM.csv\n",
            "BXP.csv\n",
            "AVB.csv\n",
            "ABC.csv\n",
            "CDNS.csv\n",
            "FB.csv\n",
            "ALB.csv\n",
            "ABT.csv\n",
            "EA.csv\n",
            "AAP.csv\n",
            "AVY.csv\n",
            "APH.csv\n",
            "MSFT.csv\n",
            "ABBV.csv\n",
            "ALLE.csv\n",
            "AZO.csv\n",
            "CDE.csv\n",
            "A.csv\n",
            "IBM.csv\n",
            "ABMD.csv\n",
            "ADP.csv\n",
            "UPS.csv\n",
            "448973\n",
            "train_data 359178\n",
            "test_data 89795\n",
            "[-0.03532456252957017, -0.06752805784967221, -0.049088585560913035]\n",
            "[0.02996251952621438, 0.008988755857864374, 0.020412060187353077]\n",
            "[-0.1357677603632792, -0.1822096656289118, -0.17977522431170953]\n",
            "[0.06666666666666667, 0.06666666666666667, 0.06666666666666667]\n",
            "[0.016853997599312324, -0.00842694522244508, 0.0]\n",
            "[18.100000381469727, 17.649999618530273, 17.799999237060547]\n",
            "[0.004119873223460046, -0.020599258962878064, -0.0007490094109442812]\n",
            "[0.008988755857864374, -0.01498125976310729, 0.0009364224953136013]\n",
            "[0.015917575103998525, -0.007865134587025785, 0.009363367717758682]\n",
            "[0.005617999199770775, -0.03183515020799745, -0.014044944422215854]\n",
            "[-0.01647938573941802, -0.04007489665491754, -0.032771465548888885]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "seed(0)\n",
        "cur_prev_n,cur_next_n=30,10\n",
        "cur_max_percent_val=5\n",
        "\n",
        "n_files=50\n",
        "\n",
        "\n",
        "def get_specs_from_next_data(normalized_next0,params0={}):\n",
        "  pred_labels0=params0.get(\"pred_labels\",[])\n",
        "  cols0=params0.get(\"columns\",[])\n",
        "\n",
        "  label_val_dict0={}\n",
        "  label_spec_dict0={}\n",
        "  for a in pred_labels0:\n",
        "    label_val_dict0[a]=0.\n",
        "    spec0=get_label_spec_dict(a)\n",
        "    label_spec_dict0[a]=spec0\n",
        "  for day0,item0 in enumerate(normalized_next0):\n",
        "    #high0,low0,open0,close0=item0\n",
        "    local_dict={}\n",
        "    for col_i,col_name0 in enumerate(cols0):\n",
        "      col_name_lower0=col_name0.lower()\n",
        "      cur_val0=item0[col_i]\n",
        "      local_dict[col_name_lower0]=cur_val0\n",
        "    # print(item0)\n",
        "    # print(local_dict)\n",
        "    #   local_dict[\"low\"]=low0\n",
        "    #   local_dict[\"open\"]=open0\n",
        "    #   local_dict[\"close\"]=close0\n",
        "\n",
        "    # local_dict[\"high\"]=high0\n",
        "    # local_dict[\"low\"]=low0\n",
        "    # local_dict[\"open\"]=open0\n",
        "    # local_dict[\"close\"]=close0\n",
        "    for lb0 in pred_labels0:\n",
        "      corr_tmp_dict=label_spec_dict0[lb0]\n",
        "      if not str(day0) in corr_tmp_dict.get(\"day\",\"\"): continue\n",
        "      for ld_key,ld_val in local_dict.items():\n",
        "        corr_rule=corr_tmp_dict.get(ld_key)\n",
        "        if corr_rule==None: continue\n",
        "        comparator=corr_rule[0]\n",
        "        compared_to=int(corr_rule[1:])\n",
        "        ld_val_percent=100*ld_val\n",
        "        outcome=0.\n",
        "        if comparator==\">\" and ld_val_percent>=compared_to: outcome=1.\n",
        "        if comparator==\"<\" and ld_val_percent<=compared_to: outcome=1.\n",
        "        if comparator==\"=\" and int(ld_val_percent)==compared_to: outcome=1.\n",
        "        if outcome==1: label_val_dict0[lb0]=1.\n",
        "  return [label_val_dict0.get(v,0.) for v in pred_labels0] #label_val_dict0\n",
        "\n",
        "cur_params={}\n",
        "cur_params[\"prev_n\"]=30#15#30\n",
        "cur_params[\"next_n\"]=5#10\n",
        "cur_params[\"max_percent\"]=5\n",
        "#cur_params[\"columns\"]=[\"High\",\"Low\",\"Open\",\"Close\"]\n",
        "cur_params[\"columns\"]=[\"High\",\"Low\",\"Close\"]\n",
        "cur_params[\"gain_threshold\"]=5\n",
        "cur_params[\"flatten\"]=False\n",
        "cur_params[\"standard_labels\"]=gen_labels(cur_params[\"max_percent\"])\n",
        "#cur_params[\"pred_labels\"]=[\"day:0;low:<0\",\"day:0;low:<-2\",\"day:0;low:<2\",\"day:1234;high:>0\",\"day:1234;high:>2\",\"day:1234;high:>5\",\"day:4;close:<0\"]\n",
        "#cur_params[\"pred_labels\"]=[\"day:0;low:<0\",\"day:1234;high:>3\",\"day:4;close:<0\"]\n",
        "cur_params[\"pred_labels\"]=[\"day:1234;high:>3\"]\n",
        "cur_params[\"summary\"]={}\n",
        "cur_params[\"summary\"][\"mean\"]=True\n",
        "cur_params[\"summary\"][\"max\"]=True\n",
        "cur_params[\"summary\"][\"min\"]=True\n",
        "cur_params[\"summary\"][\"std-dev\"]=False\n",
        "cur_params[\"summary\"][\"f0\"]=True\n",
        "cur_params[\"summary\"][\"last-item-raw\"]=True\n",
        "cur_params[\"summary\"][\"last-item-norm\"]=True\n",
        "#cur_params[\"summary\"][\"poly\"]=2\n",
        "\n",
        "\n",
        "\n",
        "#columns = [\"High\",\"Low\",\"Open\",\"Close\"]\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "files=['ABC', 'ADP', 'A', 'ABT', 'ABMD', 'ADI', 'ABBV', 'AAPL', 'ADSK', 'ADM', 'ACN', 'AAP', 'AAL', 'ALGN', 'APH', 'AOS', 'AWK', 'ALLE', 'AME', 'APD', 'ARE', 'AIZ', 'ALB', 'APA', 'ALK', 'AEE', 'AMGN', 'ANTM', 'AEP', 'AON', 'AKAM', 'AXP', 'AMD', 'AMAT', 'AMP', 'ANET', 'AJG', 'AZO', 'ATVI', 'AMZN', 'AMT', 'AVB', 'ALTR', 'AVY', 'CAH', 'CDNS', 'BIO', 'CDE', 'BXP', 'BK', 'BEN', 'C', 'BMRA', 'BAX', 'BLK', 'BF-A', 'BDX', 'BR', 'BSHI', 'CB', 'CAG', 'BIIB', 'BAC', 'BMY', 'CCI', 'BSX', 'CAT', 'BRK-A', 'BBY', 'BA', 'BWA', 'CME', 'CNWT', 'CF', 'CTXS', 'D', 'CTSH', 'CHD', 'CFG', 'DFS', 'CPICQ', 'DG', 'CRM', 'CHRW', 'CLX', 'DGX', 'CPB', 'COTY', 'CHTR', 'COP', 'CNC', 'CNP', 'DE', 'COO', 'CUK', 'CPRT', 'COST', 'CINF', 'CMG', 'CL', 'CTQ', 'CTAS', 'CMI', 'CSCO', 'COWN', 'DAL', 'DTE', 'ENS', 'EQIX', 'DRE', 'DOV', 'DHI', 'EW', 'ES', 'EQR', 'DIS', 'DPZ', 'FANG', 'EXR', 'EMR', 'DLTR', 'EMN', 'FAST', 'DVA', 'EBAY', 'EA', 'DRI', 'EOG', 'EL', 'ESS', 'EIX', 'DXCM', 'EFX', 'F', 'ECL', 'ED', 'GS-PJ', 'GILD', 'GIS', 'FMBM', 'FPLPF', 'GM', 'FBHS', 'HBAN', 'FLS', 'FIS', 'FE', 'FRT', 'FRMC', 'FFIV', 'GWW', 'GRMN', 'GGG', 'FN', 'GOOG', 'GPC', 'FLT', 'FITB', 'FCX', 'FISV', 'GPN', 'FMC', 'FRC', 'HAL', 'FDX', 'FCGN', 'FB', 'GE', 'FTI', 'GD', 'HAS', 'HD', 'INTU', 'IFF', 'IRM', 'ICE', 'HLT', 'IDXX', 'HII', 'ILMN', 'HTLF', 'HPQ', 'HON', 'IBM', 'IPGP', 'HCA', 'HRL', 'IR', 'HSY', 'HOLX', 'ISRG', 'HPE', 'HRB', 'HSIC', 'INTH', 'HFC', 'HBI', 'HUM', 'IP', 'HST', 'IEX', 'HES', 'KSU', 'LNT', 'KRA', 'KHC', 'KR', 'KMB', 'JKHY', 'JNJ', 'IT', 'LEG', 'ITW', 'KSS', 'KEY', 'JNPR', 'LKQ', 'KIM', 'IVZ', 'KO', 'LNC', 'JBHT', 'LDOS', 'KMX', 'LMT', 'K', 'JPM', 'KGNR', 'KACPF', 'JCI', 'LH', 'KEYS', 'LBTYA', 'MSFT', 'MKTX', 'LYB', 'MCO', 'MRO', 'MDLZ', 'MLM', 'LVS', 'LRCX', 'MSCI', 'MOS', 'MRK', 'MET', 'MGM', 'MNST', 'MMC', 'MO', 'MCHP', 'LYV', 'MHK', 'MDT', 'LUV', 'MCK', 'MS-PF', 'MMM', 'MAA', 'MCD', 'MPC', 'MAR', 'LOW', 'MRCR', 'NOV', 'PEG', 'NVRO', 'NTRR', 'MU', 'NTRA', 'PAYX', 'NSC', 'NRG', 'ODFL', 'NTAP', 'PBCT', 'NFLX', 'ORLY', 'OMC', 'NTRS', 'NCTKF', 'NOXL', 'OKE', 'NI', 'NVR', 'NOC', 'O', 'NOW', 'PCAR', 'NEE', 'NLSN', 'NWL', 'MSI', 'NDAQ', 'NMHLY', 'OXY', 'NOK', 'NEOG', 'NCLH', 'RF', 'PSX', 'RE', 'PPG', 'ROK', 'PXD', 'RIBT', 'RCL', 'REGN', 'RMD', 'PKI', 'RL', 'RJF', 'PG', 'QRVO', 'REG', 'PHM', 'PNWRF', 'PKG', 'PNW', 'PLD', 'PVH', 'PM', 'PNR', 'PWR', 'PH', 'RLI', 'PEP', 'PRU', 'PFE', 'RHI', 'ROST', 'TAP', 'TEL', 'SRG', 'SLG', 'RSG', 'SYK', 'SNPS', 'SCHW', 'SHW', 'RXMD', 'SEGXF', 'SWKS', 'SBUX', 'RSNHF', 'SWK', 'SONC', 'ROP', 'STZ-B', 'TCYSF', 'STT', 'SPG', 'SYF', 'T', 'STX', 'SIVB', 'SO', 'ROL', 'TJX', 'SEE', 'SLB', 'SRE', 'VZ', 'UNP', 'TMUS', 'TRAUF', 'V', 'TW', 'VRSK', 'TWTR', 'URI', 'ULTA', 'UPS', 'UDR', 'TSN', 'UAL', 'TSCO', 'TTWO', 'VRSN', 'UA', 'TMO', 'WBA', 'TXN', 'UNM', 'USB', 'TXT', 'VMC', 'WAT', 'UHS', 'UEEC', 'VTR', 'TYL', 'TROW', 'TRV', 'VFC', 'WYNN', 'WSPOF', 'WU', 'YUM', 'XYL', 'WST', 'WRK', 'WEC', 'WM', 'ZTS', 'ZBH', 'XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "sample_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "sample_files=list(set(sample_files+files[:n_files]))\n",
        "print(\"sample_files\",len(sample_files), sample_files)\n",
        "\n",
        "all_data=[]\n",
        "for fname in sample_files:\n",
        "  cur_fname= fname+\".csv\"\n",
        "  print(cur_fname)\n",
        "  cur_fpath=os.path.join(root_dir,cur_fname)\n",
        "  #tmp_data=get_data_full(cur_fpath,cur_params)\n",
        "  tmp_data=get_data_full_new(cur_fpath,cur_params)\n",
        "  all_data.extend(tmp_data)\n",
        "\n",
        "shuffle(all_data)\n",
        "print(len(all_data))\n",
        "train_data,test_data=split_train_test(all_data)\n",
        "print(\"train_data\",len(train_data))\n",
        "print(\"test_data\",len(test_data))\n",
        "# for ad in all_data[:5]:\n",
        "#   for a0 in ad:\n",
        "#     print(a0)\n",
        "#   print(\"----\")\n",
        "\n",
        "item0=train_data[0]\n",
        "train0,test0=item0\n",
        "for tr in train0:\n",
        "  print(tr)\n",
        "\n",
        "for ts in test0:\n",
        "  print(ts)  \n",
        "# import numpy as np\n",
        "# test=all_data[0][0]\n",
        "# test_array=np.array(test)\n",
        "#print(test_array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngq9XbPfkDCW"
      },
      "source": [
        "#New Training - Aug 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy2QH1DNiwON",
        "outputId": "6dabd720-76eb-4c5d-9321-10d224c51162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ft_list [[-0.03532456252957017, -0.06752805784967221, -0.049088585560913035], [0.02996251952621438, 0.008988755857864374, 0.020412060187353077], [-0.1357677603632792, -0.1822096656289118, -0.17977522431170953], [0.06666666666666667, 0.06666666666666667, 0.06666666666666667], [0.016853997599312324, -0.00842694522244508, 0.0], [18.100000381469727, 17.649999618530273, 17.799999237060547]]\n",
            "lb_list [[0.004119873223460046, -0.020599258962878064, -0.0007490094109442812], [0.008988755857864374, -0.01498125976310729, 0.0009364224953136013], [0.015917575103998525, -0.007865134587025785, 0.009363367717758682], [0.005617999199770775, -0.03183515020799745, -0.014044944422215854], [-0.01647938573941802, -0.04007489665491754, -0.032771465548888885]]\n",
            "extracted_out_specs [0.0]\n",
            "ft_list_tensor torch.Size([6, 3])\n",
            "lb_list_tensor torch.Size([1])\n",
            "torch.Size([6, 3])\n",
            "torch.Size([1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1709\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5783\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5777\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4018\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5553\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4384\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4871\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3801\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1678\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5968\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3595\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.436\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4919\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6277\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4452\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6624\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4414\n",
            "total_diff 189.463\n",
            "Epoch: 12 - batch: 3 out of 35 - train_loss_avg: 0.22 -  test_loss_avg: 0.2228 - train_eval_avg: 0.4386 - test_eval_avg: 0.4473 - elpased: 42.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-3.model\n",
            "==========\n",
            "batch_i0 4 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2479\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4743\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3629\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5091\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2409\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4198\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3217\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4963\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.603\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1866\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.295\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3015\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.707\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1267\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1467\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5003\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2016\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.588\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4416\n",
            "total_diff 231.8205\n",
            "Epoch: 12 - batch: 4 out of 35 - train_loss_avg: 0.2174 -  test_loss_avg: 0.2187 - train_eval_avg: 0.4349 - test_eval_avg: 0.4386 - elpased: 42.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-4.model\n",
            "==========\n",
            "batch_i0 5 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4735\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3363\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2831\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3446\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6311\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2001\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2753\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1828\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3871\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4053\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4132\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1281\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2213\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2371\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4522\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1697\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1665\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3632\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5601\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4448\n",
            "total_diff 266.9188\n",
            "Epoch: 12 - batch: 5 out of 35 - train_loss_avg: 0.216 -  test_loss_avg: 0.2173 - train_eval_avg: 0.4327 - test_eval_avg: 0.4289 - elpased: 42.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-5.model\n",
            "==========\n",
            "batch_i0 6 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6701\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2123\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5635\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6698\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2225\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5418\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1489\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.351\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6106\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2481\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3148\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3304\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6698\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5294\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5976\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6457\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4736\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3947\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3572\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4836\n",
            "total_diff 248.7126\n",
            "Epoch: 12 - batch: 6 out of 35 - train_loss_avg: 0.2155 -  test_loss_avg: 0.2189 - train_eval_avg: 0.4326 - test_eval_avg: 0.4375 - elpased: 43.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-6.model\n",
            "==========\n",
            "batch_i0 7 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5149\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4951\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3148\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2571\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.588\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3319\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2884\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4995\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4521\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2212\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2885\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2328\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2895\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3214\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4591\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.576\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.627\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5927\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2352\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.366\n",
            "total_diff 257.0942\n",
            "Epoch: 12 - batch: 7 out of 35 - train_loss_avg: 0.218 -  test_loss_avg: 0.2157 - train_eval_avg: 0.4357 - test_eval_avg: 0.4314 - elpased: 42.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-7.model\n",
            "==========\n",
            "batch_i0 8 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2349\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5803\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5912\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3786\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2347\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4609\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.546\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2349\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3584\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1628\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1728\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5357\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3196\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.624\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5933\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3282\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4463\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5845\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1756\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4776\n",
            "total_diff 234.1833\n",
            "Epoch: 12 - batch: 8 out of 35 - train_loss_avg: 0.2186 -  test_loss_avg: 0.2176 - train_eval_avg: 0.4369 - test_eval_avg: 0.4365 - elpased: 43.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-8.model\n",
            "==========\n",
            "batch_i0 9 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2883\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1902\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3468\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3833\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3003\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3914\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3463\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6914\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6289\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3767\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.509\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4954\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2568\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5347\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4352\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3494\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.475\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2777\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5871\n",
            "total_diff 264.2024\n",
            "Epoch: 12 - batch: 9 out of 35 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2165 - train_eval_avg: 0.4375 - test_eval_avg: 0.4313 - elpased: 43.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-9.model\n",
            "==========\n",
            "batch_i0 10 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4002\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4708\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.364\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4159\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5293\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5259\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2699\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4811\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1726\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5925\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3845\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2556\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6331\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2224\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2865\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4705\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1681\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2786\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3706\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4601\n",
            "total_diff 237.5522\n",
            "Epoch: 12 - batch: 10 out of 35 - train_loss_avg: 0.2201 -  test_loss_avg: 0.219 - train_eval_avg: 0.4396 - test_eval_avg: 0.4346 - elpased: 43.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-10.model\n",
            "==========\n",
            "batch_i0 11 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2593\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5615\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2972\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4174\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6679\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6395\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2136\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3378\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5159\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2069\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6729\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2697\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4054\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2662\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2849\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2102\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7318\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2111\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5209\n",
            "total_diff 230.2683\n",
            "Epoch: 12 - batch: 11 out of 35 - train_loss_avg: 0.2174 -  test_loss_avg: 0.2185 - train_eval_avg: 0.4364 - test_eval_avg: 0.4361 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-11.model\n",
            "==========\n",
            "batch_i0 12 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2828\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6093\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.55\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6001\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4137\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5753\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5928\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4706\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2497\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2583\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3572\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4622\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2414\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7478\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4525\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3883\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4316\n",
            "total_diff 246.1156\n",
            "Epoch: 12 - batch: 12 out of 35 - train_loss_avg: 0.216 -  test_loss_avg: 0.2168 - train_eval_avg: 0.434 - test_eval_avg: 0.437 - elpased: 43.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-12.model\n",
            "==========\n",
            "batch_i0 13 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6512\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3676\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5935\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.203\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4117\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2613\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3868\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.521\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5867\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2922\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7206\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4809\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4502\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2573\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5206\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2624\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6452\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2999\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4628\n",
            "total_diff 273.3348\n",
            "Epoch: 12 - batch: 13 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2176 - train_eval_avg: 0.4325 - test_eval_avg: 0.4321 - elpased: 44.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-13.model\n",
            "==========\n",
            "batch_i0 14 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6318\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.491\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2686\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3949\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4225\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2528\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5516\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3277\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3896\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4685\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4725\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.773\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.325\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2684\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2258\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7207\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6254\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4942\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6329\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3177\n",
            "total_diff 240.7855\n",
            "Epoch: 12 - batch: 14 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2182 - train_eval_avg: 0.4317 - test_eval_avg: 0.4364 - elpased: 42.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-14.model\n",
            "==========\n",
            "batch_i0 15 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3965\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2364\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6167\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.686\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.424\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5377\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6439\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6237\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3589\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6934\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2595\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5406\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4123\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2487\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4239\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3805\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3995\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2297\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6371\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2499\n",
            "total_diff 216.2769\n",
            "Epoch: 12 - batch: 15 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.2232 - train_eval_avg: 0.4373 - test_eval_avg: 0.4369 - elpased: 43.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-15.model\n",
            "==========\n",
            "batch_i0 16 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4206\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3489\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2968\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.239\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2731\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2734\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3434\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2536\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3119\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2022\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3948\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3157\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5653\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3223\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3187\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3289\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2846\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2025\n",
            "total_diff 223.2896\n",
            "Epoch: 12 - batch: 16 out of 35 - train_loss_avg: 0.2189 -  test_loss_avg: 0.2178 - train_eval_avg: 0.4415 - test_eval_avg: 0.4373 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-16.model\n",
            "==========\n",
            "batch_i0 17 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6611\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3143\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2404\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6419\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.296\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1747\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2887\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2318\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.287\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4241\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4363\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.362\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6155\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3912\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4602\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3117\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2347\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2051\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6013\n",
            "total_diff 227.9488\n",
            "Epoch: 12 - batch: 17 out of 35 - train_loss_avg: 0.2193 -  test_loss_avg: 0.22 - train_eval_avg: 0.4383 - test_eval_avg: 0.4421 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-17.model\n",
            "==========\n",
            "batch_i0 18 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6367\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2418\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6612\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2289\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4154\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2776\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3286\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6287\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.522\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2433\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6663\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3235\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6141\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5863\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1428\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3237\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5432\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5729\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1814\n",
            "total_diff 203.175\n",
            "Epoch: 12 - batch: 18 out of 35 - train_loss_avg: 0.2199 -  test_loss_avg: 0.224 - train_eval_avg: 0.4396 - test_eval_avg: 0.4439 - elpased: 41.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-18.model\n",
            "==========\n",
            "batch_i0 19 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5806\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6021\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3809\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5206\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.354\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3082\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1778\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7362\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4139\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4582\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3807\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4509\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4132\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5389\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4101\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4535\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.183\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5064\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5646\n",
            "total_diff 275.4334\n",
            "Epoch: 12 - batch: 19 out of 35 - train_loss_avg: 0.2187 -  test_loss_avg: 0.2158 - train_eval_avg: 0.4345 - test_eval_avg: 0.4323 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-19.model\n",
            "==========\n",
            "batch_i0 20 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5872\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3547\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4766\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.285\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6134\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.16\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.185\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2595\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3639\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2182\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.231\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4437\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5059\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1914\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3355\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4318\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4198\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7446\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.507\n",
            "total_diff 227.6698\n",
            "Epoch: 12 - batch: 20 out of 35 - train_loss_avg: 0.2204 -  test_loss_avg: 0.2211 - train_eval_avg: 0.4384 - test_eval_avg: 0.4408 - elpased: 42.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-20.model\n",
            "==========\n",
            "batch_i0 21 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6455\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2083\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4024\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2417\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4921\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2772\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3392\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2997\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3935\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7056\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2758\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4687\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2251\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2214\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6158\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4547\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2218\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2198\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6361\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1574\n",
            "total_diff 232.2563\n",
            "Epoch: 12 - batch: 21 out of 35 - train_loss_avg: 0.2171 -  test_loss_avg: 0.2202 - train_eval_avg: 0.4369 - test_eval_avg: 0.4422 - elpased: 41.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-21.model\n",
            "==========\n",
            "batch_i0 22 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3554\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3208\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.239\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7292\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2357\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3902\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3494\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.488\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5247\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.633\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3933\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1912\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2071\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1987\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4274\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7772\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.286\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2171\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3339\n",
            "total_diff 241.4001\n",
            "Epoch: 12 - batch: 22 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2161 - train_eval_avg: 0.4368 - test_eval_avg: 0.434 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-22.model\n",
            "==========\n",
            "batch_i0 23 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2425\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.307\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5598\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7144\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3242\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6257\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2454\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1831\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2716\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3895\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3322\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3616\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3538\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4194\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5423\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3309\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6983\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3094\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4128\n",
            "total_diff 231.271\n",
            "Epoch: 12 - batch: 23 out of 35 - train_loss_avg: 0.2173 -  test_loss_avg: 0.2169 - train_eval_avg: 0.4342 - test_eval_avg: 0.4389 - elpased: 41.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-23.model\n",
            "==========\n",
            "batch_i0 24 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2021\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.349\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4702\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4809\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4385\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3693\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4231\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2575\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2887\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6278\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4018\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3108\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6103\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2335\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2271\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.623\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3644\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4112\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6456\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2661\n",
            "total_diff 230.7651\n",
            "Epoch: 12 - batch: 24 out of 35 - train_loss_avg: 0.2187 -  test_loss_avg: 0.2212 - train_eval_avg: 0.4399 - test_eval_avg: 0.4332 - elpased: 41.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-24.model\n",
            "==========\n",
            "batch_i0 25 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3288\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2169\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3566\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2824\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.569\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5521\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4589\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5238\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.499\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4658\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5102\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3416\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6353\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1763\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4681\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5236\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4126\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1817\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3002\n",
            "total_diff 228.1811\n",
            "Epoch: 12 - batch: 25 out of 35 - train_loss_avg: 0.2153 -  test_loss_avg: 0.2208 - train_eval_avg: 0.4322 - test_eval_avg: 0.4385 - elpased: 42.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-25.model\n",
            "==========\n",
            "batch_i0 26 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4527\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1945\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5704\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.363\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4941\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1373\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3268\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5745\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1686\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2583\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2684\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3657\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1992\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3672\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3121\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5525\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.495\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3844\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2913\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2985\n",
            "total_diff 249.4897\n",
            "Epoch: 12 - batch: 26 out of 35 - train_loss_avg: 0.2168 -  test_loss_avg: 0.2143 - train_eval_avg: 0.4323 - test_eval_avg: 0.4313 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-26.model\n",
            "==========\n",
            "batch_i0 27 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2001\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4929\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3679\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5662\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6732\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4059\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4935\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4559\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2001\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5172\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6918\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4758\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4483\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3704\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3942\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1964\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4205\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2562\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.49\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4036\n",
            "total_diff 226.8205\n",
            "Epoch: 12 - batch: 27 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2204 - train_eval_avg: 0.436 - test_eval_avg: 0.4386 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-27.model\n",
            "==========\n",
            "batch_i0 28 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5151\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1483\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4597\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3074\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2818\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3486\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4201\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.576\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2673\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5934\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1028\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.466\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1987\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2339\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3299\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.603\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5312\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3379\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4157\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4369\n",
            "total_diff 222.8932\n",
            "Epoch: 12 - batch: 28 out of 35 - train_loss_avg: 0.2212 -  test_loss_avg: 0.2181 - train_eval_avg: 0.4401 - test_eval_avg: 0.4377 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-28.model\n",
            "==========\n",
            "batch_i0 29 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4504\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5863\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2272\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3638\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4148\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3754\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4451\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3653\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3887\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3997\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5698\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5397\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2816\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3693\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3099\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6355\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.253\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4251\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3963\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3012\n",
            "total_diff 219.6884\n",
            "Epoch: 12 - batch: 29 out of 35 - train_loss_avg: 0.2189 -  test_loss_avg: 0.2196 - train_eval_avg: 0.4401 - test_eval_avg: 0.4409 - elpased: 42.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-29.model\n",
            "==========\n",
            "batch_i0 30 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5065\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2953\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.329\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5109\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3673\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3953\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2404\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1279\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4144\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5867\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1961\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5234\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4219\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1976\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3548\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3869\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4618\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3862\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4571\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1547\n",
            "total_diff 211.3165\n",
            "Epoch: 12 - batch: 30 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4399 - test_eval_avg: 0.4398 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-30.model\n",
            "==========\n",
            "batch_i0 31 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3173\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4446\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.533\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4557\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4033\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6326\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4667\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1876\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4365\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6092\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2619\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6258\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6653\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4386\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1657\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1414\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6278\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.632\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5322\n",
            "total_diff 209.2384\n",
            "Epoch: 12 - batch: 31 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.22 - train_eval_avg: 0.4362 - test_eval_avg: 0.4427 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-31.model\n",
            "==========\n",
            "batch_i0 32 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5217\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6745\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3341\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4656\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3828\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4777\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3859\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5374\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5003\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.567\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6018\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4554\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4316\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4405\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6247\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1569\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2866\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3265\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.22\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5763\n",
            "total_diff 230.3526\n",
            "Epoch: 12 - batch: 32 out of 35 - train_loss_avg: 0.2166 -  test_loss_avg: 0.2215 - train_eval_avg: 0.4365 - test_eval_avg: 0.4327 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-32.model\n",
            "==========\n",
            "batch_i0 33 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2162\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5943\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4986\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5467\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4225\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3387\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5689\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5177\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2437\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1693\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4773\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5165\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3773\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1455\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6414\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.381\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.324\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3709\n",
            "total_diff 180.632\n",
            "Epoch: 12 - batch: 33 out of 35 - train_loss_avg: 0.2183 -  test_loss_avg: 0.2213 - train_eval_avg: 0.4355 - test_eval_avg: 0.4501 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-33.model\n",
            "==========\n",
            "batch_i0 34 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4309\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3576\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4145\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3836\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2484\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4197\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6919\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5546\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4746\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4686\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3669\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.468\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5684\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1876\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5102\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6198\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5175\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4239\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2319\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5252\n",
            "total_diff 226.235\n",
            "Epoch: 12 - batch: 34 out of 35 - train_loss_avg: 0.2185 -  test_loss_avg: 0.2191 - train_eval_avg: 0.4376 - test_eval_avg: 0.4398 - elpased: 42.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-34.model\n",
            "==========\n",
            "batch_i0 35 cur_train_items 9178 cur_test_items 20\n",
            "training 0 out of: 9178\n",
            "training 5000 out of: 9178\n",
            "testing 0 out of: 20\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4598\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.0877\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3518\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3885\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5475\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4816\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5105\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4876\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3803\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4072\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1368\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3203\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4214\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4311\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4642\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2764\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2319\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3366\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4555\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1954\n",
            "total_diff 1.6781\n",
            "Epoch: 12 - batch: 35 out of 35 - train_loss_avg: 0.2208 -  test_loss_avg: 0.216 - train_eval_avg: 0.4394 - test_eval_avg: 0.4334 - elpased: 35.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-35.model\n",
            "==========\n",
            "Epoch: 12 - train_loss_avg: 0.2182 -  test_loss_avg: 0.2188 - train_eval_avg: 0.4365 - test_eval_avg: 0.4375\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4821\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4385\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3934\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4907\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6562\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.312\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5941\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2619\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.424\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3233\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5193\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5724\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4416\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3046\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1448\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5236\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6029\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4356\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4473\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4092\n",
            "total_diff 267.8399\n",
            "Epoch: 13 - batch: 0 out of 35 - train_loss_avg: 0.2174 -  test_loss_avg: 0.2119 - train_eval_avg: 0.4376 - test_eval_avg: 0.4236 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-0.model\n",
            "==========\n",
            "batch_i0 1 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.305\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5223\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6439\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6033\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5875\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4313\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.475\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.379\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6463\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5613\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4389\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4319\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1801\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5492\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2247\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2499\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6057\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3777\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.636\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3578\n",
            "total_diff 235.2328\n",
            "Epoch: 13 - batch: 1 out of 35 - train_loss_avg: 0.2193 -  test_loss_avg: 0.2189 - train_eval_avg: 0.4351 - test_eval_avg: 0.4449 - elpased: 42.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-1.model\n",
            "==========\n",
            "batch_i0 2 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5754\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2227\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6434\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3387\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4816\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3351\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4359\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6072\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5098\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4871\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4101\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4203\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4439\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3823\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4076\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5696\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4171\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4793\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4177\n",
            "total_diff 219.9351\n",
            "Epoch: 13 - batch: 2 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.219 - train_eval_avg: 0.4318 - test_eval_avg: 0.4433 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-2.model\n",
            "==========\n",
            "batch_i0 3 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5496\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4063\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1234\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1697\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5606\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5626\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4006\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5221\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4931\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3925\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1439\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5879\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3471\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4182\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.476\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6183\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.419\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.64\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4291\n",
            "total_diff 192.9456\n",
            "Epoch: 13 - batch: 3 out of 35 - train_loss_avg: 0.2192 -  test_loss_avg: 0.2223 - train_eval_avg: 0.4392 - test_eval_avg: 0.4437 - elpased: 43.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-3.model\n",
            "==========\n",
            "batch_i0 4 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2616\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4697\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3377\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1771\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5055\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.246\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.423\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3243\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4868\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.605\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1933\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2861\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3184\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7033\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1487\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1527\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4996\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2156\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.58\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4383\n",
            "total_diff 214.4136\n",
            "Epoch: 13 - batch: 4 out of 35 - train_loss_avg: 0.2167 -  test_loss_avg: 0.2189 - train_eval_avg: 0.435 - test_eval_avg: 0.4427 - elpased: 42.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-4.model\n",
            "==========\n",
            "batch_i0 5 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4474\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3509\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2957\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3226\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6204\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2426\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3043\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1984\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3581\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3883\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4126\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.141\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2356\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1528\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4515\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1916\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1519\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3507\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5758\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4463\n",
            "total_diff 260.9162\n",
            "Epoch: 13 - batch: 5 out of 35 - train_loss_avg: 0.2159 -  test_loss_avg: 0.2171 - train_eval_avg: 0.4343 - test_eval_avg: 0.43 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-5.model\n",
            "==========\n",
            "batch_i0 6 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6745\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1843\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5455\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6789\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2244\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5379\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1543\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.336\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6012\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2543\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3128\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6841\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5441\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5914\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6621\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4867\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4101\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3531\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4844\n",
            "total_diff 256.0418\n",
            "Epoch: 13 - batch: 6 out of 35 - train_loss_avg: 0.2151 -  test_loss_avg: 0.2192 - train_eval_avg: 0.4318 - test_eval_avg: 0.4366 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-6.model\n",
            "==========\n",
            "batch_i0 7 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4965\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5714\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3339\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2662\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6263\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.307\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2912\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4945\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4222\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.192\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2379\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2536\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2646\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3026\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4875\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5429\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6422\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5838\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3632\n",
            "total_diff 272.1493\n",
            "Epoch: 13 - batch: 7 out of 35 - train_loss_avg: 0.2182 -  test_loss_avg: 0.216 - train_eval_avg: 0.4337 - test_eval_avg: 0.4278 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-7.model\n",
            "==========\n",
            "batch_i0 8 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2298\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5859\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5743\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3713\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2272\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4452\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5195\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2384\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3488\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1653\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1585\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5184\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3386\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6147\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6074\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3956\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4151\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6033\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1803\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4525\n",
            "total_diff 239.198\n",
            "Epoch: 13 - batch: 8 out of 35 - train_loss_avg: 0.2184 -  test_loss_avg: 0.2174 - train_eval_avg: 0.4355 - test_eval_avg: 0.435 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-8.model\n",
            "==========\n",
            "batch_i0 9 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2931\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1641\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3383\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3872\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3088\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1145\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3743\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3427\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6796\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6233\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3792\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4997\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4905\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2447\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5328\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.444\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4149\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.44\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2808\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5878\n",
            "total_diff 266.97\n",
            "Epoch: 13 - batch: 9 out of 35 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2159 - train_eval_avg: 0.4371 - test_eval_avg: 0.431 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-9.model\n",
            "==========\n",
            "batch_i0 10 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4115\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4835\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3918\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4319\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5171\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4659\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2652\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4902\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1642\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6053\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3856\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2671\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6453\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2078\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2877\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4842\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1348\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2841\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3469\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4526\n",
            "total_diff 233.523\n",
            "Epoch: 13 - batch: 10 out of 35 - train_loss_avg: 0.2197 -  test_loss_avg: 0.2195 - train_eval_avg: 0.4393 - test_eval_avg: 0.4354 - elpased: 41.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-10.model\n",
            "==========\n",
            "batch_i0 11 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2706\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5737\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2733\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4372\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6702\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.612\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2032\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3222\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5091\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2136\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6532\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2539\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4174\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2795\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2724\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.194\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7284\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2984\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1991\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4745\n",
            "total_diff 224.1834\n",
            "Epoch: 13 - batch: 11 out of 35 - train_loss_avg: 0.2173 -  test_loss_avg: 0.2187 - train_eval_avg: 0.4365 - test_eval_avg: 0.4371 - elpased: 40.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-11.model\n",
            "==========\n",
            "batch_i0 12 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3022\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1987\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5864\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5323\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5654\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4245\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5713\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.596\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4207\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6255\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2691\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2605\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.365\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4397\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4194\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2453\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7351\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4478\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3879\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4355\n",
            "total_diff 236.5417\n",
            "Epoch: 13 - batch: 12 out of 35 - train_loss_avg: 0.2158 -  test_loss_avg: 0.2164 - train_eval_avg: 0.4349 - test_eval_avg: 0.4386 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-12.model\n",
            "==========\n",
            "batch_i0 13 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6411\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3684\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5727\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5868\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4058\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2269\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.403\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5096\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5772\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2917\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7096\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4716\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.466\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2693\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5096\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2503\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6759\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3346\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4425\n",
            "total_diff 266.2438\n",
            "Epoch: 13 - batch: 13 out of 35 - train_loss_avg: 0.2176 -  test_loss_avg: 0.2173 - train_eval_avg: 0.4334 - test_eval_avg: 0.4341 - elpased: 43.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-13.model\n",
            "==========\n",
            "batch_i0 14 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6471\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4964\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2618\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4019\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4671\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2614\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5764\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3182\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4854\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4509\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7684\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3276\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3215\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2421\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7428\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6279\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5446\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.638\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3657\n",
            "total_diff 248.6181\n",
            "Epoch: 13 - batch: 14 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2184 - train_eval_avg: 0.4311 - test_eval_avg: 0.4367 - elpased: 42.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-14.model\n",
            "==========\n",
            "batch_i0 15 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4377\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2269\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6181\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7186\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4523\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.577\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6727\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6715\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3686\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7278\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2596\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.539\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3895\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2366\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.425\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4019\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4203\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2227\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6557\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2472\n",
            "total_diff 235.1883\n",
            "Epoch: 13 - batch: 15 out of 35 - train_loss_avg: 0.2185 -  test_loss_avg: 0.2227 - train_eval_avg: 0.4331 - test_eval_avg: 0.4352 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-15.model\n",
            "==========\n",
            "batch_i0 16 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4135\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3386\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2947\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5085\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2186\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2562\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2469\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3822\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2305\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3015\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1611\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3791\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3165\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5747\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3195\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3344\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3578\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2528\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1763\n",
            "total_diff 242.7269\n",
            "Epoch: 13 - batch: 16 out of 35 - train_loss_avg: 0.2186 -  test_loss_avg: 0.2176 - train_eval_avg: 0.4389 - test_eval_avg: 0.4326 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-16.model\n",
            "==========\n",
            "batch_i0 17 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6406\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3074\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2363\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6306\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3108\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1744\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2874\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2142\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2469\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4357\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4401\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3199\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6223\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3169\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3843\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4629\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3292\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2048\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5756\n",
            "total_diff 225.263\n",
            "Epoch: 13 - batch: 17 out of 35 - train_loss_avg: 0.2189 -  test_loss_avg: 0.2199 - train_eval_avg: 0.437 - test_eval_avg: 0.4419 - elpased: 42.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-17.model\n",
            "==========\n",
            "batch_i0 18 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6291\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2116\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6414\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2716\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4173\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3252\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3489\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6285\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5213\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2891\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4013\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6533\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3339\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6095\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5645\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1672\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3429\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5243\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5613\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2137\n",
            "total_diff 187.5658\n",
            "Epoch: 13 - batch: 18 out of 35 - train_loss_avg: 0.22 -  test_loss_avg: 0.2231 - train_eval_avg: 0.4419 - test_eval_avg: 0.4471 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-18.model\n",
            "==========\n",
            "batch_i0 19 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.568\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6045\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3799\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5064\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.353\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.32\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5937\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2095\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.723\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4692\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3709\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4633\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4209\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5294\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4026\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4616\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1884\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.494\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5621\n",
            "total_diff 253.8015\n",
            "Epoch: 13 - batch: 19 out of 35 - train_loss_avg: 0.2185 -  test_loss_avg: 0.216 - train_eval_avg: 0.4379 - test_eval_avg: 0.4363 - elpased: 40.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-19.model\n",
            "==========\n",
            "batch_i0 20 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2375\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5844\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3083\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4748\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2671\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6115\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1449\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2025\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2603\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3706\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2158\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2417\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4346\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4877\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2006\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3542\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4144\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4384\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.761\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4827\n",
            "total_diff 228.4911\n",
            "Epoch: 13 - batch: 20 out of 35 - train_loss_avg: 0.2197 -  test_loss_avg: 0.2208 - train_eval_avg: 0.4394 - test_eval_avg: 0.4401 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-20.model\n",
            "==========\n",
            "batch_i0 21 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6722\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2045\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.418\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2103\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4945\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2734\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3233\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2854\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3907\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7278\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2398\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4743\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2041\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1888\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6313\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.453\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2088\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2126\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6557\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.149\n",
            "total_diff 253.1323\n",
            "Epoch: 13 - batch: 21 out of 35 - train_loss_avg: 0.2169 -  test_loss_avg: 0.2196 - train_eval_avg: 0.4358 - test_eval_avg: 0.4382 - elpased: 45.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-21.model\n",
            "==========\n",
            "batch_i0 22 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3311\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3151\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2082\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7698\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2435\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3845\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4693\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5362\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6661\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4054\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1646\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.166\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1858\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4218\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.8127\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2704\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2275\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1751\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3451\n",
            "total_diff 270.6434\n",
            "Epoch: 13 - batch: 22 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2159 - train_eval_avg: 0.4332 - test_eval_avg: 0.4289 - elpased: 43.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-22.model\n",
            "==========\n",
            "batch_i0 23 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2454\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.293\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5904\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7452\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3246\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6467\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2362\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3961\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2026\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2745\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3886\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3158\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.354\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3336\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4395\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.546\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.335\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7143\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3009\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4148\n",
            "total_diff 248.6918\n",
            "Epoch: 13 - batch: 23 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2169 - train_eval_avg: 0.431 - test_eval_avg: 0.4364 - elpased: 44.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-23.model\n",
            "==========\n",
            "batch_i0 24 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2346\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3472\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4731\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4579\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4294\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3883\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4145\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2808\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3028\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.641\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3924\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3129\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6171\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2387\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1954\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6309\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3662\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4124\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6647\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2591\n",
            "total_diff 228.3203\n",
            "Epoch: 13 - batch: 24 out of 35 - train_loss_avg: 0.2187 -  test_loss_avg: 0.2213 - train_eval_avg: 0.4389 - test_eval_avg: 0.434 - elpased: 45.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-24.model\n",
            "==========\n",
            "batch_i0 25 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3043\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2491\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3591\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3193\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5726\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5315\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4411\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5211\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4871\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4363\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5162\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3148\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6308\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2161\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4618\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4938\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2756\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4094\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1995\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3303\n",
            "total_diff 213.2094\n",
            "Epoch: 13 - batch: 25 out of 35 - train_loss_avg: 0.2152 -  test_loss_avg: 0.2205 - train_eval_avg: 0.4335 - test_eval_avg: 0.4416 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-25.model\n",
            "==========\n",
            "batch_i0 26 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4439\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2343\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5656\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3551\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5036\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.194\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3281\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5658\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1939\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2827\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.27\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3707\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3591\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.316\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5495\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.449\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3724\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2942\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2891\n",
            "total_diff 230.6788\n",
            "Epoch: 13 - batch: 26 out of 35 - train_loss_avg: 0.2171 -  test_loss_avg: 0.2144 - train_eval_avg: 0.4354 - test_eval_avg: 0.4346 - elpased: 42.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-26.model\n",
            "==========\n",
            "batch_i0 27 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2223\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3649\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5586\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6839\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3959\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5001\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4532\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2013\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5228\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7112\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4486\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4455\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3717\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4023\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2257\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4184\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2836\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5041\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3842\n",
            "total_diff 217.6391\n",
            "Epoch: 13 - batch: 27 out of 35 - train_loss_avg: 0.2178 -  test_loss_avg: 0.2198 - train_eval_avg: 0.4379 - test_eval_avg: 0.4411 - elpased: 43.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-27.model\n",
            "==========\n",
            "batch_i0 28 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4951\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1569\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4748\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2558\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3511\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4281\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5745\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2675\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6081\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1296\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.468\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1964\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2044\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2923\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6021\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.526\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2912\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4107\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4222\n",
            "total_diff 227.1778\n",
            "Epoch: 13 - batch: 28 out of 35 - train_loss_avg: 0.2209 -  test_loss_avg: 0.2182 - train_eval_avg: 0.4405 - test_eval_avg: 0.4363 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-28.model\n",
            "==========\n",
            "batch_i0 29 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4473\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6013\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.216\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3483\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4159\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.348\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4391\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3508\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4059\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.394\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.597\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5513\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3177\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3547\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2905\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6813\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2402\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4395\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3903\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2929\n",
            "total_diff 232.0277\n",
            "Epoch: 13 - batch: 29 out of 35 - train_loss_avg: 0.2189 -  test_loss_avg: 0.22 - train_eval_avg: 0.4389 - test_eval_avg: 0.4388 - elpased: 43.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-29.model\n",
            "==========\n",
            "batch_i0 30 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4943\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3027\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.322\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5282\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3249\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4165\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2207\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1242\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3933\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6063\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1886\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5246\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4368\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.193\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3496\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3884\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4911\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3517\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4544\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1541\n",
            "total_diff 225.0859\n",
            "Epoch: 13 - batch: 30 out of 35 - train_loss_avg: 0.218 -  test_loss_avg: 0.2187 - train_eval_avg: 0.4379 - test_eval_avg: 0.4376 - elpased: 43.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-30.model\n",
            "==========\n",
            "batch_i0 31 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3053\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4989\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5383\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2054\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.46\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.41\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6466\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4564\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2104\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3915\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5739\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2885\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.648\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6789\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4042\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1759\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1682\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.631\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6477\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5692\n",
            "total_diff 219.4896\n",
            "Epoch: 13 - batch: 31 out of 35 - train_loss_avg: 0.2192 -  test_loss_avg: 0.2188 - train_eval_avg: 0.4352 - test_eval_avg: 0.4417 - elpased: 44.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-31.model\n",
            "==========\n",
            "batch_i0 32 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5188\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6855\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3265\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4522\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3649\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4742\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3624\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5464\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4923\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5751\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6133\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4432\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4552\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4285\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6404\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1691\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3014\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3155\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.252\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5657\n",
            "total_diff 231.2676\n",
            "Epoch: 13 - batch: 32 out of 35 - train_loss_avg: 0.2165 -  test_loss_avg: 0.2213 - train_eval_avg: 0.4359 - test_eval_avg: 0.4322 - elpased: 43.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-32.model\n",
            "==========\n",
            "batch_i0 33 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2423\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5927\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4914\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6247\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5538\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.423\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3458\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5614\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5111\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2741\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1917\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4739\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5213\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3611\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1671\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6338\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3839\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3426\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5916\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3758\n",
            "total_diff 178.4334\n",
            "Epoch: 13 - batch: 33 out of 35 - train_loss_avg: 0.218 -  test_loss_avg: 0.2214 - train_eval_avg: 0.4355 - test_eval_avg: 0.4509 - elpased: 43.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-33.model\n",
            "==========\n",
            "batch_i0 34 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4549\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3379\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4109\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3952\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.237\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4246\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6732\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5591\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4771\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4891\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3848\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4999\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.548\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1835\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4946\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6171\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4996\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4303\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2405\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5048\n",
            "total_diff 221.8987\n",
            "Epoch: 13 - batch: 34 out of 35 - train_loss_avg: 0.2185 -  test_loss_avg: 0.2187 - train_eval_avg: 0.4389 - test_eval_avg: 0.4405 - elpased: 43.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-34.model\n",
            "==========\n",
            "batch_i0 35 cur_train_items 9178 cur_test_items 20\n",
            "training 0 out of: 9178\n",
            "training 5000 out of: 9178\n",
            "testing 0 out of: 20\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4373\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1037\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3061\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.434\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.52\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4766\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4796\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4598\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4054\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4313\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1676\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3222\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4486\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4002\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4244\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2443\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2223\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3078\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4794\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2162\n",
            "total_diff 1.728\n",
            "Epoch: 13 - batch: 35 out of 35 - train_loss_avg: 0.2205 -  test_loss_avg: 0.2134 - train_eval_avg: 0.4404 - test_eval_avg: 0.4314 - elpased: 36.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-35.model\n",
            "==========\n",
            "Epoch: 13 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4362 - test_eval_avg: 0.4373\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4848\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3783\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.483\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6666\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3407\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5233\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2626\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4245\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3059\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5228\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.566\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4403\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.273\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1483\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5364\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.585\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4506\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4422\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4511\n",
            "total_diff 259.1215\n",
            "Epoch: 14 - batch: 0 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2123 - train_eval_avg: 0.4389 - test_eval_avg: 0.4254 - elpased: 42.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-0.model\n",
            "==========\n",
            "batch_i0 1 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2774\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5348\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6372\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6027\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5985\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4313\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4638\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3626\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6492\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5778\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4554\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4174\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1989\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5507\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2424\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2628\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.594\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3535\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6356\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3875\n",
            "total_diff 232.0188\n",
            "Epoch: 14 - batch: 1 out of 35 - train_loss_avg: 0.219 -  test_loss_avg: 0.2191 - train_eval_avg: 0.4357 - test_eval_avg: 0.4457 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-1.model\n",
            "==========\n",
            "batch_i0 2 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6015\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.206\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6492\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3321\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4768\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3443\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4301\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6233\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5026\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4876\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3957\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3936\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4362\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4216\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2971\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3649\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5956\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4324\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4948\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3941\n",
            "total_diff 232.6805\n",
            "Epoch: 14 - batch: 2 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2183 - train_eval_avg: 0.4317 - test_eval_avg: 0.4402 - elpased: 43.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-2.model\n",
            "==========\n",
            "batch_i0 3 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5585\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4562\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1301\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1703\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5892\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5838\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3927\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5714\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.457\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4906\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4035\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1637\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.593\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3523\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4171\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4984\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6266\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.445\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6676\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4373\n",
            "total_diff 200.0826\n",
            "Epoch: 14 - batch: 3 out of 35 - train_loss_avg: 0.2194 -  test_loss_avg: 0.2228 - train_eval_avg: 0.4382 - test_eval_avg: 0.4441 - elpased: 44.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-3.model\n",
            "==========\n",
            "batch_i0 4 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2857\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4983\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3624\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1849\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5243\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2683\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4308\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3046\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4784\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6049\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1991\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2884\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2958\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7033\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1592\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5217\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2134\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5946\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4301\n",
            "total_diff 226.9434\n",
            "Epoch: 14 - batch: 4 out of 35 - train_loss_avg: 0.2165 -  test_loss_avg: 0.2179 - train_eval_avg: 0.433 - test_eval_avg: 0.4402 - elpased: 46.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-4.model\n",
            "==========\n",
            "batch_i0 5 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4472\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3401\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2785\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3054\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6227\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2188\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2921\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1682\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3683\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3917\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4094\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.123\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2351\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1509\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2082\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.153\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3328\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5476\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4376\n",
            "total_diff 257.478\n",
            "Epoch: 14 - batch: 5 out of 35 - train_loss_avg: 0.2155 -  test_loss_avg: 0.218 - train_eval_avg: 0.4345 - test_eval_avg: 0.4288 - elpased: 45.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-5.model\n",
            "==========\n",
            "batch_i0 6 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6592\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1931\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5398\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6676\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2091\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5437\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1455\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3482\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6039\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2608\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3222\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3119\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6369\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5047\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5728\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6336\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4713\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4054\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3483\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5002\n",
            "total_diff 247.7174\n",
            "Epoch: 14 - batch: 6 out of 35 - train_loss_avg: 0.2151 -  test_loss_avg: 0.219 - train_eval_avg: 0.4328 - test_eval_avg: 0.437 - elpased: 45.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-6.model\n",
            "==========\n",
            "batch_i0 7 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4966\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5222\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2774\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2347\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.606\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.33\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.292\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4917\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4435\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.175\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2715\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2454\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2853\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4609\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5125\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5875\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2574\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.344\n",
            "total_diff 257.3582\n",
            "Epoch: 14 - batch: 7 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2165 - train_eval_avg: 0.4352 - test_eval_avg: 0.4296 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-7.model\n",
            "==========\n",
            "batch_i0 8 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2148\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5788\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5731\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3649\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2069\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4491\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5136\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2355\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3383\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1883\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1582\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5374\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3348\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6478\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.598\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3556\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4147\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5879\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1953\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.452\n",
            "total_diff 235.6845\n",
            "Epoch: 14 - batch: 8 out of 35 - train_loss_avg: 0.2187 -  test_loss_avg: 0.2174 - train_eval_avg: 0.437 - test_eval_avg: 0.4349 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-8.model\n",
            "==========\n",
            "batch_i0 9 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.274\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1965\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3286\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3799\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.283\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1129\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3532\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3388\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.681\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6348\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3716\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5077\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.502\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2409\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5345\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4379\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3782\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4797\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2576\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6059\n",
            "total_diff 271.3429\n",
            "Epoch: 14 - batch: 9 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2165 - train_eval_avg: 0.437 - test_eval_avg: 0.4296 - elpased: 40.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-9.model\n",
            "==========\n",
            "batch_i0 10 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4151\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4914\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3767\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4221\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5202\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5379\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.239\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4863\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1525\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.603\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.378\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2502\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.664\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2202\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3016\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4921\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1639\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2887\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3356\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4505\n",
            "total_diff 248.1583\n",
            "Epoch: 14 - batch: 10 out of 35 - train_loss_avg: 0.22 -  test_loss_avg: 0.219 - train_eval_avg: 0.4388 - test_eval_avg: 0.4322 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-10.model\n",
            "==========\n",
            "batch_i0 11 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2509\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5806\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3018\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6884\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6246\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2136\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3603\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5203\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1889\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6823\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2748\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4037\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2747\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2514\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1843\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7425\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.38\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1968\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4902\n",
            "total_diff 236.3233\n",
            "Epoch: 14 - batch: 11 out of 35 - train_loss_avg: 0.217 -  test_loss_avg: 0.2184 - train_eval_avg: 0.4346 - test_eval_avg: 0.4354 - elpased: 43.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-11.model\n",
            "==========\n",
            "batch_i0 12 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2969\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6054\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.538\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5779\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4204\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5901\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6184\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4167\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6415\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2788\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2574\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3678\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4402\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4014\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2435\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7286\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4442\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3839\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4421\n",
            "total_diff 244.1292\n",
            "Epoch: 14 - batch: 12 out of 35 - train_loss_avg: 0.2157 -  test_loss_avg: 0.2167 - train_eval_avg: 0.4331 - test_eval_avg: 0.4376 - elpased: 42.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-12.model\n",
            "==========\n",
            "batch_i0 13 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6232\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3805\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.577\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5756\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2113\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4168\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2629\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4048\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5917\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2746\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7177\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4792\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4734\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2582\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5298\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2643\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6376\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3089\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4765\n",
            "total_diff 267.8475\n",
            "Epoch: 14 - batch: 13 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2175 - train_eval_avg: 0.4326 - test_eval_avg: 0.4332 - elpased: 43.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-13.model\n",
            "==========\n",
            "batch_i0 14 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6383\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4775\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3933\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4409\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2579\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5386\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3145\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3799\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4643\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4557\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7772\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3273\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2915\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2294\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7023\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6156\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4905\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6302\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3355\n",
            "total_diff 241.8177\n",
            "Epoch: 14 - batch: 14 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2179 - train_eval_avg: 0.4318 - test_eval_avg: 0.4362 - elpased: 42.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-14.model\n",
            "==========\n",
            "batch_i0 15 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4148\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2329\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6124\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6968\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.449\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5494\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6569\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6427\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3466\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.714\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2488\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.529\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3923\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.241\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4193\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3874\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4133\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6352\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2507\n",
            "total_diff 228.0659\n",
            "Epoch: 14 - batch: 15 out of 35 - train_loss_avg: 0.2184 -  test_loss_avg: 0.2228 - train_eval_avg: 0.4361 - test_eval_avg: 0.4343 - elpased: 43.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-15.model\n",
            "==========\n",
            "batch_i0 16 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4245\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3529\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3014\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5137\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2324\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2618\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2413\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3641\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.245\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2931\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1908\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4043\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.324\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.586\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3408\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3343\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3327\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3487\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2738\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1909\n",
            "total_diff 237.6142\n",
            "Epoch: 14 - batch: 16 out of 35 - train_loss_avg: 0.2184 -  test_loss_avg: 0.2174 - train_eval_avg: 0.4392 - test_eval_avg: 0.435 - elpased: 43.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-16.model\n",
            "==========\n",
            "batch_i0 17 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6584\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3237\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2258\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6496\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1686\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2394\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2844\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4424\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3515\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6354\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3231\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3918\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4828\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3158\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2249\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2104\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5814\n",
            "total_diff 236.2643\n",
            "Epoch: 14 - batch: 17 out of 35 - train_loss_avg: 0.219 -  test_loss_avg: 0.2194 - train_eval_avg: 0.4365 - test_eval_avg: 0.4412 - elpased: 44.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-17.model\n",
            "==========\n",
            "batch_i0 18 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6624\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2691\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6713\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2438\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4144\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2996\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3153\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6438\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5211\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2769\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3885\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6717\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3132\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6135\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5636\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.141\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3323\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5137\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5672\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1691\n",
            "total_diff 204.8569\n",
            "Epoch: 14 - batch: 18 out of 35 - train_loss_avg: 0.2199 -  test_loss_avg: 0.2232 - train_eval_avg: 0.4391 - test_eval_avg: 0.4435 - elpased: 43.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-18.model\n",
            "==========\n",
            "batch_i0 19 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5826\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6116\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3884\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5186\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3555\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3114\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6143\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1803\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7366\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4017\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4794\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3724\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4671\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4214\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5075\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4023\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4662\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1829\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5067\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5804\n",
            "total_diff 270.5712\n",
            "Epoch: 14 - batch: 19 out of 35 - train_loss_avg: 0.2183 -  test_loss_avg: 0.2153 - train_eval_avg: 0.4349 - test_eval_avg: 0.433 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-19.model\n",
            "==========\n",
            "batch_i0 20 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2274\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5845\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.319\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.474\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3064\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6004\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1353\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1966\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2623\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3702\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2133\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2397\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.44\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4949\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2067\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3627\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4194\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4288\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7427\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4779\n",
            "total_diff 228.0503\n",
            "Epoch: 14 - batch: 20 out of 35 - train_loss_avg: 0.2199 -  test_loss_avg: 0.2207 - train_eval_avg: 0.4387 - test_eval_avg: 0.4404 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-20.model\n",
            "==========\n",
            "batch_i0 21 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6401\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2013\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4613\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2034\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4987\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2936\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3361\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2992\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3981\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6914\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2411\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4784\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2375\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2113\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6314\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4455\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2159\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2321\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6337\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1528\n",
            "total_diff 241.5467\n",
            "Epoch: 14 - batch: 21 out of 35 - train_loss_avg: 0.2166 -  test_loss_avg: 0.2197 - train_eval_avg: 0.4363 - test_eval_avg: 0.4403 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-21.model\n",
            "==========\n",
            "batch_i0 22 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3301\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3373\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1922\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7445\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2288\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3737\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3427\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4768\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5284\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6528\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3866\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1514\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1886\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1931\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4704\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.8035\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3023\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2272\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.181\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3481\n",
            "total_diff 258.8337\n",
            "Epoch: 14 - batch: 22 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2157 - train_eval_avg: 0.4347 - test_eval_avg: 0.4308 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-22.model\n",
            "==========\n",
            "batch_i0 23 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.238\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2822\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5735\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7265\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3351\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6262\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2589\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3416\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1995\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2485\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.32\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3285\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3442\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3443\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4274\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5381\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3235\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7013\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4268\n",
            "total_diff 248.8337\n",
            "Epoch: 14 - batch: 23 out of 35 - train_loss_avg: 0.2172 -  test_loss_avg: 0.2166 - train_eval_avg: 0.4317 - test_eval_avg: 0.4351 - elpased: 41.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-23.model\n",
            "==========\n",
            "batch_i0 24 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.215\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3786\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4651\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4855\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4385\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3916\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4099\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.273\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3196\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6339\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3911\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2761\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6166\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2333\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2122\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.624\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4169\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3978\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.658\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2712\n",
            "total_diff 241.1388\n",
            "Epoch: 14 - batch: 24 out of 35 - train_loss_avg: 0.2184 -  test_loss_avg: 0.2211 - train_eval_avg: 0.4379 - test_eval_avg: 0.4314 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-24.model\n",
            "==========\n",
            "batch_i0 25 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2128\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3441\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3437\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.567\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5353\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4562\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5196\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.494\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4327\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5222\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3204\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6337\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1884\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4559\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4948\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2756\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4278\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2085\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3423\n",
            "total_diff 223.1674\n",
            "Epoch: 14 - batch: 25 out of 35 - train_loss_avg: 0.2154 -  test_loss_avg: 0.2203 - train_eval_avg: 0.4318 - test_eval_avg: 0.44 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-25.model\n",
            "==========\n",
            "batch_i0 26 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4618\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2151\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5741\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.34\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4938\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1785\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3199\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5683\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1878\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2653\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2707\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.39\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2146\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3465\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3008\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5328\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4664\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3886\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2846\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2842\n",
            "total_diff 240.31\n",
            "Epoch: 14 - batch: 26 out of 35 - train_loss_avg: 0.2167 -  test_loss_avg: 0.2142 - train_eval_avg: 0.4331 - test_eval_avg: 0.4333 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-26.model\n",
            "==========\n",
            "batch_i0 27 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2253\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4863\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3826\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.584\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6837\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4122\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4676\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.443\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.184\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5014\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7004\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4774\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4753\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3778\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4122\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2317\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3784\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2607\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4892\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4061\n",
            "total_diff 224.3996\n",
            "Epoch: 14 - batch: 27 out of 35 - train_loss_avg: 0.2174 -  test_loss_avg: 0.2194 - train_eval_avg: 0.4358 - test_eval_avg: 0.4403 - elpased: 40.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-27.model\n",
            "==========\n",
            "batch_i0 28 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4945\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1522\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5096\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3131\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2591\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3378\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4604\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5841\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2534\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5938\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1327\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4484\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1717\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2101\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.307\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5995\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5422\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3186\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4286\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3979\n",
            "total_diff 227.8569\n",
            "Epoch: 14 - batch: 28 out of 35 - train_loss_avg: 0.221 -  test_loss_avg: 0.2181 - train_eval_avg: 0.4398 - test_eval_avg: 0.4364 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-28.model\n",
            "==========\n",
            "batch_i0 29 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4728\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5842\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1969\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3488\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.394\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3616\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4478\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3493\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3554\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3887\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5775\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5393\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2882\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3558\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2846\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6522\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2545\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4783\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4137\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.292\n",
            "total_diff 227.6363\n",
            "Epoch: 14 - batch: 29 out of 35 - train_loss_avg: 0.2189 -  test_loss_avg: 0.2199 - train_eval_avg: 0.4393 - test_eval_avg: 0.4394 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-29.model\n",
            "==========\n",
            "batch_i0 30 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5264\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3123\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.319\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5256\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3717\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4147\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.257\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1264\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4348\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5792\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5506\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.429\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2092\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3593\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3811\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4951\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3661\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4513\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1469\n",
            "total_diff 213.731\n",
            "Epoch: 14 - batch: 30 out of 35 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2188 - train_eval_avg: 0.4388 - test_eval_avg: 0.4405 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-30.model\n",
            "==========\n",
            "batch_i0 31 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3117\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4778\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5226\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2043\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4347\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4115\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6516\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4632\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2088\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.427\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5862\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2966\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.634\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6657\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1573\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6337\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6315\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5493\n",
            "total_diff 208.671\n",
            "Epoch: 14 - batch: 31 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.2194 - train_eval_avg: 0.4356 - test_eval_avg: 0.4435 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-31.model\n",
            "==========\n",
            "batch_i0 32 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5216\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.673\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3321\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.452\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3444\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.458\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3655\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5421\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4791\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5564\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5976\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4426\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4494\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4144\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6153\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1679\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2945\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2957\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2433\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5719\n",
            "total_diff 229.4406\n",
            "Epoch: 14 - batch: 32 out of 35 - train_loss_avg: 0.2163 -  test_loss_avg: 0.2214 - train_eval_avg: 0.4366 - test_eval_avg: 0.4318 - elpased: 41.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-32.model\n",
            "==========\n",
            "batch_i0 33 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.229\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5836\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4847\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6063\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5501\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4235\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3184\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.55\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5157\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2598\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1703\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4677\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5169\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3665\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1573\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6473\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4081\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.338\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5993\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3788\n",
            "total_diff 181.0771\n",
            "Epoch: 14 - batch: 33 out of 35 - train_loss_avg: 0.2178 -  test_loss_avg: 0.2211 - train_eval_avg: 0.4356 - test_eval_avg: 0.4496 - elpased: 42.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-33.model\n",
            "==========\n",
            "batch_i0 34 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4536\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3288\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4127\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.368\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.201\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4209\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6856\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5584\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4783\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4916\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3646\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4795\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5321\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1703\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4974\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6152\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5043\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4427\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2033\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.507\n",
            "total_diff 234.5112\n",
            "Epoch: 14 - batch: 34 out of 35 - train_loss_avg: 0.2183 -  test_loss_avg: 0.2191 - train_eval_avg: 0.4375 - test_eval_avg: 0.4368 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-34.model\n",
            "==========\n",
            "batch_i0 35 cur_train_items 9178 cur_test_items 20\n",
            "training 0 out of: 9178\n",
            "training 5000 out of: 9178\n",
            "testing 0 out of: 20\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4561\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.0754\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3261\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3963\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5307\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4904\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5085\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4817\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4126\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.417\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1233\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3255\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4123\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4182\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4326\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2754\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2337\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2977\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4725\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1636\n",
            "total_diff 1.8688\n",
            "Epoch: 14 - batch: 35 out of 35 - train_loss_avg: 0.2207 -  test_loss_avg: 0.213 - train_eval_avg: 0.4395 - test_eval_avg: 0.4276 - elpased: 36.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-35.model\n",
            "==========\n",
            "Epoch: 14 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4359 - test_eval_avg: 0.4368\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4893\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4614\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3998\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5049\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6572\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3326\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5552\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2616\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.422\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3117\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5163\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5571\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4352\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2935\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1597\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5209\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5952\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4376\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4443\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4269\n",
            "total_diff 266.2437\n",
            "Epoch: 15 - batch: 0 out of 35 - train_loss_avg: 0.2174 -  test_loss_avg: 0.212 - train_eval_avg: 0.4371 - test_eval_avg: 0.4244 - elpased: 42.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-0.model\n",
            "==========\n",
            "batch_i0 1 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3105\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.534\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6394\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5962\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5855\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.428\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4663\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3736\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6485\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5648\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4566\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3934\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1784\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5368\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2486\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2371\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5997\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3583\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6226\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3828\n",
            "total_diff 232.0786\n",
            "Epoch: 15 - batch: 1 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.2188 - train_eval_avg: 0.4345 - test_eval_avg: 0.4458 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-1.model\n",
            "==========\n",
            "batch_i0 2 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5827\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2134\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6342\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3186\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4674\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3343\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4028\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6104\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4814\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4725\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3748\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3788\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4126\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4131\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2982\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3636\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5775\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4253\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4677\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3853\n",
            "total_diff 234.5715\n",
            "Epoch: 15 - batch: 2 out of 35 - train_loss_avg: 0.2183 -  test_loss_avg: 0.2177 - train_eval_avg: 0.4328 - test_eval_avg: 0.4365 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-2.model\n",
            "==========\n",
            "batch_i0 3 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5523\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4289\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1282\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1478\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5654\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.567\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3987\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5444\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4432\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4851\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3923\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1398\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5961\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.37\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4206\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4819\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6242\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4399\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6481\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4108\n",
            "total_diff 198.3457\n",
            "Epoch: 15 - batch: 3 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.2221 - train_eval_avg: 0.4399 - test_eval_avg: 0.4428 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-3.model\n",
            "==========\n",
            "batch_i0 4 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3082\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.477\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.393\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1812\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3009\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4175\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3248\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5041\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6047\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1879\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2746\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2915\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.706\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1437\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1629\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5099\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2169\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5749\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4305\n",
            "total_diff 218.574\n",
            "Epoch: 15 - batch: 4 out of 35 - train_loss_avg: 0.2162 -  test_loss_avg: 0.2187 - train_eval_avg: 0.4328 - test_eval_avg: 0.4425 - elpased: 42.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-4.model\n",
            "==========\n",
            "batch_i0 5 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4244\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3207\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2678\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.32\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6228\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2183\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3023\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1816\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3778\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3897\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4071\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.138\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2078\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2057\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4668\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2066\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1375\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3636\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.55\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4311\n",
            "total_diff 260.2304\n",
            "Epoch: 15 - batch: 5 out of 35 - train_loss_avg: 0.2157 -  test_loss_avg: 0.2179 - train_eval_avg: 0.4347 - test_eval_avg: 0.4286 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-5.model\n",
            "==========\n",
            "batch_i0 6 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.655\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1851\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5448\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6601\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2228\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5275\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1374\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3164\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.595\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2304\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3306\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2908\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6434\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5259\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5666\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.644\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4866\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3943\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3346\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4883\n",
            "total_diff 249.1973\n",
            "Epoch: 15 - batch: 6 out of 35 - train_loss_avg: 0.2149 -  test_loss_avg: 0.2191 - train_eval_avg: 0.4319 - test_eval_avg: 0.437 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-6.model\n",
            "==========\n",
            "batch_i0 7 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4778\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4953\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2713\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6023\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3463\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2688\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4798\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4242\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1704\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.248\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2394\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2667\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3279\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4568\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.511\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6356\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5765\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2694\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3501\n",
            "total_diff 261.9192\n",
            "Epoch: 15 - batch: 7 out of 35 - train_loss_avg: 0.2174 -  test_loss_avg: 0.2166 - train_eval_avg: 0.4348 - test_eval_avg: 0.4279 - elpased: 40.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-7.model\n",
            "==========\n",
            "batch_i0 8 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2177\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5777\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5786\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3766\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2269\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4493\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.52\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2357\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3331\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1556\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1609\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5214\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3239\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6278\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5993\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3157\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.419\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5886\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1696\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4542\n",
            "total_diff 235.8272\n",
            "Epoch: 15 - batch: 8 out of 35 - train_loss_avg: 0.2186 -  test_loss_avg: 0.2177 - train_eval_avg: 0.4371 - test_eval_avg: 0.434 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-8.model\n",
            "==========\n",
            "batch_i0 9 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2676\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3583\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4206\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2849\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1187\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3259\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3237\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6705\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6246\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3644\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4952\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5009\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2448\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5267\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.469\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.355\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4237\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2891\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.599\n",
            "total_diff 272.6392\n",
            "Epoch: 15 - batch: 9 out of 35 - train_loss_avg: 0.2178 -  test_loss_avg: 0.2159 - train_eval_avg: 0.437 - test_eval_avg: 0.4295 - elpased: 41.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-9.model\n",
            "==========\n",
            "batch_i0 10 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4414\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5047\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4022\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4242\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4834\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5044\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2843\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4835\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1519\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6277\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3973\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2923\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6682\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2088\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2839\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4464\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1287\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3021\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3748\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.505\n",
            "total_diff 243.3726\n",
            "Epoch: 15 - batch: 10 out of 35 - train_loss_avg: 0.2197 -  test_loss_avg: 0.2192 - train_eval_avg: 0.4383 - test_eval_avg: 0.4341 - elpased: 40.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-10.model\n",
            "==========\n",
            "batch_i0 11 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2682\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5945\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.288\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4152\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6931\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6371\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1953\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3228\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5446\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2035\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6707\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2512\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4169\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2873\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2914\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1926\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7366\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.313\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1905\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5102\n",
            "total_diff 239.9195\n",
            "Epoch: 15 - batch: 11 out of 35 - train_loss_avg: 0.217 -  test_loss_avg: 0.2183 - train_eval_avg: 0.4343 - test_eval_avg: 0.4347 - elpased: 43.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-11.model\n",
            "==========\n",
            "batch_i0 12 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.284\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1987\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5979\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5418\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5862\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.436\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6002\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6307\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.444\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6403\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2682\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.25\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3692\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4611\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4004\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2254\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7398\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4585\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.398\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4301\n",
            "total_diff 251.7734\n",
            "Epoch: 15 - batch: 12 out of 35 - train_loss_avg: 0.2156 -  test_loss_avg: 0.2163 - train_eval_avg: 0.4318 - test_eval_avg: 0.4363 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-12.model\n",
            "==========\n",
            "batch_i0 13 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6393\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3687\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5747\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5812\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2083\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4052\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2362\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3838\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4953\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5778\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2893\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7065\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.471\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4933\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2775\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5116\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2538\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6619\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3171\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4942\n",
            "total_diff 273.5481\n",
            "Epoch: 15 - batch: 13 out of 35 - train_loss_avg: 0.2176 -  test_loss_avg: 0.2174 - train_eval_avg: 0.4315 - test_eval_avg: 0.4322 - elpased: 42.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-13.model\n",
            "==========\n",
            "batch_i0 14 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6528\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4806\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2702\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3736\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4534\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2469\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5596\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2906\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.357\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4802\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.424\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7526\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3096\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3217\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2287\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7166\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6161\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5087\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6197\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3394\n",
            "total_diff 242.655\n",
            "Epoch: 15 - batch: 14 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2184 - train_eval_avg: 0.4309 - test_eval_avg: 0.4356 - elpased: 42.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-14.model\n",
            "==========\n",
            "batch_i0 15 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4591\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2156\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7014\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4459\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5566\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.669\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6533\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3403\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7244\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2216\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4602\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4193\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3944\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4132\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2309\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6225\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2612\n",
            "total_diff 229.0634\n",
            "Epoch: 15 - batch: 15 out of 35 - train_loss_avg: 0.2184 -  test_loss_avg: 0.2234 - train_eval_avg: 0.4358 - test_eval_avg: 0.434 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-15.model\n",
            "==========\n",
            "batch_i0 16 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4358\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3551\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3101\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5327\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.225\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2512\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2954\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3761\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2296\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3097\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2001\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.324\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.613\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3365\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3608\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3747\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3527\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2682\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1805\n",
            "total_diff 239.4278\n",
            "Epoch: 15 - batch: 16 out of 35 - train_loss_avg: 0.2184 -  test_loss_avg: 0.2175 - train_eval_avg: 0.4386 - test_eval_avg: 0.4355 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-16.model\n",
            "==========\n",
            "batch_i0 17 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6506\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.339\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2243\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6544\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2993\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1766\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2878\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2983\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4688\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4624\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3616\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6461\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3553\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.393\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4782\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3279\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2181\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2187\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.585\n",
            "total_diff 230.6768\n",
            "Epoch: 15 - batch: 17 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.2197 - train_eval_avg: 0.4358 - test_eval_avg: 0.4433 - elpased: 42.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-17.model\n",
            "==========\n",
            "batch_i0 18 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6414\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.253\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6697\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2536\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4032\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3217\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6511\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5295\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2526\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3886\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6709\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3022\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6262\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5726\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1507\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.308\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5264\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5661\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1624\n",
            "total_diff 202.2796\n",
            "Epoch: 15 - batch: 18 out of 35 - train_loss_avg: 0.22 -  test_loss_avg: 0.223 - train_eval_avg: 0.4394 - test_eval_avg: 0.4442 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-18.model\n",
            "==========\n",
            "batch_i0 19 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5857\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6274\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4068\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5199\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3528\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3388\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6186\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1896\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3974\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4795\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3269\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.474\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4551\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5383\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3786\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4757\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1907\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5125\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5792\n",
            "total_diff 265.8063\n",
            "Epoch: 15 - batch: 19 out of 35 - train_loss_avg: 0.2185 -  test_loss_avg: 0.216 - train_eval_avg: 0.4356 - test_eval_avg: 0.4343 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-19.model\n",
            "==========\n",
            "batch_i0 20 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2251\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.599\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3089\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2824\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6107\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1307\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.181\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2475\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3748\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2052\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2275\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4341\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5032\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1881\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3523\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4282\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4554\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7624\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4808\n",
            "total_diff 237.7652\n",
            "Epoch: 15 - batch: 20 out of 35 - train_loss_avg: 0.2198 -  test_loss_avg: 0.221 - train_eval_avg: 0.4379 - test_eval_avg: 0.4385 - elpased: 42.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-20.model\n",
            "==========\n",
            "batch_i0 21 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6616\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1951\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4512\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1998\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5104\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2928\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3167\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3246\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3995\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7183\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2316\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.468\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2555\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2029\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4793\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1936\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2147\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6603\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1438\n",
            "total_diff 257.5757\n",
            "Epoch: 15 - batch: 21 out of 35 - train_loss_avg: 0.2164 -  test_loss_avg: 0.2195 - train_eval_avg: 0.434 - test_eval_avg: 0.4375 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-21.model\n",
            "==========\n",
            "batch_i0 22 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3184\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3606\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2034\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7625\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2757\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3485\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3375\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5339\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.647\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.365\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1641\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1705\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1755\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4354\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.8094\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3141\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2185\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1793\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.37\n",
            "total_diff 267.4975\n",
            "Epoch: 15 - batch: 22 out of 35 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2154 - train_eval_avg: 0.4333 - test_eval_avg: 0.4295 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-22.model\n",
            "==========\n",
            "batch_i0 23 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.231\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2856\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5962\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.731\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3153\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6111\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2275\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3586\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1959\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2444\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3548\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.308\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3368\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3485\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4151\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5147\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3089\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6871\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2938\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4279\n",
            "total_diff 246.2628\n",
            "Epoch: 15 - batch: 23 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2161 - train_eval_avg: 0.4321 - test_eval_avg: 0.4354 - elpased: 41.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-23.model\n",
            "==========\n",
            "batch_i0 24 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2454\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3574\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4601\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4723\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4378\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3814\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4412\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2636\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3049\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6436\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3825\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.324\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6184\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2334\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2545\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6308\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3798\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4134\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.661\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2858\n",
            "total_diff 221.5087\n",
            "Epoch: 15 - batch: 24 out of 35 - train_loss_avg: 0.2186 -  test_loss_avg: 0.2207 - train_eval_avg: 0.4405 - test_eval_avg: 0.4362 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-24.model\n",
            "==========\n",
            "batch_i0 25 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3163\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.231\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3602\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3444\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5286\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5717\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4529\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.551\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4921\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4664\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5128\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3747\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6446\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2311\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4535\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4948\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2925\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4197\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2176\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3486\n",
            "total_diff 203.2692\n",
            "Epoch: 15 - batch: 25 out of 35 - train_loss_avg: 0.2158 -  test_loss_avg: 0.2211 - train_eval_avg: 0.433 - test_eval_avg: 0.4466 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-25.model\n",
            "==========\n",
            "batch_i0 26 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4391\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2139\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5527\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3407\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4502\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1893\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.297\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5867\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1951\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2981\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2572\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3632\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2305\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3751\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3177\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4896\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4257\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3669\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2994\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2584\n",
            "total_diff 238.6592\n",
            "Epoch: 15 - batch: 26 out of 35 - train_loss_avg: 0.2169 -  test_loss_avg: 0.214 - train_eval_avg: 0.4347 - test_eval_avg: 0.4313 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-26.model\n",
            "==========\n",
            "batch_i0 27 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.216\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4667\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3621\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5639\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6848\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3892\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4913\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4639\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2239\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5123\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7096\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4359\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4431\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3543\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3801\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2221\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4002\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2614\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.493\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3751\n",
            "total_diff 224.2744\n",
            "Epoch: 15 - batch: 27 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2196 - train_eval_avg: 0.4379 - test_eval_avg: 0.4384 - elpased: 42.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-27.model\n",
            "==========\n",
            "batch_i0 28 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4714\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1521\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4868\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2881\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2566\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3496\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4201\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2451\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5866\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1162\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4465\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1778\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1936\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2766\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.59\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5195\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2866\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3721\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4018\n",
            "total_diff 231.6089\n",
            "Epoch: 15 - batch: 28 out of 35 - train_loss_avg: 0.2206 -  test_loss_avg: 0.2181 - train_eval_avg: 0.4406 - test_eval_avg: 0.4333 - elpased: 41.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-28.model\n",
            "==========\n",
            "batch_i0 29 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4518\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5782\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.205\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3478\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3997\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3574\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4458\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3485\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3633\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3817\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5854\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5459\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2809\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3513\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.293\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6755\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2585\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.442\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4049\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2835\n",
            "total_diff 232.2269\n",
            "Epoch: 15 - batch: 29 out of 35 - train_loss_avg: 0.2188 -  test_loss_avg: 0.22 - train_eval_avg: 0.4392 - test_eval_avg: 0.4378 - elpased: 42.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-29.model\n",
            "==========\n",
            "batch_i0 30 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5346\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3352\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3379\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5431\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3761\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4043\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2458\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1438\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4106\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5882\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2088\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5471\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4476\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2003\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3707\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4037\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5045\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.364\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4764\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1774\n",
            "total_diff 217.604\n",
            "Epoch: 15 - batch: 30 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.219 - train_eval_avg: 0.4375 - test_eval_avg: 0.442 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-30.model\n",
            "==========\n",
            "batch_i0 31 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2816\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5016\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5168\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.192\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4281\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3888\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.669\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4696\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2008\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4127\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.617\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2761\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6607\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6826\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4262\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1738\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1573\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6452\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.637\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5651\n",
            "total_diff 225.6394\n",
            "Epoch: 15 - batch: 31 out of 35 - train_loss_avg: 0.219 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4339 - test_eval_avg: 0.4393 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-31.model\n",
            "==========\n",
            "batch_i0 32 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5238\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6987\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3234\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4189\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3371\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4452\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3994\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5374\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4913\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5543\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6201\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4633\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4654\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4085\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6367\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1882\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3144\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3038\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2713\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6055\n",
            "total_diff 237.7592\n",
            "Epoch: 15 - batch: 32 out of 35 - train_loss_avg: 0.2164 -  test_loss_avg: 0.221 - train_eval_avg: 0.4371 - test_eval_avg: 0.431 - elpased: 41.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-32.model\n",
            "==========\n",
            "batch_i0 33 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2423\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5925\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4647\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6239\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5456\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4291\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3444\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5357\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5097\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2653\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1826\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4627\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.506\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3294\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1506\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6539\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.395\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2755\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.615\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3719\n",
            "total_diff 185.0554\n",
            "Epoch: 15 - batch: 33 out of 35 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2209 - train_eval_avg: 0.4322 - test_eval_avg: 0.448 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-33.model\n",
            "==========\n",
            "batch_i0 34 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3931\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2976\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3529\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3402\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1997\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3719\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6748\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5365\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4204\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4394\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3138\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5062\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5238\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1753\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4849\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.598\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4788\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.388\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2339\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4819\n",
            "total_diff 232.9331\n",
            "Epoch: 15 - batch: 34 out of 35 - train_loss_avg: 0.2191 -  test_loss_avg: 0.2193 - train_eval_avg: 0.4437 - test_eval_avg: 0.4342 - elpased: 43.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-34.model\n",
            "==========\n",
            "batch_i0 35 cur_train_items 9178 cur_test_items 20\n",
            "training 0 out of: 9178\n",
            "training 5000 out of: 9178\n",
            "testing 0 out of: 20\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1398\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3743\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4844\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.55\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4792\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4957\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4841\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4182\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4284\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2086\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3285\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5109\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4348\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.449\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3053\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2715\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3465\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4772\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2555\n",
            "total_diff 1.5963\n",
            "Epoch: 15 - batch: 35 out of 35 - train_loss_avg: 0.2208 -  test_loss_avg: 0.2137 - train_eval_avg: 0.4371 - test_eval_avg: 0.4408 - elpased: 35.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-35.model\n",
            "==========\n",
            "Epoch: 15 - train_loss_avg: 0.218 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4358 - test_eval_avg: 0.4363\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5112\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4661\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4343\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5137\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6623\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3489\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6041\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2885\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4697\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3264\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5315\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5849\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4411\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3148\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1928\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5791\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5937\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4653\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4502\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4919\n",
            "total_diff 242.2102\n",
            "Epoch: 16 - batch: 0 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2127 - train_eval_avg: 0.4385 - test_eval_avg: 0.4351 - elpased: 42.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-0.model\n",
            "==========\n",
            "batch_i0 1 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2763\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5223\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6382\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5749\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.581\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4275\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4371\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3559\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6382\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5935\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4354\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4371\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1774\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5583\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2118\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2718\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5748\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3602\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6444\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3596\n",
            "total_diff 235.9015\n",
            "Epoch: 16 - batch: 1 out of 35 - train_loss_avg: 0.2185 -  test_loss_avg: 0.2189 - train_eval_avg: 0.4359 - test_eval_avg: 0.4441 - elpased: 41.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-1.model\n",
            "==========\n",
            "batch_i0 2 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6068\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1832\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6492\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3287\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4777\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3307\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4188\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6226\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4851\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4702\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3984\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3751\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4348\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4157\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3034\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3711\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5972\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4354\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5021\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3808\n",
            "total_diff 241.2305\n",
            "Epoch: 16 - batch: 2 out of 35 - train_loss_avg: 0.2182 -  test_loss_avg: 0.218 - train_eval_avg: 0.4328 - test_eval_avg: 0.4373 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-2.model\n",
            "==========\n",
            "batch_i0 3 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5574\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4695\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1308\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5821\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5815\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3868\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5774\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4594\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4825\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4014\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1504\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5968\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3539\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.402\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5007\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6322\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6726\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4415\n",
            "total_diff 201.9783\n",
            "Epoch: 16 - batch: 3 out of 35 - train_loss_avg: 0.2193 -  test_loss_avg: 0.2228 - train_eval_avg: 0.4377 - test_eval_avg: 0.4437 - elpased: 40.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-3.model\n",
            "==========\n",
            "batch_i0 4 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2649\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4953\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.365\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5219\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2823\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4232\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2995\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4764\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5999\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.174\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.275\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2868\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7027\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1309\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1515\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5257\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2083\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5898\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4227\n",
            "total_diff 230.621\n",
            "Epoch: 16 - batch: 4 out of 35 - train_loss_avg: 0.2162 -  test_loss_avg: 0.2178 - train_eval_avg: 0.4325 - test_eval_avg: 0.438 - elpased: 40.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-4.model\n",
            "==========\n",
            "batch_i0 5 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4471\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.335\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2907\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3206\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6099\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2399\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3064\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1977\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3675\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.405\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4201\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1511\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2208\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2015\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4264\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2167\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1508\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.333\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.533\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.446\n",
            "total_diff 253.0627\n",
            "Epoch: 16 - batch: 5 out of 35 - train_loss_avg: 0.2151 -  test_loss_avg: 0.2174 - train_eval_avg: 0.4351 - test_eval_avg: 0.4305 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-5.model\n",
            "==========\n",
            "batch_i0 6 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6485\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.182\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5268\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6582\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2156\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5375\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1364\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3305\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6017\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2514\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3149\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2945\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6231\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5244\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5588\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.62\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4779\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4061\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3382\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5037\n",
            "total_diff 242.326\n",
            "Epoch: 16 - batch: 6 out of 35 - train_loss_avg: 0.2149 -  test_loss_avg: 0.2188 - train_eval_avg: 0.4327 - test_eval_avg: 0.4377 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-6.model\n",
            "==========\n",
            "batch_i0 7 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4995\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5005\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.265\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.22\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5805\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3501\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2745\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4911\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.442\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1536\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2703\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2346\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2893\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.33\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4516\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4824\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6214\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5907\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2705\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3282\n",
            "total_diff 255.6174\n",
            "Epoch: 16 - batch: 7 out of 35 - train_loss_avg: 0.2176 -  test_loss_avg: 0.2167 - train_eval_avg: 0.436 - test_eval_avg: 0.4287 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-7.model\n",
            "==========\n",
            "batch_i0 8 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.215\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5751\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5844\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.373\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2126\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4253\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5153\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2098\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3497\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1597\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1496\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.553\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3223\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6222\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5893\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4134\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.585\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1895\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4679\n",
            "total_diff 236.1047\n",
            "Epoch: 16 - batch: 8 out of 35 - train_loss_avg: 0.219 -  test_loss_avg: 0.2175 - train_eval_avg: 0.4379 - test_eval_avg: 0.4342 - elpased: 40.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-8.model\n",
            "==========\n",
            "batch_i0 9 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3095\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1464\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3552\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4104\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3156\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1197\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3681\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3721\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6422\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.637\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4124\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5279\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5217\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2535\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5519\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.458\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3629\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4464\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2854\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6006\n",
            "total_diff 267.9156\n",
            "Epoch: 16 - batch: 9 out of 35 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2162 - train_eval_avg: 0.4368 - test_eval_avg: 0.4314 - elpased: 40.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-9.model\n",
            "==========\n",
            "batch_i0 10 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4376\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5123\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4012\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4476\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4953\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5162\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2625\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4868\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1633\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6231\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4079\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2786\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6743\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2158\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2916\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4831\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1349\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3078\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.362\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4833\n",
            "total_diff 242.3595\n",
            "Epoch: 16 - batch: 10 out of 35 - train_loss_avg: 0.2195 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4382 - test_eval_avg: 0.4348 - elpased: 41.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-10.model\n",
            "==========\n",
            "batch_i0 11 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2645\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5885\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2966\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6945\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.61\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1906\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3262\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5419\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1935\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6534\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2652\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4166\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2921\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2627\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1838\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7417\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3272\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2015\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.498\n",
            "total_diff 234.6324\n",
            "Epoch: 16 - batch: 11 out of 35 - train_loss_avg: 0.2168 -  test_loss_avg: 0.2183 - train_eval_avg: 0.4347 - test_eval_avg: 0.4356 - elpased: 41.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-11.model\n",
            "==========\n",
            "batch_i0 12 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2983\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1753\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6079\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5474\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5466\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4431\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5999\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6189\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3977\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6407\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2703\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2644\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3676\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.427\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2399\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7307\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4521\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3872\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4383\n",
            "total_diff 244.7095\n",
            "Epoch: 16 - batch: 12 out of 35 - train_loss_avg: 0.2155 -  test_loss_avg: 0.2165 - train_eval_avg: 0.4325 - test_eval_avg: 0.4375 - elpased: 42.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-12.model\n",
            "==========\n",
            "batch_i0 13 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6121\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5644\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5728\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2123\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4077\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2338\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3981\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5069\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5918\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2713\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7092\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4702\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4788\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.268\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5251\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2455\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6661\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3258\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4762\n",
            "total_diff 271.7532\n",
            "Epoch: 16 - batch: 13 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2173 - train_eval_avg: 0.4322 - test_eval_avg: 0.4326 - elpased: 41.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-13.model\n",
            "==========\n",
            "batch_i0 14 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6443\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4699\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2418\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3857\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4709\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2415\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5628\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2976\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3817\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.458\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4367\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.751\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3168\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3063\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2271\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.7028\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.614\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5133\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6323\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3587\n",
            "total_diff 250.6149\n",
            "Epoch: 16 - batch: 14 out of 35 - train_loss_avg: 0.2176 -  test_loss_avg: 0.2177 - train_eval_avg: 0.4304 - test_eval_avg: 0.4336 - elpased: 40.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-14.model\n",
            "==========\n",
            "batch_i0 15 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4642\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2238\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.606\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6969\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4402\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5606\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6648\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6489\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3512\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7219\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.223\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5525\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4575\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2276\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4156\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4077\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4166\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2326\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6211\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2423\n",
            "total_diff 238.4142\n",
            "Epoch: 16 - batch: 15 out of 35 - train_loss_avg: 0.2183 -  test_loss_avg: 0.2235 - train_eval_avg: 0.4348 - test_eval_avg: 0.4319 - elpased: 41.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-15.model\n",
            "==========\n",
            "batch_i0 16 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4227\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3639\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2985\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5325\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2248\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2568\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3099\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3833\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.231\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2995\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.183\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3976\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3379\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6243\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3325\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.358\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4062\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3534\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2506\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1693\n",
            "total_diff 245.614\n",
            "Epoch: 16 - batch: 16 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2175 - train_eval_avg: 0.437 - test_eval_avg: 0.4345 - elpased: 41.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-16.model\n",
            "==========\n",
            "batch_i0 17 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6263\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3406\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2205\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6411\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3031\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1864\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3016\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.25\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2967\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4696\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4553\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3577\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6552\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3591\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3899\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4767\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3236\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2235\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2314\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5614\n",
            "total_diff 226.1615\n",
            "Epoch: 16 - batch: 17 out of 35 - train_loss_avg: 0.2189 -  test_loss_avg: 0.22 - train_eval_avg: 0.4356 - test_eval_avg: 0.4443 - elpased: 43.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-17.model\n",
            "==========\n",
            "batch_i0 18 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6713\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2644\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6608\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2735\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4026\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2965\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3153\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6456\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5503\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2835\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3827\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6686\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2953\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6119\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5482\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1708\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3296\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4968\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5495\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1738\n",
            "total_diff 192.1803\n",
            "Epoch: 16 - batch: 18 out of 35 - train_loss_avg: 0.2202 -  test_loss_avg: 0.2232 - train_eval_avg: 0.4405 - test_eval_avg: 0.4465 - elpased: 42.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-18.model\n",
            "==========\n",
            "batch_i0 19 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.581\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6122\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4124\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5173\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3389\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3409\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6178\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1922\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7421\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3791\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5115\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3329\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4716\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4516\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4958\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3621\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.469\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2096\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5119\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6128\n",
            "total_diff 259.5258\n",
            "Epoch: 16 - batch: 19 out of 35 - train_loss_avg: 0.2186 -  test_loss_avg: 0.2158 - train_eval_avg: 0.437 - test_eval_avg: 0.4356 - elpased: 44.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-19.model\n",
            "==========\n",
            "batch_i0 20 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2407\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5871\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3041\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4713\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2604\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6062\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1343\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2001\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2573\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3702\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2127\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2501\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4394\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4844\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1788\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3406\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4028\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4393\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7632\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4861\n",
            "total_diff 237.8463\n",
            "Epoch: 16 - batch: 20 out of 35 - train_loss_avg: 0.2196 -  test_loss_avg: 0.2207 - train_eval_avg: 0.4384 - test_eval_avg: 0.4385 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-20.model\n",
            "==========\n",
            "batch_i0 21 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6681\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1996\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4688\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2057\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5371\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2747\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3234\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2933\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4186\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7332\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.232\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4808\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2215\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1868\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6598\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4714\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.208\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2329\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6649\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1459\n",
            "total_diff 263.2088\n",
            "Epoch: 16 - batch: 21 out of 35 - train_loss_avg: 0.2163 -  test_loss_avg: 0.2192 - train_eval_avg: 0.4336 - test_eval_avg: 0.437 - elpased: 41.5\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-21.model\n",
            "==========\n",
            "batch_i0 22 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3236\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3365\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1964\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7811\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2474\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3705\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3432\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5389\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.654\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3826\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1617\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1752\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1689\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4512\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.839\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2924\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.215\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1699\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3568\n",
            "total_diff 276.8755\n",
            "Epoch: 16 - batch: 22 out of 35 - train_loss_avg: 0.2178 -  test_loss_avg: 0.2154 - train_eval_avg: 0.4319 - test_eval_avg: 0.4281 - elpased: 42.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-22.model\n",
            "==========\n",
            "batch_i0 23 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2246\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2874\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6011\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7431\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3256\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.62\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2409\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3518\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1834\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2496\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3445\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3097\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3382\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3505\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3949\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5223\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3056\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6964\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.31\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4342\n",
            "total_diff 254.5259\n",
            "Epoch: 16 - batch: 23 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2167 - train_eval_avg: 0.4306 - test_eval_avg: 0.434 - elpased: 41.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-23.model\n",
            "==========\n",
            "batch_i0 24 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.251\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3754\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4602\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4536\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4234\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3832\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4322\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2686\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3105\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.662\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3715\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3011\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.641\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2473\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.242\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6348\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3951\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4082\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.673\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.28\n",
            "total_diff 231.4977\n",
            "Epoch: 16 - batch: 24 out of 35 - train_loss_avg: 0.2187 -  test_loss_avg: 0.2209 - train_eval_avg: 0.4392 - test_eval_avg: 0.4345 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-24.model\n",
            "==========\n",
            "batch_i0 25 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3178\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2185\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3066\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3471\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5555\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5752\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4765\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5654\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4971\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4795\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5188\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3777\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6557\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2332\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4262\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4762\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2971\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4237\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2261\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3527\n",
            "total_diff 210.4589\n",
            "Epoch: 16 - batch: 25 out of 35 - train_loss_avg: 0.2158 -  test_loss_avg: 0.2212 - train_eval_avg: 0.4317 - test_eval_avg: 0.4457 - elpased: 43.3\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-25.model\n",
            "==========\n",
            "batch_i0 26 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4073\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2037\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5679\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3422\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4568\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1824\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.266\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5918\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2106\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3178\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2691\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3305\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.248\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3967\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3306\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5063\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4017\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3398\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3197\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2697\n",
            "total_diff 238.178\n",
            "Epoch: 16 - batch: 26 out of 35 - train_loss_avg: 0.2171 -  test_loss_avg: 0.2148 - train_eval_avg: 0.434 - test_eval_avg: 0.432 - elpased: 42.8\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-26.model\n",
            "==========\n",
            "batch_i0 27 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2154\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4363\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3369\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5441\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6914\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3846\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5032\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4722\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2445\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5136\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7166\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4472\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4323\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3281\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3875\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2033\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4163\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2651\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5103\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3423\n",
            "total_diff 226.5656\n",
            "Epoch: 16 - batch: 27 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2197 - train_eval_avg: 0.4378 - test_eval_avg: 0.438 - elpased: 44.1\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-27.model\n",
            "==========\n",
            "batch_i0 28 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4576\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1726\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.493\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2681\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2245\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3686\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4246\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5554\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2432\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5966\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1384\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4499\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.169\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2081\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2737\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5666\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5163\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2849\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3762\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4193\n",
            "total_diff 230.9744\n",
            "Epoch: 16 - batch: 28 out of 35 - train_loss_avg: 0.2204 -  test_loss_avg: 0.2182 - train_eval_avg: 0.4405 - test_eval_avg: 0.4332 - elpased: 42.7\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-28.model\n",
            "==========\n",
            "batch_i0 29 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4369\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.587\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2393\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3412\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4168\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3509\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.438\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.34\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3791\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3789\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5934\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5241\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2713\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3432\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2832\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6668\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2561\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4533\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4013\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2893\n",
            "total_diff 226.042\n",
            "Epoch: 16 - batch: 29 out of 35 - train_loss_avg: 0.2187 -  test_loss_avg: 0.2201 - train_eval_avg: 0.4395 - test_eval_avg: 0.4399 - elpased: 44.2\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-29.model\n",
            "==========\n",
            "batch_i0 30 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5303\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.33\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3418\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5402\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3981\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4249\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2622\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.161\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4304\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5827\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2141\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5582\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4296\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2062\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3757\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3967\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4995\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3679\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4632\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.1826\n",
            "total_diff 212.7285\n",
            "Epoch: 16 - batch: 30 out of 35 - train_loss_avg: 0.2181 -  test_loss_avg: 0.2192 - train_eval_avg: 0.4373 - test_eval_avg: 0.4439 - elpased: 42.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-30.model\n",
            "==========\n",
            "batch_i0 31 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2721\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4667\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5189\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.184\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.414\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3968\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6769\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4518\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.18\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3871\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6147\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2784\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6527\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6882\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3895\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1621\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1582\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6197\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6406\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5477\n",
            "total_diff 227.1602\n",
            "Epoch: 16 - batch: 31 out of 35 - train_loss_avg: 0.219 -  test_loss_avg: 0.2188 - train_eval_avg: 0.4349 - test_eval_avg: 0.437 - elpased: 41.0\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-31.model\n",
            "==========\n",
            "batch_i0 32 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5189\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.7272\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3383\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4359\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3892\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4606\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3769\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5494\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.502\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5742\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6481\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4647\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4719\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4409\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6578\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1843\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3223\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2965\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2591\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5957\n",
            "total_diff 231.1972\n",
            "Epoch: 16 - batch: 32 out of 35 - train_loss_avg: 0.2166 -  test_loss_avg: 0.2205 - train_eval_avg: 0.4387 - test_eval_avg: 0.4343 - elpased: 41.4\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-32.model\n",
            "==========\n",
            "batch_i0 33 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2236\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.588\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4571\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6033\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5399\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4155\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3141\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.544\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5028\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2361\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1635\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4542\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5023\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3401\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1419\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6943\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3625\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2719\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6194\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3581\n",
            "total_diff 191.4953\n",
            "Epoch: 16 - batch: 33 out of 35 - train_loss_avg: 0.2177 -  test_loss_avg: 0.2202 - train_eval_avg: 0.4298 - test_eval_avg: 0.4436 - elpased: 40.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-33.model\n",
            "==========\n",
            "batch_i0 34 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4066\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3399\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3717\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3516\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2368\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3861\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6897\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5466\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4484\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4394\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.335\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4981\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5378\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2097\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4983\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.614\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5075\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3975\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2407\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5076\n",
            "total_diff 223.451\n",
            "Epoch: 16 - batch: 34 out of 35 - train_loss_avg: 0.2191 -  test_loss_avg: 0.2192 - train_eval_avg: 0.4455 - test_eval_avg: 0.439 - elpased: 41.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-34.model\n",
            "==========\n",
            "batch_i0 35 cur_train_items 9178 cur_test_items 20\n",
            "training 0 out of: 9178\n",
            "training 5000 out of: 9178\n",
            "testing 0 out of: 20\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4599\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1344\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.3375\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4233\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5582\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4573\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5027\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4793\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3805\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4056\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.1782\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3027\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4582\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4329\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4373\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2813\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.2441\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3357\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.457\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2366\n",
            "total_diff 1.5052\n",
            "Epoch: 16 - batch: 35 out of 35 - train_loss_avg: 0.2204 -  test_loss_avg: 0.2169 - train_eval_avg: 0.4342 - test_eval_avg: 0.4389 - elpased: 34.6\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-35.model\n",
            "==========\n",
            "Epoch: 16 - train_loss_avg: 0.2179 -  test_loss_avg: 0.2186 - train_eval_avg: 0.4357 - test_eval_avg: 0.4368\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n",
            "testing 0 out of: 2565\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5083\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4599\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4265\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5145\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6583\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.337\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.6339\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2744\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4321\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3333\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5214\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.5834\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4381\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.3344\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.2036\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.5476\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.6052\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4721\n",
            "day:1234;high:>3 actual: 0.0 predicted: 0.4558\n",
            "day:1234;high:>3 actual: 1.0 predicted: 0.4674\n",
            "total_diff 247.7162\n",
            "Epoch: 17 - batch: 0 out of 35 - train_loss_avg: 0.2175 -  test_loss_avg: 0.2124 - train_eval_avg: 0.4391 - test_eval_avg: 0.4332 - elpased: 40.9\n",
            "models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/tmp/model-batch-0.model\n",
            "==========\n",
            "batch_i0 1 cur_train_items 10000 cur_test_items 2565\n",
            "training 0 out of: 10000\n",
            "training 5000 out of: 10000\n"
          ]
        }
      ],
      "source": [
        "import torch, random, time, math\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "#params={}\n",
        "# def extract_features():\n",
        "#   return\n",
        "\n",
        "def eval_pred2(rnn_out0,actual_outcome0):\n",
        "  eval_list=[]\n",
        "  rnn_out_flat=rnn_out0.ravel()\n",
        "  for ac0,pred0 in zip(actual_outcome0,rnn_out_flat):\n",
        "    cur_pred0=pred0.item()\n",
        "    if ac0==1: dist0=1-cur_pred0\n",
        "    else: dist0=cur_pred0\n",
        "    eval_list.append(dist0)\n",
        "  return sum(eval_list)/len(eval_list)\n",
        "\n",
        "\n",
        "exp_name=\"new-stock-pred-test29-0\"\n",
        "exp_name=\"new-stock-pred-test29-64-L2\"\n",
        "exp_name=\"new-stock-pred-test29-64-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-pred-test39-64-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-pred-test39-128-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-unflattened-000001\"\n",
        "exp_name=\"new-stock-unflattened-128-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample4-32-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample-30-10-128-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-00000001\"\n",
        "exp_name=\"test-batches-1layer-000001-256\"\n",
        "exp_name=\"batches-1layer-000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-1layer-0000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-2layer-0000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-limited-preds-29-256-2\"\n",
        "exp_name=\"batches-limited-preds-48-128-1-0000001\"\n",
        "exp_name=\"test-batches-limited-preds-58-128-6layer-00000001\"\n",
        "#exp_name=\"test-batches-limited-preds-15prev-58-512-1layer-0000001\"\n",
        "#exp_name=\"test-batches-limited-preds-15prev-58-1024-1layer-00000001\"\n",
        "exp_name=\"test-batches-limited-preds-15prev-10-1024-1layer-00000001\"\n",
        "exp_name=\"test-batches-3preds-20prev-29-1024-1layer-000000001\"\n",
        "exp_name=\"test-batches-init-zeros-3preds-20prev-29-1024-1layer-000000001\"\n",
        "#exp_name=\"test-batches-init-zeros-3preds-30prev-48-512-2layer-00000005\"\n",
        "exp_name=\"test-batches-init-zeros-1preds-30prev-10-1024-1layer-0000001\"\n",
        "exp_name=\"test-batches-init-zeros-1preds-30prev-19-512-1layer-00000001\"\n",
        "exp_name=\"test-batches-init-rand-1preds-15prev-29-512-1layer-00000001\"\n",
        "exp_name=\"test-batches-init-rand-1preds-15prev-39-512-1layer-0000001\"\n",
        "exp_name=\"test-batches-sigmoid-1preds-30prev-10-64-1layer-0000001\"\n",
        "exp_name=\"test-batches-sigmoid-1preds-30prev-10-512-1layer-0000001\"\n",
        "exp_name=\"test-batches-sigmoid-1preds-30prev-10-512-1layer-0000001\"\n",
        "exp_name=\"test-batches-sigmoid-1preds-15prev-29-512-1layer-0000001\"\n",
        "exp_name=\"test-batches-sigmoid-1preds-15prev-29-512-3layer-0000001\"\n",
        "exp_name=\"test-batches-sigmoid-1preds-10prev-48-512-1layer-00000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-10prev-10-64-1layer-00000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-30prev-29-64-1layer-00000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-30prev-29-64-1layer-000000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-30prev-29-64-1layer-0000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-30prev-29-64-2layer-0000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-30prev-58-64-2layer-0000001\"\n",
        "exp_name=\"test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001\"\n",
        "\n",
        "n_layers=2#4#3\n",
        "n_hidden=128 #1024#512#256#256 #128 #128 #256 #64#64\n",
        "#LR=0.0000001 #0.0000001\n",
        "#LR=0.0000001\n",
        "#LR=0.00000001\n",
        "#LR=0.0000001\n",
        "#LR=0.000001\n",
        "#LR=0.000000001\n",
        "LR=0.0000001\n",
        "n_epochs=100\n",
        "n_data=None #number of items per source\n",
        "train_batch_size=10000\n",
        "cur_matching_in_out=False\n",
        "cur_init_zeros=False\n",
        "cur_apply_sigmoid=True\n",
        "train_ratio=0.8\n",
        "model_dir=\"models\"\n",
        "#output_labels=standard_labels=ipa_symbol_list #combined_ipa_list\n",
        "standard_labels=cur_params[\"standard_labels\"]\n",
        "\n",
        "n_batches=math.floor(len(train_data)/train_batch_size)\n",
        "if n_batches==0: test_batch_size=len(test_data)\n",
        "else: test_batch_size=math.floor(len(test_data)/n_batches)\n",
        "\n",
        "\n",
        "\n",
        "item0=train_data[0]\n",
        "ft_list,lb_list=item0[:2]\n",
        "ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "extracted_out_specs=get_specs_from_next_data(lb_list,cur_params)\n",
        "lb_list_tensor=torch.tensor(extracted_out_specs,dtype=torch.float32)\n",
        "n_input=ft_list_tensor.shape[-1]\n",
        "n_output=lb_list_tensor.shape[-1]\n",
        "\n",
        "#get_specs_from_next_data()\n",
        "print(\"ft_list\",ft_list)\n",
        "print(\"lb_list\",lb_list)\n",
        "print(\"extracted_out_specs\",extracted_out_specs)\n",
        "print(\"ft_list_tensor\",ft_list_tensor.shape)\n",
        "print(\"lb_list_tensor\",lb_list_tensor.shape)\n",
        "#hello\n",
        "loss_func = nn.MSELoss()\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=cur_matching_in_out,apply_sigmoid=cur_apply_sigmoid).to(device)\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) \n",
        "\n",
        "\n",
        "\n",
        "print(ft_list_tensor.shape)\n",
        "print(lb_list_tensor.shape)\n",
        "exp_dir_path=os.path.join(model_dir,exp_name)\n",
        "if not os.path.exists(exp_dir_path): os.makedirs(exp_dir_path)\n",
        "tmp_model_dir=os.path.join(exp_dir_path,\"tmp\") \n",
        "if not os.path.exists(tmp_model_dir): os.makedirs(tmp_model_dir)\n",
        "\n",
        "\n",
        "log_fpath=os.path.join(exp_dir_path,\"log.txt\")\n",
        "log_something(str(rnn),log_fpath)\n",
        "\n",
        "\n",
        "\n",
        "#ft_list_tensor=ft_list_tensor.reshape([1,n_input])\n",
        "rnn_out=rnn(ft_list_tensor)\n",
        "print(\"rnn_out\",rnn_out.shape)\n",
        "\n",
        "for epoch_i in range(n_epochs):\n",
        "  PATH=os.path.join(exp_dir_path,\"model-%s.model\"%epoch_i) #tmp_path\n",
        "  if os.path.exists(PATH):\n",
        "    try: checkpoint = torch.load(PATH)\n",
        "    except: checkpoint = dill_unpickle(PATH)\n",
        "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"loaded model for this epoch\",PATH)\n",
        "    rnn.train()\n",
        "    continue  \n",
        "  rnn.zero_grad()\n",
        "  train_loss_items,test_loss_items=[],[]\n",
        "  train_eval_items,test_eval_items=[],[]\n",
        "  \n",
        "  for batch_i0 in range(n_batches+1):\n",
        "    t0=time.time()\n",
        "    batch_train_eval_dict,batch_test_eval_dict={},{}\n",
        "    \n",
        "    #pred_count,correct_count=0,0\n",
        "    batch_i1=batch_i0+1\n",
        "    cur_train_items=train_data[batch_i0*train_batch_size:batch_i1*train_batch_size]\n",
        "    cur_test_items=test_data[batch_i0*test_batch_size:batch_i1*test_batch_size]\n",
        "    print(\"batch_i0\",batch_i0, \"cur_train_items\",len(cur_train_items),\"cur_test_items\",len(cur_test_items))\n",
        "    tmp_path=os.path.join(tmp_model_dir, \"model-batch-%s.model\"%batch_i0)\n",
        "    if os.path.exists(tmp_path):\n",
        "      try: checkpoint = torch.load(tmp_path)\n",
        "      except: checkpoint = dill_unpickle(tmp_path)\n",
        "      rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "      print(\"loaded model for this epoch\",tmp_path)\n",
        "      rnn.train()\n",
        "      continue  \n",
        "\n",
        "\n",
        "    for item_i,item0 in enumerate(cur_train_items):\n",
        "      if item_i%5000==0: print(\"training\",item_i, \"out of:\",len(cur_train_items))\n",
        "      ft_list,lb_list=item0[:2]\n",
        "      ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "      extracted_out_specs=get_specs_from_next_data(lb_list,cur_params)\n",
        "      lb_list_tensor=torch.tensor(extracted_out_specs,dtype=torch.float32)\n",
        "\n",
        "      #actual_outcome=item0[2]\n",
        "      \n",
        "      #lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "      lb_tensor_flat=lb_list_tensor.ravel()\n",
        "      rnn_out=rnn(ft_list_tensor)\n",
        "      rnn_out_flat=rnn_out.ravel()\n",
        "      loss = loss_func(rnn_out_flat,lb_tensor_flat) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "      train_loss_items.append(loss.item())\n",
        "      #cur_eval_item=eval_pred(rnn_out,actual_outcome,cur_params[\"standard_labels\"])\n",
        "      #cur_eval_item=0\n",
        "      cur_eval_item=eval_pred2(rnn_out,extracted_out_specs)\n",
        "      train_eval_items.append(cur_eval_item)\n",
        "\n",
        "    for item_i,item0 in enumerate(cur_test_items):\n",
        "      if item_i%5000==0: print(\"testing\",item_i, \"out of:\",len(cur_test_items))\n",
        "      ft_list,lb_list=item0[:2]\n",
        "      #actual_outcome=item0[2]\n",
        "      ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "      extracted_out_specs=get_specs_from_next_data(lb_list,cur_params)\n",
        "      lb_list_tensor=torch.tensor(extracted_out_specs,dtype=torch.float32)\n",
        "\n",
        "      \n",
        "      #lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "      lb_tensor_flat=lb_list_tensor.ravel()\n",
        "      rnn_out=rnn(ft_list_tensor)\n",
        "      rnn_out_flat=rnn_out.ravel()\n",
        "      if True: #item_i<20:\n",
        "        for lb,a,b in zip(cur_params[\"pred_labels\"],extracted_out_specs,rnn_out_flat.tolist()):\n",
        "          key0=(lb,int(a))\n",
        "          tmp_dict=batch_test_eval_dict.get(lb,{})\n",
        "          tmp_dict[int(a)]=tmp_dict.get(int(a),[])+[b]\n",
        "          batch_test_eval_dict[lb]=tmp_dict#batch_test_eval_dict.get(key0,[])+[b]\n",
        "          if item_i<20: print(lb,\"actual:\", a, \"predicted:\", round(b,4))\n",
        "        # print(\"actual:\",extracted_out_specs)\n",
        "        # print(\"predict\",[round(v,4) for v in rnn_out_flat.tolist()])\n",
        "        #print(\"-----\")\n",
        "      loss = loss_func(rnn_out_flat,lb_tensor_flat) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      test_loss_items.append(loss.item())\n",
        "      #cur_eval_item=eval_pred(rnn_out,actual_outcome,cur_params[\"standard_labels\"])\n",
        "      cur_eval_item=eval_pred2(rnn_out,extracted_out_specs)\n",
        "      \n",
        "      test_eval_items.append(cur_eval_item)\n",
        "\n",
        "    train_batch_loss_avg=sum(train_loss_items[-len(cur_train_items):])/len(cur_train_items)  \n",
        "    test_batch_loss_avg=sum(test_loss_items[-len(cur_test_items):])/len(cur_test_items)\n",
        "    train_batch_eval_avg=sum(train_eval_items[-len(cur_train_items):])/len(cur_train_items)  \n",
        "    test_batch_eval_avg=sum(test_eval_items[-len(cur_test_items):])/len(cur_test_items)\n",
        "    some_eval_list=sorted(list(batch_test_eval_dict.items()))\n",
        "    total_diff=0\n",
        "    for a,b_dict in some_eval_list:\n",
        "      vals_0=b_dict.get(0,[])\n",
        "      vals_1=b_dict.get(1,[])\n",
        "      avg_vals_0,avg_vals_1=0,0\n",
        "      if vals_0: avg_vals_0=sum(vals_0)/len(vals_0)\n",
        "      if vals_1: avg_vals_1=sum(vals_1)/len(vals_1)\n",
        "      diff01=avg_vals_1-avg_vals_0\n",
        "      total_diff+=diff01\n",
        "\n",
        "      #avg_b=sum(b)/len(b)#round(,4)\n",
        "      #print(a,round(avg_b,6), len(b),b[:5])\n",
        "      #print(a,\"avg_vals_0\", round(avg_vals_0,6),\"avg_vals_1\", round(avg_vals_1,6),\"diff01\",round(diff01,6))\n",
        "    print(\"total_diff\", round(total_diff*len(cur_test_items),4))\n",
        "\n",
        "\n",
        "    train_loss_avg=sum(train_loss_items)/len(train_loss_items)  \n",
        "    test_loss_avg=sum(test_loss_items)/len(test_loss_items)\n",
        "    train_eval_avg=sum(train_eval_items)/len(train_eval_items)  \n",
        "    test_eval_avg=sum(test_eval_items)/len(test_eval_items)\n",
        "\n",
        "    cur_checkpoint={\n",
        "              'epoch': epoch_i,\n",
        "              'n_input': n_input,\n",
        "              'n_hidden': n_hidden,\n",
        "              'n_layers': n_layers,\n",
        "              'n_output': n_output,\n",
        "              'output_labels': standard_labels,\n",
        "              'model_state_dict': rnn.state_dict(),\n",
        "              'LR': LR,\n",
        "              'matching_in_out':cur_matching_in_out,\n",
        "              'train_loss': train_loss_avg,\n",
        "              'test_loss': test_loss_avg,\n",
        "              'train_eval': train_eval_avg,\n",
        "              'test_eval': test_eval_avg,\n",
        "              'feature_extraction_parameters':cur_params,\n",
        "              'parameters':cur_params,\n",
        "              'feature_extraction_function':normalize_analyze_features,\n",
        "              'label_extraction_function':get_specs_from_next_data   \n",
        "              }\n",
        "    dill_pickle(cur_checkpoint, tmp_path)\n",
        "    t1=time.time()\n",
        "    elapsed=round(t1-t0,1)\n",
        "    line=\"Epoch: %s - batch: %s out of %s - train_loss_avg: %s -  test_loss_avg: %s - train_eval_avg: %s - test_eval_avg: %s - elpased: %s\"%(epoch_i,batch_i0,n_batches,round(train_batch_loss_avg,4),round(test_batch_loss_avg,4),round(train_batch_eval_avg,4),round(test_batch_eval_avg,4),elapsed)\n",
        "    print(line)\n",
        "    print(tmp_path)\n",
        "    log_something(line,log_fpath)\n",
        "    print(\"==========\")\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #train_eval_avg,test_eval_avg=0,0  \n",
        "  #print(\"epoch_i\",epoch_i,\"train_loss_avg\",round(train_loss_avg,4),\"test_loss_avg\",round(test_loss_avg,4)) \n",
        "  line=\"Epoch: %s - train_loss_avg: %s -  test_loss_avg: %s - train_eval_avg: %s - test_eval_avg: %s\"%(epoch_i,round(train_loss_avg,4),round(test_loss_avg,4),round(train_eval_avg,4),round(test_eval_avg,4))\n",
        "  #line=\"Epoch # %s  - Batch: %s / %s -  train loss: %s - test loss: %s - train eval: %s - test eval: %s - elapsed: %s\"%(epoch0, batch_i0, n_batches, avg_train_loss,avg_test_loss, avg_train_eval,avg_test_eval, elapsed)\n",
        "  print(line)\n",
        "  log_something(line,log_fpath)\n",
        "  dill_pickle(cur_checkpoint, PATH)\n",
        "  print(\"model saved\")\n",
        "  for f in os.listdir(tmp_model_dir):\n",
        "    tmp_fpath=os.path.join(tmp_model_dir,f)\n",
        "    os.remove(tmp_fpath)\n",
        "  print(\"deleted temporary files\")\n",
        "  print(\"-----------\")\n",
        "\n",
        "\n",
        "\n",
        "    #torch.save(cur_checkpoint, tmp_path)\n",
        "  \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKqCtDuxQUXG"
      },
      "source": [
        "#Testing and evaluating - Aug 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jVDIfPyrlVW",
        "outputId": "358ebdd9-bb1c-461a-f84d-409e6e4dfc60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "item_i 0 out of: 6750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7595\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7926\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8143\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7657\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7676\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8072\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.912\n",
            ">>> correct\n",
            "=====\n",
            "item_i 500 out of: 6750\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8446\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7914\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8003\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8383\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8075\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.777\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8343\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7897\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7602\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8032\n",
            ">>> correct\n",
            "=====\n",
            "item_i 1000 out of: 6750\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8639\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7667\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7629\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8757\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8167\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8584\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.798\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.774\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8504\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7906\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7632\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.809\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8633\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.951\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8794\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8088\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8857\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7729\n",
            ">>> correct\n",
            "=====\n",
            "item_i 1500 out of: 6750\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7644\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7872\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7736\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7515\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7707\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8234\n",
            ">>> correct\n",
            "=====\n",
            "item_i 2000 out of: 6750\n",
            "item_i 2500 out of: 6750\n",
            "item_i 3000 out of: 6750\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7743\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8454\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7987\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7691\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8003\n",
            ">>> correct\n",
            "=====\n",
            "item_i 3500 out of: 6750\n",
            "item_i 4000 out of: 6750\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.8475\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7546\n",
            "xxx incorrect\n",
            "=====\n",
            "item_i 4500 out of: 6750\n",
            "item_i 5000 out of: 6750\n",
            "item_i 5500 out of: 6750\n",
            "extracted_out_specs [0.0]\n",
            "rnn_out 0.7534\n",
            "xxx incorrect\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.843\n",
            ">>> correct\n",
            "=====\n",
            "item_i 6000 out of: 6750\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 1.0562\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.9598\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 1.1081\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.9822\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7571\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8881\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.8056\n",
            ">>> correct\n",
            "=====\n",
            "extracted_out_specs [1.0]\n",
            "rnn_out 0.7515\n",
            ">>> correct\n",
            "=====\n",
            "item_i 6500 out of: 6750\n",
            "cur_fpath stock_market_data/sp500/csv/DLTR.csv\n",
            "model_path models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/model-12.model\n",
            "average eval: 0.2911\n",
            "accuracy 0.6724\n"
          ]
        }
      ],
      "source": [
        "#New\n",
        "import torch, random\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "def eval_pred1(rnn_out0,actual_outcome0,standard_labels0,gain_threshold0=5):\n",
        "  gain_loss=0\n",
        "  rnn_out_flat=rnn_out0.ravel()\n",
        "  preds=out2labels(rnn_out_flat,standard_labels0)\n",
        "  eval_list=[]\n",
        "  for ac0,pred0 in zip(actual_outcome0,preds):\n",
        "    cur_pred=[(v[0],round(v[1].item(),4)) for v in pred0]\n",
        "    eval_list.append((ac0,cur_pred))\n",
        "  for i0 in range(0,len(eval_list),2):\n",
        "    cur_high=eval_list[i0]\n",
        "    cur_low=eval_list[i0+1]\n",
        "    high_ac,high_preds=cur_high\n",
        "    low_ac,low_preds=cur_low\n",
        "    sorted_high_preds=[(int(v[0]),v[1]) for v in high_preds]\n",
        "    sorted_high_preds=sorted(sorted_high_preds,key=lambda x:int(x[0]))\n",
        "    sum_high_pred_vals=sum([v[1] for v in sorted_high_preds])\n",
        "    sorted_low_preds=[(int(v[0]),v[1]) for v in low_preds]\n",
        "    sorted_low_preds=sorted(sorted_low_preds,key=lambda x:int(x[0]))\n",
        "    sum_low_pred_vals=sum([v[1] for v in sorted_low_preds])\n",
        "\n",
        "    #print(\"actual high:\",high_ac,\"sorted_high_preds\",sorted_high_preds)\n",
        "    print(int(i0/2), \"actual high:\",high_ac, \"actual low:\",low_ac)\n",
        "    for hi,hi_lb_wt in enumerate(sorted_high_preds):\n",
        "      hi_lb0,hi_wt0=hi_lb_wt\n",
        "      low_lb0,low_wt0=low_lb_wt=sorted_low_preds[hi]\n",
        "      \n",
        "      if hi_lb0<0: hi_cum_wt=sum([v[1] for v in sorted_high_preds[:hi+1]])\n",
        "      elif hi_lb0>0: hi_cum_wt=sum([v[1] for v in sorted_high_preds[hi:]])\n",
        "      else: hi_cum_wt=hi_wt0\n",
        "      avg_hi_cum_wt=hi_cum_wt/sum_high_pred_vals\n",
        "\n",
        "      if low_lb0<0: low_cum_wt=sum([v[1] for v in sorted_low_preds[:hi+1]])\n",
        "      elif low_lb0>0: low_cum_wt=sum([v[1] for v in sorted_low_preds[hi:]])\n",
        "      else: low_cum_wt=low_wt0\n",
        "      avg_low_cum_wt=low_cum_wt/sum_low_pred_vals\n",
        "\n",
        "      #print(hi,hi_lb_wt,round(avg_cum_wt,4))\n",
        "      print(\"high:\", hi_lb_wt,round(avg_hi_cum_wt,4),\"low:\", low_lb_wt,round(avg_low_cum_wt,4))\n",
        "  return gain_loss\n",
        "\n",
        "\n",
        "def eval_pred2(rnn_out0,actual_outcome0):\n",
        "  eval_list=[]\n",
        "  rnn_out_flat=rnn_out0.ravel()\n",
        "  for ac0,pred0 in zip(actual_outcome0,rnn_out_flat):\n",
        "    cur_pred0=pred0.item()\n",
        "    if ac0==1: dist0=1-cur_pred0\n",
        "    else: dist0=cur_pred0\n",
        "    eval_list.append(dist0)\n",
        "    #print(\"ac0,pred0 >>>\",ac0,cur_pred0, \"dist0\",dist0)\n",
        "  return sum(eval_list)/len(eval_list)\n",
        "\n",
        "\n",
        "epoch_i=3\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-0000001\"\n",
        "\n",
        "epoch_i=0\n",
        "exp_name=\"test-batches-1layer-000001-256\"\n",
        "\n",
        "model_dir=\"models\"\n",
        "exp_dir_path=os.path.join(model_dir,exp_name)\n",
        "tmp_path=os.path.join(exp_dir_path,\"model-%s.model\"%epoch_i)\n",
        "\n",
        "tmp_path='models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-25.model'\n",
        "tmp_path='models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-70.model'\n",
        "tmp_path='models/batches-limited-preds-small-16-1/tmp/model-batch-44.model'\n",
        "tmp_path='models/batches-limited-preds-29-256-2/tmp/model-batch-24.model'\n",
        "tmp_path='models/batches-limited-preds-48-128-1/tmp/model-batch-29.model'\n",
        "tmp_path='models/test-batches-limited-preds-15prev-58-512-1layer-0000001/tmp/model-batch-413.model'\n",
        "tmp_path='models/test2-batches-summary-sigmoid-1preds-30prev-29-64-2layer-0000001/tmp/model-batch-8.model'\n",
        "tmp_path='models/test2-batches-summary-sigmoid-1preds-30prev-29-64-2layer-0000001/model-15.model'\n",
        "tmp_path='models/test2-batches-summary-sigmoid-1preds-30prev-58-64-2layer-0000001/model-8.model' \n",
        "#initial: 12:70, 11:73 10:70 9:71 8:78 7:75 6:70 5:69\n",
        "#file_i=80: 12:56, 11:63, 8:57\n",
        "#file_i=110: 12:67, 11:73, 8:88\n",
        "#file_i=120: 12:69, 11:73, 8:81\n",
        "#file_i=150: 12:76, 11:76, 8:67\n",
        "#file_i=200: 12:67, 11:75, 8:92\n",
        "#file_i=250: 12:72, 11:77, 8:92\n",
        "tmp_path='models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/model-13.model' \n",
        "#file_i=250: 15: 87, 14:87, 13:87, 12:89, 11: 83, 10:87\n",
        "#file_i=80: 12: 69, 13: 69.3\n",
        "#file_i=110: 12: 76, 13: 90, 14: 75, 15: 83\n",
        "#file_i=120: 12: 67, 13: 75, 14: 73, 15: 67\n",
        "#file_i=200: 12: 83, 13: 80, 14: 81, 15: 78\n",
        "#file_i=200: 12: 84, 13: 75, 14: 83, 15: 84\n",
        "\n",
        "file_i=120 #48\n",
        "\n",
        "try: checkpoint = torch.load(tmp_path)\n",
        "except: checkpoint = dill_unpickle(tmp_path)\n",
        "rnn = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , checkpoint[\"matching_in_out\"]).to(device)\n",
        "n_input=checkpoint[\"n_input\"]\n",
        "standard_labels=checkpoint[\"output_labels\"]\n",
        "cur_parameters=checkpoint[\"parameters\"]\n",
        "pred_labels=cur_parameters[\"pred_labels\"]\n",
        "cur_parameters=checkpoint[\"feature_extraction_parameters\"]\n",
        "label_func=checkpoint['label_extraction_function']\n",
        "feature_func=checkpoint['feature_extraction_function']\n",
        "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn.eval()\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "files=['ABC', 'ADP', 'A', 'ABT', 'ABMD', 'ADI', 'ABBV', 'AAPL', 'ADSK', 'ADM', 'ACN', 'AAP', 'AAL', 'ALGN', 'APH', 'AOS', 'AWK', 'ALLE', 'AME', 'APD', 'ARE', 'AIZ', 'ALB', 'APA', 'ALK', 'AEE', 'AMGN', 'ANTM', 'AEP', 'AON', 'AKAM', 'AXP', 'AMD', 'AMAT', 'AMP', 'ANET', 'AJG', 'AZO', 'ATVI', 'AMZN', 'AMT', 'AVB', 'ALTR', 'AVY', 'CAH', 'CDNS', 'BIO', 'CDE', 'BXP', 'BK', 'BEN', 'C', 'BMRA', 'BAX', 'BLK', 'BF-A', 'BDX', 'BR', 'BSHI', 'CB', 'CAG', 'BIIB', 'BAC', 'BMY', 'CCI', 'BSX', 'CAT', 'BRK-A', 'BBY', 'BA', 'BWA', 'CME', 'CNWT', 'CF', 'CTXS', 'D', 'CTSH', 'CHD', 'CFG', 'DFS', 'CPICQ', 'DG', 'CRM', 'CHRW', 'CLX', 'DGX', 'CPB', 'COTY', 'CHTR', 'COP', 'CNC', 'CNP', 'DE', 'COO', 'CUK', 'CPRT', 'COST', 'CINF', 'CMG', 'CL', 'CTQ', 'CTAS', 'CMI', 'CSCO', 'COWN', 'DAL', 'DTE', 'ENS', 'EQIX', 'DRE', 'DOV', 'DHI', 'EW', 'ES', 'EQR', 'DIS', 'DPZ', 'FANG', 'EXR', 'EMR', 'DLTR', 'EMN', 'FAST', 'DVA', 'EBAY', 'EA', 'DRI', 'EOG', 'EL', 'ESS', 'EIX', 'DXCM', 'EFX', 'F', 'ECL', 'ED', 'GS-PJ', 'GILD', 'GIS', 'FMBM', 'FPLPF', 'GM', 'FBHS', 'HBAN', 'FLS', 'FIS', 'FE', 'FRT', 'FRMC', 'FFIV', 'GWW', 'GRMN', 'GGG', 'FN', 'GOOG', 'GPC', 'FLT', 'FITB', 'FCX', 'FISV', 'GPN', 'FMC', 'FRC', 'HAL', 'FDX', 'FCGN', 'FB', 'GE', 'FTI', 'GD', 'HAS', 'HD', 'INTU', 'IFF', 'IRM', 'ICE', 'HLT', 'IDXX', 'HII', 'ILMN', 'HTLF', 'HPQ', 'HON', 'IBM', 'IPGP', 'HCA', 'HRL', 'IR', 'HSY', 'HOLX', 'ISRG', 'HPE', 'HRB', 'HSIC', 'INTH', 'HFC', 'HBI', 'HUM', 'IP', 'HST', 'IEX', 'HES', 'KSU', 'LNT', 'KRA', 'KHC', 'KR', 'KMB', 'JKHY', 'JNJ', 'IT', 'LEG', 'ITW', 'KSS', 'KEY', 'JNPR', 'LKQ', 'KIM', 'IVZ', 'KO', 'LNC', 'JBHT', 'LDOS', 'KMX', 'LMT', 'K', 'JPM', 'KGNR', 'KACPF', 'JCI', 'LH', 'KEYS', 'LBTYA', 'MSFT', 'MKTX', 'LYB', 'MCO', 'MRO', 'MDLZ', 'MLM', 'LVS', 'LRCX', 'MSCI', 'MOS', 'MRK', 'MET', 'MGM', 'MNST', 'MMC', 'MO', 'MCHP', 'LYV', 'MHK', 'MDT', 'LUV', 'MCK', 'MS-PF', 'MMM', 'MAA', 'MCD', 'MPC', 'MAR', 'LOW', 'MRCR', 'NOV', 'PEG', 'NVRO', 'NTRR', 'MU', 'NTRA', 'PAYX', 'NSC', 'NRG', 'ODFL', 'NTAP', 'PBCT', 'NFLX', 'ORLY', 'OMC', 'NTRS', 'NCTKF', 'NOXL', 'OKE', 'NI', 'NVR', 'NOC', 'O', 'NOW', 'PCAR', 'NEE', 'NLSN', 'NWL', 'MSI', 'NDAQ', 'NMHLY', 'OXY', 'NOK', 'NEOG', 'NCLH', 'RF', 'PSX', 'RE', 'PPG', 'ROK', 'PXD', 'RIBT', 'RCL', 'REGN', 'RMD', 'PKI', 'RL', 'RJF', 'PG', 'QRVO', 'REG', 'PHM', 'PNWRF', 'PKG', 'PNW', 'PLD', 'PVH', 'PM', 'PNR', 'PWR', 'PH', 'RLI', 'PEP', 'PRU', 'PFE', 'RHI', 'ROST', 'TAP', 'TEL', 'SRG', 'SLG', 'RSG', 'SYK', 'SNPS', 'SCHW', 'SHW', 'RXMD', 'SEGXF', 'SWKS', 'SBUX', 'RSNHF', 'SWK', 'SONC', 'ROP', 'STZ-B', 'TCYSF', 'STT', 'SPG', 'SYF', 'T', 'STX', 'SIVB', 'SO', 'ROL', 'TJX', 'SEE', 'SLB', 'SRE', 'VZ', 'UNP', 'TMUS', 'TRAUF', 'V', 'TW', 'VRSK', 'TWTR', 'URI', 'ULTA', 'UPS', 'UDR', 'TSN', 'UAL', 'TSCO', 'TTWO', 'VRSN', 'UA', 'TMO', 'WBA', 'TXN', 'UNM', 'USB', 'TXT', 'VMC', 'WAT', 'UHS', 'UEEC', 'VTR', 'TYL', 'TROW', 'TRV', 'VFC', 'WYNN', 'WSPOF', 'WU', 'YUM', 'XYL', 'WST', 'WRK', 'WEC', 'WM', 'ZTS', 'ZBH', 'XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "\n",
        "fname=\"%s.csv\"%files[file_i]\n",
        "cur_fpath=os.path.join(root_dir,fname)\n",
        "tmp_data=get_data_full_new(cur_fpath,cur_parameters)\n",
        "#tmp_data=get_data_full(cur_fpath,cur_parameters)\n",
        "\n",
        "arbitrary_i=3\n",
        "gain_loss_items=[]\n",
        "arbitrary_items=[]\n",
        "random_items=[]\n",
        "eval_items=[]\n",
        "correct_counter,total_counter=0,0\n",
        "threshold=0.75\n",
        "#for item_i in range(len(tmp_data)):\n",
        "for item_i in range(len(tmp_data)):  \n",
        "  if item_i%500==0:print(\"item_i\",item_i, \"out of:\", len(tmp_data))\n",
        "  #print(\"item_i\",item_i)\n",
        "  item0=tmp_data[item_i]\n",
        "  ft_list,lb_list=item0[:2]\n",
        "  ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "  extracted_out_specs=label_func(lb_list,cur_parameters)\n",
        "  lb_list_tensor=torch.tensor(extracted_out_specs,dtype=torch.float32)\n",
        "  lb_tensor_flat=lb_list_tensor.ravel()\n",
        "  rnn_out=rnn(ft_list_tensor)\n",
        "  rnn_out_flat=rnn_out.ravel()\n",
        "  rnn_out_flat_item=rnn_out_flat.item()\n",
        "  cur_eval=eval_pred2(rnn_out,extracted_out_specs)\n",
        "  eval_items.append(cur_eval)\n",
        "\n",
        "  if rnn_out_flat_item>threshold:\n",
        "    total_counter+=1\n",
        "    print(\"extracted_out_specs\",extracted_out_specs)\n",
        "    print(\"rnn_out\",round(rnn_out_flat_item,4))\n",
        "    if int(round(extracted_out_specs[0]))==int(round(rnn_out_flat_item)):\n",
        "      print(\">>> correct\")\n",
        "      correct_counter+=1\n",
        "    else: print(\"xxx incorrect\")\n",
        "    print(\"=====\")\n",
        "\n",
        "print(\"cur_fpath\",cur_fpath)\n",
        "print(\"model_path\",tmp_path)\n",
        "avg_eval=sum(eval_items)/len(eval_items)\n",
        "print(\"average eval:\", round(avg_eval,4))\n",
        "accuracy=0\n",
        "if total_counter>0:\n",
        "  accuracy=float(correct_counter)/total_counter\n",
        "  print(\"accuracy\",round(accuracy,4))\n",
        "  #overall_gain_loss=sum(gain_loss_items)\n",
        "if len(gain_loss_items)>0:\n",
        "  avg_gain_loss=sum(gain_loss_items)/len(gain_loss_items)\n",
        "  n_gain_items=len([v for v in gain_loss_items if v>0])\n",
        "  n_loss_items=len([v for v in gain_loss_items if v<0])\n",
        "  percent_gain_items=round(100*n_gain_items/len(gain_loss_items))\n",
        "  percent_loss_items=round(100*n_loss_items/len(gain_loss_items))\n",
        "\n",
        "  avg_arbitrary=sum(arbitrary_items)/len(arbitrary_items)\n",
        "  avg_random=sum(random_items)/len(random_items)\n",
        "\n",
        "  print(fname, \"epoch\",epoch_i)\n",
        "  print(\"overall_gain_loss\",overall_gain_loss)\n",
        "  print(\"avg_gain_loss\",round(avg_gain_loss,2))\n",
        "  print(\"avg_arbitrary\",round(avg_arbitrary,2))\n",
        "  print(\"avg_random\",round(avg_random,2))\n",
        "  print(\"n_gain_items\",n_gain_items)\n",
        "  print(\"n_loss_items\",n_loss_items)\n",
        "  print(\"percent_gain_items\",percent_gain_items)\n",
        "  print(\"percent_loss_items\",percent_loss_items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkimAFjugfPV",
        "outputId": "6dd66c79-f781-4028-a49f-6e58e2655ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0026, -0.0240, -0.0233],\n",
            "        [-0.0110, -0.0238, -0.0122],\n",
            "        [-0.0111, -0.0179, -0.0157],\n",
            "        [-0.0100, -0.0197, -0.0177],\n",
            "        [-0.0136, -0.0318, -0.0160],\n",
            "        [ 0.0061, -0.0178,  0.0044],\n",
            "        [ 0.0092,  0.0001,  0.0066],\n",
            "        [ 0.0093,  0.0023,  0.0053],\n",
            "        [ 0.0077, -0.0011,  0.0034],\n",
            "        [ 0.0066, -0.0008,  0.0037],\n",
            "        [ 0.0120,  0.0000,  0.0112],\n",
            "        [ 0.0160,  0.0045,  0.0059],\n",
            "        [ 0.0094,  0.0020,  0.0034],\n",
            "        [ 0.0071,  0.0007,  0.0012],\n",
            "        [ 0.0050, -0.0019,  0.0000]])\n",
            "tensor([ 0.0027, -0.0086, -0.0026])\n",
            "tensor([-0.0136, -0.0318, -0.0233])\n",
            "tensor([0.0160, 0.0045, 0.0112])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "train_item,test_item=item0[:2]\n",
        "train_item_tensor=to_tensor(train_item)\n",
        "train_item_tensor_avg=torch.mean(train_item_tensor,dim=0)\n",
        "train_item_tensor_min=torch.min(train_item_tensor,dim=0)\n",
        "train_item_tensor_max=torch.max(train_item_tensor,dim=0)\n",
        "print(train_item_tensor)\n",
        "print(train_item_tensor_avg)\n",
        "print(train_item_tensor_min.values)\n",
        "print(train_item_tensor_max.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMceE1enNdyK",
        "outputId": "95cd3de1-7a99-415e-d0e4-c75f7d11f8af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1015629991889, 0.0885419994592666, 0.0972220003604888]\n",
            "[0.1024309992790222, 0.0972220003604888, 0.1006940007209777]\n",
            "[0.1032989993691444, 0.1006940007209777, 0.1024309992790222]\n",
            "[0.1032989993691444, 0.0989580005407333, 0.0998260006308555]\n",
            "[0.1006940007209777, 0.0972220003604888, 0.0980900004506111]\n",
            "[0.0980900004506111, 0.0946180000901222, 0.0954860001802444]\n",
            "[0.0972220003604888, 0.0911459997296333, 0.0928819999098777]\n",
            "[0.0928819999098777, 0.0894099995493888, 0.0902779996395111]\n",
            "[0.0920139998197555, 0.0894099995493888, 0.0920139998197555]\n",
            "[0.0954860001802444, 0.0911459997296333, 0.0946180000901222]\n",
            "[0.0963540002703666, 0.0946180000901222, 0.0963540002703666]\n",
            "[0.0963540002703666, 0.09375, 0.0954860001802444]\n",
            "[0.0954860001802444, 0.0946180000901222, 0.0946180000901222]\n",
            "[0.0972220003604888, 0.0946180000901222, 0.0954860001802444]\n",
            "[0.0989580005407333, 0.0963540002703666, 0.0963540002703666]\n",
            "[0.0972220003604888, 0.0963540002703666, 0.0963540002703666]\n",
            "[0.0972220003604888, 0.0928819999098777, 0.0946180000901222]\n",
            "[0.0972220003604888, 0.0946180000901222, 0.0954860001802444]\n",
            "[0.0980900004506111, 0.0954860001802444, 0.0972220003604888]\n",
            "[0.0989580005407333, 0.0954860001802444, 0.0980900004506111]\n",
            "------\n",
            "cur_summary\n",
            "[0.035406246532107714, -0.09733918796495451, -0.008849017087723935]\n",
            "[0.044255263619830655, -0.008849017087723935, 0.02654705126316855]\n",
            "[0.053104280707553464, 0.02654705126316855, 0.044255263619830655]\n",
            "[0.053104280707553464, 0.008849017087722804, 0.01769803417544575]\n",
            "[0.02654705126316855, -0.008849017087723935, 0.0]\n",
            "[0.0, -0.03539606835089249, -0.026547051263169544]\n",
            "[-0.008849017087723935, -0.07079213670178483, -0.053094102526339226]\n",
            "[-0.053094102526339226, -0.08849017087723157, -0.07964115378950778]\n",
            "[-0.06194311961406203, -0.08849017087723157, -0.06194311961406203]\n",
            "[-0.026547051263169544, -0.07079213670178483, -0.03539606835089249]\n",
            "[-0.01769803417544674, -0.03539606835089249, -0.01769803417544674]\n",
            "[-0.01769803417544674, -0.04424508543861529, -0.026547051263169544]\n",
            "[-0.026547051263169544, -0.03539606835089249, -0.03539606835089249]\n",
            "[-0.008849017087723935, -0.03539606835089249, -0.026547051263169544]\n",
            "[0.008849017087722804, -0.01769803417544674, -0.01769803417544674]\n",
            "[-0.008849017087723935, -0.01769803417544674, -0.01769803417544674]\n",
            "[-0.008849017087723935, -0.053094102526339226, -0.03539606835089249]\n",
            "[-0.008849017087723935, -0.03539606835089249, -0.026547051263169544]\n",
            "[0.0, -0.026547051263169544, -0.008849017087723935]\n",
            "[0.008849017087722804, -0.026547051263169544, 0.0]\n"
          ]
        }
      ],
      "source": [
        "import numpy.polynomial as poly\n",
        "# x = [1, 2, 3, 4, 5]\n",
        "# y = [16, 42.25, 81, 132.25, 196]\n",
        "# c = poly.Polynomial.fit(x, y, deg = 2)\n",
        "\n",
        "# def transpose(list1):\n",
        "#   return list(map(list, zip(*list1)))\n",
        "\n",
        "# def get_poly(list1,deg0=2):\n",
        "#   indexes0=list(range(1,len(list1)+1))\n",
        "#   c = poly.Polynomial.fit(indexes0, list1, deg = deg0)\n",
        "#   return c.convert().coef.tolist()\n",
        "\n",
        "# def get_f0(list1):\n",
        "#   avg0=sum(list1)/len(list1)\n",
        "#   counter=0\n",
        "#   for i0 in range(1,len(list1)):\n",
        "#     prev_val,cur_val=list1[i0-1],list1[i0]\n",
        "#     if prev_val<avg0 and cur_val>avg0: counter+=1\n",
        "#   avg_freq=counter/len(list1)\n",
        "#   return avg_freq\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cur_params={}\n",
        "cur_params[\"summary\"]={}\n",
        "cur_params[\"summary\"][\"mean\"]=True\n",
        "cur_params[\"summary\"][\"max\"]=True\n",
        "cur_params[\"summary\"][\"min\"]=True\n",
        "cur_params[\"summary\"][\"std-dev\"]=False\n",
        "cur_params[\"summary\"][\"f0\"]=True\n",
        "cur_params[\"summary\"][\"last-item-raw\"]=True\n",
        "cur_params[\"summary\"][\"last-item-norm\"]=True\n",
        "cur_params[\"summary\"][\"poly\"]=2\n",
        "cur_params[\"columns\"]=[\"High\",\"Low\",\"Close\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# item0=train_data[0]\n",
        "# train_item,test_item=item0[:2]\n",
        "# train_item_transpose=transpose(train_item)\n",
        "# print(\"train_item\")\n",
        "# for ti in train_item:\n",
        "#   print(ti)\n",
        "# print(\"---\")\n",
        "# final_list=[]\n",
        "# cur_summary=params0.get(\"summary\",{})\n",
        "# for ti in train_item_transpose:\n",
        "#   cur_tmp_list=[]\n",
        "#   #print(ti)\n",
        "#   mean_val=sum(ti)/len(ti)\n",
        "#   max_val,min_val=max(ti),min(ti)\n",
        "#   if cur_summary.get(\"mean\",False): cur_tmp_list.append(mean_val)\n",
        "#   if cur_summary.get(\"max\",False): cur_tmp_list.append(max_val)\n",
        "#   if cur_summary.get(\"min\",False): cur_tmp_list.append(min_val)\n",
        "#   poly_deg=cur_summary.get(\"poly\",0)\n",
        "#   if poly_deg>0: cur_tmp_list.extend(get_poly(ti,poly_deg))\n",
        "#   if cur_summary.get(\"f0\",False): cur_tmp_list.append(get_f0(ti))\n",
        "#   final_list.append(cur_tmp_list)\n",
        "    \n",
        "\n",
        "  # print(\"mean,max,min\",mean_val,max_val,min_val)\n",
        "  # poly0=get_poly(ti)\n",
        "  # print(\"poly0\",poly0)\n",
        "  # f0_test=get_f0(ti)\n",
        "\n",
        "# for fl in final_list:\n",
        "#   print(fl)\n",
        "\n",
        "\n",
        "\n",
        "# root_dir='stock_market_data/sp500/csv'\n",
        "# sample_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "\n",
        "# all_data=[]\n",
        "# for fname in sample_files:\n",
        "#   cur_fname= fname+\".csv\"\n",
        "#   print(cur_fname)\n",
        "#   cur_fpath=os.path.join(root_dir,cur_fname)\n",
        "#   print(cur_fpath)\n",
        "csv_fpath0='stock_market_data/sp500/csv/MSFT.csv'\n",
        "test=get_csv_data_new(csv_fpath0,cur_params)\n",
        "\n",
        "cur_raw_ft_list=test[:20]\n",
        "for ts in cur_raw_ft_list:\n",
        "  print(ts)\n",
        "print(\"------\")\n",
        "cur_params[\"summary\"]={}\n",
        "cur_summary=normalize_analyze_features(cur_raw_ft_list,cur_params)\n",
        "print(\"cur_summary\")\n",
        "for a in cur_summary:\n",
        "  print(a)\n",
        "\n",
        "# test=[1,2,3,4,5]\n",
        "# test=[1,5,16,22]\n",
        "# test=[7,8,9,10]\n",
        "# poly0=get_poly(test,2)\n",
        "# print(poly0)\n",
        "\n",
        "# test2=[1,4,1,3,1,5,2]\n",
        "# test2_avg=sum(test2)/len(test2)\n",
        "# print(\"avg\",test2_avg)\n",
        "# f0_test=get_f0(test2)\n",
        "# print(test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErBO3N20i7u-",
        "outputId": "65a1568a-a337-4f9b-82ba-f7c15ad5028c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30, 4])\n",
            "torch.Size([130])\n",
            "rnn_out torch.Size([1, 1, 130])\n",
            "batch_i0 0 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-0.model\n",
            "batch_i0 1 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-1.model\n",
            "batch_i0 2 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-2.model\n",
            "batch_i0 3 cur_train_items 1000 cur_test_items 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-3.model\n",
            "batch_i0 4 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-4.model\n",
            "batch_i0 5 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-5.model\n",
            "batch_i0 6 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-6.model\n",
            "batch_i0 7 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-7.model\n",
            "batch_i0 8 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-8.model\n",
            "batch_i0 9 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-9.model\n",
            "batch_i0 10 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-10.model\n",
            "batch_i0 11 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-11.model\n",
            "batch_i0 12 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-12.model\n",
            "batch_i0 13 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-13.model\n",
            "batch_i0 14 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-14.model\n",
            "batch_i0 15 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-15.model\n",
            "batch_i0 16 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-16.model\n",
            "batch_i0 17 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-17.model\n",
            "batch_i0 18 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-18.model\n",
            "batch_i0 19 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-19.model\n",
            "batch_i0 20 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-20.model\n",
            "batch_i0 21 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-21.model\n",
            "batch_i0 22 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-22.model\n",
            "batch_i0 23 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-23.model\n",
            "batch_i0 24 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-24.model\n",
            "batch_i0 25 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-25.model\n",
            "batch_i0 26 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-26.model\n",
            "batch_i0 27 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-27.model\n",
            "batch_i0 28 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-28.model\n",
            "batch_i0 29 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-29.model\n",
            "batch_i0 30 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-30.model\n",
            "batch_i0 31 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-31.model\n",
            "batch_i0 32 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-32.model\n",
            "batch_i0 33 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-33.model\n",
            "batch_i0 34 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-34.model\n",
            "batch_i0 35 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-35.model\n",
            "batch_i0 36 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-36.model\n",
            "batch_i0 37 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-37.model\n",
            "batch_i0 38 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-38.model\n",
            "batch_i0 39 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-39.model\n",
            "batch_i0 40 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-40.model\n",
            "batch_i0 41 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-41.model\n",
            "batch_i0 42 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-42.model\n",
            "batch_i0 43 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-43.model\n",
            "batch_i0 44 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-44.model\n",
            "batch_i0 45 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-45.model\n",
            "batch_i0 46 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-46.model\n",
            "batch_i0 47 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-47.model\n",
            "batch_i0 48 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-48.model\n",
            "batch_i0 49 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-49.model\n",
            "batch_i0 50 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-50.model\n",
            "batch_i0 51 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-51.model\n",
            "batch_i0 52 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-52.model\n",
            "batch_i0 53 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-53.model\n",
            "batch_i0 54 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-54.model\n",
            "batch_i0 55 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-55.model\n",
            "batch_i0 56 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-56.model\n",
            "batch_i0 57 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-57.model\n",
            "batch_i0 58 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-58.model\n",
            "batch_i0 59 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-59.model\n",
            "batch_i0 60 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-60.model\n",
            "batch_i0 61 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-61.model\n",
            "batch_i0 62 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-62.model\n",
            "batch_i0 63 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-63.model\n",
            "batch_i0 64 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-64.model\n",
            "batch_i0 65 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-65.model\n",
            "batch_i0 66 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-66.model\n",
            "batch_i0 67 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-67.model\n",
            "batch_i0 68 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-68.model\n",
            "batch_i0 69 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-69.model\n",
            "batch_i0 70 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-70.model\n",
            "batch_i0 71 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-71.model\n",
            "batch_i0 72 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 72 out of 236 - train_loss_avg: 0.0632 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 111.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-72.model\n",
            "batch_i0 73 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 73 out of 236 - train_loss_avg: 0.0624 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 118.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-73.model\n",
            "batch_i0 74 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 74 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 112.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-74.model\n",
            "batch_i0 75 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 75 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.061 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 111.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-75.model\n",
            "batch_i0 76 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 76 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 110.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-76.model\n",
            "batch_i0 77 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 77 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 105.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-77.model\n",
            "batch_i0 78 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 78 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0607 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 110.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-78.model\n",
            "batch_i0 79 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 79 out of 236 - train_loss_avg: 0.0625 -  test_loss_avg: 0.0612 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 105.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-79.model\n",
            "batch_i0 80 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 80 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-80.model\n",
            "batch_i0 81 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 81 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-81.model\n",
            "batch_i0 82 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 82 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-82.model\n",
            "batch_i0 83 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 83 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-83.model\n",
            "batch_i0 84 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 84 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-84.model\n",
            "batch_i0 85 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 85 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 98.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-85.model\n",
            "batch_i0 86 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 86 out of 236 - train_loss_avg: 0.0624 -  test_loss_avg: 0.0613 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 100.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-86.model\n",
            "batch_i0 87 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 87 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 103.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-87.model\n",
            "batch_i0 88 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 88 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0614 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 101.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-88.model\n",
            "batch_i0 89 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 89 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 105.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-89.model\n",
            "batch_i0 90 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 90 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 108.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-90.model\n",
            "batch_i0 91 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 91 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0623 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 103.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-91.model\n",
            "batch_i0 92 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 92 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 94.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-92.model\n",
            "batch_i0 93 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 93 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 100.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-93.model\n",
            "batch_i0 94 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 94 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 99.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-94.model\n",
            "batch_i0 95 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 95 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 104.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-95.model\n",
            "batch_i0 96 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 96 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 102.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-96.model\n",
            "batch_i0 97 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 97 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0611 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-97.model\n",
            "batch_i0 98 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 98 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0623 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 89.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-98.model\n",
            "batch_i0 99 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 99 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-99.model\n",
            "batch_i0 100 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 100 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.061 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-100.model\n",
            "batch_i0 101 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 101 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-101.model\n",
            "batch_i0 102 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 102 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-102.model\n",
            "batch_i0 103 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 103 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0607 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 87.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-103.model\n",
            "batch_i0 104 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 104 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-104.model\n",
            "batch_i0 105 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 105 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 95.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-105.model\n",
            "batch_i0 106 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 106 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-106.model\n",
            "batch_i0 107 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 107 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-107.model\n",
            "batch_i0 108 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 108 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-108.model\n",
            "batch_i0 109 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 109 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0622 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-109.model\n",
            "batch_i0 110 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 110 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0622 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-110.model\n",
            "batch_i0 111 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 111 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-111.model\n",
            "batch_i0 112 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 112 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-112.model\n",
            "batch_i0 113 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 113 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0613 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 86.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-113.model\n",
            "batch_i0 114 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 114 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-114.model\n",
            "batch_i0 115 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 115 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 87.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-115.model\n",
            "batch_i0 116 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 116 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-116.model\n",
            "batch_i0 117 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 117 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-117.model\n",
            "batch_i0 118 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 118 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0613 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 86.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-118.model\n",
            "batch_i0 119 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 119 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-119.model\n",
            "batch_i0 120 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 120 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.061 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-120.model\n",
            "batch_i0 121 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 121 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0625 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 95.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-121.model\n",
            "batch_i0 122 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 122 out of 236 - train_loss_avg: 0.0624 -  test_loss_avg: 0.0625 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-122.model\n",
            "batch_i0 123 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 123 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-123.model\n",
            "batch_i0 124 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 124 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0626 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 94.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-124.model\n",
            "batch_i0 125 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 125 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0626 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-125.model\n",
            "batch_i0 126 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 126 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-126.model\n",
            "batch_i0 127 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 127 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0615 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 93.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-127.model\n",
            "batch_i0 128 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 128 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-128.model\n",
            "batch_i0 129 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 129 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0618 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-129.model\n",
            "batch_i0 130 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 130 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 104.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-130.model\n",
            "batch_i0 131 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 131 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-131.model\n",
            "batch_i0 132 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 132 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-132.model\n",
            "batch_i0 133 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 133 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0634 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 102.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-133.model\n",
            "batch_i0 134 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 134 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0605 - train_eval_avg: 0.0 - test_eval_avg: -0.024 - elpased: 92.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-134.model\n",
            "batch_i0 135 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 135 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-135.model\n",
            "batch_i0 136 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 136 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-136.model\n",
            "batch_i0 137 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 137 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 95.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-137.model\n",
            "batch_i0 138 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 138 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0625 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-138.model\n",
            "batch_i0 139 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 139 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-139.model\n",
            "batch_i0 140 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 140 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-140.model\n",
            "batch_i0 141 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 141 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0611 - train_eval_avg: -0.001 - test_eval_avg: 0.0 - elpased: 90.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-141.model\n",
            "batch_i0 142 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 142 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0626 - train_eval_avg: 0.0 - test_eval_avg: 0.024 - elpased: 89.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-142.model\n",
            "batch_i0 143 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 143 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0615 - train_eval_avg: 0.002 - test_eval_avg: 0.0 - elpased: 90.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-143.model\n",
            "batch_i0 144 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 144 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0598 - train_eval_avg: 0.006 - test_eval_avg: 0.012 - elpased: 92.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-144.model\n",
            "batch_i0 145 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 145 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0613 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 94.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-145.model\n",
            "batch_i0 146 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 146 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0614 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 93.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-146.model\n",
            "batch_i0 147 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 147 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-147.model\n",
            "batch_i0 148 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 148 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0619 - train_eval_avg: 0.006 - test_eval_avg: 0.048 - elpased: 98.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-148.model\n",
            "batch_i0 149 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 149 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0613 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 94.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-149.model\n",
            "batch_i0 150 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 150 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0617 - train_eval_avg: 0.01 - test_eval_avg: 0.024 - elpased: 95.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-150.model\n",
            "batch_i0 151 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 151 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 100.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-151.model\n",
            "batch_i0 152 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 152 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0618 - train_eval_avg: -0.006 - test_eval_avg: 0.0 - elpased: 117.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-152.model\n",
            "batch_i0 153 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 153 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0605 - train_eval_avg: -0.012 - test_eval_avg: 0.024 - elpased: 118.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-153.model\n",
            "batch_i0 154 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 154 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0608 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 126.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-154.model\n",
            "batch_i0 155 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 155 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0625 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 122.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-155.model\n",
            "batch_i0 156 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 156 out of 236 - train_loss_avg: 0.0625 -  test_loss_avg: 0.0612 - train_eval_avg: 0.0 - test_eval_avg: 0.024 - elpased: 110.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-156.model\n",
            "batch_i0 157 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 157 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0609 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 104.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-157.model\n",
            "batch_i0 158 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 158 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-158.model\n",
            "batch_i0 159 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 159 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0612 - train_eval_avg: 0.024 - test_eval_avg: 0.0 - elpased: 94.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-159.model\n",
            "batch_i0 160 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 160 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0624 - train_eval_avg: -0.011 - test_eval_avg: 0.0 - elpased: 100.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-160.model\n",
            "batch_i0 161 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 161 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0617 - train_eval_avg: 0.008 - test_eval_avg: -0.024 - elpased: 106.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-161.model\n",
            "batch_i0 162 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 162 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0624 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 93.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-162.model\n",
            "batch_i0 163 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 163 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.063 - train_eval_avg: 0.014 - test_eval_avg: 0.04 - elpased: 89.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-163.model\n",
            "batch_i0 164 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 164 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0624 - train_eval_avg: 0.008 - test_eval_avg: 0.024 - elpased: 90.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-164.model\n",
            "batch_i0 165 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 165 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0615 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 92.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-165.model\n",
            "batch_i0 166 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 166 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0615 - train_eval_avg: 0.01 - test_eval_avg: -0.024 - elpased: 106.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-166.model\n",
            "batch_i0 167 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 167 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.061 - train_eval_avg: 0.01 - test_eval_avg: -0.016 - elpased: 106.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-167.model\n",
            "batch_i0 168 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 168 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0624 - train_eval_avg: 0.015 - test_eval_avg: -0.004 - elpased: 102.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-168.model\n",
            "batch_i0 169 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 169 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0615 - train_eval_avg: 0.022 - test_eval_avg: 0.048 - elpased: 101.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-169.model\n",
            "batch_i0 170 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 170 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0619 - train_eval_avg: -0.006 - test_eval_avg: -0.024 - elpased: 108.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-170.model\n",
            "batch_i0 171 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 171 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0614 - train_eval_avg: 0.007 - test_eval_avg: 0.048 - elpased: 122.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-171.model\n",
            "batch_i0 172 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 172 out of 236 - train_loss_avg: 0.0627 -  test_loss_avg: 0.0619 - train_eval_avg: 0.035 - test_eval_avg: 0.052 - elpased: 130.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-172.model\n",
            "batch_i0 173 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 173 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0618 - train_eval_avg: -0.01 - test_eval_avg: 0.048 - elpased: 108.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-173.model\n",
            "batch_i0 174 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 174 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0618 - train_eval_avg: 0.009 - test_eval_avg: -0.024 - elpased: 97.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-174.model\n",
            "batch_i0 175 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 175 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0606 - train_eval_avg: 0.04 - test_eval_avg: 0.0 - elpased: 114.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-175.model\n",
            "batch_i0 176 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 176 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0622 - train_eval_avg: -0.008 - test_eval_avg: 0.048 - elpased: 130.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-176.model\n",
            "batch_i0 177 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 177 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0618 - train_eval_avg: 0.021 - test_eval_avg: 0.108 - elpased: 126.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-177.model\n",
            "batch_i0 178 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 178 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0612 - train_eval_avg: 0.007 - test_eval_avg: 0.0 - elpased: 163.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-178.model\n",
            "batch_i0 179 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 179 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0613 - train_eval_avg: 0.01 - test_eval_avg: 0.024 - elpased: 114.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-179.model\n",
            "batch_i0 180 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 180 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0607 - train_eval_avg: 0.002 - test_eval_avg: 0.024 - elpased: 106.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-180.model\n",
            "batch_i0 181 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 181 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.062 - train_eval_avg: 0.043 - test_eval_avg: 0.084 - elpased: 99.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-181.model\n",
            "batch_i0 182 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 182 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0614 - train_eval_avg: -0.002 - test_eval_avg: 0.024 - elpased: 97.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-182.model\n",
            "batch_i0 183 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 183 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0615 - train_eval_avg: 0.007 - test_eval_avg: 0.0 - elpased: 100.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-183.model\n",
            "batch_i0 184 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 184 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0618 - train_eval_avg: 0.033 - test_eval_avg: 0.024 - elpased: 111.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-184.model\n",
            "batch_i0 185 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 185 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0625 - train_eval_avg: 0.032 - test_eval_avg: -0.024 - elpased: 103.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-185.model\n",
            "batch_i0 186 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 186 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0605 - train_eval_avg: 0.034 - test_eval_avg: 0.024 - elpased: 94.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-186.model\n",
            "batch_i0 187 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 187 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0626 - train_eval_avg: 0.034 - test_eval_avg: 0.016 - elpased: 92.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-187.model\n",
            "batch_i0 188 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 188 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0623 - train_eval_avg: -0.001 - test_eval_avg: 0.072 - elpased: 101.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-188.model\n",
            "batch_i0 189 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 189 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0613 - train_eval_avg: 0.041 - test_eval_avg: 0.0 - elpased: 94.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-189.model\n",
            "batch_i0 190 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 190 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0622 - train_eval_avg: 0.009 - test_eval_avg: -0.048 - elpased: 86.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-190.model\n",
            "batch_i0 191 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 191 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0621 - train_eval_avg: 0.025 - test_eval_avg: 0.0 - elpased: 86.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-191.model\n",
            "batch_i0 192 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 192 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0618 - train_eval_avg: 0.053 - test_eval_avg: 0.072 - elpased: 86.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-192.model\n",
            "batch_i0 193 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 193 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0613 - train_eval_avg: -0.002 - test_eval_avg: -0.004 - elpased: 84.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-193.model\n",
            "batch_i0 194 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 194 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0613 - train_eval_avg: 0.024 - test_eval_avg: 0.012 - elpased: 85.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-194.model\n",
            "batch_i0 195 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 195 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0606 - train_eval_avg: 0.022 - test_eval_avg: 0.024 - elpased: 87.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-195.model\n",
            "batch_i0 196 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 196 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0605 - train_eval_avg: 0.038 - test_eval_avg: 0.012 - elpased: 96.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-196.model\n",
            "batch_i0 197 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 197 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0621 - train_eval_avg: 0.015 - test_eval_avg: 0.072 - elpased: 99.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-197.model\n",
            "batch_i0 198 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 198 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0625 - train_eval_avg: 0.035 - test_eval_avg: 0.056 - elpased: 97.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-198.model\n",
            "batch_i0 199 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 199 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0607 - train_eval_avg: 0.039 - test_eval_avg: -0.024 - elpased: 88.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-199.model\n",
            "batch_i0 200 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 200 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.062 - train_eval_avg: 0.043 - test_eval_avg: 0.16 - elpased: 86.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-200.model\n",
            "batch_i0 201 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 201 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0609 - train_eval_avg: 0.063 - test_eval_avg: 0.044 - elpased: 87.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-201.model\n",
            "batch_i0 202 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 202 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.062 - train_eval_avg: 0.035 - test_eval_avg: 0.02 - elpased: 86.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-202.model\n",
            "batch_i0 203 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 203 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0618 - train_eval_avg: 0.049 - test_eval_avg: 0.06 - elpased: 85.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-203.model\n",
            "batch_i0 204 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 204 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0611 - train_eval_avg: 0.009 - test_eval_avg: -0.04 - elpased: 87.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-204.model\n",
            "batch_i0 205 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 205 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0612 - train_eval_avg: 0.013 - test_eval_avg: 0.052 - elpased: 87.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-205.model\n",
            "batch_i0 206 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 206 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0626 - train_eval_avg: 0.073 - test_eval_avg: 0.176 - elpased: 88.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-206.model\n",
            "batch_i0 207 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 207 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0607 - train_eval_avg: 0.056 - test_eval_avg: 0.088 - elpased: 85.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-207.model\n",
            "batch_i0 208 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 208 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0616 - train_eval_avg: 0.044 - test_eval_avg: 0.02 - elpased: 85.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-208.model\n",
            "batch_i0 209 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 209 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0619 - train_eval_avg: 0.047 - test_eval_avg: 0.156 - elpased: 96.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-209.model\n",
            "batch_i0 210 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 210 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0611 - train_eval_avg: 0.051 - test_eval_avg: 0.012 - elpased: 105.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-210.model\n",
            "batch_i0 211 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 211 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.061 - train_eval_avg: 0.048 - test_eval_avg: -0.012 - elpased: 101.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-211.model\n",
            "batch_i0 212 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 212 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.061 - train_eval_avg: 0.038 - test_eval_avg: 0.028 - elpased: 98.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-212.model\n",
            "batch_i0 213 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 213 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.062 - train_eval_avg: 0.072 - test_eval_avg: 0.104 - elpased: 95.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-213.model\n",
            "batch_i0 214 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 214 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0615 - train_eval_avg: 0.094 - test_eval_avg: 0.096 - elpased: 93.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-214.model\n",
            "batch_i0 215 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 215 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.062 - train_eval_avg: 0.065 - test_eval_avg: 0.1 - elpased: 97.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-215.model\n",
            "batch_i0 216 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 216 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0608 - train_eval_avg: 0.061 - test_eval_avg: -0.008 - elpased: 101.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-216.model\n",
            "batch_i0 217 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 217 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0605 - train_eval_avg: 0.128 - test_eval_avg: 0.092 - elpased: 100.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-217.model\n",
            "batch_i0 218 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 218 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0612 - train_eval_avg: 0.07 - test_eval_avg: 0.024 - elpased: 100.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-218.model\n",
            "batch_i0 219 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 219 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0617 - train_eval_avg: 0.121 - test_eval_avg: 0.22 - elpased: 102.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-219.model\n",
            "batch_i0 220 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 220 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0612 - train_eval_avg: 0.108 - test_eval_avg: 0.096 - elpased: 102.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-220.model\n",
            "batch_i0 221 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 221 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0615 - train_eval_avg: 0.149 - test_eval_avg: 0.212 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-221.model\n",
            "batch_i0 222 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 222 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.061 - train_eval_avg: 0.132 - test_eval_avg: 0.052 - elpased: 103.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-222.model\n",
            "batch_i0 223 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 223 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0606 - train_eval_avg: 0.144 - test_eval_avg: 0.02 - elpased: 107.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-223.model\n",
            "batch_i0 224 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 224 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0608 - train_eval_avg: 0.084 - test_eval_avg: 0.112 - elpased: 106.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-224.model\n",
            "batch_i0 225 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 225 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0606 - train_eval_avg: 0.139 - test_eval_avg: 0.088 - elpased: 104.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-225.model\n",
            "batch_i0 226 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 226 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0604 - train_eval_avg: 0.231 - test_eval_avg: 0.032 - elpased: 102.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-226.model\n",
            "batch_i0 227 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 227 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.062 - train_eval_avg: 0.02 - test_eval_avg: 0.08 - elpased: 104.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-227.model\n",
            "batch_i0 228 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 228 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0603 - train_eval_avg: 0.182 - test_eval_avg: 0.312 - elpased: 100.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-228.model\n",
            "batch_i0 229 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 229 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0607 - train_eval_avg: 0.127 - test_eval_avg: -0.008 - elpased: 105.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-229.model\n",
            "batch_i0 230 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 230 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0602 - train_eval_avg: 0.153 - test_eval_avg: 0.28 - elpased: 99.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-230.model\n",
            "batch_i0 231 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 231 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0622 - train_eval_avg: 0.168 - test_eval_avg: 0.22 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-231.model\n",
            "batch_i0 232 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 232 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.0614 - train_eval_avg: 0.175 - test_eval_avg: 0.044 - elpased: 95.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-232.model\n",
            "batch_i0 233 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 233 out of 236 - train_loss_avg: 0.0608 -  test_loss_avg: 0.0603 - train_eval_avg: 0.117 - test_eval_avg: 0.124 - elpased: 97.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-233.model\n",
            "batch_i0 234 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 234 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0606 - train_eval_avg: 0.208 - test_eval_avg: 0.24 - elpased: 98.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-234.model\n",
            "batch_i0 235 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 235 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0599 - train_eval_avg: 0.198 - test_eval_avg: 0.248 - elpased: 97.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-235.model\n",
            "batch_i0 236 cur_train_items 207 cur_test_items 52\n",
            "training 0 out of: 207\n",
            "testing 0 out of: 52\n",
            "Epoch: 0 - batch: 236 out of 236 - train_loss_avg: 0.0603 -  test_loss_avg: 0.0616 - train_eval_avg: 0.1787 - test_eval_avg: 0.0769 - elpased: 21.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-236.model\n",
            "Epoch: 0 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0265 - test_eval_avg: 0.027\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 0 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.0607 - train_eval_avg: 0.268 - test_eval_avg: 0.044 - elpased: 101.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-0.model\n",
            "batch_i0 1 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 1 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0612 - train_eval_avg: 0.243 - test_eval_avg: 0.336 - elpased: 103.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-1.model\n",
            "batch_i0 2 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 2 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0606 - train_eval_avg: 0.198 - test_eval_avg: 0.136 - elpased: 104.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-2.model\n",
            "batch_i0 3 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 3 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0608 - train_eval_avg: 0.316 - test_eval_avg: 0.448 - elpased: 101.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-3.model\n",
            "batch_i0 4 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 4 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0605 - train_eval_avg: 0.149 - test_eval_avg: 0.256 - elpased: 105.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-4.model\n",
            "batch_i0 5 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 5 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0596 - train_eval_avg: 0.26 - test_eval_avg: 0.188 - elpased: 107.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-5.model\n",
            "batch_i0 6 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 6 out of 236 - train_loss_avg: 0.0607 -  test_loss_avg: 0.0609 - train_eval_avg: 0.284 - test_eval_avg: 0.14 - elpased: 110.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-6.model\n",
            "batch_i0 7 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 7 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.061 - train_eval_avg: 0.297 - test_eval_avg: 0.268 - elpased: 108.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-7.model\n",
            "batch_i0 8 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 8 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0599 - train_eval_avg: 0.261 - test_eval_avg: 0.112 - elpased: 126.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-8.model\n",
            "batch_i0 9 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 9 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0613 - train_eval_avg: 0.106 - test_eval_avg: 0.292 - elpased: 122.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-9.model\n",
            "batch_i0 10 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 10 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0621 - train_eval_avg: 0.284 - test_eval_avg: 0.14 - elpased: 115.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-10.model\n",
            "batch_i0 11 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 11 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0605 - train_eval_avg: 0.2 - test_eval_avg: 0.304 - elpased: 106.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-11.model\n",
            "batch_i0 12 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 12 out of 236 - train_loss_avg: 0.0605 -  test_loss_avg: 0.0592 - train_eval_avg: 0.193 - test_eval_avg: 0.172 - elpased: 108.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-12.model\n",
            "batch_i0 13 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 13 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0614 - train_eval_avg: 0.276 - test_eval_avg: 0.392 - elpased: 105.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-13.model\n",
            "batch_i0 14 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 14 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0608 - train_eval_avg: 0.204 - test_eval_avg: 0.332 - elpased: 97.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-14.model\n",
            "batch_i0 15 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 15 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0606 - train_eval_avg: 0.297 - test_eval_avg: 0.336 - elpased: 96.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-15.model\n",
            "batch_i0 16 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 16 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0606 - train_eval_avg: 0.222 - test_eval_avg: 0.336 - elpased: 95.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-16.model\n",
            "batch_i0 17 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 17 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.061 - train_eval_avg: 0.224 - test_eval_avg: 0.284 - elpased: 88.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-17.model\n",
            "batch_i0 18 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 18 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0599 - train_eval_avg: 0.238 - test_eval_avg: 0.444 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-18.model\n",
            "batch_i0 19 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 19 out of 236 - train_loss_avg: 0.0605 -  test_loss_avg: 0.0608 - train_eval_avg: 0.222 - test_eval_avg: 0.096 - elpased: 95.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-19.model\n",
            "batch_i0 20 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 20 out of 236 - train_loss_avg: 0.0602 -  test_loss_avg: 0.0599 - train_eval_avg: 0.503 - test_eval_avg: 0.444 - elpased: 86.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-20.model\n",
            "batch_i0 21 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 21 out of 236 - train_loss_avg: 0.0602 -  test_loss_avg: 0.06 - train_eval_avg: 0.261 - test_eval_avg: 0.256 - elpased: 85.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-21.model\n",
            "batch_i0 22 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 22 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0599 - train_eval_avg: 0.371 - test_eval_avg: 0.22 - elpased: 90.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-22.model\n",
            "batch_i0 23 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 23 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0597 - train_eval_avg: 0.469 - test_eval_avg: 0.552 - elpased: 97.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-23.model\n",
            "batch_i0 24 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 24 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0602 - train_eval_avg: 0.401 - test_eval_avg: 0.284 - elpased: 85.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-24.model\n",
            "batch_i0 25 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 25 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0615 - train_eval_avg: 0.437 - test_eval_avg: 0.268 - elpased: 86.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-25.model\n",
            "batch_i0 26 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 26 out of 236 - train_loss_avg: 0.0603 -  test_loss_avg: 0.0613 - train_eval_avg: 0.362 - test_eval_avg: 0.332 - elpased: 84.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-26.model\n",
            "batch_i0 27 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 27 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0599 - train_eval_avg: 0.364 - test_eval_avg: 0.32 - elpased: 85.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-27.model\n",
            "batch_i0 28 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 28 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0597 - train_eval_avg: 0.287 - test_eval_avg: 0.084 - elpased: 86.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-28.model\n",
            "batch_i0 29 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 29 out of 236 - train_loss_avg: 0.0608 -  test_loss_avg: 0.0612 - train_eval_avg: 0.296 - test_eval_avg: 0.4 - elpased: 91.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-29.model\n",
            "batch_i0 30 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 30 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0608 - train_eval_avg: 0.275 - test_eval_avg: 0.176 - elpased: 93.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-30.model\n",
            "batch_i0 31 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 31 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0603 - train_eval_avg: 0.304 - test_eval_avg: 0.452 - elpased: 88.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-31.model\n",
            "batch_i0 32 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 32 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.06 - train_eval_avg: 0.3 - test_eval_avg: 0.408 - elpased: 85.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-32.model\n",
            "batch_i0 33 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 33 out of 236 - train_loss_avg: 0.0602 -  test_loss_avg: 0.061 - train_eval_avg: 0.321 - test_eval_avg: 0.316 - elpased: 85.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-33.model\n",
            "batch_i0 34 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 34 out of 236 - train_loss_avg: 0.0607 -  test_loss_avg: 0.0607 - train_eval_avg: 0.428 - test_eval_avg: 0.4 - elpased: 88.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-34.model\n",
            "batch_i0 35 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 35 out of 236 - train_loss_avg: 0.06 -  test_loss_avg: 0.06 - train_eval_avg: 0.282 - test_eval_avg: 0.452 - elpased: 88.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-35.model\n",
            "batch_i0 36 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 36 out of 236 - train_loss_avg: 0.0603 -  test_loss_avg: 0.0605 - train_eval_avg: 0.344 - test_eval_avg: 0.344 - elpased: 87.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-36.model\n",
            "batch_i0 37 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 37 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0614 - train_eval_avg: 0.416 - test_eval_avg: 0.332 - elpased: 89.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-37.model\n",
            "batch_i0 38 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 38 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.0604 - train_eval_avg: 0.529 - test_eval_avg: 0.436 - elpased: 90.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-38.model\n",
            "batch_i0 39 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n"
          ]
        }
      ],
      "source": [
        "#OLD\n",
        "import torch, random, time, math\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "#params={}\n",
        "def extract_features():\n",
        "  return\n",
        "exp_name=\"new-stock-pred-test29-0\"\n",
        "exp_name=\"new-stock-pred-test29-64-L2\"\n",
        "exp_name=\"new-stock-pred-test29-64-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-pred-test39-64-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-pred-test39-128-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-unflattened-000001\"\n",
        "exp_name=\"new-stock-unflattened-128-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample4-32-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample-30-10-128-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-00000001\"\n",
        "exp_name=\"test-batches-1layer-000001-256\"\n",
        "exp_name=\"batches-1layer-000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-1layer-0000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-2layer-0000001-256-5day-pred-39stocks\"\n",
        "\n",
        "n_layers=2#4#3\n",
        "n_hidden=256 #128 #128 #256 #64#64\n",
        "#LR=0.0000001 #0.0000001\n",
        "#LR=0.0000001\n",
        "#LR=0.00000001\n",
        "LR=0.0000001\n",
        "n_epochs=100\n",
        "n_data=None #number of items per source\n",
        "train_batch_size=1000\n",
        "cur_matching_in_out=False\n",
        "train_ratio=0.8\n",
        "model_dir=\"models\"\n",
        "#output_labels=standard_labels=ipa_symbol_list #combined_ipa_list\n",
        "standard_labels=cur_params[\"standard_labels\"]\n",
        "\n",
        "n_batches=math.floor(len(train_data)/train_batch_size)\n",
        "if n_batches==0: test_batch_size=len(test_data)\n",
        "else: test_batch_size=math.floor(len(test_data)/n_batches)\n",
        "\n",
        "\n",
        "\n",
        "item0=train_data[0]\n",
        "ft_list,lb_list=item0[:2]\n",
        "ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "n_input=ft_list_tensor.shape[-1]\n",
        "n_output=lb_list_tensor.shape[0]\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=cur_matching_in_out).to(device)\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) \n",
        "\n",
        "\n",
        "\n",
        "print(ft_list_tensor.shape)\n",
        "print(lb_list_tensor.shape)\n",
        "exp_dir_path=os.path.join(model_dir,exp_name)\n",
        "if not os.path.exists(exp_dir_path): os.makedirs(exp_dir_path)\n",
        "tmp_model_dir=os.path.join(exp_dir_path,\"tmp\") \n",
        "if not os.path.exists(tmp_model_dir): os.makedirs(tmp_model_dir)\n",
        "\n",
        "\n",
        "log_fpath=os.path.join(exp_dir_path,\"log.txt\")\n",
        "log_something(str(rnn),log_fpath)\n",
        "\n",
        "\n",
        "\n",
        "#ft_list_tensor=ft_list_tensor.reshape([1,n_input])\n",
        "rnn_out=rnn(ft_list_tensor)\n",
        "print(\"rnn_out\",rnn_out.shape)\n",
        "\n",
        "for epoch_i in range(n_epochs):\n",
        "  PATH=os.path.join(exp_dir_path,\"model-%s.model\"%epoch_i) #tmp_path\n",
        "  if os.path.exists(PATH):\n",
        "    try: checkpoint = torch.load(PATH)\n",
        "    except: checkpoint = dill_unpickle(PATH)\n",
        "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"loaded model for this epoch\",PATH)\n",
        "    rnn.train()\n",
        "    continue  \n",
        "  rnn.zero_grad()\n",
        "  train_loss_items,test_loss_items=[],[]\n",
        "  train_eval_items,test_eval_items=[],[]\n",
        "  for batch_i0 in range(n_batches+1):\n",
        "    t0=time.time()\n",
        "    \n",
        "    #pred_count,correct_count=0,0\n",
        "    batch_i1=batch_i0+1\n",
        "    cur_train_items=train_data[batch_i0*train_batch_size:batch_i1*train_batch_size]\n",
        "    cur_test_items=test_data[batch_i0*test_batch_size:batch_i1*test_batch_size]\n",
        "    print(\"batch_i0\",batch_i0, \"cur_train_items\",len(cur_train_items),\"cur_test_items\",len(cur_test_items))\n",
        "    tmp_path=os.path.join(tmp_model_dir, \"model-batch-%s.model\"%batch_i0)\n",
        "    if os.path.exists(tmp_path):\n",
        "      try: checkpoint = torch.load(tmp_path)\n",
        "      except: checkpoint = dill_unpickle(tmp_path)\n",
        "      rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "      print(\"loaded model for this epoch\",tmp_path)\n",
        "      rnn.train()\n",
        "      continue  \n",
        "\n",
        "\n",
        "    for item_i,item0 in enumerate(cur_train_items):\n",
        "      if item_i%5000==0: print(\"training\",item_i, \"out of:\",len(cur_train_items))\n",
        "      ft_list,lb_list=item0[:2]\n",
        "      actual_outcome=item0[2]\n",
        "      ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "      lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "      lb_tensor_flat=lb_list_tensor.ravel()\n",
        "      rnn_out=rnn(ft_list_tensor)\n",
        "      rnn_out_flat=rnn_out.ravel()\n",
        "      loss = loss_func(rnn_out_flat,lb_tensor_flat) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "      train_loss_items.append(loss.item())\n",
        "      cur_eval_item=eval_pred(rnn_out,actual_outcome,cur_params[\"standard_labels\"])\n",
        "      train_eval_items.append(cur_eval_item)\n",
        "\n",
        "    for item_i,item0 in enumerate(cur_test_items):\n",
        "      if item_i%5000==0: print(\"testing\",item_i, \"out of:\",len(cur_test_items))\n",
        "      ft_list,lb_list=item0[:2]\n",
        "      actual_outcome=item0[2]\n",
        "      ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "      lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "      lb_tensor_flat=lb_list_tensor.ravel()\n",
        "      rnn_out=rnn(ft_list_tensor)\n",
        "      rnn_out_flat=rnn_out.ravel()\n",
        "      loss = loss_func(rnn_out_flat,lb_tensor_flat) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      test_loss_items.append(loss.item())\n",
        "      cur_eval_item=eval_pred(rnn_out,actual_outcome,cur_params[\"standard_labels\"])\n",
        "      test_eval_items.append(cur_eval_item)\n",
        "\n",
        "    train_batch_loss_avg=sum(train_loss_items[-len(cur_train_items):])/len(cur_train_items)  \n",
        "    test_batch_loss_avg=sum(test_loss_items[-len(cur_test_items):])/len(cur_test_items)\n",
        "    train_batch_eval_avg=sum(train_eval_items[-len(cur_train_items):])/len(cur_train_items)  \n",
        "    test_batch_eval_avg=sum(test_eval_items[-len(cur_test_items):])/len(cur_test_items)\n",
        "\n",
        "    train_loss_avg=sum(train_loss_items)/len(train_loss_items)  \n",
        "    test_loss_avg=sum(test_loss_items)/len(test_loss_items)\n",
        "    train_eval_avg=sum(train_eval_items)/len(train_eval_items)  \n",
        "    test_eval_avg=sum(test_eval_items)/len(test_eval_items)\n",
        "\n",
        "    cur_checkpoint={\n",
        "              'epoch': epoch_i,\n",
        "              'n_input': n_input,\n",
        "              'n_hidden': n_hidden,\n",
        "              'n_layers': n_layers,\n",
        "              'n_output': n_output,\n",
        "              'output_labels': standard_labels,\n",
        "              'model_state_dict': rnn.state_dict(),\n",
        "              'LR': LR,\n",
        "              'matching_in_out':cur_matching_in_out,\n",
        "              'train_loss': train_loss_avg,\n",
        "              'test_loss': test_loss_avg,\n",
        "              'train_eval': train_eval_avg,\n",
        "              'test_eval': test_eval_avg,\n",
        "              'feature_extraction_parameters':cur_params,\n",
        "              'feature_extraction_function':extract_features   \n",
        "              }\n",
        "    dill_pickle(cur_checkpoint, tmp_path)\n",
        "    t1=time.time()\n",
        "    elapsed=round(t1-t0,1)\n",
        "    line=\"Epoch: %s - batch: %s out of %s - train_loss_avg: %s -  test_loss_avg: %s - train_eval_avg: %s - test_eval_avg: %s - elpased: %s\"%(epoch_i,batch_i0,n_batches,round(train_batch_loss_avg,4),round(test_batch_loss_avg,4),round(train_batch_eval_avg,4),round(test_batch_eval_avg,4),elapsed)\n",
        "    print(line)\n",
        "    print(tmp_path)\n",
        "    log_something(line,log_fpath)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #train_eval_avg,test_eval_avg=0,0  \n",
        "  #print(\"epoch_i\",epoch_i,\"train_loss_avg\",round(train_loss_avg,4),\"test_loss_avg\",round(test_loss_avg,4)) \n",
        "  line=\"Epoch: %s - train_loss_avg: %s -  test_loss_avg: %s - train_eval_avg: %s - test_eval_avg: %s\"%(epoch_i,round(train_loss_avg,4),round(test_loss_avg,4),round(train_eval_avg,4),round(test_eval_avg,4))\n",
        "  #line=\"Epoch # %s  - Batch: %s / %s -  train loss: %s - test loss: %s - train eval: %s - test eval: %s - elapsed: %s\"%(epoch0, batch_i0, n_batches, avg_train_loss,avg_test_loss, avg_train_eval,avg_test_eval, elapsed)\n",
        "  print(line)\n",
        "  log_something(line,log_fpath)\n",
        "  dill_pickle(cur_checkpoint, PATH)\n",
        "  print(\"model saved\")\n",
        "  for f in os.listdir(tmp_model_dir):\n",
        "    tmp_fpath=os.path.join(tmp_model_dir,f)\n",
        "    os.remove(tmp_fpath)\n",
        "  print(\"deleted temporary files\")\n",
        "  print(\"-----------\")\n",
        "\n",
        "\n",
        "\n",
        "    #torch.save(cur_checkpoint, tmp_path)\n",
        "  \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "quY7i2ooeSfd",
        "outputId": "65a1568a-a337-4f9b-82ba-f7c15ad5028c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30, 4])\n",
            "torch.Size([130])\n",
            "rnn_out torch.Size([1, 1, 130])\n",
            "batch_i0 0 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-0.model\n",
            "batch_i0 1 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-1.model\n",
            "batch_i0 2 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-2.model\n",
            "batch_i0 3 cur_train_items 1000 cur_test_items 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-3.model\n",
            "batch_i0 4 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-4.model\n",
            "batch_i0 5 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-5.model\n",
            "batch_i0 6 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-6.model\n",
            "batch_i0 7 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-7.model\n",
            "batch_i0 8 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-8.model\n",
            "batch_i0 9 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-9.model\n",
            "batch_i0 10 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-10.model\n",
            "batch_i0 11 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-11.model\n",
            "batch_i0 12 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-12.model\n",
            "batch_i0 13 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-13.model\n",
            "batch_i0 14 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-14.model\n",
            "batch_i0 15 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-15.model\n",
            "batch_i0 16 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-16.model\n",
            "batch_i0 17 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-17.model\n",
            "batch_i0 18 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-18.model\n",
            "batch_i0 19 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-19.model\n",
            "batch_i0 20 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-20.model\n",
            "batch_i0 21 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-21.model\n",
            "batch_i0 22 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-22.model\n",
            "batch_i0 23 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-23.model\n",
            "batch_i0 24 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-24.model\n",
            "batch_i0 25 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-25.model\n",
            "batch_i0 26 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-26.model\n",
            "batch_i0 27 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-27.model\n",
            "batch_i0 28 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-28.model\n",
            "batch_i0 29 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-29.model\n",
            "batch_i0 30 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-30.model\n",
            "batch_i0 31 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-31.model\n",
            "batch_i0 32 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-32.model\n",
            "batch_i0 33 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-33.model\n",
            "batch_i0 34 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-34.model\n",
            "batch_i0 35 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-35.model\n",
            "batch_i0 36 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-36.model\n",
            "batch_i0 37 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-37.model\n",
            "batch_i0 38 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-38.model\n",
            "batch_i0 39 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-39.model\n",
            "batch_i0 40 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-40.model\n",
            "batch_i0 41 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-41.model\n",
            "batch_i0 42 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-42.model\n",
            "batch_i0 43 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-43.model\n",
            "batch_i0 44 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-44.model\n",
            "batch_i0 45 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-45.model\n",
            "batch_i0 46 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-46.model\n",
            "batch_i0 47 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-47.model\n",
            "batch_i0 48 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-48.model\n",
            "batch_i0 49 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-49.model\n",
            "batch_i0 50 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-50.model\n",
            "batch_i0 51 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-51.model\n",
            "batch_i0 52 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-52.model\n",
            "batch_i0 53 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-53.model\n",
            "batch_i0 54 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-54.model\n",
            "batch_i0 55 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-55.model\n",
            "batch_i0 56 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-56.model\n",
            "batch_i0 57 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-57.model\n",
            "batch_i0 58 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-58.model\n",
            "batch_i0 59 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-59.model\n",
            "batch_i0 60 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-60.model\n",
            "batch_i0 61 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-61.model\n",
            "batch_i0 62 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-62.model\n",
            "batch_i0 63 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-63.model\n",
            "batch_i0 64 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-64.model\n",
            "batch_i0 65 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-65.model\n",
            "batch_i0 66 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-66.model\n",
            "batch_i0 67 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-67.model\n",
            "batch_i0 68 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-68.model\n",
            "batch_i0 69 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-69.model\n",
            "batch_i0 70 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-70.model\n",
            "batch_i0 71 cur_train_items 1000 cur_test_items 250\n",
            "loaded model for this epoch models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-71.model\n",
            "batch_i0 72 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 72 out of 236 - train_loss_avg: 0.0632 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 111.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-72.model\n",
            "batch_i0 73 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 73 out of 236 - train_loss_avg: 0.0624 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 118.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-73.model\n",
            "batch_i0 74 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 74 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 112.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-74.model\n",
            "batch_i0 75 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 75 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.061 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 111.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-75.model\n",
            "batch_i0 76 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 76 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 110.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-76.model\n",
            "batch_i0 77 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 77 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 105.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-77.model\n",
            "batch_i0 78 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 78 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0607 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 110.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-78.model\n",
            "batch_i0 79 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 79 out of 236 - train_loss_avg: 0.0625 -  test_loss_avg: 0.0612 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 105.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-79.model\n",
            "batch_i0 80 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 80 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-80.model\n",
            "batch_i0 81 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 81 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-81.model\n",
            "batch_i0 82 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 82 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-82.model\n",
            "batch_i0 83 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 83 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-83.model\n",
            "batch_i0 84 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 84 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-84.model\n",
            "batch_i0 85 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 85 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 98.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-85.model\n",
            "batch_i0 86 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 86 out of 236 - train_loss_avg: 0.0624 -  test_loss_avg: 0.0613 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 100.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-86.model\n",
            "batch_i0 87 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 87 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 103.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-87.model\n",
            "batch_i0 88 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 88 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0614 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 101.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-88.model\n",
            "batch_i0 89 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 89 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 105.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-89.model\n",
            "batch_i0 90 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 90 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 108.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-90.model\n",
            "batch_i0 91 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 91 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0623 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 103.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-91.model\n",
            "batch_i0 92 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 92 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 94.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-92.model\n",
            "batch_i0 93 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 93 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 100.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-93.model\n",
            "batch_i0 94 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 94 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 99.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-94.model\n",
            "batch_i0 95 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 95 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 104.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-95.model\n",
            "batch_i0 96 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 96 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 102.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-96.model\n",
            "batch_i0 97 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 97 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0611 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-97.model\n",
            "batch_i0 98 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 98 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0623 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 89.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-98.model\n",
            "batch_i0 99 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 99 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-99.model\n",
            "batch_i0 100 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 100 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.061 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-100.model\n",
            "batch_i0 101 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 101 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-101.model\n",
            "batch_i0 102 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 102 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-102.model\n",
            "batch_i0 103 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 103 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0607 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 87.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-103.model\n",
            "batch_i0 104 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 104 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-104.model\n",
            "batch_i0 105 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 105 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 95.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-105.model\n",
            "batch_i0 106 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 106 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-106.model\n",
            "batch_i0 107 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 107 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-107.model\n",
            "batch_i0 108 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 108 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-108.model\n",
            "batch_i0 109 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 109 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0622 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-109.model\n",
            "batch_i0 110 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 110 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0622 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-110.model\n",
            "batch_i0 111 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 111 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 88.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-111.model\n",
            "batch_i0 112 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 112 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-112.model\n",
            "batch_i0 113 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 113 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0613 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 86.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-113.model\n",
            "batch_i0 114 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 114 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0621 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-114.model\n",
            "batch_i0 115 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 115 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0617 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 87.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-115.model\n",
            "batch_i0 116 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 116 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-116.model\n",
            "batch_i0 117 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 117 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-117.model\n",
            "batch_i0 118 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 118 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0613 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 86.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-118.model\n",
            "batch_i0 119 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 119 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-119.model\n",
            "batch_i0 120 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 120 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.061 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-120.model\n",
            "batch_i0 121 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 121 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0625 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 95.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-121.model\n",
            "batch_i0 122 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 122 out of 236 - train_loss_avg: 0.0624 -  test_loss_avg: 0.0625 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-122.model\n",
            "batch_i0 123 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 123 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 89.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-123.model\n",
            "batch_i0 124 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 124 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0626 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 94.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-124.model\n",
            "batch_i0 125 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 125 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0626 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-125.model\n",
            "batch_i0 126 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 126 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-126.model\n",
            "batch_i0 127 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 127 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0615 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 93.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-127.model\n",
            "batch_i0 128 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 128 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-128.model\n",
            "batch_i0 129 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 129 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0618 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 101.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-129.model\n",
            "batch_i0 130 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 130 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0619 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 104.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-130.model\n",
            "batch_i0 131 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 131 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0627 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-131.model\n",
            "batch_i0 132 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 132 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-132.model\n",
            "batch_i0 133 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 133 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0634 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 102.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-133.model\n",
            "batch_i0 134 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 134 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0605 - train_eval_avg: 0.0 - test_eval_avg: -0.024 - elpased: 92.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-134.model\n",
            "batch_i0 135 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 135 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0628 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-135.model\n",
            "batch_i0 136 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 136 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0615 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 92.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-136.model\n",
            "batch_i0 137 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 137 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 95.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-137.model\n",
            "batch_i0 138 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 138 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0625 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-138.model\n",
            "batch_i0 139 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 139 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 90.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-139.model\n",
            "batch_i0 140 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 140 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0624 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 91.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-140.model\n",
            "batch_i0 141 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 141 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0611 - train_eval_avg: -0.001 - test_eval_avg: 0.0 - elpased: 90.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-141.model\n",
            "batch_i0 142 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 142 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0626 - train_eval_avg: 0.0 - test_eval_avg: 0.024 - elpased: 89.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-142.model\n",
            "batch_i0 143 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 143 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0615 - train_eval_avg: 0.002 - test_eval_avg: 0.0 - elpased: 90.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-143.model\n",
            "batch_i0 144 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 144 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0598 - train_eval_avg: 0.006 - test_eval_avg: 0.012 - elpased: 92.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-144.model\n",
            "batch_i0 145 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 145 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0613 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 94.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-145.model\n",
            "batch_i0 146 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 146 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0614 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 93.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-146.model\n",
            "batch_i0 147 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 147 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.062 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 93.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-147.model\n",
            "batch_i0 148 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 148 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0619 - train_eval_avg: 0.006 - test_eval_avg: 0.048 - elpased: 98.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-148.model\n",
            "batch_i0 149 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 149 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0613 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 94.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-149.model\n",
            "batch_i0 150 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 150 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0617 - train_eval_avg: 0.01 - test_eval_avg: 0.024 - elpased: 95.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-150.model\n",
            "batch_i0 151 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 151 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0614 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 100.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-151.model\n",
            "batch_i0 152 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 152 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0618 - train_eval_avg: -0.006 - test_eval_avg: 0.0 - elpased: 117.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-152.model\n",
            "batch_i0 153 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 153 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0605 - train_eval_avg: -0.012 - test_eval_avg: 0.024 - elpased: 118.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-153.model\n",
            "batch_i0 154 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 154 out of 236 - train_loss_avg: 0.0622 -  test_loss_avg: 0.0608 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 126.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-154.model\n",
            "batch_i0 155 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 155 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0625 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 122.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-155.model\n",
            "batch_i0 156 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 156 out of 236 - train_loss_avg: 0.0625 -  test_loss_avg: 0.0612 - train_eval_avg: 0.0 - test_eval_avg: 0.024 - elpased: 110.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-156.model\n",
            "batch_i0 157 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 157 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0609 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 104.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-157.model\n",
            "batch_i0 158 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 158 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0 - test_eval_avg: 0.0 - elpased: 96.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-158.model\n",
            "batch_i0 159 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 159 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0612 - train_eval_avg: 0.024 - test_eval_avg: 0.0 - elpased: 94.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-159.model\n",
            "batch_i0 160 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 160 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0624 - train_eval_avg: -0.011 - test_eval_avg: 0.0 - elpased: 100.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-160.model\n",
            "batch_i0 161 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 161 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0617 - train_eval_avg: 0.008 - test_eval_avg: -0.024 - elpased: 106.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-161.model\n",
            "batch_i0 162 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 162 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0624 - train_eval_avg: 0.006 - test_eval_avg: 0.0 - elpased: 93.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-162.model\n",
            "batch_i0 163 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 163 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.063 - train_eval_avg: 0.014 - test_eval_avg: 0.04 - elpased: 89.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-163.model\n",
            "batch_i0 164 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 164 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0624 - train_eval_avg: 0.008 - test_eval_avg: 0.024 - elpased: 90.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-164.model\n",
            "batch_i0 165 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 165 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0615 - train_eval_avg: 0.012 - test_eval_avg: 0.0 - elpased: 92.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-165.model\n",
            "batch_i0 166 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 166 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0615 - train_eval_avg: 0.01 - test_eval_avg: -0.024 - elpased: 106.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-166.model\n",
            "batch_i0 167 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 167 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.061 - train_eval_avg: 0.01 - test_eval_avg: -0.016 - elpased: 106.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-167.model\n",
            "batch_i0 168 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 168 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0624 - train_eval_avg: 0.015 - test_eval_avg: -0.004 - elpased: 102.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-168.model\n",
            "batch_i0 169 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 169 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0615 - train_eval_avg: 0.022 - test_eval_avg: 0.048 - elpased: 101.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-169.model\n",
            "batch_i0 170 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 170 out of 236 - train_loss_avg: 0.0623 -  test_loss_avg: 0.0619 - train_eval_avg: -0.006 - test_eval_avg: -0.024 - elpased: 108.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-170.model\n",
            "batch_i0 171 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 171 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0614 - train_eval_avg: 0.007 - test_eval_avg: 0.048 - elpased: 122.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-171.model\n",
            "batch_i0 172 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 172 out of 236 - train_loss_avg: 0.0627 -  test_loss_avg: 0.0619 - train_eval_avg: 0.035 - test_eval_avg: 0.052 - elpased: 130.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-172.model\n",
            "batch_i0 173 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 173 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0618 - train_eval_avg: -0.01 - test_eval_avg: 0.048 - elpased: 108.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-173.model\n",
            "batch_i0 174 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 174 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0618 - train_eval_avg: 0.009 - test_eval_avg: -0.024 - elpased: 97.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-174.model\n",
            "batch_i0 175 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 175 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0606 - train_eval_avg: 0.04 - test_eval_avg: 0.0 - elpased: 114.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-175.model\n",
            "batch_i0 176 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 176 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0622 - train_eval_avg: -0.008 - test_eval_avg: 0.048 - elpased: 130.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-176.model\n",
            "batch_i0 177 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 177 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0618 - train_eval_avg: 0.021 - test_eval_avg: 0.108 - elpased: 126.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-177.model\n",
            "batch_i0 178 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 178 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0612 - train_eval_avg: 0.007 - test_eval_avg: 0.0 - elpased: 163.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-178.model\n",
            "batch_i0 179 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 179 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0613 - train_eval_avg: 0.01 - test_eval_avg: 0.024 - elpased: 114.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-179.model\n",
            "batch_i0 180 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 180 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0607 - train_eval_avg: 0.002 - test_eval_avg: 0.024 - elpased: 106.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-180.model\n",
            "batch_i0 181 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 181 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.062 - train_eval_avg: 0.043 - test_eval_avg: 0.084 - elpased: 99.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-181.model\n",
            "batch_i0 182 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 182 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0614 - train_eval_avg: -0.002 - test_eval_avg: 0.024 - elpased: 97.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-182.model\n",
            "batch_i0 183 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 183 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0615 - train_eval_avg: 0.007 - test_eval_avg: 0.0 - elpased: 100.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-183.model\n",
            "batch_i0 184 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 184 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0618 - train_eval_avg: 0.033 - test_eval_avg: 0.024 - elpased: 111.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-184.model\n",
            "batch_i0 185 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 185 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0625 - train_eval_avg: 0.032 - test_eval_avg: -0.024 - elpased: 103.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-185.model\n",
            "batch_i0 186 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 186 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0605 - train_eval_avg: 0.034 - test_eval_avg: 0.024 - elpased: 94.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-186.model\n",
            "batch_i0 187 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 187 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0626 - train_eval_avg: 0.034 - test_eval_avg: 0.016 - elpased: 92.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-187.model\n",
            "batch_i0 188 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 188 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0623 - train_eval_avg: -0.001 - test_eval_avg: 0.072 - elpased: 101.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-188.model\n",
            "batch_i0 189 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 189 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0613 - train_eval_avg: 0.041 - test_eval_avg: 0.0 - elpased: 94.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-189.model\n",
            "batch_i0 190 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 190 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0622 - train_eval_avg: 0.009 - test_eval_avg: -0.048 - elpased: 86.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-190.model\n",
            "batch_i0 191 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 191 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0621 - train_eval_avg: 0.025 - test_eval_avg: 0.0 - elpased: 86.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-191.model\n",
            "batch_i0 192 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 192 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0618 - train_eval_avg: 0.053 - test_eval_avg: 0.072 - elpased: 86.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-192.model\n",
            "batch_i0 193 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 193 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0613 - train_eval_avg: -0.002 - test_eval_avg: -0.004 - elpased: 84.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-193.model\n",
            "batch_i0 194 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 194 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0613 - train_eval_avg: 0.024 - test_eval_avg: 0.012 - elpased: 85.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-194.model\n",
            "batch_i0 195 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 195 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0606 - train_eval_avg: 0.022 - test_eval_avg: 0.024 - elpased: 87.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-195.model\n",
            "batch_i0 196 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 196 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0605 - train_eval_avg: 0.038 - test_eval_avg: 0.012 - elpased: 96.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-196.model\n",
            "batch_i0 197 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 197 out of 236 - train_loss_avg: 0.0619 -  test_loss_avg: 0.0621 - train_eval_avg: 0.015 - test_eval_avg: 0.072 - elpased: 99.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-197.model\n",
            "batch_i0 198 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 198 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0625 - train_eval_avg: 0.035 - test_eval_avg: 0.056 - elpased: 97.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-198.model\n",
            "batch_i0 199 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 199 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0607 - train_eval_avg: 0.039 - test_eval_avg: -0.024 - elpased: 88.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-199.model\n",
            "batch_i0 200 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 200 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.062 - train_eval_avg: 0.043 - test_eval_avg: 0.16 - elpased: 86.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-200.model\n",
            "batch_i0 201 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 201 out of 236 - train_loss_avg: 0.062 -  test_loss_avg: 0.0609 - train_eval_avg: 0.063 - test_eval_avg: 0.044 - elpased: 87.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-201.model\n",
            "batch_i0 202 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 202 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.062 - train_eval_avg: 0.035 - test_eval_avg: 0.02 - elpased: 86.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-202.model\n",
            "batch_i0 203 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 203 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0618 - train_eval_avg: 0.049 - test_eval_avg: 0.06 - elpased: 85.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-203.model\n",
            "batch_i0 204 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 204 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0611 - train_eval_avg: 0.009 - test_eval_avg: -0.04 - elpased: 87.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-204.model\n",
            "batch_i0 205 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 205 out of 236 - train_loss_avg: 0.0621 -  test_loss_avg: 0.0612 - train_eval_avg: 0.013 - test_eval_avg: 0.052 - elpased: 87.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-205.model\n",
            "batch_i0 206 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 206 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0626 - train_eval_avg: 0.073 - test_eval_avg: 0.176 - elpased: 88.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-206.model\n",
            "batch_i0 207 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 207 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0607 - train_eval_avg: 0.056 - test_eval_avg: 0.088 - elpased: 85.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-207.model\n",
            "batch_i0 208 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 208 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0616 - train_eval_avg: 0.044 - test_eval_avg: 0.02 - elpased: 85.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-208.model\n",
            "batch_i0 209 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 209 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0619 - train_eval_avg: 0.047 - test_eval_avg: 0.156 - elpased: 96.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-209.model\n",
            "batch_i0 210 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 210 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0611 - train_eval_avg: 0.051 - test_eval_avg: 0.012 - elpased: 105.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-210.model\n",
            "batch_i0 211 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 211 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.061 - train_eval_avg: 0.048 - test_eval_avg: -0.012 - elpased: 101.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-211.model\n",
            "batch_i0 212 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 212 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.061 - train_eval_avg: 0.038 - test_eval_avg: 0.028 - elpased: 98.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-212.model\n",
            "batch_i0 213 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 213 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.062 - train_eval_avg: 0.072 - test_eval_avg: 0.104 - elpased: 95.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-213.model\n",
            "batch_i0 214 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 214 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0615 - train_eval_avg: 0.094 - test_eval_avg: 0.096 - elpased: 93.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-214.model\n",
            "batch_i0 215 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 215 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.062 - train_eval_avg: 0.065 - test_eval_avg: 0.1 - elpased: 97.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-215.model\n",
            "batch_i0 216 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 216 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0608 - train_eval_avg: 0.061 - test_eval_avg: -0.008 - elpased: 101.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-216.model\n",
            "batch_i0 217 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 217 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0605 - train_eval_avg: 0.128 - test_eval_avg: 0.092 - elpased: 100.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-217.model\n",
            "batch_i0 218 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 218 out of 236 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0612 - train_eval_avg: 0.07 - test_eval_avg: 0.024 - elpased: 100.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-218.model\n",
            "batch_i0 219 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 219 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0617 - train_eval_avg: 0.121 - test_eval_avg: 0.22 - elpased: 102.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-219.model\n",
            "batch_i0 220 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 220 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0612 - train_eval_avg: 0.108 - test_eval_avg: 0.096 - elpased: 102.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-220.model\n",
            "batch_i0 221 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 221 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0615 - train_eval_avg: 0.149 - test_eval_avg: 0.212 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-221.model\n",
            "batch_i0 222 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 222 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.061 - train_eval_avg: 0.132 - test_eval_avg: 0.052 - elpased: 103.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-222.model\n",
            "batch_i0 223 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 223 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0606 - train_eval_avg: 0.144 - test_eval_avg: 0.02 - elpased: 107.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-223.model\n",
            "batch_i0 224 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 224 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0608 - train_eval_avg: 0.084 - test_eval_avg: 0.112 - elpased: 106.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-224.model\n",
            "batch_i0 225 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 225 out of 236 - train_loss_avg: 0.0618 -  test_loss_avg: 0.0606 - train_eval_avg: 0.139 - test_eval_avg: 0.088 - elpased: 104.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-225.model\n",
            "batch_i0 226 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 226 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0604 - train_eval_avg: 0.231 - test_eval_avg: 0.032 - elpased: 102.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-226.model\n",
            "batch_i0 227 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 227 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.062 - train_eval_avg: 0.02 - test_eval_avg: 0.08 - elpased: 104.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-227.model\n",
            "batch_i0 228 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 228 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0603 - train_eval_avg: 0.182 - test_eval_avg: 0.312 - elpased: 100.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-228.model\n",
            "batch_i0 229 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 229 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0607 - train_eval_avg: 0.127 - test_eval_avg: -0.008 - elpased: 105.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-229.model\n",
            "batch_i0 230 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 230 out of 236 - train_loss_avg: 0.0614 -  test_loss_avg: 0.0602 - train_eval_avg: 0.153 - test_eval_avg: 0.28 - elpased: 99.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-230.model\n",
            "batch_i0 231 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 231 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0622 - train_eval_avg: 0.168 - test_eval_avg: 0.22 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-231.model\n",
            "batch_i0 232 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 232 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.0614 - train_eval_avg: 0.175 - test_eval_avg: 0.044 - elpased: 95.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-232.model\n",
            "batch_i0 233 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 233 out of 236 - train_loss_avg: 0.0608 -  test_loss_avg: 0.0603 - train_eval_avg: 0.117 - test_eval_avg: 0.124 - elpased: 97.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-233.model\n",
            "batch_i0 234 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 234 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0606 - train_eval_avg: 0.208 - test_eval_avg: 0.24 - elpased: 98.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-234.model\n",
            "batch_i0 235 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 0 - batch: 235 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0599 - train_eval_avg: 0.198 - test_eval_avg: 0.248 - elpased: 97.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-235.model\n",
            "batch_i0 236 cur_train_items 207 cur_test_items 52\n",
            "training 0 out of: 207\n",
            "testing 0 out of: 52\n",
            "Epoch: 0 - batch: 236 out of 236 - train_loss_avg: 0.0603 -  test_loss_avg: 0.0616 - train_eval_avg: 0.1787 - test_eval_avg: 0.0769 - elpased: 21.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-236.model\n",
            "Epoch: 0 - train_loss_avg: 0.0617 -  test_loss_avg: 0.0616 - train_eval_avg: 0.0265 - test_eval_avg: 0.027\n",
            "model saved\n",
            "deleted temporary files\n",
            "-----------\n",
            "batch_i0 0 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 0 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.0607 - train_eval_avg: 0.268 - test_eval_avg: 0.044 - elpased: 101.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-0.model\n",
            "batch_i0 1 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 1 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0612 - train_eval_avg: 0.243 - test_eval_avg: 0.336 - elpased: 103.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-1.model\n",
            "batch_i0 2 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 2 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0606 - train_eval_avg: 0.198 - test_eval_avg: 0.136 - elpased: 104.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-2.model\n",
            "batch_i0 3 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 3 out of 236 - train_loss_avg: 0.0616 -  test_loss_avg: 0.0608 - train_eval_avg: 0.316 - test_eval_avg: 0.448 - elpased: 101.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-3.model\n",
            "batch_i0 4 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 4 out of 236 - train_loss_avg: 0.0615 -  test_loss_avg: 0.0605 - train_eval_avg: 0.149 - test_eval_avg: 0.256 - elpased: 105.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-4.model\n",
            "batch_i0 5 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 5 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0596 - train_eval_avg: 0.26 - test_eval_avg: 0.188 - elpased: 107.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-5.model\n",
            "batch_i0 6 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 6 out of 236 - train_loss_avg: 0.0607 -  test_loss_avg: 0.0609 - train_eval_avg: 0.284 - test_eval_avg: 0.14 - elpased: 110.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-6.model\n",
            "batch_i0 7 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 7 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.061 - train_eval_avg: 0.297 - test_eval_avg: 0.268 - elpased: 108.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-7.model\n",
            "batch_i0 8 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 8 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0599 - train_eval_avg: 0.261 - test_eval_avg: 0.112 - elpased: 126.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-8.model\n",
            "batch_i0 9 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 9 out of 236 - train_loss_avg: 0.0612 -  test_loss_avg: 0.0613 - train_eval_avg: 0.106 - test_eval_avg: 0.292 - elpased: 122.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-9.model\n",
            "batch_i0 10 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 10 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0621 - train_eval_avg: 0.284 - test_eval_avg: 0.14 - elpased: 115.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-10.model\n",
            "batch_i0 11 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 11 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0605 - train_eval_avg: 0.2 - test_eval_avg: 0.304 - elpased: 106.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-11.model\n",
            "batch_i0 12 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 12 out of 236 - train_loss_avg: 0.0605 -  test_loss_avg: 0.0592 - train_eval_avg: 0.193 - test_eval_avg: 0.172 - elpased: 108.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-12.model\n",
            "batch_i0 13 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 13 out of 236 - train_loss_avg: 0.0611 -  test_loss_avg: 0.0614 - train_eval_avg: 0.276 - test_eval_avg: 0.392 - elpased: 105.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-13.model\n",
            "batch_i0 14 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 14 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0608 - train_eval_avg: 0.204 - test_eval_avg: 0.332 - elpased: 97.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-14.model\n",
            "batch_i0 15 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 15 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0606 - train_eval_avg: 0.297 - test_eval_avg: 0.336 - elpased: 96.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-15.model\n",
            "batch_i0 16 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 16 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0606 - train_eval_avg: 0.222 - test_eval_avg: 0.336 - elpased: 95.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-16.model\n",
            "batch_i0 17 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 17 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.061 - train_eval_avg: 0.224 - test_eval_avg: 0.284 - elpased: 88.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-17.model\n",
            "batch_i0 18 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 18 out of 236 - train_loss_avg: 0.0609 -  test_loss_avg: 0.0599 - train_eval_avg: 0.238 - test_eval_avg: 0.444 - elpased: 102.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-18.model\n",
            "batch_i0 19 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 19 out of 236 - train_loss_avg: 0.0605 -  test_loss_avg: 0.0608 - train_eval_avg: 0.222 - test_eval_avg: 0.096 - elpased: 95.7\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-19.model\n",
            "batch_i0 20 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 20 out of 236 - train_loss_avg: 0.0602 -  test_loss_avg: 0.0599 - train_eval_avg: 0.503 - test_eval_avg: 0.444 - elpased: 86.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-20.model\n",
            "batch_i0 21 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 21 out of 236 - train_loss_avg: 0.0602 -  test_loss_avg: 0.06 - train_eval_avg: 0.261 - test_eval_avg: 0.256 - elpased: 85.5\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-21.model\n",
            "batch_i0 22 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 22 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0599 - train_eval_avg: 0.371 - test_eval_avg: 0.22 - elpased: 90.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-22.model\n",
            "batch_i0 23 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 23 out of 236 - train_loss_avg: 0.0613 -  test_loss_avg: 0.0597 - train_eval_avg: 0.469 - test_eval_avg: 0.552 - elpased: 97.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-23.model\n",
            "batch_i0 24 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 24 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0602 - train_eval_avg: 0.401 - test_eval_avg: 0.284 - elpased: 85.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-24.model\n",
            "batch_i0 25 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 25 out of 236 - train_loss_avg: 0.061 -  test_loss_avg: 0.0615 - train_eval_avg: 0.437 - test_eval_avg: 0.268 - elpased: 86.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-25.model\n",
            "batch_i0 26 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 26 out of 236 - train_loss_avg: 0.0603 -  test_loss_avg: 0.0613 - train_eval_avg: 0.362 - test_eval_avg: 0.332 - elpased: 84.8\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-26.model\n",
            "batch_i0 27 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 27 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0599 - train_eval_avg: 0.364 - test_eval_avg: 0.32 - elpased: 85.1\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-27.model\n",
            "batch_i0 28 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 28 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.0597 - train_eval_avg: 0.287 - test_eval_avg: 0.084 - elpased: 86.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-28.model\n",
            "batch_i0 29 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 29 out of 236 - train_loss_avg: 0.0608 -  test_loss_avg: 0.0612 - train_eval_avg: 0.296 - test_eval_avg: 0.4 - elpased: 91.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-29.model\n",
            "batch_i0 30 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 30 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0608 - train_eval_avg: 0.275 - test_eval_avg: 0.176 - elpased: 93.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-30.model\n",
            "batch_i0 31 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 31 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0603 - train_eval_avg: 0.304 - test_eval_avg: 0.452 - elpased: 88.0\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-31.model\n",
            "batch_i0 32 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 32 out of 236 - train_loss_avg: 0.0604 -  test_loss_avg: 0.06 - train_eval_avg: 0.3 - test_eval_avg: 0.408 - elpased: 85.2\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-32.model\n",
            "batch_i0 33 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 33 out of 236 - train_loss_avg: 0.0602 -  test_loss_avg: 0.061 - train_eval_avg: 0.321 - test_eval_avg: 0.316 - elpased: 85.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-33.model\n",
            "batch_i0 34 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 34 out of 236 - train_loss_avg: 0.0607 -  test_loss_avg: 0.0607 - train_eval_avg: 0.428 - test_eval_avg: 0.4 - elpased: 88.3\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-34.model\n",
            "batch_i0 35 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 35 out of 236 - train_loss_avg: 0.06 -  test_loss_avg: 0.06 - train_eval_avg: 0.282 - test_eval_avg: 0.452 - elpased: 88.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-35.model\n",
            "batch_i0 36 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 36 out of 236 - train_loss_avg: 0.0603 -  test_loss_avg: 0.0605 - train_eval_avg: 0.344 - test_eval_avg: 0.344 - elpased: 87.9\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-36.model\n",
            "batch_i0 37 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 37 out of 236 - train_loss_avg: 0.0601 -  test_loss_avg: 0.0614 - train_eval_avg: 0.416 - test_eval_avg: 0.332 - elpased: 89.6\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-37.model\n",
            "batch_i0 38 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n",
            "testing 0 out of: 250\n",
            "Epoch: 1 - batch: 38 out of 236 - train_loss_avg: 0.0606 -  test_loss_avg: 0.0604 - train_eval_avg: 0.529 - test_eval_avg: 0.436 - elpased: 90.4\n",
            "models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-38.model\n",
            "batch_i0 39 cur_train_items 1000 cur_test_items 250\n",
            "training 0 out of: 1000\n"
          ]
        }
      ],
      "source": [
        "#OLD\n",
        "import torch, random, time, math\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "#params={}\n",
        "def extract_features():\n",
        "  return\n",
        "exp_name=\"new-stock-pred-test29-0\"\n",
        "exp_name=\"new-stock-pred-test29-64-L2\"\n",
        "exp_name=\"new-stock-pred-test29-64-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-pred-test39-64-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-pred-test39-128-L2-LR1e-6\"\n",
        "exp_name=\"new-stock-unflattened-000001\"\n",
        "exp_name=\"new-stock-unflattened-128-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample4-32-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample-30-10-128-000001\"\n",
        "exp_name=\"new-stock-unflattened-sample-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-00000001\"\n",
        "exp_name=\"test-batches-1layer-000001-256\"\n",
        "exp_name=\"batches-1layer-000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-1layer-0000001-256-5day-pred-39stocks\"\n",
        "exp_name=\"batches-2layer-0000001-256-5day-pred-39stocks\"\n",
        "\n",
        "n_layers=2#4#3\n",
        "n_hidden=256 #128 #128 #256 #64#64\n",
        "#LR=0.0000001 #0.0000001\n",
        "#LR=0.0000001\n",
        "#LR=0.00000001\n",
        "LR=0.0000001\n",
        "n_epochs=100\n",
        "n_data=None #number of items per source\n",
        "train_batch_size=1000\n",
        "cur_matching_in_out=False\n",
        "train_ratio=0.8\n",
        "model_dir=\"models\"\n",
        "#output_labels=standard_labels=ipa_symbol_list #combined_ipa_list\n",
        "standard_labels=cur_params[\"standard_labels\"]\n",
        "\n",
        "n_batches=math.floor(len(train_data)/train_batch_size)\n",
        "if n_batches==0: test_batch_size=len(test_data)\n",
        "else: test_batch_size=math.floor(len(test_data)/n_batches)\n",
        "\n",
        "\n",
        "\n",
        "item0=train_data[0]\n",
        "ft_list,lb_list=item0[:2]\n",
        "ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "n_input=ft_list_tensor.shape[-1]\n",
        "n_output=lb_list_tensor.shape[0]\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=cur_matching_in_out).to(device)\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) \n",
        "\n",
        "\n",
        "\n",
        "print(ft_list_tensor.shape)\n",
        "print(lb_list_tensor.shape)\n",
        "exp_dir_path=os.path.join(model_dir,exp_name)\n",
        "if not os.path.exists(exp_dir_path): os.makedirs(exp_dir_path)\n",
        "tmp_model_dir=os.path.join(exp_dir_path,\"tmp\") \n",
        "if not os.path.exists(tmp_model_dir): os.makedirs(tmp_model_dir)\n",
        "\n",
        "\n",
        "log_fpath=os.path.join(exp_dir_path,\"log.txt\")\n",
        "log_something(str(rnn),log_fpath)\n",
        "\n",
        "\n",
        "\n",
        "#ft_list_tensor=ft_list_tensor.reshape([1,n_input])\n",
        "rnn_out=rnn(ft_list_tensor)\n",
        "print(\"rnn_out\",rnn_out.shape)\n",
        "\n",
        "for epoch_i in range(n_epochs):\n",
        "  PATH=os.path.join(exp_dir_path,\"model-%s.model\"%epoch_i) #tmp_path\n",
        "  if os.path.exists(PATH):\n",
        "    try: checkpoint = torch.load(PATH)\n",
        "    except: checkpoint = dill_unpickle(PATH)\n",
        "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"loaded model for this epoch\",PATH)\n",
        "    rnn.train()\n",
        "    continue  \n",
        "  rnn.zero_grad()\n",
        "  train_loss_items,test_loss_items=[],[]\n",
        "  train_eval_items,test_eval_items=[],[]\n",
        "  for batch_i0 in range(n_batches+1):\n",
        "    t0=time.time()\n",
        "    \n",
        "    #pred_count,correct_count=0,0\n",
        "    batch_i1=batch_i0+1\n",
        "    cur_train_items=train_data[batch_i0*train_batch_size:batch_i1*train_batch_size]\n",
        "    cur_test_items=test_data[batch_i0*test_batch_size:batch_i1*test_batch_size]\n",
        "    print(\"batch_i0\",batch_i0, \"cur_train_items\",len(cur_train_items),\"cur_test_items\",len(cur_test_items))\n",
        "    tmp_path=os.path.join(tmp_model_dir, \"model-batch-%s.model\"%batch_i0)\n",
        "    if os.path.exists(tmp_path):\n",
        "      try: checkpoint = torch.load(tmp_path)\n",
        "      except: checkpoint = dill_unpickle(tmp_path)\n",
        "      rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "      print(\"loaded model for this epoch\",tmp_path)\n",
        "      rnn.train()\n",
        "      continue  \n",
        "\n",
        "\n",
        "    for item_i,item0 in enumerate(cur_train_items):\n",
        "      if item_i%5000==0: print(\"training\",item_i, \"out of:\",len(cur_train_items))\n",
        "      ft_list,lb_list=item0[:2]\n",
        "      actual_outcome=item0[2]\n",
        "      ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "      lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "      lb_tensor_flat=lb_list_tensor.ravel()\n",
        "      rnn_out=rnn(ft_list_tensor)\n",
        "      rnn_out_flat=rnn_out.ravel()\n",
        "      loss = loss_func(rnn_out_flat,lb_tensor_flat) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "      train_loss_items.append(loss.item())\n",
        "      cur_eval_item=eval_pred(rnn_out,actual_outcome,cur_params[\"standard_labels\"])\n",
        "      train_eval_items.append(cur_eval_item)\n",
        "\n",
        "    for item_i,item0 in enumerate(cur_test_items):\n",
        "      if item_i%5000==0: print(\"testing\",item_i, \"out of:\",len(cur_test_items))\n",
        "      ft_list,lb_list=item0[:2]\n",
        "      actual_outcome=item0[2]\n",
        "      ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "      lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "      lb_tensor_flat=lb_list_tensor.ravel()\n",
        "      rnn_out=rnn(ft_list_tensor)\n",
        "      rnn_out_flat=rnn_out.ravel()\n",
        "      loss = loss_func(rnn_out_flat,lb_tensor_flat) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      test_loss_items.append(loss.item())\n",
        "      cur_eval_item=eval_pred(rnn_out,actual_outcome,cur_params[\"standard_labels\"])\n",
        "      test_eval_items.append(cur_eval_item)\n",
        "\n",
        "    train_batch_loss_avg=sum(train_loss_items[-len(cur_train_items):])/len(cur_train_items)  \n",
        "    test_batch_loss_avg=sum(test_loss_items[-len(cur_test_items):])/len(cur_test_items)\n",
        "    train_batch_eval_avg=sum(train_eval_items[-len(cur_train_items):])/len(cur_train_items)  \n",
        "    test_batch_eval_avg=sum(test_eval_items[-len(cur_test_items):])/len(cur_test_items)\n",
        "\n",
        "    train_loss_avg=sum(train_loss_items)/len(train_loss_items)  \n",
        "    test_loss_avg=sum(test_loss_items)/len(test_loss_items)\n",
        "    train_eval_avg=sum(train_eval_items)/len(train_eval_items)  \n",
        "    test_eval_avg=sum(test_eval_items)/len(test_eval_items)\n",
        "\n",
        "    cur_checkpoint={\n",
        "              'epoch': epoch_i,\n",
        "              'n_input': n_input,\n",
        "              'n_hidden': n_hidden,\n",
        "              'n_layers': n_layers,\n",
        "              'n_output': n_output,\n",
        "              'output_labels': standard_labels,\n",
        "              'model_state_dict': rnn.state_dict(),\n",
        "              'LR': LR,\n",
        "              'matching_in_out':cur_matching_in_out,\n",
        "              'train_loss': train_loss_avg,\n",
        "              'test_loss': test_loss_avg,\n",
        "              'train_eval': train_eval_avg,\n",
        "              'test_eval': test_eval_avg,\n",
        "              'feature_extraction_parameters':cur_params,\n",
        "              'feature_extraction_function':extract_features   \n",
        "              }\n",
        "    dill_pickle(cur_checkpoint, tmp_path)\n",
        "    t1=time.time()\n",
        "    elapsed=round(t1-t0,1)\n",
        "    line=\"Epoch: %s - batch: %s out of %s - train_loss_avg: %s -  test_loss_avg: %s - train_eval_avg: %s - test_eval_avg: %s - elpased: %s\"%(epoch_i,batch_i0,n_batches,round(train_batch_loss_avg,4),round(test_batch_loss_avg,4),round(train_batch_eval_avg,4),round(test_batch_eval_avg,4),elapsed)\n",
        "    print(line)\n",
        "    print(tmp_path)\n",
        "    log_something(line,log_fpath)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  #train_eval_avg,test_eval_avg=0,0  \n",
        "  #print(\"epoch_i\",epoch_i,\"train_loss_avg\",round(train_loss_avg,4),\"test_loss_avg\",round(test_loss_avg,4)) \n",
        "  line=\"Epoch: %s - train_loss_avg: %s -  test_loss_avg: %s - train_eval_avg: %s - test_eval_avg: %s\"%(epoch_i,round(train_loss_avg,4),round(test_loss_avg,4),round(train_eval_avg,4),round(test_eval_avg,4))\n",
        "  #line=\"Epoch # %s  - Batch: %s / %s -  train loss: %s - test loss: %s - train eval: %s - test eval: %s - elapsed: %s\"%(epoch0, batch_i0, n_batches, avg_train_loss,avg_test_loss, avg_train_eval,avg_test_eval, elapsed)\n",
        "  print(line)\n",
        "  log_something(line,log_fpath)\n",
        "  dill_pickle(cur_checkpoint, PATH)\n",
        "  print(\"model saved\")\n",
        "  for f in os.listdir(tmp_model_dir):\n",
        "    tmp_fpath=os.path.join(tmp_model_dir,f)\n",
        "    os.remove(tmp_fpath)\n",
        "  print(\"deleted temporary files\")\n",
        "  print(\"-----------\")\n",
        "\n",
        "\n",
        "\n",
        "    #torch.save(cur_checkpoint, tmp_path)\n",
        "  \n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S6CrmnNppgkw",
        "outputId": "4003c2fe-0445-4e0b-a09c-20227c312585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "item_i 0 out of: 6706\n",
            "0 actual high: 0 actual low: -3\n",
            "high: (-6, -0.0065) -0.0081 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0215) 0.0318\n",
            "high: (-4, -0.0197) 0.0149 low: (-4, -0.0224) 0.0091\n",
            "high: (-3, -0.0234) -0.0141 low: (-3, 0.0478) 0.0574\n",
            "high: (-2, 0.0068) -0.0057 low: (-2, 0.08) 0.1383\n",
            "high: (-1, 0.0214) 0.0208 low: (-1, 0.3261) 0.4681\n",
            "high: (0, 0.5202) 0.6448 low: (0, 0.5382) 0.5442\n",
            "high: (1, 0.1761) 0.3344 low: (1, 0.0066) -0.0123\n",
            "high: (2, 0.0836) 0.1161 low: (2, 0.0014) -0.019\n",
            "high: (3, 0.0018) 0.0125 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0349) 0.0103 low: (4, -0.0373) -0.0418\n",
            "high: (5, -0.007) 0.0535 low: (5, -0.0002) -0.004\n",
            "high: (6, 0.0502) 0.0622 low: (6, -0.0038) -0.0038\n",
            "1 actual high: -2 actual low: -6\n",
            "high: (-6, 0.025) 0.026 low: (-6, 0.0374) 0.0435\n",
            "high: (-5, 0.0115) 0.038 low: (-5, -0.034) 0.004\n",
            "high: (-4, 0.0242) 0.0632 low: (-4, 0.0366) 0.0465\n",
            "high: (-3, 0.0144) 0.0782 low: (-3, 0.0598) 0.116\n",
            "high: (-2, 0.026) 0.1053 low: (-2, 0.1027) 0.2354\n",
            "high: (-1, 0.0534) 0.1609 low: (-1, 0.1676) 0.4302\n",
            "high: (0, 0.3918) 0.4081 low: (0, 0.4176) 0.4855\n",
            "high: (1, 0.1241) 0.4309 low: (1, 0.0489) 0.0843\n",
            "high: (2, 0.0903) 0.3017 low: (2, 0.0348) 0.0274\n",
            "high: (3, 0.0883) 0.2076 low: (3, 0.0182) -0.013\n",
            "high: (4, 0.0415) 0.1156 low: (4, -0.0111) -0.0342\n",
            "high: (5, -0.0042) 0.0724 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0737) 0.0768 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 0 actual low: -3\n",
            "high: (-6, 0.0163) 0.0222 low: (-6, 0.104) 0.097\n",
            "high: (-5, 0.0163) 0.0443 low: (-5, -0.0014) 0.0957\n",
            "high: (-4, 0.0108) 0.059 low: (-4, 0.0418) 0.1347\n",
            "high: (-3, 0.0113) 0.0744 low: (-3, 0.0668) 0.197\n",
            "high: (-2, 0.029) 0.1138 low: (-2, 0.0422) 0.2364\n",
            "high: (-1, 0.0838) 0.2278 low: (-1, 0.1979) 0.4209\n",
            "high: (0, 0.3179) 0.4324 low: (0, 0.3918) 0.3655\n",
            "high: (1, 0.0926) 0.3398 low: (1, 0.0754) 0.2136\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0672) 0.1433\n",
            "high: (3, 0.0538) 0.1721 low: (3, 0.0519) 0.0806\n",
            "high: (4, -0.0356) 0.0989 low: (4, 0.0166) 0.0322\n",
            "high: (5, -0.0001) 0.1473 low: (5, -0.0098) 0.0167\n",
            "high: (6, 0.1084) 0.1474 low: (6, 0.0277) 0.0258\n",
            "3 actual high: -1 actual low: -4\n",
            "high: (-6, 0.0273) 0.032 low: (-6, 0.1259) 0.1571\n",
            "high: (-5, 0.0001) 0.0321 low: (-5, 0.0199) 0.1819\n",
            "high: (-4, 0.0181) 0.0534 low: (-4, 0.0369) 0.2279\n",
            "high: (-3, 0.0461) 0.1074 low: (-3, 0.039) 0.2766\n",
            "high: (-2, 0.0594) 0.1771 low: (-2, 0.0704) 0.3644\n",
            "high: (-1, 0.0894) 0.282 low: (-1, 0.0455) 0.4212\n",
            "high: (0, 0.2802) 0.3286 low: (0, 0.2609) 0.3255\n",
            "high: (1, 0.0596) 0.3894 low: (1, 0.0696) 0.2534\n",
            "high: (2, 0.1148) 0.3195 low: (2, 0.0687) 0.1665\n",
            "high: (3, 0.0704) 0.1848 low: (3, 0.0009) 0.0808\n",
            "high: (4, 0.0396) 0.1023 low: (4, 0.0267) 0.0797\n",
            "high: (5, 0.0) 0.0558 low: (5, -0.0037) 0.0464\n",
            "high: (6, 0.0476) 0.0558 low: (6, 0.0409) 0.051\n",
            "4 actual high: 0 actual low: -4\n",
            "high: (-6, 0.057) 0.0757 low: (-6, 0.1696) 0.1614\n",
            "high: (-5, 0.0025) 0.079 low: (-5, -0.0007) 0.1607\n",
            "high: (-4, -0.0646) -0.0068 low: (-4, 0.0086) 0.1689\n",
            "high: (-3, 0.0379) 0.0436 low: (-3, 0.0918) 0.2563\n",
            "high: (-2, 0.0182) 0.0677 low: (-2, 0.057) 0.3105\n",
            "high: (-1, 0.0488) 0.1325 low: (-1, 0.0912) 0.3973\n",
            "high: (0, 0.2475) 0.3286 low: (0, 0.3008) 0.2862\n",
            "high: (1, 0.0756) 0.5388 low: (1, 0.118) 0.3165\n",
            "high: (2, 0.0538) 0.4385 low: (2, 0.0665) 0.2042\n",
            "high: (3, 0.0914) 0.367 low: (3, 0.0576) 0.1409\n",
            "high: (4, 0.0568) 0.2457 low: (4, 0.0192) 0.0861\n",
            "high: (5, 0.0021) 0.1702 low: (5, 0.0153) 0.0678\n",
            "high: (6, 0.1261) 0.1674 low: (6, 0.056) 0.0533\n",
            "------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 actual high: 1 actual low: -2\n",
            "high: (-6, -0.007) -0.0087 low: (-6, 0.01) 0.0102\n",
            "high: (-5, 0.0383) 0.0391 low: (-5, 0.0216) 0.0321\n",
            "high: (-4, -0.0197) 0.0145 low: (-4, -0.0221) 0.0096\n",
            "high: (-3, -0.0235) -0.0148 low: (-3, 0.048) 0.0584\n",
            "high: (-2, 0.0064) -0.0069 low: (-2, 0.0797) 0.1393\n",
            "high: (-1, 0.0213) 0.0197 low: (-1, 0.3242) 0.4684\n",
            "high: (0, 0.5174) 0.6456 low: (0, 0.5352) 0.5434\n",
            "high: (1, 0.175) 0.3347 low: (1, 0.0064) -0.0118\n",
            "high: (2, 0.0834) 0.1163 low: (2, 0.0019) -0.0183\n",
            "high: (3, 0.0021) 0.0122 low: (3, 0.0212) -0.0202\n",
            "high: (4, -0.035) 0.0096 low: (4, -0.0376) -0.0417\n",
            "high: (5, -0.0072) 0.0533 low: (5, 0.0002) -0.0036\n",
            "high: (6, 0.0499) 0.0623 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0249) 0.0261 low: (-6, 0.0369) 0.0432\n",
            "high: (-5, 0.0113) 0.0379 low: (-5, -0.0336) 0.0039\n",
            "high: (-4, 0.0237) 0.0628 low: (-4, 0.0365) 0.0466\n",
            "high: (-3, 0.0147) 0.0782 low: (-3, 0.0592) 0.1159\n",
            "high: (-2, 0.0262) 0.1056 low: (-2, 0.1025) 0.2358\n",
            "high: (-1, 0.0526) 0.1607 low: (-1, 0.1667) 0.4309\n",
            "high: (0, 0.3893) 0.4079 low: (0, 0.4148) 0.4855\n",
            "high: (1, 0.1238) 0.4314 low: (1, 0.0481) 0.0836\n",
            "high: (2, 0.0902) 0.3017 low: (2, 0.0348) 0.0273\n",
            "high: (3, 0.0875) 0.2071 low: (3, 0.0183) -0.0135\n",
            "high: (4, 0.0414) 0.1155 low: (4, -0.0115) -0.0349\n",
            "high: (5, -0.0043) 0.0721 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0731) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 2 actual low: -1\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.1033) 0.0969\n",
            "high: (-5, 0.0164) 0.0449 low: (-5, -0.0018) 0.0952\n",
            "high: (-4, 0.0108) 0.0597 low: (-4, 0.0416) 0.1343\n",
            "high: (-3, 0.0107) 0.0744 low: (-3, 0.0667) 0.1968\n",
            "high: (-2, 0.0287) 0.1137 low: (-2, 0.0417) 0.236\n",
            "high: (-1, 0.083) 0.2273 low: (-1, 0.1969) 0.4207\n",
            "high: (0, 0.3162) 0.433 low: (0, 0.3891) 0.3651\n",
            "high: (1, 0.0921) 0.3397 low: (1, 0.0753) 0.2142\n",
            "high: (2, 0.0306) 0.2136 low: (2, 0.0672) 0.1436\n",
            "high: (3, 0.053) 0.1717 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0351) 0.0991 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0003) 0.1472 low: (5, -0.01) 0.0166\n",
            "high: (6, 0.1078) 0.1476 low: (6, 0.0277) 0.026\n",
            "3 actual high: 3 actual low: -1\n",
            "high: (-6, 0.027) 0.0317 low: (-6, 0.1258) 0.1576\n",
            "high: (-5, 0.0) 0.0317 low: (-5, 0.0198) 0.1824\n",
            "high: (-4, 0.0181) 0.053 low: (-4, 0.0369) 0.2286\n",
            "high: (-3, 0.0463) 0.1074 low: (-3, 0.0388) 0.2772\n",
            "high: (-2, 0.0595) 0.1774 low: (-2, 0.0698) 0.3646\n",
            "high: (-1, 0.0891) 0.2821 low: (-1, 0.0456) 0.4218\n",
            "high: (0, 0.279) 0.3279 low: (0, 0.2597) 0.3253\n",
            "high: (1, 0.0594) 0.39 low: (1, 0.0694) 0.2529\n",
            "high: (2, 0.1149) 0.3202 low: (2, 0.0684) 0.166\n",
            "high: (3, 0.07) 0.1851 low: (3, 0.0005) 0.0803\n",
            "high: (4, 0.0393) 0.1028 low: (4, 0.0267) 0.0797\n",
            "high: (5, 0.0006) 0.0567 low: (5, -0.0043) 0.0462\n",
            "high: (6, 0.0476) 0.0559 low: (6, 0.0412) 0.0516\n",
            "4 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0568) 0.0756 low: (-6, 0.1686) 0.1612\n",
            "high: (-5, 0.0024) 0.0788 low: (-5, -0.0007) 0.1605\n",
            "high: (-4, -0.064) -0.0064 low: (-4, 0.0084) 0.1685\n",
            "high: (-3, 0.0381) 0.0443 low: (-3, 0.0918) 0.2563\n",
            "high: (-2, 0.018) 0.0683 low: (-2, 0.0564) 0.3102\n",
            "high: (-1, 0.049) 0.1335 low: (-1, 0.0909) 0.3971\n",
            "high: (0, 0.246) 0.3275 low: (0, 0.2992) 0.286\n",
            "high: (1, 0.0757) 0.5389 low: (1, 0.1173) 0.3169\n",
            "high: (2, 0.0532) 0.4382 low: (2, 0.0661) 0.2048\n",
            "high: (3, 0.091) 0.3673 low: (3, 0.0574) 0.1416\n",
            "high: (4, 0.057) 0.2462 low: (4, 0.019) 0.0867\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0154) 0.0685\n",
            "high: (6, 0.1258) 0.1675 low: (6, 0.0563) 0.0538\n",
            "------\n",
            "0 actual high: 4 actual low: 1\n",
            "high: (-6, -0.007) -0.0087 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0383) 0.0391 low: (-5, 0.0216) 0.0322\n",
            "high: (-4, -0.0197) 0.0145 low: (-4, -0.0221) 0.0098\n",
            "high: (-3, -0.0234) -0.0147 low: (-3, 0.048) 0.0585\n",
            "high: (-2, 0.0063) -0.0069 low: (-2, 0.0796) 0.1395\n",
            "high: (-1, 0.0212) 0.0196 low: (-1, 0.3237) 0.4685\n",
            "high: (0, 0.5167) 0.6457 low: (0, 0.5343) 0.5431\n",
            "high: (1, 0.1747) 0.3347 low: (1, 0.0064) -0.0116\n",
            "high: (2, 0.0833) 0.1163 low: (2, 0.002) -0.0181\n",
            "high: (3, 0.0021) 0.0122 low: (3, 0.0212) -0.0201\n",
            "high: (4, -0.035) 0.0096 low: (4, -0.0376) -0.0417\n",
            "high: (5, -0.0072) 0.0534 low: (5, 0.0003) -0.0035\n",
            "high: (6, 0.0499) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0249) 0.0261 low: (-6, 0.0368) 0.0431\n",
            "high: (-5, 0.0112) 0.0379 low: (-5, -0.0336) 0.0038\n",
            "high: (-4, 0.0236) 0.0627 low: (-4, 0.0365) 0.0465\n",
            "high: (-3, 0.0148) 0.0782 low: (-3, 0.0591) 0.1158\n",
            "high: (-2, 0.0262) 0.1057 low: (-2, 0.1025) 0.2359\n",
            "high: (-1, 0.0524) 0.1607 low: (-1, 0.1665) 0.4311\n",
            "high: (0, 0.3886) 0.4079 low: (0, 0.4141) 0.4853\n",
            "high: (1, 0.1236) 0.4314 low: (1, 0.0479) 0.0836\n",
            "high: (2, 0.0902) 0.3017 low: (2, 0.0349) 0.0274\n",
            "high: (3, 0.0873) 0.207 low: (3, 0.0183) -0.0135\n",
            "high: (4, 0.0413) 0.1154 low: (4, -0.0115) -0.0349\n",
            "high: (5, -0.0043) 0.072 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0729) 0.0765 low: (6, -0.0154) -0.018\n",
            "2 actual high: 4 actual low: 0\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.1031) 0.0969\n",
            "high: (-5, 0.0163) 0.0448 low: (-5, -0.0019) 0.0951\n",
            "high: (-4, 0.0108) 0.0597 low: (-4, 0.0415) 0.1341\n",
            "high: (-3, 0.0106) 0.0742 low: (-3, 0.0667) 0.1968\n",
            "high: (-2, 0.0286) 0.1134 low: (-2, 0.0415) 0.2358\n",
            "high: (-1, 0.0828) 0.227 low: (-1, 0.1966) 0.4205\n",
            "high: (0, 0.3158) 0.4331 low: (0, 0.3884) 0.365\n",
            "high: (1, 0.092) 0.34 low: (1, 0.0753) 0.2145\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0672) 0.1437\n",
            "high: (3, 0.0529) 0.1717 low: (3, 0.0516) 0.0805\n",
            "high: (4, -0.035) 0.0991 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0003) 0.1471 low: (5, -0.01) 0.0166\n",
            "high: (6, 0.1076) 0.1476 low: (6, 0.0277) 0.026\n",
            "3 actual high: 4 actual low: 1\n",
            "high: (-6, 0.027) 0.0317 low: (-6, 0.1257) 0.1576\n",
            "high: (-5, 0.0001) 0.0319 low: (-5, 0.0198) 0.1825\n",
            "high: (-4, 0.0181) 0.0531 low: (-4, 0.0369) 0.2287\n",
            "high: (-3, 0.0464) 0.1077 low: (-3, 0.0388) 0.2774\n",
            "high: (-2, 0.0595) 0.1776 low: (-2, 0.0696) 0.3647\n",
            "high: (-1, 0.0891) 0.2824 low: (-1, 0.0455) 0.4217\n",
            "high: (0, 0.2787) 0.3277 low: (0, 0.2594) 0.3253\n",
            "high: (1, 0.0593) 0.39 low: (1, 0.0693) 0.2529\n",
            "high: (2, 0.1149) 0.3202 low: (2, 0.0684) 0.166\n",
            "high: (3, 0.07) 0.1852 low: (3, 0.0004) 0.0803\n",
            "high: (4, 0.0392) 0.1029 low: (4, 0.0267) 0.0798\n",
            "high: (5, 0.0007) 0.0568 low: (5, -0.0044) 0.0463\n",
            "high: (6, 0.0476) 0.056 low: (6, 0.0413) 0.0518\n",
            "4 actual high: 6 actual low: 2\n",
            "high: (-6, 0.0568) 0.0757 low: (-6, 0.1683) 0.1611\n",
            "high: (-5, 0.0024) 0.0789 low: (-5, -0.0007) 0.1604\n",
            "high: (-4, -0.0639) -0.0063 low: (-4, 0.0083) 0.1684\n",
            "high: (-3, 0.0381) 0.0445 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.018) 0.0685 low: (-2, 0.0562) 0.31\n",
            "high: (-1, 0.049) 0.1338 low: (-1, 0.0908) 0.3969\n",
            "high: (0, 0.2456) 0.3273 low: (0, 0.2988) 0.286\n",
            "high: (1, 0.0756) 0.5389 low: (1, 0.1172) 0.3171\n",
            "high: (2, 0.0531) 0.4382 low: (2, 0.066) 0.2049\n",
            "high: (3, 0.0909) 0.3674 low: (3, 0.0574) 0.1417\n",
            "high: (4, 0.057) 0.2463 low: (4, 0.019) 0.0868\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0154) 0.0686\n",
            "high: (6, 0.1257) 0.1675 low: (6, 0.0563) 0.0539\n",
            "------\n",
            "0 actual high: 2 actual low: -1\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.01) 0.0102\n",
            "high: (-5, 0.0382) 0.0391 low: (-5, 0.0216) 0.0321\n",
            "high: (-4, -0.0197) 0.0146 low: (-4, -0.0223) 0.0094\n",
            "high: (-3, -0.0234) -0.0146 low: (-3, 0.0479) 0.0581\n",
            "high: (-2, 0.0066) -0.0064 low: (-2, 0.0798) 0.1391\n",
            "high: (-1, 0.0212) 0.0201 low: (-1, 0.3245) 0.4685\n",
            "high: (0, 0.5177) 0.6454 low: (0, 0.5353) 0.5434\n",
            "high: (1, 0.1751) 0.3345 low: (1, 0.0065) -0.0119\n",
            "high: (2, 0.0834) 0.1162 low: (2, 0.0018) -0.0185\n",
            "high: (3, 0.002) 0.0122 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0097 low: (4, -0.0375) -0.0417\n",
            "high: (5, -0.0072) 0.0534 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.05) 0.0623 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 3 actual low: -1\n",
            "high: (-6, 0.0249) 0.0261 low: (-6, 0.037) 0.0432\n",
            "high: (-5, 0.0113) 0.0379 low: (-5, -0.0337) 0.0039\n",
            "high: (-4, 0.0238) 0.0628 low: (-4, 0.0365) 0.0465\n",
            "high: (-3, 0.0147) 0.0782 low: (-3, 0.0594) 0.1159\n",
            "high: (-2, 0.0262) 0.1057 low: (-2, 0.1026) 0.2358\n",
            "high: (-1, 0.0528) 0.1609 low: (-1, 0.1669) 0.4308\n",
            "high: (0, 0.3895) 0.4079 low: (0, 0.4152) 0.4852\n",
            "high: (1, 0.1238) 0.4312 low: (1, 0.0483) 0.084\n",
            "high: (2, 0.0902) 0.3016 low: (2, 0.0349) 0.0276\n",
            "high: (3, 0.0876) 0.2071 low: (3, 0.0183) -0.0132\n",
            "high: (4, 0.0413) 0.1154 low: (4, -0.0113) -0.0346\n",
            "high: (5, -0.0043) 0.0721 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0732) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0164) 0.0224 low: (-6, 0.1034) 0.097\n",
            "high: (-5, 0.0162) 0.0446 low: (-5, -0.0017) 0.0954\n",
            "high: (-4, 0.0108) 0.0594 low: (-4, 0.0416) 0.1344\n",
            "high: (-3, 0.0109) 0.0743 low: (-3, 0.0667) 0.1969\n",
            "high: (-2, 0.0287) 0.1136 low: (-2, 0.0418) 0.2361\n",
            "high: (-1, 0.0831) 0.2273 low: (-1, 0.1969) 0.4207\n",
            "high: (0, 0.3164) 0.4329 low: (0, 0.3895) 0.3652\n",
            "high: (1, 0.0922) 0.3399 low: (1, 0.0754) 0.2141\n",
            "high: (2, 0.0307) 0.2137 low: (2, 0.0671) 0.1434\n",
            "high: (3, 0.0532) 0.1717 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0353) 0.0989 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0002) 0.1472 low: (5, -0.0099) 0.0166\n",
            "high: (6, 0.1078) 0.1475 low: (6, 0.0276) 0.0259\n",
            "3 actual high: 4 actual low: 1\n",
            "high: (-6, 0.0271) 0.0318 low: (-6, 0.1258) 0.1574\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.0199) 0.1824\n",
            "high: (-4, 0.0182) 0.0533 low: (-4, 0.0369) 0.2285\n",
            "high: (-3, 0.0462) 0.1076 low: (-3, 0.0389) 0.2772\n",
            "high: (-2, 0.0595) 0.1775 low: (-2, 0.0699) 0.3647\n",
            "high: (-1, 0.0892) 0.2823 low: (-1, 0.0455) 0.4217\n",
            "high: (0, 0.2792) 0.328 low: (0, 0.2599) 0.3253\n",
            "high: (1, 0.0594) 0.3897 low: (1, 0.0694) 0.2531\n",
            "high: (2, 0.1148) 0.3199 low: (2, 0.0686) 0.1662\n",
            "high: (3, 0.0701) 0.185 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0394) 0.1027 low: (4, 0.0266) 0.0796\n",
            "high: (5, 0.0004) 0.0564 low: (5, -0.0041) 0.0463\n",
            "high: (6, 0.0476) 0.0559 low: (6, 0.0411) 0.0514\n",
            "4 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0569) 0.0758 low: (-6, 0.1687) 0.1612\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0008) 0.1604\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0084) 0.1685\n",
            "high: (-3, 0.038) 0.0441 low: (-3, 0.0917) 0.2561\n",
            "high: (-2, 0.018) 0.068 low: (-2, 0.0566) 0.3102\n",
            "high: (-1, 0.0489) 0.1332 low: (-1, 0.0909) 0.397\n",
            "high: (0, 0.2461) 0.3277 low: (0, 0.2994) 0.2861\n",
            "high: (1, 0.0756) 0.5391 low: (1, 0.1174) 0.3169\n",
            "high: (2, 0.0533) 0.4385 low: (2, 0.0661) 0.2047\n",
            "high: (3, 0.0911) 0.3675 low: (3, 0.0575) 0.1415\n",
            "high: (4, 0.0569) 0.2462 low: (4, 0.0191) 0.0866\n",
            "high: (5, 0.0021) 0.1704 low: (5, 0.0154) 0.0683\n",
            "high: (6, 0.1259) 0.1676 low: (6, 0.0561) 0.0536\n",
            "------\n",
            "0 actual high: 4 actual low: 0\n",
            "high: (-6, -0.0069) -0.0086 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0383) 0.0392 low: (-5, 0.0216) 0.0322\n",
            "high: (-4, -0.0197) 0.0146 low: (-4, -0.0222) 0.0097\n",
            "high: (-3, -0.0234) -0.0146 low: (-3, 0.048) 0.0584\n",
            "high: (-2, 0.0065) -0.0065 low: (-2, 0.0797) 0.1394\n",
            "high: (-1, 0.0212) 0.02 low: (-1, 0.324) 0.4686\n",
            "high: (0, 0.517) 0.6455 low: (0, 0.5345) 0.5431\n",
            "high: (1, 0.1748) 0.3345 low: (1, 0.0065) -0.0117\n",
            "high: (2, 0.0833) 0.1162 low: (2, 0.0019) -0.0183\n",
            "high: (3, 0.0021) 0.0122 low: (3, 0.0212) -0.0202\n",
            "high: (4, -0.035) 0.0096 low: (4, -0.0376) -0.0418\n",
            "high: (5, -0.0072) 0.0533 low: (5, 0.0002) -0.0036\n",
            "high: (6, 0.0499) 0.0623 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 4 actual low: 1\n",
            "high: (-6, 0.0249) 0.0261 low: (-6, 0.0369) 0.0432\n",
            "high: (-5, 0.0112) 0.0379 low: (-5, -0.0336) 0.0039\n",
            "high: (-4, 0.0236) 0.0626 low: (-4, 0.0365) 0.0466\n",
            "high: (-3, 0.0148) 0.0781 low: (-3, 0.0592) 0.1159\n",
            "high: (-2, 0.0262) 0.1056 low: (-2, 0.1026) 0.236\n",
            "high: (-1, 0.0526) 0.1608 low: (-1, 0.1666) 0.431\n",
            "high: (0, 0.3889) 0.4079 low: (0, 0.4145) 0.4852\n",
            "high: (1, 0.1237) 0.4313 low: (1, 0.0481) 0.0838\n",
            "high: (2, 0.0901) 0.3016 low: (2, 0.0349) 0.0275\n",
            "high: (3, 0.0874) 0.207 low: (3, 0.0183) -0.0133\n",
            "high: (4, 0.0413) 0.1154 low: (4, -0.0114) -0.0348\n",
            "high: (5, -0.0043) 0.0721 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.073) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 6 actual low: 2\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.1033) 0.097\n",
            "high: (-5, 0.0162) 0.0447 low: (-5, -0.0017) 0.0954\n",
            "high: (-4, 0.0108) 0.0595 low: (-4, 0.0415) 0.1344\n",
            "high: (-3, 0.0107) 0.0742 low: (-3, 0.0667) 0.197\n",
            "high: (-2, 0.0286) 0.1134 low: (-2, 0.0417) 0.2362\n",
            "high: (-1, 0.0829) 0.227 low: (-1, 0.1966) 0.4208\n",
            "high: (0, 0.3159) 0.433 low: (0, 0.3887) 0.365\n",
            "high: (1, 0.0921) 0.34 low: (1, 0.0754) 0.2143\n",
            "high: (2, 0.0307) 0.2137 low: (2, 0.0671) 0.1435\n",
            "high: (3, 0.053) 0.1716 low: (3, 0.0516) 0.0805\n",
            "high: (4, -0.0352) 0.099 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0002) 0.1472 low: (5, -0.0099) 0.0166\n",
            "high: (6, 0.1076) 0.1475 low: (6, 0.0276) 0.0259\n",
            "3 actual high: 4 actual low: 1\n",
            "high: (-6, 0.027) 0.0317 low: (-6, 0.1257) 0.1576\n",
            "high: (-5, 0.0002) 0.032 low: (-5, 0.0199) 0.1825\n",
            "high: (-4, 0.0182) 0.0534 low: (-4, 0.0368) 0.2287\n",
            "high: (-3, 0.0463) 0.1078 low: (-3, 0.0388) 0.2773\n",
            "high: (-2, 0.0595) 0.1778 low: (-2, 0.0697) 0.3647\n",
            "high: (-1, 0.0891) 0.2825 low: (-1, 0.0455) 0.4217\n",
            "high: (0, 0.2788) 0.3278 low: (0, 0.2595) 0.3253\n",
            "high: (1, 0.0593) 0.3897 low: (1, 0.0694) 0.253\n",
            "high: (2, 0.1148) 0.32 low: (2, 0.0685) 0.166\n",
            "high: (3, 0.0701) 0.185 low: (3, 0.0005) 0.0801\n",
            "high: (4, 0.0392) 0.1026 low: (4, 0.0266) 0.0795\n",
            "high: (5, 0.0005) 0.0565 low: (5, -0.0043) 0.0461\n",
            "high: (6, 0.0476) 0.056 low: (6, 0.0411) 0.0515\n",
            "4 actual high: 2 actual low: 0\n",
            "high: (-6, 0.0568) 0.0757 low: (-6, 0.1685) 0.1612\n",
            "high: (-5, 0.0024) 0.0789 low: (-5, -0.0008) 0.1604\n",
            "high: (-4, -0.0641) -0.0065 low: (-4, 0.0083) 0.1684\n",
            "high: (-3, 0.0381) 0.0442 low: (-3, 0.0917) 0.2561\n",
            "high: (-2, 0.018) 0.0682 low: (-2, 0.0564) 0.31\n",
            "high: (-1, 0.0489) 0.1334 low: (-1, 0.0908) 0.3969\n",
            "high: (0, 0.2457) 0.3275 low: (0, 0.2989) 0.2859\n",
            "high: (1, 0.0755) 0.5391 low: (1, 0.1173) 0.3172\n",
            "high: (2, 0.0532) 0.4385 low: (2, 0.0661) 0.205\n",
            "high: (3, 0.091) 0.3676 low: (3, 0.0575) 0.1418\n",
            "high: (4, 0.0569) 0.2463 low: (4, 0.0191) 0.0868\n",
            "high: (5, 0.0021) 0.1705 low: (5, 0.0154) 0.0685\n",
            "high: (6, 0.1258) 0.1677 low: (6, 0.0562) 0.0538\n",
            "------\n",
            "0 actual high: 1 actual low: -2\n",
            "high: (-6, -0.0064) -0.0079 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.015 low: (-4, -0.0225) 0.0091\n",
            "high: (-3, -0.0234) -0.014 low: (-3, 0.0478) 0.0575\n",
            "high: (-2, 0.0069) -0.0055 low: (-2, 0.08) 0.1384\n",
            "high: (-1, 0.0213) 0.021 low: (-1, 0.3259) 0.4683\n",
            "high: (0, 0.5198) 0.6448 low: (0, 0.5376) 0.5441\n",
            "high: (1, 0.176) 0.3343 low: (1, 0.0066) -0.0123\n",
            "high: (2, 0.0834) 0.116 low: (2, 0.0014) -0.019\n",
            "high: (3, 0.0018) 0.0125 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0349) 0.0103 low: (4, -0.0373) -0.0418\n",
            "high: (5, -0.007) 0.0536 low: (5, -0.0002) -0.004\n",
            "high: (6, 0.0502) 0.0623 low: (6, -0.0038) -0.0038\n",
            "1 actual high: 2 actual low: -1\n",
            "high: (-6, 0.025) 0.0261 low: (-6, 0.0374) 0.0435\n",
            "high: (-5, 0.0114) 0.0379 low: (-5, -0.034) 0.004\n",
            "high: (-4, 0.0242) 0.0632 low: (-4, 0.0366) 0.0465\n",
            "high: (-3, 0.0146) 0.0784 low: (-3, 0.0598) 0.116\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1028) 0.2355\n",
            "high: (-1, 0.0534) 0.1612 low: (-1, 0.1676) 0.4303\n",
            "high: (0, 0.3914) 0.408 low: (0, 0.4174) 0.4851\n",
            "high: (1, 0.124) 0.4308 low: (1, 0.049) 0.0846\n",
            "high: (2, 0.0902) 0.3015 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0882) 0.2075 low: (3, 0.0182) -0.0129\n",
            "high: (4, 0.0414) 0.1156 low: (4, -0.011) -0.0341\n",
            "high: (5, -0.0041) 0.0724 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0736) 0.0767 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.104) 0.0971\n",
            "high: (-5, 0.0161) 0.044 low: (-5, -0.0014) 0.0958\n",
            "high: (-4, 0.0108) 0.0587 low: (-4, 0.0418) 0.1348\n",
            "high: (-3, 0.0114) 0.0742 low: (-3, 0.0667) 0.1971\n",
            "high: (-2, 0.029) 0.1137 low: (-2, 0.0422) 0.2364\n",
            "high: (-1, 0.0838) 0.2277 low: (-1, 0.1976) 0.4209\n",
            "high: (0, 0.3177) 0.4324 low: (0, 0.3916) 0.3655\n",
            "high: (1, 0.0926) 0.3399 low: (1, 0.0755) 0.2136\n",
            "high: (2, 0.0308) 0.2138 low: (2, 0.0671) 0.1431\n",
            "high: (3, 0.0538) 0.1719 low: (3, 0.0518) 0.0805\n",
            "high: (4, -0.0358) 0.0987 low: (4, 0.0165) 0.0321\n",
            "high: (5, -0.0) 0.1474 low: (5, -0.0097) 0.0167\n",
            "high: (6, 0.1083) 0.1474 low: (6, 0.0276) 0.0258\n",
            "3 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0274) 0.0321 low: (-6, 0.1258) 0.1571\n",
            "high: (-5, 0.0002) 0.0324 low: (-5, 0.02) 0.182\n",
            "high: (-4, 0.0182) 0.0537 low: (-4, 0.0368) 0.228\n",
            "high: (-3, 0.046) 0.1077 low: (-3, 0.0389) 0.2765\n",
            "high: (-2, 0.0595) 0.1775 low: (-2, 0.0703) 0.3643\n",
            "high: (-1, 0.0894) 0.2823 low: (-1, 0.0454) 0.421\n",
            "high: (0, 0.2801) 0.3286 low: (0, 0.2608) 0.3256\n",
            "high: (1, 0.0595) 0.3891 low: (1, 0.0696) 0.2534\n",
            "high: (2, 0.1147) 0.3193 low: (2, 0.0689) 0.1665\n",
            "high: (3, 0.0704) 0.1848 low: (3, 0.0009) 0.0805\n",
            "high: (4, 0.0397) 0.1022 low: (4, 0.0266) 0.0794\n",
            "high: (5, -0.0001) 0.0556 low: (5, -0.0037) 0.0462\n",
            "high: (6, 0.0475) 0.0557 low: (6, 0.0407) 0.0508\n",
            "4 actual high: 1 actual low: -2\n",
            "high: (-6, 0.057) 0.0758 low: (-6, 0.1695) 0.1614\n",
            "high: (-5, 0.0025) 0.0791 low: (-5, -0.0008) 0.1606\n",
            "high: (-4, -0.0648) -0.007 low: (-4, 0.0085) 0.1687\n",
            "high: (-3, 0.0379) 0.0433 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.0181) 0.0674 low: (-2, 0.0571) 0.3104\n",
            "high: (-1, 0.0487) 0.1322 low: (-1, 0.0911) 0.3971\n",
            "high: (0, 0.2472) 0.3287 low: (0, 0.3006) 0.2862\n",
            "high: (1, 0.0755) 0.5392 low: (1, 0.118) 0.3167\n",
            "high: (2, 0.0539) 0.4388 low: (2, 0.0665) 0.2044\n",
            "high: (3, 0.0913) 0.3671 low: (3, 0.0577) 0.1411\n",
            "high: (4, 0.0567) 0.2457 low: (4, 0.0194) 0.0862\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0153) 0.0677\n",
            "high: (6, 0.1261) 0.1677 low: (6, 0.0558) 0.0531\n",
            "------\n",
            "0 actual high: 1 actual low: -2\n",
            "high: (-6, -0.0063) -0.0078 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0395 low: (-5, 0.0216) 0.0318\n",
            "high: (-4, -0.0197) 0.0151 low: (-4, -0.0226) 0.009\n",
            "high: (-3, -0.0234) -0.0139 low: (-3, 0.0477) 0.0572\n",
            "high: (-2, 0.007) -0.0052 low: (-2, 0.0801) 0.1382\n",
            "high: (-1, 0.0213) 0.0212 low: (-1, 0.3264) 0.4682\n",
            "high: (0, 0.5206) 0.6445 low: (0, 0.5385) 0.5444\n",
            "high: (1, 0.1763) 0.3344 low: (1, 0.0067) -0.0126\n",
            "high: (2, 0.0835) 0.1161 low: (2, 0.0012) -0.0194\n",
            "high: (3, 0.0017) 0.0128 low: (3, 0.021) -0.0206\n",
            "high: (4, -0.0348) 0.0106 low: (4, -0.0372) -0.0419\n",
            "high: (5, -0.0069) 0.0537 low: (5, -0.0004) -0.0042\n",
            "high: (6, 0.0503) 0.0623 low: (6, -0.0038) -0.0038\n",
            "1 actual high: 0 actual low: -3\n",
            "high: (-6, 0.0251) 0.0261 low: (-6, 0.0376) 0.0436\n",
            "high: (-5, 0.0115) 0.0381 low: (-5, -0.0341) 0.0041\n",
            "high: (-4, 0.0244) 0.0634 low: (-4, 0.0367) 0.0466\n",
            "high: (-3, 0.0145) 0.0785 low: (-3, 0.06) 0.1162\n",
            "high: (-2, 0.0261) 0.1057 low: (-2, 0.1028) 0.2354\n",
            "high: (-1, 0.0536) 0.1614 low: (-1, 0.1679) 0.4301\n",
            "high: (0, 0.3922) 0.4079 low: (0, 0.4182) 0.485\n",
            "high: (1, 0.1241) 0.4306 low: (1, 0.0493) 0.0849\n",
            "high: (2, 0.0902) 0.3015 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0885) 0.2077 low: (3, 0.0182) -0.0128\n",
            "high: (4, 0.0414) 0.1157 low: (4, -0.0109) -0.0339\n",
            "high: (5, -0.004) 0.0726 low: (5, -0.003) -0.0212\n",
            "high: (6, 0.0738) 0.0768 low: (6, -0.0153) -0.0177\n",
            "2 actual high: -2 actual low: -4\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.1042) 0.0971\n",
            "high: (-5, 0.0161) 0.0439 low: (-5, -0.0013) 0.0959\n",
            "high: (-4, 0.0108) 0.0585 low: (-4, 0.0419) 0.1349\n",
            "high: (-3, 0.0116) 0.0743 low: (-3, 0.0667) 0.1971\n",
            "high: (-2, 0.0291) 0.1138 low: (-2, 0.0423) 0.2365\n",
            "high: (-1, 0.084) 0.2279 low: (-1, 0.198) 0.4209\n",
            "high: (0, 0.3183) 0.4322 low: (0, 0.3925) 0.3657\n",
            "high: (1, 0.0928) 0.3399 low: (1, 0.0755) 0.2134\n",
            "high: (2, 0.0308) 0.2139 low: (2, 0.0671) 0.143\n",
            "high: (3, 0.054) 0.1721 low: (3, 0.0519) 0.0805\n",
            "high: (4, -0.0359) 0.0987 low: (4, 0.0166) 0.0321\n",
            "high: (5, 0.0) 0.1475 low: (5, -0.0097) 0.0167\n",
            "high: (6, 0.1086) 0.1475 low: (6, 0.0276) 0.0257\n",
            "3 actual high: 0 actual low: -3\n",
            "high: (-6, 0.0274) 0.0321 low: (-6, 0.1259) 0.1569\n",
            "high: (-5, 0.0001) 0.0323 low: (-5, 0.02) 0.1819\n",
            "high: (-4, 0.0182) 0.0536 low: (-4, 0.0368) 0.2277\n",
            "high: (-3, 0.0459) 0.1074 low: (-3, 0.039) 0.2763\n",
            "high: (-2, 0.0594) 0.1771 low: (-2, 0.0706) 0.3643\n",
            "high: (-1, 0.0895) 0.2821 low: (-1, 0.0454) 0.4209\n",
            "high: (0, 0.2804) 0.3289 low: (0, 0.2612) 0.3256\n",
            "high: (1, 0.0595) 0.389 low: (1, 0.0697) 0.2535\n",
            "high: (2, 0.1147) 0.3193 low: (2, 0.0689) 0.1666\n",
            "high: (3, 0.0705) 0.1847 low: (3, 0.001) 0.0808\n",
            "high: (4, 0.0398) 0.102 low: (4, 0.0267) 0.0795\n",
            "high: (5, -0.0002) 0.0554 low: (5, -0.0035) 0.0462\n",
            "high: (6, 0.0474) 0.0556 low: (6, 0.0406) 0.0506\n",
            "4 actual high: 0 actual low: -3\n",
            "high: (-6, 0.0571) 0.0759 low: (-6, 0.1698) 0.1614\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1607\n",
            "high: (-4, -0.065) -0.0072 low: (-4, 0.0086) 0.1688\n",
            "high: (-3, 0.0379) 0.0432 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.0181) 0.0672 low: (-2, 0.0573) 0.3105\n",
            "high: (-1, 0.0487) 0.1319 low: (-1, 0.0912) 0.3972\n",
            "high: (0, 0.2476) 0.3289 low: (0, 0.3011) 0.2862\n",
            "high: (1, 0.0755) 0.5391 low: (1, 0.1182) 0.3166\n",
            "high: (2, 0.0541) 0.4388 low: (2, 0.0666) 0.2042\n",
            "high: (3, 0.0914) 0.3669 low: (3, 0.0577) 0.1409\n",
            "high: (4, 0.0566) 0.2455 low: (4, 0.0195) 0.086\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0153) 0.0675\n",
            "high: (6, 0.1262) 0.1677 low: (6, 0.0557) 0.053\n",
            "------\n",
            "0 actual high: 1 actual low: -2\n",
            "high: (-6, -0.0065) -0.0081 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.0149 low: (-4, -0.0224) 0.0092\n",
            "high: (-3, -0.0234) -0.0141 low: (-3, 0.0478) 0.0576\n",
            "high: (-2, 0.0068) -0.0057 low: (-2, 0.08) 0.1385\n",
            "high: (-1, 0.0214) 0.0208 low: (-1, 0.3259) 0.4683\n",
            "high: (0, 0.5198) 0.6448 low: (0, 0.5377) 0.5441\n",
            "high: (1, 0.176) 0.3344 low: (1, 0.0066) -0.0123\n",
            "high: (2, 0.0835) 0.1161 low: (2, 0.0014) -0.019\n",
            "high: (3, 0.0018) 0.0125 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0349) 0.0103 low: (4, -0.0373) -0.0418\n",
            "high: (5, -0.007) 0.0536 low: (5, -0.0002) -0.004\n",
            "high: (6, 0.0502) 0.0623 low: (6, -0.0038) -0.0038\n",
            "1 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0251) 0.0262 low: (-6, 0.0374) 0.0435\n",
            "high: (-5, 0.0115) 0.0381 low: (-5, -0.034) 0.004\n",
            "high: (-4, 0.0242) 0.0633 low: (-4, 0.0367) 0.0466\n",
            "high: (-3, 0.0145) 0.0785 low: (-3, 0.0598) 0.1161\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1027) 0.2355\n",
            "high: (-1, 0.0534) 0.1613 low: (-1, 0.1675) 0.4302\n",
            "high: (0, 0.3915) 0.4079 low: (0, 0.4173) 0.4851\n",
            "high: (1, 0.124) 0.4308 low: (1, 0.049) 0.0846\n",
            "high: (2, 0.0902) 0.3016 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0882) 0.2076 low: (3, 0.0182) -0.0129\n",
            "high: (4, 0.0415) 0.1158 low: (4, -0.011) -0.0341\n",
            "high: (5, -0.004) 0.0725 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0736) 0.0767 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0162) 0.0221 low: (-6, 0.104) 0.0971\n",
            "high: (-5, 0.0162) 0.0441 low: (-5, -0.0014) 0.0958\n",
            "high: (-4, 0.0108) 0.0588 low: (-4, 0.0418) 0.1348\n",
            "high: (-3, 0.0113) 0.0742 low: (-3, 0.0667) 0.1971\n",
            "high: (-2, 0.0289) 0.1135 low: (-2, 0.0421) 0.2363\n",
            "high: (-1, 0.0837) 0.2275 low: (-1, 0.1978) 0.421\n",
            "high: (0, 0.3177) 0.4325 low: (0, 0.3916) 0.3655\n",
            "high: (1, 0.0926) 0.34 low: (1, 0.0754) 0.2135\n",
            "high: (2, 0.0307) 0.2139 low: (2, 0.0672) 0.1431\n",
            "high: (3, 0.0538) 0.1721 low: (3, 0.0518) 0.0804\n",
            "high: (4, -0.0357) 0.0988 low: (4, 0.0165) 0.032\n",
            "high: (5, -0.0001) 0.1474 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1084) 0.1476 low: (6, 0.0276) 0.0258\n",
            "3 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0273) 0.032 low: (-6, 0.1259) 0.1571\n",
            "high: (-5, 0.0001) 0.0322 low: (-5, 0.02) 0.1821\n",
            "high: (-4, 0.0182) 0.0535 low: (-4, 0.0368) 0.228\n",
            "high: (-3, 0.046) 0.1075 low: (-3, 0.0389) 0.2765\n",
            "high: (-2, 0.0594) 0.1772 low: (-2, 0.0704) 0.3644\n",
            "high: (-1, 0.0894) 0.2821 low: (-1, 0.0454) 0.421\n",
            "high: (0, 0.28) 0.3286 low: (0, 0.2608) 0.3254\n",
            "high: (1, 0.0595) 0.3893 low: (1, 0.0697) 0.2536\n",
            "high: (2, 0.1147) 0.3194 low: (2, 0.0688) 0.1666\n",
            "high: (3, 0.0704) 0.1848 low: (3, 0.0009) 0.0807\n",
            "high: (4, 0.0396) 0.1022 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0) 0.0557 low: (5, -0.0037) 0.0463\n",
            "high: (6, 0.0475) 0.0557 low: (6, 0.0408) 0.0509\n",
            "4 actual high: 1 actual low: -4\n",
            "high: (-6, 0.0571) 0.0759 low: (-6, 0.1695) 0.1613\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1606\n",
            "high: (-4, -0.0647) -0.0068 low: (-4, 0.0086) 0.1688\n",
            "high: (-3, 0.0379) 0.0436 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.0181) 0.0677 low: (-2, 0.0571) 0.3104\n",
            "high: (-1, 0.0488) 0.1325 low: (-1, 0.0911) 0.3971\n",
            "high: (0, 0.2472) 0.3286 low: (0, 0.3006) 0.2861\n",
            "high: (1, 0.0755) 0.5388 low: (1, 0.118) 0.3168\n",
            "high: (2, 0.0538) 0.4384 low: (2, 0.0665) 0.2045\n",
            "high: (3, 0.0913) 0.3669 low: (3, 0.0576) 0.1412\n",
            "high: (4, 0.0567) 0.2455 low: (4, 0.0194) 0.0863\n",
            "high: (5, 0.002) 0.1702 low: (5, 0.0154) 0.0679\n",
            "high: (6, 0.126) 0.1675 low: (6, 0.0559) 0.0532\n",
            "------\n",
            "0 actual high: 1 actual low: -1\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.01) 0.0101\n",
            "high: (-5, 0.0383) 0.0392 low: (-5, 0.0216) 0.0321\n",
            "high: (-4, -0.0197) 0.0147 low: (-4, -0.0222) 0.0095\n",
            "high: (-3, -0.0234) -0.0145 low: (-3, 0.0479) 0.0581\n",
            "high: (-2, 0.0066) -0.0062 low: (-2, 0.0798) 0.1391\n",
            "high: (-1, 0.0213) 0.0203 low: (-1, 0.3246) 0.4684\n",
            "high: (0, 0.518) 0.6453 low: (0, 0.5358) 0.5436\n",
            "high: (1, 0.1752) 0.3344 low: (1, 0.0065) -0.012\n",
            "high: (2, 0.0834) 0.1161 low: (2, 0.0017) -0.0186\n",
            "high: (3, 0.002) 0.0122 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0097 low: (4, -0.0375) -0.0417\n",
            "high: (5, -0.0072) 0.0533 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.05) 0.0623 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 3 actual low: 0\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.0371) 0.0433\n",
            "high: (-5, 0.0113) 0.038 low: (-5, -0.0337) 0.004\n",
            "high: (-4, 0.0238) 0.0629 low: (-4, 0.0366) 0.0467\n",
            "high: (-3, 0.0146) 0.0782 low: (-3, 0.0594) 0.1161\n",
            "high: (-2, 0.0262) 0.1056 low: (-2, 0.1026) 0.2359\n",
            "high: (-1, 0.0528) 0.1608 low: (-1, 0.1669) 0.4309\n",
            "high: (0, 0.3898) 0.4079 low: (0, 0.4154) 0.4852\n",
            "high: (1, 0.1238) 0.4312 low: (1, 0.0484) 0.084\n",
            "high: (2, 0.0902) 0.3017 low: (2, 0.0349) 0.0274\n",
            "high: (3, 0.0877) 0.2073 low: (3, 0.0182) -0.0133\n",
            "high: (4, 0.0414) 0.1155 low: (4, -0.0114) -0.0346\n",
            "high: (5, -0.0042) 0.0722 low: (5, -0.0029) -0.0213\n",
            "high: (6, 0.0732) 0.0766 low: (6, -0.0153) -0.0179\n",
            "2 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1035) 0.097\n",
            "high: (-5, 0.0163) 0.0446 low: (-5, -0.0016) 0.0955\n",
            "high: (-4, 0.0108) 0.0593 low: (-4, 0.0417) 0.1345\n",
            "high: (-3, 0.0109) 0.0742 low: (-3, 0.0667) 0.197\n",
            "high: (-2, 0.0287) 0.1135 low: (-2, 0.0418) 0.2362\n",
            "high: (-1, 0.0832) 0.2272 low: (-1, 0.1971) 0.4208\n",
            "high: (0, 0.3166) 0.4329 low: (0, 0.3898) 0.3652\n",
            "high: (1, 0.0923) 0.3399 low: (1, 0.0754) 0.214\n",
            "high: (2, 0.0307) 0.2137 low: (2, 0.0672) 0.1433\n",
            "high: (3, 0.0532) 0.1717 low: (3, 0.0517) 0.0804\n",
            "high: (4, -0.0353) 0.099 low: (4, 0.0165) 0.0319\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.01) 0.0165\n",
            "high: (6, 0.1079) 0.1475 low: (6, 0.0276) 0.0259\n",
            "3 actual high: 3 actual low: -2\n",
            "high: (-6, 0.0271) 0.0318 low: (-6, 0.1258) 0.1574\n",
            "high: (-5, 0.0) 0.0318 low: (-5, 0.0199) 0.1823\n",
            "high: (-4, 0.0182) 0.0532 low: (-4, 0.0368) 0.2284\n",
            "high: (-3, 0.0462) 0.1075 low: (-3, 0.0389) 0.277\n",
            "high: (-2, 0.0594) 0.1773 low: (-2, 0.07) 0.3646\n",
            "high: (-1, 0.0892) 0.2821 low: (-1, 0.0455) 0.4215\n",
            "high: (0, 0.2793) 0.3282 low: (0, 0.26) 0.3253\n",
            "high: (1, 0.0594) 0.3897 low: (1, 0.0695) 0.2531\n",
            "high: (2, 0.1148) 0.3199 low: (2, 0.0685) 0.1662\n",
            "high: (3, 0.0701) 0.185 low: (3, 0.0006) 0.0805\n",
            "high: (4, 0.0394) 0.1026 low: (4, 0.0267) 0.0797\n",
            "high: (5, 0.0004) 0.0563 low: (5, -0.0041) 0.0463\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.0411) 0.0514\n",
            "4 actual high: 2 actual low: -1\n",
            "high: (-6, 0.0569) 0.0757 low: (-6, 0.1689) 0.1613\n",
            "high: (-5, 0.0024) 0.0789 low: (-5, -0.0008) 0.1605\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0084) 0.1686\n",
            "high: (-3, 0.0381) 0.0442 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.018) 0.0681 low: (-2, 0.0566) 0.3103\n",
            "high: (-1, 0.0489) 0.1332 low: (-1, 0.0909) 0.3971\n",
            "high: (0, 0.2463) 0.3278 low: (0, 0.2995) 0.286\n",
            "high: (1, 0.0756) 0.539 low: (1, 0.1175) 0.3169\n",
            "high: (2, 0.0534) 0.4384 low: (2, 0.0662) 0.2047\n",
            "high: (3, 0.0911) 0.3673 low: (3, 0.0575) 0.1414\n",
            "high: (4, 0.0569) 0.2461 low: (4, 0.0191) 0.0865\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0154) 0.0683\n",
            "high: (6, 0.1259) 0.1676 low: (6, 0.0561) 0.0536\n",
            "------\n",
            "0 actual high: 2 actual low: -1\n",
            "high: (-6, -0.0066) -0.0082 low: (-6, 0.01) 0.0101\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0216) 0.032\n",
            "high: (-4, -0.0197) 0.0148 low: (-4, -0.0224) 0.0093\n",
            "high: (-3, -0.0234) -0.0143 low: (-3, 0.0479) 0.0579\n",
            "high: (-2, 0.0067) -0.006 low: (-2, 0.0799) 0.1388\n",
            "high: (-1, 0.0213) 0.0205 low: (-1, 0.3253) 0.4684\n",
            "high: (0, 0.5189) 0.6451 low: (0, 0.5367) 0.5438\n",
            "high: (1, 0.1756) 0.3344 low: (1, 0.0066) -0.0122\n",
            "high: (2, 0.0834) 0.1161 low: (2, 0.0016) -0.0188\n",
            "high: (3, 0.0019) 0.0124 low: (3, 0.0211) -0.0205\n",
            "high: (4, -0.0349) 0.0101 low: (4, -0.0374) -0.0418\n",
            "high: (5, -0.0071) 0.0535 low: (5, -0.0001) -0.004\n",
            "high: (6, 0.0501) 0.0623 low: (6, -0.0038) -0.0039\n",
            "1 actual high: 2 actual low: -1\n",
            "high: (-6, 0.025) 0.0261 low: (-6, 0.0373) 0.0435\n",
            "high: (-5, 0.0114) 0.038 low: (-5, -0.0338) 0.0041\n",
            "high: (-4, 0.024) 0.0631 low: (-4, 0.0366) 0.0467\n",
            "high: (-3, 0.0146) 0.0783 low: (-3, 0.0596) 0.1161\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1027) 0.2358\n",
            "high: (-1, 0.0531) 0.161 low: (-1, 0.1673) 0.4307\n",
            "high: (0, 0.3907) 0.408 low: (0, 0.4164) 0.4851\n",
            "high: (1, 0.1239) 0.431 low: (1, 0.0487) 0.0842\n",
            "high: (2, 0.0902) 0.3017 low: (2, 0.0349) 0.0275\n",
            "high: (3, 0.088) 0.2075 low: (3, 0.0182) -0.0132\n",
            "high: (4, 0.0414) 0.1156 low: (4, -0.0112) -0.0344\n",
            "high: (5, -0.0041) 0.0724 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0734) 0.0766 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 2 actual low: -3\n",
            "high: (-6, 0.0163) 0.0222 low: (-6, 0.1038) 0.0971\n",
            "high: (-5, 0.0162) 0.0443 low: (-5, -0.0015) 0.0957\n",
            "high: (-4, 0.0108) 0.0591 low: (-4, 0.0417) 0.1346\n",
            "high: (-3, 0.0111) 0.0742 low: (-3, 0.0667) 0.197\n",
            "high: (-2, 0.0288) 0.1135 low: (-2, 0.042) 0.2363\n",
            "high: (-1, 0.0835) 0.2274 low: (-1, 0.1974) 0.4209\n",
            "high: (0, 0.3172) 0.4327 low: (0, 0.3907) 0.3653\n",
            "high: (1, 0.0924) 0.3399 low: (1, 0.0754) 0.2138\n",
            "high: (2, 0.0307) 0.2139 low: (2, 0.0672) 0.1433\n",
            "high: (3, 0.0535) 0.172 low: (3, 0.0518) 0.0805\n",
            "high: (4, -0.0355) 0.099 low: (4, 0.0165) 0.0321\n",
            "high: (5, -0.0001) 0.1475 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1082) 0.1476 low: (6, 0.0276) 0.0258\n",
            "3 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0272) 0.0319 low: (-6, 0.1258) 0.1572\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.02) 0.1822\n",
            "high: (-4, 0.0182) 0.0534 low: (-4, 0.0368) 0.2282\n",
            "high: (-3, 0.0461) 0.1075 low: (-3, 0.0389) 0.2768\n",
            "high: (-2, 0.0595) 0.1774 low: (-2, 0.0702) 0.3645\n",
            "high: (-1, 0.0893) 0.2822 low: (-1, 0.0455) 0.4213\n",
            "high: (0, 0.2797) 0.3284 low: (0, 0.2604) 0.3254\n",
            "high: (1, 0.0595) 0.3894 low: (1, 0.0696) 0.2533\n",
            "high: (2, 0.1147) 0.3196 low: (2, 0.0687) 0.1663\n",
            "high: (3, 0.0703) 0.1849 low: (3, 0.0007) 0.0805\n",
            "high: (4, 0.0395) 0.1024 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0002) 0.056 low: (5, -0.0039) 0.0462\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.0409) 0.0511\n",
            "4 actual high: 0 actual low: -3\n",
            "high: (-6, 0.057) 0.0758 low: (-6, 0.1692) 0.1613\n",
            "high: (-5, 0.0025) 0.0791 low: (-5, -0.0008) 0.1605\n",
            "high: (-4, -0.0645) -0.0066 low: (-4, 0.0085) 0.1686\n",
            "high: (-3, 0.038) 0.0439 low: (-3, 0.0918) 0.2561\n",
            "high: (-2, 0.0181) 0.068 low: (-2, 0.0569) 0.3104\n",
            "high: (-1, 0.0488) 0.1329 low: (-1, 0.091) 0.3971\n",
            "high: (0, 0.2468) 0.3282 low: (0, 0.3001) 0.2861\n",
            "high: (1, 0.0756) 0.5389 low: (1, 0.1178) 0.3168\n",
            "high: (2, 0.0536) 0.4384 low: (2, 0.0663) 0.2046\n",
            "high: (3, 0.0912) 0.3671 low: (3, 0.0576) 0.1414\n",
            "high: (4, 0.0568) 0.2458 low: (4, 0.0193) 0.0865\n",
            "high: (5, 0.002) 0.1702 low: (5, 0.0154) 0.0681\n",
            "high: (6, 0.126) 0.1676 low: (6, 0.056) 0.0534\n",
            "------\n",
            "0 actual high: 1 actual low: -2\n",
            "high: (-6, -0.0064) -0.0079 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.015 low: (-4, -0.0225) 0.0091\n",
            "high: (-3, -0.0234) -0.014 low: (-3, 0.0478) 0.0575\n",
            "high: (-2, 0.0068) -0.0056 low: (-2, 0.08) 0.1384\n",
            "high: (-1, 0.0213) 0.0208 low: (-1, 0.3259) 0.4682\n",
            "high: (0, 0.5198) 0.6448 low: (0, 0.5377) 0.5441\n",
            "high: (1, 0.176) 0.3344 low: (1, 0.0066) -0.0123\n",
            "high: (2, 0.0835) 0.1161 low: (2, 0.0014) -0.019\n",
            "high: (3, 0.0018) 0.0125 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0349) 0.0103 low: (4, -0.0373) -0.0418\n",
            "high: (5, -0.007) 0.0536 low: (5, -0.0002) -0.004\n",
            "high: (6, 0.0502) 0.0623 low: (6, -0.0038) -0.0038\n",
            "1 actual high: 1 actual low: -4\n",
            "high: (-6, 0.0251) 0.0262 low: (-6, 0.0374) 0.0435\n",
            "high: (-5, 0.0114) 0.038 low: (-5, -0.034) 0.004\n",
            "high: (-4, 0.0242) 0.0632 low: (-4, 0.0367) 0.0466\n",
            "high: (-3, 0.0145) 0.0784 low: (-3, 0.0598) 0.1161\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1027) 0.2355\n",
            "high: (-1, 0.0534) 0.1612 low: (-1, 0.1676) 0.4303\n",
            "high: (0, 0.3915) 0.4079 low: (0, 0.4174) 0.4851\n",
            "high: (1, 0.124) 0.4309 low: (1, 0.049) 0.0846\n",
            "high: (2, 0.0903) 0.3017 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0883) 0.2076 low: (3, 0.0182) -0.0129\n",
            "high: (4, 0.0414) 0.1156 low: (4, -0.011) -0.0341\n",
            "high: (5, -0.0041) 0.0724 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0736) 0.0767 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 0 actual low: -3\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.104) 0.0971\n",
            "high: (-5, 0.0162) 0.0441 low: (-5, -0.0014) 0.0958\n",
            "high: (-4, 0.0108) 0.0588 low: (-4, 0.0418) 0.1348\n",
            "high: (-3, 0.0113) 0.0741 low: (-3, 0.0667) 0.1971\n",
            "high: (-2, 0.029) 0.1136 low: (-2, 0.0421) 0.2364\n",
            "high: (-1, 0.0838) 0.2276 low: (-1, 0.1977) 0.4209\n",
            "high: (0, 0.3178) 0.4324 low: (0, 0.3916) 0.3656\n",
            "high: (1, 0.0926) 0.34 low: (1, 0.0754) 0.2135\n",
            "high: (2, 0.0308) 0.214 low: (2, 0.0672) 0.1431\n",
            "high: (3, 0.0538) 0.1721 low: (3, 0.0518) 0.0804\n",
            "high: (4, -0.0357) 0.0989 low: (4, 0.0165) 0.032\n",
            "high: (5, -0.0) 0.1475 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1084) 0.1475 low: (6, 0.0276) 0.0258\n",
            "3 actual high: -1 actual low: -4\n",
            "high: (-6, 0.0273) 0.032 low: (-6, 0.1258) 0.157\n",
            "high: (-5, 0.0001) 0.0321 low: (-5, 0.02) 0.182\n",
            "high: (-4, 0.0182) 0.0535 low: (-4, 0.0368) 0.2279\n",
            "high: (-3, 0.046) 0.1075 low: (-3, 0.0389) 0.2765\n",
            "high: (-2, 0.0594) 0.1772 low: (-2, 0.0704) 0.3644\n",
            "high: (-1, 0.0894) 0.2821 low: (-1, 0.0454) 0.421\n",
            "high: (0, 0.2801) 0.3286 low: (0, 0.2608) 0.3256\n",
            "high: (1, 0.0595) 0.3893 low: (1, 0.0696) 0.2534\n",
            "high: (2, 0.1147) 0.3195 low: (2, 0.0688) 0.1665\n",
            "high: (3, 0.0704) 0.1849 low: (3, 0.0008) 0.0806\n",
            "high: (4, 0.0397) 0.1023 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0) 0.0557 low: (5, -0.0037) 0.0463\n",
            "high: (6, 0.0475) 0.0557 low: (6, 0.0408) 0.0509\n",
            "4 actual high: -3 actual low: -6\n",
            "high: (-6, 0.0571) 0.0759 low: (-6, 0.1695) 0.1613\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1606\n",
            "high: (-4, -0.0647) -0.0068 low: (-4, 0.0085) 0.1687\n",
            "high: (-3, 0.0379) 0.0436 low: (-3, 0.0918) 0.256\n",
            "high: (-2, 0.0181) 0.0677 low: (-2, 0.0571) 0.3104\n",
            "high: (-1, 0.0488) 0.1325 low: (-1, 0.0911) 0.3971\n",
            "high: (0, 0.2472) 0.3285 low: (0, 0.3006) 0.2861\n",
            "high: (1, 0.0755) 0.5389 low: (1, 0.118) 0.3168\n",
            "high: (2, 0.0539) 0.4386 low: (2, 0.0665) 0.2045\n",
            "high: (3, 0.0913) 0.367 low: (3, 0.0576) 0.1412\n",
            "high: (4, 0.0567) 0.2456 low: (4, 0.0194) 0.0863\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0154) 0.0679\n",
            "high: (6, 0.1261) 0.1676 low: (6, 0.0559) 0.0532\n",
            "------\n",
            "0 actual high: 3 actual low: -2\n",
            "high: (-6, -0.0067) -0.0083 low: (-6, 0.01) 0.0101\n",
            "high: (-5, 0.0383) 0.0393 low: (-5, 0.0216) 0.0321\n",
            "high: (-4, -0.0197) 0.0148 low: (-4, -0.0223) 0.0094\n",
            "high: (-3, -0.0234) -0.0143 low: (-3, 0.0479) 0.058\n",
            "high: (-2, 0.0066) -0.0061 low: (-2, 0.0798) 0.139\n",
            "high: (-1, 0.0213) 0.0204 low: (-1, 0.3247) 0.4684\n",
            "high: (0, 0.5182) 0.6452 low: (0, 0.5359) 0.5436\n",
            "high: (1, 0.1753) 0.3344 low: (1, 0.0065) -0.012\n",
            "high: (2, 0.0834) 0.1162 low: (2, 0.0017) -0.0186\n",
            "high: (3, 0.002) 0.0123 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0098 low: (4, -0.0375) -0.0417\n",
            "high: (5, -0.0071) 0.0534 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.05) 0.0623 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 2 actual low: -1\n",
            "high: (-6, 0.025) 0.0261 low: (-6, 0.0371) 0.0433\n",
            "high: (-5, 0.0113) 0.038 low: (-5, -0.0337) 0.004\n",
            "high: (-4, 0.0239) 0.063 low: (-4, 0.0366) 0.0467\n",
            "high: (-3, 0.0146) 0.0782 low: (-3, 0.0594) 0.116\n",
            "high: (-2, 0.0262) 0.1056 low: (-2, 0.1026) 0.2358\n",
            "high: (-1, 0.0529) 0.161 low: (-1, 0.167) 0.4307\n",
            "high: (0, 0.39) 0.4079 low: (0, 0.4156) 0.4851\n",
            "high: (1, 0.1238) 0.4311 low: (1, 0.0484) 0.0842\n",
            "high: (2, 0.0902) 0.3016 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0877) 0.2073 low: (3, 0.0183) -0.0131\n",
            "high: (4, 0.0414) 0.1156 low: (4, -0.0113) -0.0344\n",
            "high: (5, -0.0042) 0.0723 low: (5, -0.0029) -0.0212\n",
            "high: (6, 0.0733) 0.0767 low: (6, -0.0153) -0.0179\n",
            "2 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1036) 0.097\n",
            "high: (-5, 0.0163) 0.0445 low: (-5, -0.0016) 0.0955\n",
            "high: (-4, 0.0108) 0.0593 low: (-4, 0.0417) 0.1346\n",
            "high: (-3, 0.0109) 0.0742 low: (-3, 0.0667) 0.197\n",
            "high: (-2, 0.0288) 0.1135 low: (-2, 0.0418) 0.2362\n",
            "high: (-1, 0.0833) 0.2274 low: (-1, 0.1972) 0.4208\n",
            "high: (0, 0.3167) 0.4327 low: (0, 0.3899) 0.3651\n",
            "high: (1, 0.0923) 0.3399 low: (1, 0.0754) 0.2141\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0672) 0.1435\n",
            "high: (3, 0.0533) 0.1719 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0353) 0.0991 low: (4, 0.0165) 0.0321\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.0099) 0.0167\n",
            "high: (6, 0.108) 0.1476 low: (6, 0.0277) 0.0259\n",
            "3 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0271) 0.0318 low: (-6, 0.1258) 0.1574\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.0199) 0.1823\n",
            "high: (-4, 0.0182) 0.0533 low: (-4, 0.0369) 0.2284\n",
            "high: (-3, 0.0462) 0.1076 low: (-3, 0.0389) 0.2771\n",
            "high: (-2, 0.0595) 0.1775 low: (-2, 0.07) 0.3646\n",
            "high: (-1, 0.0892) 0.2823 low: (-1, 0.0455) 0.4216\n",
            "high: (0, 0.2793) 0.3281 low: (0, 0.2601) 0.3254\n",
            "high: (1, 0.0594) 0.3896 low: (1, 0.0695) 0.2531\n",
            "high: (2, 0.1148) 0.3198 low: (2, 0.0685) 0.1661\n",
            "high: (3, 0.0702) 0.1849 low: (3, 0.0007) 0.0804\n",
            "high: (4, 0.0394) 0.1024 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0003) 0.0562 low: (5, -0.0041) 0.0462\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.041) 0.0513\n",
            "4 actual high: -3 actual low: -6\n",
            "high: (-6, 0.0569) 0.0757 low: (-6, 0.1689) 0.1613\n",
            "high: (-5, 0.0024) 0.0789 low: (-5, -0.0008) 0.1605\n",
            "high: (-4, -0.0643) -0.0067 low: (-4, 0.0084) 0.1685\n",
            "high: (-3, 0.038) 0.0439 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.018) 0.0679 low: (-2, 0.0566) 0.3102\n",
            "high: (-1, 0.0489) 0.133 low: (-1, 0.0909) 0.397\n",
            "high: (0, 0.2464) 0.328 low: (0, 0.2996) 0.286\n",
            "high: (1, 0.0756) 0.539 low: (1, 0.1176) 0.317\n",
            "high: (2, 0.0534) 0.4384 low: (2, 0.0662) 0.2047\n",
            "high: (3, 0.0911) 0.3673 low: (3, 0.0575) 0.1415\n",
            "high: (4, 0.0568) 0.246 low: (4, 0.0192) 0.0866\n",
            "high: (5, 0.0021) 0.1704 low: (5, 0.0154) 0.0683\n",
            "high: (6, 0.1259) 0.1676 low: (6, 0.0561) 0.0536\n",
            "------\n",
            "0 actual high: 0 actual low: -3\n",
            "high: (-6, -0.0064) -0.0079 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.015 low: (-4, -0.0225) 0.0091\n",
            "high: (-3, -0.0234) -0.014 low: (-3, 0.0478) 0.0575\n",
            "high: (-2, 0.0068) -0.0056 low: (-2, 0.08) 0.1384\n",
            "high: (-1, 0.0213) 0.0208 low: (-1, 0.326) 0.4682\n",
            "high: (0, 0.52) 0.6448 low: (0, 0.5379) 0.5442\n",
            "high: (1, 0.176) 0.3344 low: (1, 0.0066) -0.0124\n",
            "high: (2, 0.0835) 0.1162 low: (2, 0.0014) -0.0191\n",
            "high: (3, 0.0018) 0.0126 low: (3, 0.021) -0.0205\n",
            "high: (4, -0.0348) 0.0104 low: (4, -0.0373) -0.0418\n",
            "high: (5, -0.007) 0.0536 low: (5, -0.0002) -0.004\n",
            "high: (6, 0.0502) 0.0622 low: (6, -0.0038) -0.0038\n",
            "1 actual high: -1 actual low: -4\n",
            "high: (-6, 0.0251) 0.0261 low: (-6, 0.0375) 0.0436\n",
            "high: (-5, 0.0114) 0.038 low: (-5, -0.034) 0.0041\n",
            "high: (-4, 0.0242) 0.0632 low: (-4, 0.0366) 0.0466\n",
            "high: (-3, 0.0145) 0.0783 low: (-3, 0.0598) 0.1161\n",
            "high: (-2, 0.0261) 0.1055 low: (-2, 0.1027) 0.2354\n",
            "high: (-1, 0.0534) 0.1612 low: (-1, 0.1676) 0.4302\n",
            "high: (0, 0.3916) 0.408 low: (0, 0.4175) 0.4852\n",
            "high: (1, 0.124) 0.4309 low: (1, 0.049) 0.0846\n",
            "high: (2, 0.0903) 0.3017 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0883) 0.2076 low: (3, 0.0182) -0.0129\n",
            "high: (4, 0.0414) 0.1156 low: (4, -0.011) -0.034\n",
            "high: (5, -0.0041) 0.0725 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0737) 0.0768 low: (6, -0.0153) -0.0178\n",
            "2 actual high: -3 actual low: -6\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.104) 0.097\n",
            "high: (-5, 0.0162) 0.0441 low: (-5, -0.0014) 0.0957\n",
            "high: (-4, 0.0108) 0.0588 low: (-4, 0.0419) 0.1348\n",
            "high: (-3, 0.0114) 0.0743 low: (-3, 0.0667) 0.1971\n",
            "high: (-2, 0.029) 0.1137 low: (-2, 0.0421) 0.2364\n",
            "high: (-1, 0.0838) 0.2277 low: (-1, 0.1978) 0.4209\n",
            "high: (0, 0.3179) 0.4324 low: (0, 0.3918) 0.3656\n",
            "high: (1, 0.0926) 0.3399 low: (1, 0.0755) 0.2135\n",
            "high: (2, 0.0308) 0.214 low: (2, 0.0672) 0.143\n",
            "high: (3, 0.0538) 0.1721 low: (3, 0.0518) 0.0803\n",
            "high: (4, -0.0357) 0.0989 low: (4, 0.0165) 0.032\n",
            "high: (5, -0.0) 0.1474 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1084) 0.1474 low: (6, 0.0276) 0.0258\n",
            "3 actual high: -6 actual low: -6\n",
            "high: (-6, 0.0274) 0.0321 low: (-6, 0.1259) 0.1571\n",
            "high: (-5, 0.0001) 0.0323 low: (-5, 0.02) 0.182\n",
            "high: (-4, 0.0182) 0.0536 low: (-4, 0.0369) 0.228\n",
            "high: (-3, 0.046) 0.1075 low: (-3, 0.0389) 0.2766\n",
            "high: (-2, 0.0594) 0.1772 low: (-2, 0.0704) 0.3644\n",
            "high: (-1, 0.0894) 0.282 low: (-1, 0.0455) 0.4212\n",
            "high: (0, 0.2802) 0.3286 low: (0, 0.2609) 0.3255\n",
            "high: (1, 0.0596) 0.3894 low: (1, 0.0696) 0.2534\n",
            "high: (2, 0.1148) 0.3195 low: (2, 0.0688) 0.1665\n",
            "high: (3, 0.0704) 0.1848 low: (3, 0.0009) 0.0807\n",
            "high: (4, 0.0397) 0.1023 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0) 0.0557 low: (5, -0.0037) 0.0463\n",
            "high: (6, 0.0475) 0.0557 low: (6, 0.0408) 0.0509\n",
            "4 actual high: -6 actual low: -6\n",
            "high: (-6, 0.0571) 0.0759 low: (-6, 0.1696) 0.1614\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1607\n",
            "high: (-4, -0.0647) -0.0068 low: (-4, 0.0085) 0.1688\n",
            "high: (-3, 0.0379) 0.0436 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.0181) 0.0676 low: (-2, 0.0571) 0.3105\n",
            "high: (-1, 0.0488) 0.1325 low: (-1, 0.0911) 0.3972\n",
            "high: (0, 0.2473) 0.3286 low: (0, 0.3007) 0.2862\n",
            "high: (1, 0.0755) 0.5389 low: (1, 0.118) 0.3165\n",
            "high: (2, 0.0539) 0.4385 low: (2, 0.0665) 0.2042\n",
            "high: (3, 0.0913) 0.3669 low: (3, 0.0576) 0.1409\n",
            "high: (4, 0.0567) 0.2456 low: (4, 0.0193) 0.0861\n",
            "high: (5, 0.002) 0.1702 low: (5, 0.0153) 0.0677\n",
            "high: (6, 0.1261) 0.1676 low: (6, 0.0558) 0.0531\n",
            "------\n",
            "0 actual high: 2 actual low: -1\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.01) 0.0102\n",
            "high: (-5, 0.0383) 0.0393 low: (-5, 0.0216) 0.0321\n",
            "high: (-4, -0.0197) 0.0147 low: (-4, -0.0222) 0.0095\n",
            "high: (-3, -0.0234) -0.0145 low: (-3, 0.048) 0.0583\n",
            "high: (-2, 0.0065) -0.0064 low: (-2, 0.0797) 0.1392\n",
            "high: (-1, 0.0213) 0.0202 low: (-1, 0.3242) 0.4685\n",
            "high: (0, 0.5174) 0.6453 low: (0, 0.535) 0.5433\n",
            "high: (1, 0.175) 0.3345 low: (1, 0.0065) -0.0118\n",
            "high: (2, 0.0833) 0.1162 low: (2, 0.0018) -0.0184\n",
            "high: (3, 0.0021) 0.0123 low: (3, 0.0211) -0.0202\n",
            "high: (4, -0.035) 0.0097 low: (4, -0.0375) -0.0416\n",
            "high: (5, -0.0072) 0.0534 low: (5, 0.0002) -0.0036\n",
            "high: (6, 0.05) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0249) 0.0261 low: (-6, 0.0369) 0.0432\n",
            "high: (-5, 0.0113) 0.0379 low: (-5, -0.0336) 0.0039\n",
            "high: (-4, 0.0237) 0.0628 low: (-4, 0.0365) 0.0466\n",
            "high: (-3, 0.0147) 0.0782 low: (-3, 0.0593) 0.1159\n",
            "high: (-2, 0.0262) 0.1056 low: (-2, 0.1026) 0.2359\n",
            "high: (-1, 0.0527) 0.1608 low: (-1, 0.1667) 0.4309\n",
            "high: (0, 0.3893) 0.4079 low: (0, 0.4148) 0.4852\n",
            "high: (1, 0.1237) 0.4313 low: (1, 0.0482) 0.0839\n",
            "high: (2, 0.0902) 0.3017 low: (2, 0.0349) 0.0275\n",
            "high: (3, 0.0875) 0.2072 low: (3, 0.0183) -0.0133\n",
            "high: (4, 0.0414) 0.1156 low: (4, -0.0114) -0.0347\n",
            "high: (5, -0.0042) 0.0722 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0731) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: -2 actual low: -6\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1034) 0.097\n",
            "high: (-5, 0.0163) 0.0446 low: (-5, -0.0017) 0.0954\n",
            "high: (-4, 0.0108) 0.0594 low: (-4, 0.0416) 0.1344\n",
            "high: (-3, 0.0108) 0.0742 low: (-3, 0.0667) 0.197\n",
            "high: (-2, 0.0287) 0.1135 low: (-2, 0.0417) 0.2361\n",
            "high: (-1, 0.083) 0.2271 low: (-1, 0.1969) 0.4208\n",
            "high: (0, 0.3162) 0.4329 low: (0, 0.3891) 0.365\n",
            "high: (1, 0.0921) 0.34 low: (1, 0.0754) 0.2142\n",
            "high: (2, 0.0307) 0.2139 low: (2, 0.0672) 0.1434\n",
            "high: (3, 0.0531) 0.1718 low: (3, 0.0517) 0.0804\n",
            "high: (4, -0.0352) 0.0991 low: (4, 0.0164) 0.0319\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.01) 0.0165\n",
            "high: (6, 0.1078) 0.1476 low: (6, 0.0276) 0.0259\n",
            "3 actual high: -2 actual low: -6\n",
            "high: (-6, 0.027) 0.0317 low: (-6, 0.1258) 0.1575\n",
            "high: (-5, 0.0) 0.0317 low: (-5, 0.0199) 0.1825\n",
            "high: (-4, 0.0182) 0.0531 low: (-4, 0.0369) 0.2287\n",
            "high: (-3, 0.0463) 0.1075 low: (-3, 0.0388) 0.2773\n",
            "high: (-2, 0.0595) 0.1775 low: (-2, 0.0698) 0.3647\n",
            "high: (-1, 0.0891) 0.2822 low: (-1, 0.0455) 0.4217\n",
            "high: (0, 0.279) 0.3279 low: (0, 0.2597) 0.3252\n",
            "high: (1, 0.0594) 0.3899 low: (1, 0.0694) 0.2531\n",
            "high: (2, 0.1148) 0.3201 low: (2, 0.0685) 0.1662\n",
            "high: (3, 0.0701) 0.1851 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0393) 0.1027 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0005) 0.0565 low: (5, -0.0042) 0.0462\n",
            "high: (6, 0.0476) 0.0559 low: (6, 0.0411) 0.0515\n",
            "4 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0569) 0.0758 low: (-6, 0.1686) 0.1612\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0007) 0.1605\n",
            "high: (-4, -0.0641) -0.0064 low: (-4, 0.0083) 0.1685\n",
            "high: (-3, 0.0381) 0.0443 low: (-3, 0.0917) 0.2561\n",
            "high: (-2, 0.018) 0.0683 low: (-2, 0.0564) 0.31\n",
            "high: (-1, 0.049) 0.1336 low: (-1, 0.0908) 0.3968\n",
            "high: (0, 0.2459) 0.3275 low: (0, 0.2992) 0.286\n",
            "high: (1, 0.0756) 0.539 low: (1, 0.1174) 0.3171\n",
            "high: (2, 0.0533) 0.4383 low: (2, 0.0661) 0.2049\n",
            "high: (3, 0.091) 0.3673 low: (3, 0.0575) 0.1417\n",
            "high: (4, 0.0569) 0.2461 low: (4, 0.0191) 0.0867\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0154) 0.0685\n",
            "high: (6, 0.1258) 0.1675 low: (6, 0.0562) 0.0537\n",
            "------\n",
            "0 actual high: 0 actual low: -2\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.01) 0.0102\n",
            "high: (-5, 0.0383) 0.0393 low: (-5, 0.0216) 0.0321\n",
            "high: (-4, -0.0197) 0.0147 low: (-4, -0.0222) 0.0095\n",
            "high: (-3, -0.0234) -0.0145 low: (-3, 0.0479) 0.0582\n",
            "high: (-2, 0.0065) -0.0064 low: (-2, 0.0797) 0.1391\n",
            "high: (-1, 0.0213) 0.0202 low: (-1, 0.3243) 0.4684\n",
            "high: (0, 0.5175) 0.6454 low: (0, 0.5352) 0.5435\n",
            "high: (1, 0.175) 0.3344 low: (1, 0.0065) -0.0119\n",
            "high: (2, 0.0833) 0.1161 low: (2, 0.0018) -0.0185\n",
            "high: (3, 0.002) 0.0122 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0097 low: (4, -0.0375) -0.0417\n",
            "high: (5, -0.0072) 0.0534 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.05) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: -2 actual low: -6\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.037) 0.0432\n",
            "high: (-5, 0.0113) 0.038 low: (-5, -0.0337) 0.0039\n",
            "high: (-4, 0.0237) 0.0628 low: (-4, 0.0366) 0.0466\n",
            "high: (-3, 0.0147) 0.0782 low: (-3, 0.0593) 0.116\n",
            "high: (-2, 0.0262) 0.1057 low: (-2, 0.1026) 0.2359\n",
            "high: (-1, 0.0527) 0.1609 low: (-1, 0.1668) 0.4309\n",
            "high: (0, 0.3894) 0.4079 low: (0, 0.415) 0.4851\n",
            "high: (1, 0.1237) 0.4312 low: (1, 0.0483) 0.084\n",
            "high: (2, 0.0902) 0.3017 low: (2, 0.0349) 0.0276\n",
            "high: (3, 0.0876) 0.2072 low: (3, 0.0183) -0.0132\n",
            "high: (4, 0.0413) 0.1154 low: (4, -0.0113) -0.0346\n",
            "high: (5, -0.0042) 0.0722 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0731) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: -2 actual low: -6\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1034) 0.097\n",
            "high: (-5, 0.0162) 0.0445 low: (-5, -0.0017) 0.0954\n",
            "high: (-4, 0.0108) 0.0593 low: (-4, 0.0416) 0.1344\n",
            "high: (-3, 0.0109) 0.0742 low: (-3, 0.0667) 0.1969\n",
            "high: (-2, 0.0287) 0.1135 low: (-2, 0.0417) 0.236\n",
            "high: (-1, 0.0831) 0.2272 low: (-1, 0.1969) 0.4207\n",
            "high: (0, 0.3163) 0.4329 low: (0, 0.3893) 0.3651\n",
            "high: (1, 0.0922) 0.3399 low: (1, 0.0754) 0.2142\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0672) 0.1435\n",
            "high: (3, 0.0532) 0.1718 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0353) 0.0989 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.0099) 0.0166\n",
            "high: (6, 0.1078) 0.1475 low: (6, 0.0276) 0.0259\n",
            "3 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0271) 0.0318 low: (-6, 0.1258) 0.1575\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.0199) 0.1824\n",
            "high: (-4, 0.0182) 0.0534 low: (-4, 0.0368) 0.2285\n",
            "high: (-3, 0.0462) 0.1077 low: (-3, 0.0388) 0.2771\n",
            "high: (-2, 0.0595) 0.1776 low: (-2, 0.0699) 0.3646\n",
            "high: (-1, 0.0892) 0.2824 low: (-1, 0.0455) 0.4216\n",
            "high: (0, 0.2791) 0.328 low: (0, 0.2598) 0.3253\n",
            "high: (1, 0.0593) 0.3896 low: (1, 0.0694) 0.2531\n",
            "high: (2, 0.1148) 0.3199 low: (2, 0.0685) 0.1662\n",
            "high: (3, 0.0701) 0.185 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0394) 0.1026 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0004) 0.0563 low: (5, -0.0042) 0.0462\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.0411) 0.0515\n",
            "4 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0569) 0.0758 low: (-6, 0.1687) 0.1612\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0008) 0.1605\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0084) 0.1685\n",
            "high: (-3, 0.0381) 0.0442 low: (-3, 0.0917) 0.2561\n",
            "high: (-2, 0.018) 0.0682 low: (-2, 0.0565) 0.3101\n",
            "high: (-1, 0.0489) 0.1334 low: (-1, 0.0908) 0.3969\n",
            "high: (0, 0.246) 0.3277 low: (0, 0.2993) 0.286\n",
            "high: (1, 0.0755) 0.5389 low: (1, 0.1174) 0.3171\n",
            "high: (2, 0.0533) 0.4383 low: (2, 0.0662) 0.2049\n",
            "high: (3, 0.091) 0.3673 low: (3, 0.0575) 0.1416\n",
            "high: (4, 0.0569) 0.2461 low: (4, 0.0192) 0.0867\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0154) 0.0683\n",
            "high: (6, 0.1258) 0.1676 low: (6, 0.0561) 0.0536\n",
            "------\n",
            "0 actual high: 0 actual low: -3\n",
            "high: (-6, -0.007) -0.0088 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0383) 0.0392 low: (-5, 0.0217) 0.0324\n",
            "high: (-4, -0.0196) 0.0146 low: (-4, -0.0221) 0.0099\n",
            "high: (-3, -0.0234) -0.0146 low: (-3, 0.048) 0.0587\n",
            "high: (-2, 0.0064) -0.0066 low: (-2, 0.0795) 0.1396\n",
            "high: (-1, 0.0212) 0.0199 low: (-1, 0.3232) 0.4686\n",
            "high: (0, 0.5159) 0.6456 low: (0, 0.5333) 0.5427\n",
            "high: (1, 0.1744) 0.3345 low: (1, 0.0065) -0.0113\n",
            "high: (2, 0.0832) 0.1163 low: (2, 0.0021) -0.0179\n",
            "high: (3, 0.0022) 0.0121 low: (3, 0.0212) -0.02\n",
            "high: (4, -0.0351) 0.0094 low: (4, -0.0377) -0.0416\n",
            "high: (5, -0.0073) 0.0533 low: (5, 0.0004) -0.0033\n",
            "high: (6, 0.0499) 0.0624 low: (6, -0.0036) -0.0037\n",
            "1 actual high: 0 actual low: -3\n",
            "high: (-6, 0.0249) 0.0262 low: (-6, 0.0367) 0.0431\n",
            "high: (-5, 0.0111) 0.0378 low: (-5, -0.0335) 0.0038\n",
            "high: (-4, 0.0234) 0.0624 low: (-4, 0.0365) 0.0466\n",
            "high: (-3, 0.0149) 0.0781 low: (-3, 0.059) 0.1158\n",
            "high: (-2, 0.0263) 0.1057 low: (-2, 0.1025) 0.2361\n",
            "high: (-1, 0.0523) 0.1607 low: (-1, 0.1663) 0.4312\n",
            "high: (0, 0.3879) 0.4077 low: (0, 0.4134) 0.4851\n",
            "high: (1, 0.1235) 0.4316 low: (1, 0.0479) 0.0837\n",
            "high: (2, 0.0901) 0.3018 low: (2, 0.0349) 0.0275\n",
            "high: (3, 0.0871) 0.2071 low: (3, 0.0183) -0.0135\n",
            "high: (4, 0.0413) 0.1155 low: (4, -0.0115) -0.035\n",
            "high: (5, -0.0042) 0.0721 low: (5, -0.0029) -0.0215\n",
            "high: (6, 0.0728) 0.0765 low: (6, -0.0154) -0.0181\n",
            "2 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.103) 0.0969\n",
            "high: (-5, 0.0162) 0.0448 low: (-5, -0.0019) 0.0951\n",
            "high: (-4, 0.0109) 0.0597 low: (-4, 0.0415) 0.1342\n",
            "high: (-3, 0.0105) 0.0741 low: (-3, 0.0666) 0.1969\n",
            "high: (-2, 0.0285) 0.1133 low: (-2, 0.0415) 0.2359\n",
            "high: (-1, 0.0827) 0.2268 low: (-1, 0.1963) 0.4206\n",
            "high: (0, 0.3154) 0.4331 low: (0, 0.3877) 0.3648\n",
            "high: (1, 0.0919) 0.3401 low: (1, 0.0753) 0.2145\n",
            "high: (2, 0.0307) 0.2139 low: (2, 0.0671) 0.1437\n",
            "high: (3, 0.0528) 0.1718 low: (3, 0.0516) 0.0805\n",
            "high: (4, -0.035) 0.0993 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.01) 0.0166\n",
            "high: (6, 0.1075) 0.1476 low: (6, 0.0276) 0.026\n",
            "3 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0269) 0.0317 low: (-6, 0.1257) 0.1578\n",
            "high: (-5, 0.0001) 0.0318 low: (-5, 0.0199) 0.1827\n",
            "high: (-4, 0.0182) 0.0532 low: (-4, 0.0368) 0.2289\n",
            "high: (-3, 0.0464) 0.1078 low: (-3, 0.0388) 0.2776\n",
            "high: (-2, 0.0595) 0.1778 low: (-2, 0.0695) 0.3648\n",
            "high: (-1, 0.089) 0.2826 low: (-1, 0.0455) 0.4219\n",
            "high: (0, 0.2783) 0.3275 low: (0, 0.2591) 0.3252\n",
            "high: (1, 0.0592) 0.3899 low: (1, 0.0693) 0.2529\n",
            "high: (2, 0.1148) 0.3202 low: (2, 0.0684) 0.1659\n",
            "high: (3, 0.0699) 0.1851 low: (3, 0.0004) 0.0801\n",
            "high: (4, 0.0391) 0.1029 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0007) 0.0568 low: (5, -0.0045) 0.0461\n",
            "high: (6, 0.0476) 0.056 low: (6, 0.0412) 0.0517\n",
            "4 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0568) 0.0758 low: (-6, 0.1681) 0.161\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0008) 0.1603\n",
            "high: (-4, -0.0638) -0.0061 low: (-4, 0.0083) 0.1682\n",
            "high: (-3, 0.0382) 0.0448 low: (-3, 0.0917) 0.2561\n",
            "high: (-2, 0.0179) 0.0687 low: (-2, 0.0562) 0.3099\n",
            "high: (-1, 0.049) 0.1341 low: (-1, 0.0906) 0.3967\n",
            "high: (0, 0.2452) 0.3271 low: (0, 0.2983) 0.2858\n",
            "high: (1, 0.0756) 0.5389 low: (1, 0.1171) 0.3175\n",
            "high: (2, 0.053) 0.438 low: (2, 0.0659) 0.2053\n",
            "high: (3, 0.0908) 0.3673 low: (3, 0.0575) 0.1422\n",
            "high: (4, 0.0569) 0.2462 low: (4, 0.0191) 0.0871\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0155) 0.0688\n",
            "high: (6, 0.1256) 0.1675 low: (6, 0.0563) 0.0539\n",
            "------\n",
            "0 actual high: 0 actual low: -3\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0383) 0.0394 low: (-5, 0.0217) 0.0323\n",
            "high: (-4, -0.0196) 0.0149 low: (-4, -0.0222) 0.0098\n",
            "high: (-3, -0.0234) -0.0144 low: (-3, 0.048) 0.0586\n",
            "high: (-2, 0.0065) -0.0062 low: (-2, 0.0796) 0.1396\n",
            "high: (-1, 0.0212) 0.0202 low: (-1, 0.3236) 0.4687\n",
            "high: (0, 0.5164) 0.6453 low: (0, 0.5338) 0.543\n",
            "high: (1, 0.1746) 0.3344 low: (1, 0.0065) -0.0117\n",
            "high: (2, 0.0832) 0.1162 low: (2, 0.0019) -0.0183\n",
            "high: (3, 0.0021) 0.0122 low: (3, 0.0211) -0.0202\n",
            "high: (4, -0.035) 0.0096 low: (4, -0.0376) -0.0417\n",
            "high: (5, -0.0072) 0.0534 low: (5, 0.0003) -0.0035\n",
            "high: (6, 0.0499) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0249) 0.0261 low: (-6, 0.0368) 0.0431\n",
            "high: (-5, 0.0111) 0.0378 low: (-5, -0.0336) 0.0037\n",
            "high: (-4, 0.0236) 0.0626 low: (-4, 0.0366) 0.0466\n",
            "high: (-3, 0.0149) 0.0782 low: (-3, 0.0591) 0.1158\n",
            "high: (-2, 0.0263) 0.1058 low: (-2, 0.1026) 0.236\n",
            "high: (-1, 0.0525) 0.161 low: (-1, 0.1666) 0.4312\n",
            "high: (0, 0.3884) 0.4078 low: (0, 0.4139) 0.4848\n",
            "high: (1, 0.1235) 0.4312 low: (1, 0.0482) 0.084\n",
            "high: (2, 0.0901) 0.3016 low: (2, 0.0349) 0.0275\n",
            "high: (3, 0.0872) 0.207 low: (3, 0.0183) -0.0134\n",
            "high: (4, 0.0412) 0.1154 low: (4, -0.0114) -0.0348\n",
            "high: (5, -0.0042) 0.0721 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0729) 0.0765 low: (6, -0.0154) -0.018\n",
            "2 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.1032) 0.097\n",
            "high: (-5, 0.0161) 0.0446 low: (-5, -0.0018) 0.0953\n",
            "high: (-4, 0.0109) 0.0595 low: (-4, 0.0415) 0.1343\n",
            "high: (-3, 0.0106) 0.0741 low: (-3, 0.0666) 0.1969\n",
            "high: (-2, 0.0285) 0.1132 low: (-2, 0.0415) 0.2359\n",
            "high: (-1, 0.0829) 0.2269 low: (-1, 0.1964) 0.4206\n",
            "high: (0, 0.3157) 0.433 low: (0, 0.3883) 0.365\n",
            "high: (1, 0.092) 0.3401 low: (1, 0.0754) 0.2144\n",
            "high: (2, 0.0307) 0.214 low: (2, 0.0671) 0.1435\n",
            "high: (3, 0.053) 0.1719 low: (3, 0.0516) 0.0805\n",
            "high: (4, -0.0351) 0.0992 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.0099) 0.0165\n",
            "high: (6, 0.1076) 0.1476 low: (6, 0.0275) 0.0259\n",
            "3 actual high: 0 actual low: -2\n",
            "high: (-6, 0.027) 0.0318 low: (-6, 0.1257) 0.1576\n",
            "high: (-5, 0.0001) 0.0319 low: (-5, 0.0199) 0.1826\n",
            "high: (-4, 0.0182) 0.0533 low: (-4, 0.0368) 0.2287\n",
            "high: (-3, 0.0463) 0.1077 low: (-3, 0.0388) 0.2774\n",
            "high: (-2, 0.0596) 0.1778 low: (-2, 0.0697) 0.3648\n",
            "high: (-1, 0.0891) 0.2826 low: (-1, 0.0454) 0.4217\n",
            "high: (0, 0.2786) 0.3277 low: (0, 0.2593) 0.3251\n",
            "high: (1, 0.0592) 0.3897 low: (1, 0.0694) 0.2532\n",
            "high: (2, 0.1147) 0.32 low: (2, 0.0685) 0.1661\n",
            "high: (3, 0.07) 0.1851 low: (3, 0.0005) 0.0803\n",
            "high: (4, 0.0393) 0.1028 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0006) 0.0566 low: (5, -0.0043) 0.0461\n",
            "high: (6, 0.0475) 0.0559 low: (6, 0.0411) 0.0515\n",
            "4 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0568) 0.0758 low: (-6, 0.1683) 0.1611\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0008) 0.1603\n",
            "high: (-4, -0.064) -0.0064 low: (-4, 0.0082) 0.1682\n",
            "high: (-3, 0.0381) 0.0444 low: (-3, 0.0917) 0.2559\n",
            "high: (-2, 0.0179) 0.0683 low: (-2, 0.0564) 0.3099\n",
            "high: (-1, 0.0489) 0.1335 low: (-1, 0.0907) 0.3967\n",
            "high: (0, 0.2454) 0.3274 low: (0, 0.2986) 0.2858\n",
            "high: (1, 0.0755) 0.5391 low: (1, 0.1172) 0.3175\n",
            "high: (2, 0.0532) 0.4384 low: (2, 0.0661) 0.2053\n",
            "high: (3, 0.0909) 0.3674 low: (3, 0.0575) 0.142\n",
            "high: (4, 0.0568) 0.2461 low: (4, 0.0192) 0.087\n",
            "high: (5, 0.002) 0.1704 low: (5, 0.0155) 0.0686\n",
            "high: (6, 0.1257) 0.1677 low: (6, 0.0562) 0.0538\n",
            "------\n",
            "0 actual high: 1 actual low: -1\n",
            "high: (-6, -0.0067) -0.0084 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0217) 0.0323\n",
            "high: (-4, -0.0196) 0.0149 low: (-4, -0.0223) 0.0097\n",
            "high: (-3, -0.0234) -0.0144 low: (-3, 0.048) 0.0585\n",
            "high: (-2, 0.0066) -0.0061 low: (-2, 0.0796) 0.1394\n",
            "high: (-1, 0.0212) 0.0203 low: (-1, 0.3239) 0.4686\n",
            "high: (0, 0.5169) 0.6452 low: (0, 0.5343) 0.5432\n",
            "high: (1, 0.1748) 0.3345 low: (1, 0.0066) -0.0118\n",
            "high: (2, 0.0832) 0.1163 low: (2, 0.0018) -0.0185\n",
            "high: (3, 0.0021) 0.0125 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0099 low: (4, -0.0375) -0.0418\n",
            "high: (5, -0.0071) 0.0535 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.05) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.037) 0.0433\n",
            "high: (-5, 0.0111) 0.0379 low: (-5, -0.0336) 0.004\n",
            "high: (-4, 0.0237) 0.0627 low: (-4, 0.0366) 0.0468\n",
            "high: (-3, 0.0149) 0.0783 low: (-3, 0.0593) 0.1161\n",
            "high: (-2, 0.0262) 0.1058 low: (-2, 0.1026) 0.2361\n",
            "high: (-1, 0.0527) 0.1611 low: (-1, 0.1668) 0.4311\n",
            "high: (0, 0.3889) 0.4078 low: (0, 0.4144) 0.4846\n",
            "high: (1, 0.1235) 0.4312 low: (1, 0.0484) 0.0843\n",
            "high: (2, 0.0901) 0.3017 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0874) 0.2072 low: (3, 0.0183) -0.0131\n",
            "high: (4, 0.0412) 0.1155 low: (4, -0.0112) -0.0345\n",
            "high: (5, -0.004) 0.0723 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.073) 0.0765 low: (6, -0.0154) -0.018\n",
            "2 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.1034) 0.0971\n",
            "high: (-5, 0.016) 0.0444 low: (-5, -0.0018) 0.0954\n",
            "high: (-4, 0.0109) 0.0593 low: (-4, 0.0416) 0.1345\n",
            "high: (-3, 0.0108) 0.0741 low: (-3, 0.0665) 0.1969\n",
            "high: (-2, 0.0286) 0.1132 low: (-2, 0.0416) 0.236\n",
            "high: (-1, 0.0831) 0.227 low: (-1, 0.1965) 0.4205\n",
            "high: (0, 0.3161) 0.4328 low: (0, 0.3888) 0.3651\n",
            "high: (1, 0.0922) 0.3401 low: (1, 0.0754) 0.2143\n",
            "high: (2, 0.0308) 0.2139 low: (2, 0.0671) 0.1435\n",
            "high: (3, 0.0531) 0.1717 low: (3, 0.0516) 0.0805\n",
            "high: (4, -0.0353) 0.099 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0001) 0.1473 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1077) 0.1475 low: (6, 0.0275) 0.0258\n",
            "3 actual high: 0 actual low: -2\n",
            "high: (-6, 0.027) 0.0317 low: (-6, 0.1256) 0.1574\n",
            "high: (-5, 0.0001) 0.0319 low: (-5, 0.02) 0.1824\n",
            "high: (-4, 0.0183) 0.0534 low: (-4, 0.0368) 0.2285\n",
            "high: (-3, 0.0463) 0.1078 low: (-3, 0.0389) 0.2773\n",
            "high: (-2, 0.0596) 0.1779 low: (-2, 0.0699) 0.3649\n",
            "high: (-1, 0.0892) 0.2827 low: (-1, 0.0453) 0.4216\n",
            "high: (0, 0.2788) 0.3278 low: (0, 0.2596) 0.3253\n",
            "high: (1, 0.0592) 0.3895 low: (1, 0.0694) 0.2531\n",
            "high: (2, 0.1147) 0.3199 low: (2, 0.0686) 0.1661\n",
            "high: (3, 0.0701) 0.185 low: (3, 0.0005) 0.0802\n",
            "high: (4, 0.0394) 0.1026 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0004) 0.0563 low: (5, -0.0042) 0.0461\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.041) 0.0514\n",
            "4 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0569) 0.0759 low: (-6, 0.1685) 0.1611\n",
            "high: (-5, 0.0024) 0.0791 low: (-5, -0.0008) 0.1604\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0083) 0.1683\n",
            "high: (-3, 0.0381) 0.0443 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.0179) 0.0682 low: (-2, 0.0565) 0.31\n",
            "high: (-1, 0.0488) 0.1333 low: (-1, 0.0907) 0.3967\n",
            "high: (0, 0.2457) 0.3278 low: (0, 0.2989) 0.2858\n",
            "high: (1, 0.0754) 0.539 low: (1, 0.1174) 0.3175\n",
            "high: (2, 0.0533) 0.4384 low: (2, 0.0662) 0.2052\n",
            "high: (3, 0.0909) 0.3673 low: (3, 0.0576) 0.1419\n",
            "high: (4, 0.0567) 0.246 low: (4, 0.0193) 0.0868\n",
            "high: (5, 0.002) 0.1704 low: (5, 0.0155) 0.0684\n",
            "high: (6, 0.1257) 0.1677 low: (6, 0.056) 0.0535\n",
            "------\n",
            "0 actual high: 2 actual low: 0\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0383) 0.0394 low: (-5, 0.0217) 0.0323\n",
            "high: (-4, -0.0196) 0.0149 low: (-4, -0.0222) 0.0098\n",
            "high: (-3, -0.0234) -0.0144 low: (-3, 0.048) 0.0586\n",
            "high: (-2, 0.0066) -0.0061 low: (-2, 0.0796) 0.1396\n",
            "high: (-1, 0.0212) 0.0204 low: (-1, 0.3236) 0.4687\n",
            "high: (0, 0.5164) 0.6452 low: (0, 0.5338) 0.543\n",
            "high: (1, 0.1746) 0.3345 low: (1, 0.0066) -0.0117\n",
            "high: (2, 0.0831) 0.1163 low: (2, 0.0019) -0.0184\n",
            "high: (3, 0.0021) 0.0125 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0099 low: (4, -0.0376) -0.0418\n",
            "high: (5, -0.0071) 0.0536 low: (5, 0.0002) -0.0036\n",
            "high: (6, 0.05) 0.0625 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.0369) 0.0432\n",
            "high: (-5, 0.0111) 0.0379 low: (-5, -0.0336) 0.0039\n",
            "high: (-4, 0.0236) 0.0627 low: (-4, 0.0366) 0.0467\n",
            "high: (-3, 0.0149) 0.0783 low: (-3, 0.0592) 0.116\n",
            "high: (-2, 0.0262) 0.1058 low: (-2, 0.1026) 0.2361\n",
            "high: (-1, 0.0526) 0.161 low: (-1, 0.1666) 0.4312\n",
            "high: (0, 0.3885) 0.4078 low: (0, 0.414) 0.4847\n",
            "high: (1, 0.1235) 0.4311 low: (1, 0.0483) 0.0842\n",
            "high: (2, 0.09) 0.3015 low: (2, 0.0349) 0.0276\n",
            "high: (3, 0.0872) 0.207 low: (3, 0.0183) -0.0132\n",
            "high: (4, 0.0412) 0.1155 low: (4, -0.0113) -0.0347\n",
            "high: (5, -0.0041) 0.0722 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0729) 0.0765 low: (6, -0.0154) -0.018\n",
            "2 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0164) 0.0225 low: (-6, 0.1033) 0.0971\n",
            "high: (-5, 0.016) 0.0444 low: (-5, -0.0018) 0.0954\n",
            "high: (-4, 0.0109) 0.0594 low: (-4, 0.0416) 0.1345\n",
            "high: (-3, 0.0107) 0.074 low: (-3, 0.0665) 0.197\n",
            "high: (-2, 0.0285) 0.1131 low: (-2, 0.0416) 0.2361\n",
            "high: (-1, 0.083) 0.2269 low: (-1, 0.1964) 0.4206\n",
            "high: (0, 0.3158) 0.433 low: (0, 0.3884) 0.365\n",
            "high: (1, 0.0921) 0.3401 low: (1, 0.0754) 0.2144\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0671) 0.1435\n",
            "high: (3, 0.053) 0.1717 low: (3, 0.0516) 0.0804\n",
            "high: (4, -0.0352) 0.099 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.0099) 0.0165\n",
            "high: (6, 0.1076) 0.1475 low: (6, 0.0275) 0.0258\n",
            "3 actual high: 1 actual low: -1\n",
            "high: (-6, 0.027) 0.0318 low: (-6, 0.1256) 0.1575\n",
            "high: (-5, 0.0001) 0.0319 low: (-5, 0.02) 0.1825\n",
            "high: (-4, 0.0183) 0.0534 low: (-4, 0.0368) 0.2287\n",
            "high: (-3, 0.0463) 0.1078 low: (-3, 0.0389) 0.2774\n",
            "high: (-2, 0.0596) 0.1779 low: (-2, 0.0698) 0.3649\n",
            "high: (-1, 0.0892) 0.2828 low: (-1, 0.0453) 0.4217\n",
            "high: (0, 0.2786) 0.3276 low: (0, 0.2594) 0.3252\n",
            "high: (1, 0.0592) 0.3895 low: (1, 0.0694) 0.2531\n",
            "high: (2, 0.1147) 0.3199 low: (2, 0.0686) 0.1661\n",
            "high: (3, 0.07) 0.185 low: (3, 0.0005) 0.0801\n",
            "high: (4, 0.0393) 0.1027 low: (4, 0.0267) 0.0795\n",
            "high: (5, 0.0005) 0.0565 low: (5, -0.0043) 0.046\n",
            "high: (6, 0.0475) 0.0559 low: (6, 0.041) 0.0514\n",
            "4 actual high: 3 actual low: -1\n",
            "high: (-6, 0.0569) 0.0759 low: (-6, 0.1684) 0.1611\n",
            "high: (-5, 0.0024) 0.0791 low: (-5, -0.0009) 0.1603\n",
            "high: (-4, -0.0641) -0.0064 low: (-4, 0.0083) 0.1682\n",
            "high: (-3, 0.0381) 0.0444 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.0179) 0.0683 low: (-2, 0.0564) 0.3099\n",
            "high: (-1, 0.0488) 0.1335 low: (-1, 0.0907) 0.3967\n",
            "high: (0, 0.2454) 0.3275 low: (0, 0.2986) 0.2857\n",
            "high: (1, 0.0754) 0.539 low: (1, 0.1173) 0.3176\n",
            "high: (2, 0.0532) 0.4384 low: (2, 0.0661) 0.2053\n",
            "high: (3, 0.0909) 0.3674 low: (3, 0.0576) 0.1421\n",
            "high: (4, 0.0567) 0.2461 low: (4, 0.0193) 0.087\n",
            "high: (5, 0.002) 0.1704 low: (5, 0.0155) 0.0685\n",
            "high: (6, 0.1257) 0.1678 low: (6, 0.0561) 0.0537\n",
            "------\n",
            "0 actual high: 0 actual low: -2\n",
            "high: (-6, -0.0066) -0.0082 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0217) 0.0323\n",
            "high: (-4, -0.0196) 0.0149 low: (-4, -0.0224) 0.0095\n",
            "high: (-3, -0.0234) -0.0142 low: (-3, 0.0479) 0.0582\n",
            "high: (-2, 0.0068) -0.0057 low: (-2, 0.0797) 0.1391\n",
            "high: (-1, 0.0213) 0.0208 low: (-1, 0.3244) 0.4685\n",
            "high: (0, 0.5177) 0.6449 low: (0, 0.5352) 0.5434\n",
            "high: (1, 0.1751) 0.3343 low: (1, 0.0066) -0.0119\n",
            "high: (2, 0.0832) 0.1162 low: (2, 0.0017) -0.0186\n",
            "high: (3, 0.002) 0.0126 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.0101 low: (4, -0.0374) -0.0417\n",
            "high: (5, -0.007) 0.0537 low: (5, 0.0) -0.0038\n",
            "high: (6, 0.0501) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 0 actual low: -2\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.0371) 0.0433\n",
            "high: (-5, 0.0112) 0.0379 low: (-5, -0.0337) 0.004\n",
            "high: (-4, 0.0239) 0.0629 low: (-4, 0.0367) 0.0468\n",
            "high: (-3, 0.0148) 0.0784 low: (-3, 0.0595) 0.1162\n",
            "high: (-2, 0.0262) 0.1058 low: (-2, 0.1028) 0.2362\n",
            "high: (-1, 0.053) 0.1613 low: (-1, 0.167) 0.431\n",
            "high: (0, 0.3897) 0.4078 low: (0, 0.4153) 0.4846\n",
            "high: (1, 0.1236) 0.4309 low: (1, 0.0486) 0.0844\n",
            "high: (2, 0.0901) 0.3016 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0876) 0.2073 low: (3, 0.0183) -0.0131\n",
            "high: (4, 0.0413) 0.1156 low: (4, -0.0111) -0.0344\n",
            "high: (5, -0.004) 0.0724 low: (5, -0.003) -0.0215\n",
            "high: (6, 0.0732) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1036) 0.0971\n",
            "high: (-5, 0.016) 0.0441 low: (-5, -0.0016) 0.0956\n",
            "high: (-4, 0.0109) 0.059 low: (-4, 0.0417) 0.1347\n",
            "high: (-3, 0.011) 0.0741 low: (-3, 0.0666) 0.1971\n",
            "high: (-2, 0.0287) 0.1133 low: (-2, 0.0418) 0.2363\n",
            "high: (-1, 0.0833) 0.2272 low: (-1, 0.1969) 0.4208\n",
            "high: (0, 0.3166) 0.4328 low: (0, 0.3897) 0.3652\n",
            "high: (1, 0.0923) 0.3401 low: (1, 0.0754) 0.214\n",
            "high: (2, 0.0308) 0.2139 low: (2, 0.0671) 0.1433\n",
            "high: (3, 0.0534) 0.1718 low: (3, 0.0517) 0.0804\n",
            "high: (4, -0.0355) 0.0988 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0001) 0.1473 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1079) 0.1475 low: (6, 0.0275) 0.0258\n",
            "3 actual high: 2 actual low: -2\n",
            "high: (-6, 0.0271) 0.0319 low: (-6, 0.1256) 0.1572\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.02) 0.1823\n",
            "high: (-4, 0.0183) 0.0535 low: (-4, 0.0368) 0.2283\n",
            "high: (-3, 0.0462) 0.1078 low: (-3, 0.0389) 0.277\n",
            "high: (-2, 0.0595) 0.1777 low: (-2, 0.07) 0.3646\n",
            "high: (-1, 0.0893) 0.2827 low: (-1, 0.0453) 0.4213\n",
            "high: (0, 0.2792) 0.3282 low: (0, 0.2599) 0.3253\n",
            "high: (1, 0.0593) 0.3891 low: (1, 0.0695) 0.2533\n",
            "high: (2, 0.1146) 0.3194 low: (2, 0.0687) 0.1664\n",
            "high: (3, 0.0701) 0.1847 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0394) 0.1023 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0002) 0.056 low: (5, -0.004) 0.0462\n",
            "high: (6, 0.0474) 0.0557 low: (6, 0.0409) 0.0512\n",
            "4 actual high: 3 actual low: 0\n",
            "high: (-6, 0.057) 0.0759 low: (-6, 0.1688) 0.1612\n",
            "high: (-5, 0.0024) 0.0791 low: (-5, -0.0009) 0.1603\n",
            "high: (-4, -0.0644) -0.0067 low: (-4, 0.0084) 0.1683\n",
            "high: (-3, 0.0381) 0.0441 low: (-3, 0.0917) 0.2559\n",
            "high: (-2, 0.018) 0.0681 low: (-2, 0.0568) 0.3101\n",
            "high: (-1, 0.0488) 0.1331 low: (-1, 0.0908) 0.3968\n",
            "high: (0, 0.2461) 0.3279 low: (0, 0.2993) 0.2858\n",
            "high: (1, 0.0754) 0.539 low: (1, 0.1176) 0.3174\n",
            "high: (2, 0.0535) 0.4385 low: (2, 0.0663) 0.2051\n",
            "high: (3, 0.0911) 0.3672 low: (3, 0.0577) 0.1418\n",
            "high: (4, 0.0567) 0.2458 low: (4, 0.0194) 0.0867\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0155) 0.0682\n",
            "high: (6, 0.1258) 0.1676 low: (6, 0.0559) 0.0534\n",
            "------\n",
            "0 actual high: 1 actual low: -1\n",
            "high: (-6, -0.0067) -0.0084 low: (-6, 0.0101) 0.0103\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0217) 0.0323\n",
            "high: (-4, -0.0196) 0.0148 low: (-4, -0.0223) 0.0096\n",
            "high: (-3, -0.0234) -0.0143 low: (-3, 0.048) 0.0584\n",
            "high: (-2, 0.0067) -0.006 low: (-2, 0.0797) 0.1394\n",
            "high: (-1, 0.0213) 0.0206 low: (-1, 0.3241) 0.4686\n",
            "high: (0, 0.5172) 0.645 low: (0, 0.5347) 0.5431\n",
            "high: (1, 0.1749) 0.3344 low: (1, 0.0066) -0.0117\n",
            "high: (2, 0.0832) 0.1162 low: (2, 0.0018) -0.0184\n",
            "high: (3, 0.0021) 0.0125 low: (3, 0.0211) -0.0202\n",
            "high: (4, -0.035) 0.0099 low: (4, -0.0375) -0.0416\n",
            "high: (5, -0.0071) 0.0535 low: (5, 0.0002) -0.0036\n",
            "high: (6, 0.05) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.037) 0.0432\n",
            "high: (-5, 0.0112) 0.0379 low: (-5, -0.0336) 0.004\n",
            "high: (-4, 0.0238) 0.0629 low: (-4, 0.0367) 0.0469\n",
            "high: (-3, 0.0148) 0.0784 low: (-3, 0.0594) 0.1163\n",
            "high: (-2, 0.0262) 0.1058 low: (-2, 0.1027) 0.2363\n",
            "high: (-1, 0.0528) 0.1611 low: (-1, 0.1668) 0.4312\n",
            "high: (0, 0.3892) 0.4078 low: (0, 0.4147) 0.4846\n",
            "high: (1, 0.1236) 0.4311 low: (1, 0.0484) 0.0841\n",
            "high: (2, 0.0901) 0.3016 low: (2, 0.0349) 0.0276\n",
            "high: (3, 0.0875) 0.2072 low: (3, 0.0183) -0.0132\n",
            "high: (4, 0.0413) 0.1156 low: (4, -0.0113) -0.0346\n",
            "high: (5, -0.0041) 0.0723 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0731) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 3 actual low: -1\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1035) 0.0971\n",
            "high: (-5, 0.0161) 0.0443 low: (-5, -0.0017) 0.0955\n",
            "high: (-4, 0.0109) 0.0592 low: (-4, 0.0417) 0.1347\n",
            "high: (-3, 0.0109) 0.0742 low: (-3, 0.0666) 0.1971\n",
            "high: (-2, 0.0287) 0.1134 low: (-2, 0.0417) 0.2363\n",
            "high: (-1, 0.0832) 0.2273 low: (-1, 0.1967) 0.4209\n",
            "high: (0, 0.3163) 0.4328 low: (0, 0.3891) 0.3651\n",
            "high: (1, 0.0922) 0.34 low: (1, 0.0754) 0.214\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0671) 0.1433\n",
            "high: (3, 0.0532) 0.1718 low: (3, 0.0516) 0.0803\n",
            "high: (4, -0.0353) 0.0991 low: (4, 0.0164) 0.0319\n",
            "high: (5, -0.0001) 0.1474 low: (5, -0.0099) 0.0165\n",
            "high: (6, 0.1078) 0.1475 low: (6, 0.0275) 0.0258\n",
            "3 actual high: 4 actual low: 1\n",
            "high: (-6, 0.0271) 0.0319 low: (-6, 0.1256) 0.1573\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.02) 0.1823\n",
            "high: (-4, 0.0183) 0.0535 low: (-4, 0.0368) 0.2284\n",
            "high: (-3, 0.0462) 0.1078 low: (-3, 0.0389) 0.2771\n",
            "high: (-2, 0.0595) 0.1778 low: (-2, 0.0699) 0.3647\n",
            "high: (-1, 0.0892) 0.2826 low: (-1, 0.0454) 0.4215\n",
            "high: (0, 0.279) 0.328 low: (0, 0.2597) 0.3252\n",
            "high: (1, 0.0592) 0.3894 low: (1, 0.0694) 0.2532\n",
            "high: (2, 0.1147) 0.3198 low: (2, 0.0686) 0.1663\n",
            "high: (3, 0.0701) 0.1849 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0393) 0.1025 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0004) 0.0563 low: (5, -0.0041) 0.0462\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.041) 0.0513\n",
            "4 actual high: 3 actual low: 1\n",
            "high: (-6, 0.0569) 0.0758 low: (-6, 0.1686) 0.1612\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0009) 0.1603\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0083) 0.1682\n",
            "high: (-3, 0.0381) 0.0442 low: (-3, 0.0917) 0.2559\n",
            "high: (-2, 0.018) 0.0682 low: (-2, 0.0566) 0.31\n",
            "high: (-1, 0.0488) 0.1333 low: (-1, 0.0908) 0.3968\n",
            "high: (0, 0.2459) 0.3277 low: (0, 0.299) 0.2858\n",
            "high: (1, 0.0755) 0.539 low: (1, 0.1174) 0.3174\n",
            "high: (2, 0.0534) 0.4384 low: (2, 0.0662) 0.2051\n",
            "high: (3, 0.091) 0.3672 low: (3, 0.0576) 0.1419\n",
            "high: (4, 0.0567) 0.2459 low: (4, 0.0193) 0.0868\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0155) 0.0683\n",
            "high: (6, 0.1258) 0.1677 low: (6, 0.056) 0.0535\n",
            "------\n",
            "0 actual high: 1 actual low: -1\n",
            "high: (-6, -0.0067) -0.0084 low: (-6, 0.01) 0.0102\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0217) 0.0322\n",
            "high: (-4, -0.0196) 0.0148 low: (-4, -0.0223) 0.0095\n",
            "high: (-3, -0.0234) -0.0143 low: (-3, 0.0479) 0.0582\n",
            "high: (-2, 0.0067) -0.006 low: (-2, 0.0797) 0.1391\n",
            "high: (-1, 0.0213) 0.0206 low: (-1, 0.3243) 0.4684\n",
            "high: (0, 0.5175) 0.6451 low: (0, 0.5351) 0.5434\n",
            "high: (1, 0.175) 0.3343 low: (1, 0.0066) -0.0118\n",
            "high: (2, 0.0832) 0.1162 low: (2, 0.0018) -0.0185\n",
            "high: (3, 0.002) 0.0125 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.035) 0.01 low: (4, -0.0375) -0.0417\n",
            "high: (5, -0.0071) 0.0536 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.0501) 0.0625 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 3 actual low: -1\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.0371) 0.0433\n",
            "high: (-5, 0.0112) 0.0379 low: (-5, -0.0337) 0.004\n",
            "high: (-4, 0.0239) 0.0629 low: (-4, 0.0367) 0.0468\n",
            "high: (-3, 0.0148) 0.0784 low: (-3, 0.0594) 0.1162\n",
            "high: (-2, 0.0262) 0.1059 low: (-2, 0.1027) 0.2361\n",
            "high: (-1, 0.0529) 0.1612 low: (-1, 0.1669) 0.431\n",
            "high: (0, 0.3895) 0.4078 low: (0, 0.4151) 0.4847\n",
            "high: (1, 0.1236) 0.4309 low: (1, 0.0485) 0.0843\n",
            "high: (2, 0.0901) 0.3015 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0876) 0.2072 low: (3, 0.0183) -0.0131\n",
            "high: (4, 0.0413) 0.1155 low: (4, -0.0112) -0.0344\n",
            "high: (5, -0.0041) 0.0722 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0731) 0.0765 low: (6, -0.0154) -0.018\n",
            "2 actual high: 4 actual low: 1\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1035) 0.097\n",
            "high: (-5, 0.0161) 0.0443 low: (-5, -0.0017) 0.0955\n",
            "high: (-4, 0.0109) 0.0592 low: (-4, 0.0417) 0.1346\n",
            "high: (-3, 0.0109) 0.0741 low: (-3, 0.0666) 0.197\n",
            "high: (-2, 0.0287) 0.1133 low: (-2, 0.0418) 0.2362\n",
            "high: (-1, 0.0833) 0.2272 low: (-1, 0.1968) 0.4207\n",
            "high: (0, 0.3165) 0.4327 low: (0, 0.3895) 0.3652\n",
            "high: (1, 0.0923) 0.34 low: (1, 0.0754) 0.2141\n",
            "high: (2, 0.0308) 0.2138 low: (2, 0.0671) 0.1434\n",
            "high: (3, 0.0533) 0.1717 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0354) 0.0989 low: (4, 0.0164) 0.032\n",
            "high: (5, -0.0001) 0.1473 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1078) 0.1474 low: (6, 0.0275) 0.0258\n",
            "3 actual high: 3 actual low: 1\n",
            "high: (-6, 0.0271) 0.0319 low: (-6, 0.1256) 0.1572\n",
            "high: (-5, 0.0001) 0.032 low: (-5, 0.02) 0.1823\n",
            "high: (-4, 0.0182) 0.0534 low: (-4, 0.0368) 0.2283\n",
            "high: (-3, 0.0462) 0.1077 low: (-3, 0.0389) 0.277\n",
            "high: (-2, 0.0595) 0.1776 low: (-2, 0.07) 0.3647\n",
            "high: (-1, 0.0893) 0.2826 low: (-1, 0.0454) 0.4215\n",
            "high: (0, 0.2791) 0.328 low: (0, 0.2598) 0.3252\n",
            "high: (1, 0.0593) 0.3894 low: (1, 0.0695) 0.2533\n",
            "high: (2, 0.1147) 0.3197 low: (2, 0.0686) 0.1662\n",
            "high: (3, 0.0701) 0.1849 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0394) 0.1025 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0003) 0.0562 low: (5, -0.004) 0.0462\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.0409) 0.0512\n",
            "4 actual high: 6 actual low: 2\n",
            "high: (-6, 0.0569) 0.0758 low: (-6, 0.1688) 0.1612\n",
            "high: (-5, 0.0024) 0.079 low: (-5, -0.0009) 0.1604\n",
            "high: (-4, -0.0643) -0.0067 low: (-4, 0.0084) 0.1684\n",
            "high: (-3, 0.0381) 0.0441 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.018) 0.0681 low: (-2, 0.0567) 0.3102\n",
            "high: (-1, 0.0488) 0.1331 low: (-1, 0.0909) 0.397\n",
            "high: (0, 0.246) 0.3278 low: (0, 0.2992) 0.2858\n",
            "high: (1, 0.0755) 0.539 low: (1, 0.1175) 0.3172\n",
            "high: (2, 0.0535) 0.4384 low: (2, 0.0662) 0.205\n",
            "high: (3, 0.091) 0.3671 low: (3, 0.0576) 0.1418\n",
            "high: (4, 0.0567) 0.2459 low: (4, 0.0193) 0.0867\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0155) 0.0683\n",
            "high: (6, 0.1258) 0.1676 low: (6, 0.056) 0.0535\n",
            "------\n",
            "0 actual high: 3 actual low: -1\n",
            "high: (-6, -0.0067) -0.0083 low: (-6, 0.01) 0.0101\n",
            "high: (-5, 0.0382) 0.0392 low: (-5, 0.0217) 0.0322\n",
            "high: (-4, -0.0196) 0.0148 low: (-4, -0.0224) 0.0094\n",
            "high: (-3, -0.0234) -0.0143 low: (-3, 0.0479) 0.058\n",
            "high: (-2, 0.0067) -0.006 low: (-2, 0.0798) 0.139\n",
            "high: (-1, 0.0213) 0.0205 low: (-1, 0.3246) 0.4684\n",
            "high: (0, 0.5179) 0.645 low: (0, 0.5355) 0.5434\n",
            "high: (1, 0.1752) 0.3345 low: (1, 0.0066) -0.0119\n",
            "high: (2, 0.0833) 0.1163 low: (2, 0.0017) -0.0186\n",
            "high: (3, 0.002) 0.0126 low: (3, 0.0211) -0.0203\n",
            "high: (4, -0.0349) 0.0101 low: (4, -0.0375) -0.0417\n",
            "high: (5, -0.0071) 0.0535 low: (5, 0.0001) -0.0037\n",
            "high: (6, 0.0501) 0.0624 low: (6, -0.0037) -0.0038\n",
            "1 actual high: 4 actual low: 1\n",
            "high: (-6, 0.025) 0.0262 low: (-6, 0.0372) 0.0434\n",
            "high: (-5, 0.0112) 0.0379 low: (-5, -0.0337) 0.0041\n",
            "high: (-4, 0.0239) 0.0629 low: (-4, 0.0367) 0.0469\n",
            "high: (-3, 0.0148) 0.0784 low: (-3, 0.0595) 0.1163\n",
            "high: (-2, 0.0262) 0.1058 low: (-2, 0.1028) 0.2363\n",
            "high: (-1, 0.053) 0.1612 low: (-1, 0.167) 0.4311\n",
            "high: (0, 0.3898) 0.4078 low: (0, 0.4154) 0.4847\n",
            "high: (1, 0.1237) 0.4309 low: (1, 0.0486) 0.0842\n",
            "high: (2, 0.0901) 0.3015 low: (2, 0.0349) 0.0275\n",
            "high: (3, 0.0877) 0.2073 low: (3, 0.0183) -0.0132\n",
            "high: (4, 0.0413) 0.1155 low: (4, -0.0112) -0.0345\n",
            "high: (5, -0.0041) 0.0723 low: (5, -0.003) -0.0215\n",
            "high: (6, 0.0732) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 3 actual low: 1\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1036) 0.0971\n",
            "high: (-5, 0.0161) 0.0443 low: (-5, -0.0016) 0.0956\n",
            "high: (-4, 0.0109) 0.0592 low: (-4, 0.0417) 0.1346\n",
            "high: (-3, 0.011) 0.0742 low: (-3, 0.0666) 0.197\n",
            "high: (-2, 0.0287) 0.1134 low: (-2, 0.0419) 0.2363\n",
            "high: (-1, 0.0833) 0.2272 low: (-1, 0.197) 0.4208\n",
            "high: (0, 0.3167) 0.4327 low: (0, 0.3898) 0.3652\n",
            "high: (1, 0.0923) 0.3401 low: (1, 0.0754) 0.214\n",
            "high: (2, 0.0308) 0.214 low: (2, 0.0671) 0.1433\n",
            "high: (3, 0.0534) 0.1719 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0354) 0.0989 low: (4, 0.0165) 0.032\n",
            "high: (5, -0.0001) 0.1473 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1079) 0.1474 low: (6, 0.0275) 0.0258\n",
            "3 actual high: 6 actual low: 2\n",
            "high: (-6, 0.0272) 0.032 low: (-6, 0.1256) 0.1572\n",
            "high: (-5, 0.0001) 0.0321 low: (-5, 0.02) 0.1822\n",
            "high: (-4, 0.0182) 0.0535 low: (-4, 0.0368) 0.2283\n",
            "high: (-3, 0.0462) 0.1078 low: (-3, 0.0389) 0.2769\n",
            "high: (-2, 0.0595) 0.1777 low: (-2, 0.07) 0.3645\n",
            "high: (-1, 0.0893) 0.2826 low: (-1, 0.0454) 0.4213\n",
            "high: (0, 0.2793) 0.3282 low: (0, 0.26) 0.3254\n",
            "high: (1, 0.0593) 0.3892 low: (1, 0.0695) 0.2533\n",
            "high: (2, 0.1147) 0.3195 low: (2, 0.0687) 0.1663\n",
            "high: (3, 0.0701) 0.1847 low: (3, 0.0006) 0.0803\n",
            "high: (4, 0.0394) 0.1024 low: (4, 0.0267) 0.0796\n",
            "high: (5, 0.0002) 0.0561 low: (5, -0.004) 0.0462\n",
            "high: (6, 0.0475) 0.0558 low: (6, 0.0409) 0.0512\n",
            "4 actual high: 4 actual low: 2\n",
            "high: (-6, 0.057) 0.0759 low: (-6, 0.1689) 0.1613\n",
            "high: (-5, 0.0024) 0.0791 low: (-5, -0.0009) 0.1604\n",
            "high: (-4, -0.0644) -0.0067 low: (-4, 0.0084) 0.1684\n",
            "high: (-3, 0.038) 0.044 low: (-3, 0.0917) 0.256\n",
            "high: (-2, 0.018) 0.0679 low: (-2, 0.0567) 0.3101\n",
            "high: (-1, 0.0488) 0.1329 low: (-1, 0.0909) 0.3969\n",
            "high: (0, 0.2462) 0.328 low: (0, 0.2994) 0.2859\n",
            "high: (1, 0.0755) 0.5391 low: (1, 0.1176) 0.3173\n",
            "high: (2, 0.0535) 0.4385 low: (2, 0.0663) 0.205\n",
            "high: (3, 0.0911) 0.3673 low: (3, 0.0576) 0.1417\n",
            "high: (4, 0.0567) 0.2459 low: (4, 0.0193) 0.0867\n",
            "high: (5, 0.002) 0.1704 low: (5, 0.0155) 0.0683\n",
            "high: (6, 0.1259) 0.1677 low: (6, 0.056) 0.0535\n",
            "------\n",
            "0 actual high: 2 actual low: -1\n",
            "high: (-6, -0.0064) -0.0079 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.015 low: (-4, -0.0226) 0.009\n",
            "high: (-3, -0.0234) -0.014 low: (-3, 0.0478) 0.0574\n",
            "high: (-2, 0.007) -0.0053 low: (-2, 0.08) 0.1383\n",
            "high: (-1, 0.0213) 0.0211 low: (-1, 0.3259) 0.4681\n",
            "high: (0, 0.5199) 0.6445 low: (0, 0.5377) 0.5441\n",
            "high: (1, 0.176) 0.3344 low: (1, 0.0067) -0.0122\n",
            "high: (2, 0.0834) 0.1163 low: (2, 0.0014) -0.019\n",
            "high: (3, 0.0018) 0.0129 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0348) 0.0107 low: (4, -0.0372) -0.0418\n",
            "high: (5, -0.0069) 0.0538 low: (5, -0.0003) -0.0041\n",
            "high: (6, 0.0503) 0.0624 low: (6, -0.0038) -0.0038\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0251) 0.0261 low: (-6, 0.0375) 0.0435\n",
            "high: (-5, 0.0114) 0.038 low: (-5, -0.034) 0.0041\n",
            "high: (-4, 0.0243) 0.0633 low: (-4, 0.0367) 0.0467\n",
            "high: (-3, 0.0146) 0.0785 low: (-3, 0.0599) 0.1162\n",
            "high: (-2, 0.0261) 0.1057 low: (-2, 0.1029) 0.2357\n",
            "high: (-1, 0.0535) 0.1614 low: (-1, 0.1677) 0.4304\n",
            "high: (0, 0.3917) 0.4079 low: (0, 0.4175) 0.4848\n",
            "high: (1, 0.124) 0.4307 low: (1, 0.0491) 0.0848\n",
            "high: (2, 0.0902) 0.3016 low: (2, 0.0349) 0.0278\n",
            "high: (3, 0.0883) 0.2076 low: (3, 0.0182) -0.0128\n",
            "high: (4, 0.0414) 0.1157 low: (4, -0.0109) -0.0339\n",
            "high: (5, -0.004) 0.0726 low: (5, -0.003) -0.0212\n",
            "high: (6, 0.0737) 0.0767 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.1041) 0.0971\n",
            "high: (-5, 0.0161) 0.0439 low: (-5, -0.0014) 0.0958\n",
            "high: (-4, 0.0108) 0.0586 low: (-4, 0.0419) 0.1349\n",
            "high: (-3, 0.0115) 0.0742 low: (-3, 0.0667) 0.1972\n",
            "high: (-2, 0.029) 0.1137 low: (-2, 0.0422) 0.2365\n",
            "high: (-1, 0.0839) 0.2278 low: (-1, 0.1977) 0.421\n",
            "high: (0, 0.3179) 0.4323 low: (0, 0.3918) 0.3656\n",
            "high: (1, 0.0927) 0.34 low: (1, 0.0755) 0.2134\n",
            "high: (2, 0.0308) 0.2139 low: (2, 0.0671) 0.143\n",
            "high: (3, 0.0539) 0.172 low: (3, 0.0518) 0.0803\n",
            "high: (4, -0.0358) 0.0987 low: (4, 0.0165) 0.032\n",
            "high: (5, 0.0) 0.1474 low: (5, -0.0097) 0.0166\n",
            "high: (6, 0.1084) 0.1474 low: (6, 0.0275) 0.0257\n",
            "3 actual high: 2 actual low: 0\n",
            "high: (-6, 0.0274) 0.0321 low: (-6, 0.1258) 0.1569\n",
            "high: (-5, 0.0001) 0.0323 low: (-5, 0.02) 0.1819\n",
            "high: (-4, 0.0182) 0.0536 low: (-4, 0.0368) 0.2278\n",
            "high: (-3, 0.046) 0.1076 low: (-3, 0.039) 0.2764\n",
            "high: (-2, 0.0595) 0.1774 low: (-2, 0.0705) 0.3644\n",
            "high: (-1, 0.0895) 0.2824 low: (-1, 0.0454) 0.421\n",
            "high: (0, 0.2802) 0.3287 low: (0, 0.2609) 0.3255\n",
            "high: (1, 0.0595) 0.3889 low: (1, 0.0696) 0.2535\n",
            "high: (2, 0.1147) 0.3191 low: (2, 0.0689) 0.1667\n",
            "high: (3, 0.0704) 0.1845 low: (3, 0.0009) 0.0807\n",
            "high: (4, 0.0397) 0.1019 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0002) 0.0554 low: (5, -0.0036) 0.0463\n",
            "high: (6, 0.0474) 0.0556 low: (6, 0.0407) 0.0508\n",
            "4 actual high: 2 actual low: 0\n",
            "high: (-6, 0.0571) 0.0759 low: (-6, 0.1696) 0.1614\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1606\n",
            "high: (-4, -0.0648) -0.0069 low: (-4, 0.0085) 0.1687\n",
            "high: (-3, 0.0379) 0.0435 low: (-3, 0.0918) 0.2561\n",
            "high: (-2, 0.0181) 0.0675 low: (-2, 0.0572) 0.3105\n",
            "high: (-1, 0.0487) 0.1323 low: (-1, 0.0911) 0.3972\n",
            "high: (0, 0.2473) 0.3287 low: (0, 0.3006) 0.286\n",
            "high: (1, 0.0755) 0.539 low: (1, 0.118) 0.3168\n",
            "high: (2, 0.054) 0.4387 low: (2, 0.0666) 0.2045\n",
            "high: (3, 0.0913) 0.3669 low: (3, 0.0577) 0.1411\n",
            "high: (4, 0.0566) 0.2455 low: (4, 0.0195) 0.0862\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0154) 0.0677\n",
            "high: (6, 0.1261) 0.1676 low: (6, 0.0557) 0.053\n",
            "------\n",
            "0 actual high: 1 actual low: -1\n",
            "high: (-6, -0.0064) -0.0079 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.015 low: (-4, -0.0225) 0.0091\n",
            "high: (-3, -0.0234) -0.014 low: (-3, 0.0478) 0.0575\n",
            "high: (-2, 0.0069) -0.0055 low: (-2, 0.08) 0.1384\n",
            "high: (-1, 0.0214) 0.0211 low: (-1, 0.326) 0.4682\n",
            "high: (0, 0.52) 0.6448 low: (0, 0.5378) 0.5441\n",
            "high: (1, 0.176) 0.3342 low: (1, 0.0067) -0.0122\n",
            "high: (2, 0.0834) 0.1159 low: (2, 0.0014) -0.019\n",
            "high: (3, 0.0018) 0.0125 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0349) 0.0103 low: (4, -0.0373) -0.0418\n",
            "high: (5, -0.007) 0.0536 low: (5, -0.0002) -0.004\n",
            "high: (6, 0.0502) 0.0622 low: (6, -0.0038) -0.0038\n",
            "1 actual high: 3 actual low: 0\n",
            "high: (-6, 0.0251) 0.0261 low: (-6, 0.0375) 0.0436\n",
            "high: (-5, 0.0114) 0.038 low: (-5, -0.034) 0.0041\n",
            "high: (-4, 0.0243) 0.0633 low: (-4, 0.0367) 0.0467\n",
            "high: (-3, 0.0145) 0.0784 low: (-3, 0.0599) 0.1163\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1028) 0.2357\n",
            "high: (-1, 0.0535) 0.1613 low: (-1, 0.1676) 0.4304\n",
            "high: (0, 0.3917) 0.4079 low: (0, 0.4175) 0.485\n",
            "high: (1, 0.124) 0.4307 low: (1, 0.0491) 0.0847\n",
            "high: (2, 0.0902) 0.3016 low: (2, 0.0349) 0.0276\n",
            "high: (3, 0.0883) 0.2077 low: (3, 0.0182) -0.0129\n",
            "high: (4, 0.0414) 0.1157 low: (4, -0.011) -0.034\n",
            "high: (5, -0.004) 0.0726 low: (5, -0.003) -0.0213\n",
            "high: (6, 0.0737) 0.0768 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 2 actual low: 0\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.1041) 0.0971\n",
            "high: (-5, 0.0162) 0.0441 low: (-5, -0.0014) 0.0958\n",
            "high: (-4, 0.0108) 0.0587 low: (-4, 0.0419) 0.1349\n",
            "high: (-3, 0.0114) 0.0742 low: (-3, 0.0667) 0.1971\n",
            "high: (-2, 0.029) 0.1137 low: (-2, 0.0422) 0.2365\n",
            "high: (-1, 0.0839) 0.2278 low: (-1, 0.1978) 0.421\n",
            "high: (0, 0.3179) 0.4323 low: (0, 0.3918) 0.3655\n",
            "high: (1, 0.0927) 0.34 low: (1, 0.0755) 0.2135\n",
            "high: (2, 0.0308) 0.2139 low: (2, 0.0672) 0.143\n",
            "high: (3, 0.0539) 0.172 low: (3, 0.0518) 0.0803\n",
            "high: (4, -0.0358) 0.0987 low: (4, 0.0165) 0.032\n",
            "high: (5, -0.0) 0.1474 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1084) 0.1474 low: (6, 0.0276) 0.0257\n",
            "3 actual high: 2 actual low: 0\n",
            "high: (-6, 0.0274) 0.0321 low: (-6, 0.1258) 0.1569\n",
            "high: (-5, 0.0001) 0.0323 low: (-5, 0.02) 0.1819\n",
            "high: (-4, 0.0182) 0.0536 low: (-4, 0.0368) 0.2278\n",
            "high: (-3, 0.046) 0.1076 low: (-3, 0.039) 0.2764\n",
            "high: (-2, 0.0594) 0.1773 low: (-2, 0.0705) 0.3644\n",
            "high: (-1, 0.0895) 0.2823 low: (-1, 0.0454) 0.421\n",
            "high: (0, 0.2802) 0.3287 low: (0, 0.2609) 0.3254\n",
            "high: (1, 0.0595) 0.389 low: (1, 0.0697) 0.2536\n",
            "high: (2, 0.1147) 0.3192 low: (2, 0.0689) 0.1666\n",
            "high: (3, 0.0704) 0.1847 low: (3, 0.0009) 0.0807\n",
            "high: (4, 0.0397) 0.1021 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0001) 0.0555 low: (5, -0.0036) 0.0463\n",
            "high: (6, 0.0474) 0.0556 low: (6, 0.0407) 0.0508\n",
            "4 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0571) 0.0759 low: (-6, 0.1696) 0.1614\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1606\n",
            "high: (-4, -0.0648) -0.0069 low: (-4, 0.0086) 0.1688\n",
            "high: (-3, 0.0379) 0.0435 low: (-3, 0.0918) 0.2561\n",
            "high: (-2, 0.0181) 0.0675 low: (-2, 0.0571) 0.3105\n",
            "high: (-1, 0.0487) 0.1323 low: (-1, 0.0912) 0.3972\n",
            "high: (0, 0.2473) 0.3287 low: (0, 0.3007) 0.2861\n",
            "high: (1, 0.0755) 0.539 low: (1, 0.118) 0.3167\n",
            "high: (2, 0.0539) 0.4387 low: (2, 0.0665) 0.2044\n",
            "high: (3, 0.0913) 0.367 low: (3, 0.0577) 0.1411\n",
            "high: (4, 0.0567) 0.2456 low: (4, 0.0194) 0.0862\n",
            "high: (5, 0.002) 0.1703 low: (5, 0.0154) 0.0677\n",
            "high: (6, 0.1261) 0.1676 low: (6, 0.0558) 0.0531\n",
            "------\n",
            "0 actual high: 2 actual low: -1\n",
            "high: (-6, -0.0063) -0.0078 low: (-6, 0.0098) 0.0099\n",
            "high: (-5, 0.0382) 0.0395 low: (-5, 0.0216) 0.0317\n",
            "high: (-4, -0.0197) 0.0151 low: (-4, -0.0226) 0.0089\n",
            "high: (-3, -0.0234) -0.0139 low: (-3, 0.0477) 0.0571\n",
            "high: (-2, 0.007) -0.0052 low: (-2, 0.0801) 0.1381\n",
            "high: (-1, 0.0214) 0.0213 low: (-1, 0.3265) 0.4681\n",
            "high: (0, 0.5208) 0.6444 low: (0, 0.5388) 0.5446\n",
            "high: (1, 0.1764) 0.3343 low: (1, 0.0067) -0.0126\n",
            "high: (2, 0.0835) 0.1161 low: (2, 0.0013) -0.0194\n",
            "high: (3, 0.0017) 0.0127 low: (3, 0.021) -0.0207\n",
            "high: (4, -0.0348) 0.0106 low: (4, -0.0372) -0.0419\n",
            "high: (5, -0.0069) 0.0537 low: (5, -0.0004) -0.0043\n",
            "high: (6, 0.0503) 0.0622 low: (6, -0.0039) -0.0039\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0251) 0.0261 low: (-6, 0.0377) 0.0437\n",
            "high: (-5, 0.0115) 0.038 low: (-5, -0.0341) 0.0042\n",
            "high: (-4, 0.0244) 0.0634 low: (-4, 0.0367) 0.0467\n",
            "high: (-3, 0.0144) 0.0784 low: (-3, 0.06) 0.1163\n",
            "high: (-2, 0.026) 0.1054 low: (-2, 0.1028) 0.2355\n",
            "high: (-1, 0.0537) 0.1612 low: (-1, 0.1679) 0.4301\n",
            "high: (0, 0.3924) 0.4079 low: (0, 0.4184) 0.4851\n",
            "high: (1, 0.1241) 0.4308 low: (1, 0.0492) 0.0848\n",
            "high: (2, 0.0903) 0.3018 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0886) 0.2079 low: (3, 0.0182) -0.0128\n",
            "high: (4, 0.0415) 0.1158 low: (4, -0.0109) -0.0339\n",
            "high: (5, -0.004) 0.0727 low: (5, -0.003) -0.0212\n",
            "high: (6, 0.0739) 0.0768 low: (6, -0.0153) -0.0177\n",
            "2 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.1043) 0.0971\n",
            "high: (-5, 0.0162) 0.044 low: (-5, -0.0013) 0.0959\n",
            "high: (-4, 0.0108) 0.0586 low: (-4, 0.042) 0.135\n",
            "high: (-3, 0.0116) 0.0744 low: (-3, 0.0667) 0.1972\n",
            "high: (-2, 0.0291) 0.1139 low: (-2, 0.0423) 0.2365\n",
            "high: (-1, 0.0841) 0.228 low: (-1, 0.1982) 0.4211\n",
            "high: (0, 0.3184) 0.4322 low: (0, 0.3926) 0.3656\n",
            "high: (1, 0.0928) 0.3398 low: (1, 0.0755) 0.2133\n",
            "high: (2, 0.0308) 0.2138 low: (2, 0.0672) 0.143\n",
            "high: (3, 0.054) 0.172 low: (3, 0.0519) 0.0804\n",
            "high: (4, -0.0359) 0.0987 low: (4, 0.0166) 0.032\n",
            "high: (5, 0.0) 0.1474 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1086) 0.1474 low: (6, 0.0276) 0.0257\n",
            "3 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0274) 0.0321 low: (-6, 0.1258) 0.1568\n",
            "high: (-5, 0.0001) 0.0322 low: (-5, 0.02) 0.1817\n",
            "high: (-4, 0.0182) 0.0536 low: (-4, 0.0368) 0.2275\n",
            "high: (-3, 0.046) 0.1075 low: (-3, 0.039) 0.2761\n",
            "high: (-2, 0.0594) 0.1771 low: (-2, 0.0706) 0.3641\n",
            "high: (-1, 0.0895) 0.282 low: (-1, 0.0455) 0.4208\n",
            "high: (0, 0.2806) 0.3289 low: (0, 0.2613) 0.3256\n",
            "high: (1, 0.0596) 0.3891 low: (1, 0.0697) 0.2536\n",
            "high: (2, 0.1147) 0.3192 low: (2, 0.0689) 0.1667\n",
            "high: (3, 0.0705) 0.1847 low: (3, 0.001) 0.0809\n",
            "high: (4, 0.0398) 0.1021 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0002) 0.0554 low: (5, -0.0035) 0.0464\n",
            "high: (6, 0.0475) 0.0557 low: (6, 0.0407) 0.0507\n",
            "4 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0571) 0.0758 low: (-6, 0.1699) 0.1615\n",
            "high: (-5, 0.0025) 0.0791 low: (-5, -0.0008) 0.1607\n",
            "high: (-4, -0.0649) -0.007 low: (-4, 0.0086) 0.1689\n",
            "high: (-3, 0.0379) 0.0433 low: (-3, 0.0918) 0.2561\n",
            "high: (-2, 0.0182) 0.0674 low: (-2, 0.0573) 0.3106\n",
            "high: (-1, 0.0487) 0.1321 low: (-1, 0.0913) 0.3974\n",
            "high: (0, 0.2478) 0.329 low: (0, 0.3012) 0.2863\n",
            "high: (1, 0.0755) 0.5389 low: (1, 0.1182) 0.3164\n",
            "high: (2, 0.0541) 0.4387 low: (2, 0.0666) 0.204\n",
            "high: (3, 0.0914) 0.3668 low: (3, 0.0577) 0.1408\n",
            "high: (4, 0.0567) 0.2455 low: (4, 0.0194) 0.0859\n",
            "high: (5, 0.002) 0.1702 low: (5, 0.0153) 0.0675\n",
            "high: (6, 0.1262) 0.1676 low: (6, 0.0557) 0.0529\n",
            "------\n",
            "0 actual high: 0 actual low: -2\n",
            "high: (-6, -0.0063) -0.0078 low: (-6, 0.0098) 0.0099\n",
            "high: (-5, 0.0382) 0.0394 low: (-5, 0.0216) 0.0317\n",
            "high: (-4, -0.0197) 0.0151 low: (-4, -0.0226) 0.0089\n",
            "high: (-3, -0.0234) -0.0138 low: (-3, 0.0477) 0.057\n",
            "high: (-2, 0.007) -0.0052 low: (-2, 0.0802) 0.138\n",
            "high: (-1, 0.0214) 0.0213 low: (-1, 0.327) 0.468\n",
            "high: (0, 0.5215) 0.6444 low: (0, 0.5397) 0.5447\n",
            "high: (1, 0.1767) 0.3344 low: (1, 0.0067) -0.0127\n",
            "high: (2, 0.0836) 0.116 low: (2, 0.0012) -0.0195\n",
            "high: (3, 0.0017) 0.0127 low: (3, 0.021) -0.0207\n",
            "high: (4, -0.0348) 0.0106 low: (4, -0.0371) -0.0419\n",
            "high: (5, -0.0069) 0.0536 low: (5, -0.0005) -0.0044\n",
            "high: (6, 0.0503) 0.0622 low: (6, -0.0039) -0.0039\n",
            "1 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0252) 0.0261 low: (-6, 0.0378) 0.0438\n",
            "high: (-5, 0.0116) 0.0382 low: (-5, -0.0342) 0.0042\n",
            "high: (-4, 0.0246) 0.0637 low: (-4, 0.0367) 0.0467\n",
            "high: (-3, 0.0144) 0.0787 low: (-3, 0.0601) 0.1162\n",
            "high: (-2, 0.026) 0.1056 low: (-2, 0.1029) 0.2354\n",
            "high: (-1, 0.0538) 0.1615 low: (-1, 0.1681) 0.43\n",
            "high: (0, 0.3931) 0.4079 low: (0, 0.4191) 0.4852\n",
            "high: (1, 0.1243) 0.4306 low: (1, 0.0494) 0.0849\n",
            "high: (2, 0.0904) 0.3016 low: (2, 0.0349) 0.0277\n",
            "high: (3, 0.0888) 0.2078 low: (3, 0.0181) -0.0127\n",
            "high: (4, 0.0415) 0.1157 low: (4, -0.0109) -0.0337\n",
            "high: (5, -0.004) 0.0726 low: (5, -0.003) -0.0211\n",
            "high: (6, 0.074) 0.0768 low: (6, -0.0152) -0.0176\n",
            "2 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.1044) 0.0971\n",
            "high: (-5, 0.0162) 0.0439 low: (-5, -0.0012) 0.096\n",
            "high: (-4, 0.0108) 0.0586 low: (-4, 0.042) 0.135\n",
            "high: (-3, 0.0117) 0.0744 low: (-3, 0.0668) 0.1972\n",
            "high: (-2, 0.0292) 0.114 low: (-2, 0.0424) 0.2366\n",
            "high: (-1, 0.0842) 0.2281 low: (-1, 0.1985) 0.4212\n",
            "high: (0, 0.3188) 0.4321 low: (0, 0.3933) 0.3658\n",
            "high: (1, 0.0929) 0.3398 low: (1, 0.0755) 0.213\n",
            "high: (2, 0.0308) 0.2139 low: (2, 0.0672) 0.1428\n",
            "high: (3, 0.0542) 0.1721 low: (3, 0.0519) 0.0803\n",
            "high: (4, -0.036) 0.0987 low: (4, 0.0166) 0.032\n",
            "high: (5, 0.0) 0.1475 low: (5, -0.0098) 0.0166\n",
            "high: (6, 0.1088) 0.1475 low: (6, 0.0276) 0.0257\n",
            "3 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0275) 0.0322 low: (-6, 0.1259) 0.1567\n",
            "high: (-5, 0.0001) 0.0323 low: (-5, 0.02) 0.1816\n",
            "high: (-4, 0.0182) 0.0537 low: (-4, 0.0369) 0.2276\n",
            "high: (-3, 0.0459) 0.1074 low: (-3, 0.039) 0.2761\n",
            "high: (-2, 0.0594) 0.177 low: (-2, 0.0708) 0.3642\n",
            "high: (-1, 0.0896) 0.282 low: (-1, 0.0455) 0.4209\n",
            "high: (0, 0.2809) 0.3291 low: (0, 0.2616) 0.3257\n",
            "high: (1, 0.0597) 0.3889 low: (1, 0.0698) 0.2535\n",
            "high: (2, 0.1147) 0.319 low: (2, 0.0689) 0.1666\n",
            "high: (3, 0.0705) 0.1846 low: (3, 0.001) 0.0808\n",
            "high: (4, 0.0399) 0.102 low: (4, 0.0267) 0.0795\n",
            "high: (5, -0.0003) 0.0553 low: (5, -0.0034) 0.0463\n",
            "high: (6, 0.0475) 0.0556 low: (6, 0.0406) 0.0505\n",
            "4 actual high: -2 actual low: -3\n",
            "high: (-6, 0.0572) 0.0759 low: (-6, 0.1701) 0.1615\n",
            "high: (-5, 0.0025) 0.0792 low: (-5, -0.0008) 0.1608\n",
            "high: (-4, -0.065) -0.007 low: (-4, 0.0086) 0.1689\n",
            "high: (-3, 0.0378) 0.0431 low: (-3, 0.0918) 0.2561\n",
            "high: (-2, 0.0182) 0.0673 low: (-2, 0.0574) 0.3106\n",
            "high: (-1, 0.0487) 0.1319 low: (-1, 0.0913) 0.3973\n",
            "high: (0, 0.2481) 0.3292 low: (0, 0.3016) 0.2864\n",
            "high: (1, 0.0755) 0.5389 low: (1, 0.1183) 0.3163\n",
            "high: (2, 0.0542) 0.4388 low: (2, 0.0667) 0.204\n",
            "high: (3, 0.0915) 0.3669 low: (3, 0.0577) 0.1406\n",
            "high: (4, 0.0567) 0.2455 low: (4, 0.0194) 0.0858\n",
            "high: (5, 0.002) 0.1702 low: (5, 0.0153) 0.0674\n",
            "high: (6, 0.1263) 0.1676 low: (6, 0.0557) 0.0529\n",
            "------\n",
            "0 actual high: 2 actual low: 0\n",
            "high: (-6, -0.0066) -0.0082 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0382) 0.0392 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.0148 low: (-4, -0.0223) 0.0093\n",
            "high: (-3, -0.0234) -0.0143 low: (-3, 0.0478) 0.0577\n",
            "high: (-2, 0.0067) -0.006 low: (-2, 0.08) 0.1386\n",
            "high: (-1, 0.0214) 0.0206 low: (-1, 0.3257) 0.4683\n",
            "high: (0, 0.5196) 0.6449 low: (0, 0.5376) 0.5441\n",
            "high: (1, 0.1759) 0.3345 low: (1, 0.0065) -0.0123\n",
            "high: (2, 0.0835) 0.1162 low: (2, 0.0015) -0.0189\n",
            "high: (3, 0.0019) 0.0125 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.0349) 0.0102 low: (4, -0.0374) -0.0418\n",
            "high: (5, -0.007) 0.0535 low: (5, -0.0001) -0.0039\n",
            "high: (6, 0.0501) 0.0622 low: (6, -0.0038) -0.0038\n",
            "1 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0251) 0.0262 low: (-6, 0.0374) 0.0435\n",
            "high: (-5, 0.0115) 0.0381 low: (-5, -0.0339) 0.0041\n",
            "high: (-4, 0.0241) 0.0633 low: (-4, 0.0367) 0.0468\n",
            "high: (-3, 0.0145) 0.0784 low: (-3, 0.0597) 0.1162\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1027) 0.2357\n",
            "high: (-1, 0.0532) 0.161 low: (-1, 0.1674) 0.4305\n",
            "high: (0, 0.3913) 0.4079 low: (0, 0.4171) 0.4853\n",
            "high: (1, 0.1241) 0.4311 low: (1, 0.0487) 0.0841\n",
            "high: (2, 0.0903) 0.3018 low: (2, 0.0348) 0.0275\n",
            "high: (3, 0.0882) 0.2076 low: (3, 0.0182) -0.013\n",
            "high: (4, 0.0415) 0.1157 low: (4, -0.0112) -0.0342\n",
            "high: (5, -0.0041) 0.0724 low: (5, -0.0029) -0.0212\n",
            "high: (6, 0.0736) 0.0767 low: (6, -0.0153) -0.0178\n",
            "2 actual high: 1 actual low: -1\n",
            "high: (-6, 0.0163) 0.0222 low: (-6, 0.1039) 0.097\n",
            "high: (-5, 0.0163) 0.0444 low: (-5, -0.0015) 0.0956\n",
            "high: (-4, 0.0108) 0.0591 low: (-4, 0.0418) 0.1346\n",
            "high: (-3, 0.0112) 0.0744 low: (-3, 0.0668) 0.197\n",
            "high: (-2, 0.0289) 0.1137 low: (-2, 0.0421) 0.2363\n",
            "high: (-1, 0.0836) 0.2276 low: (-1, 0.1978) 0.421\n",
            "high: (0, 0.3176) 0.4326 low: (0, 0.3913) 0.3653\n",
            "high: (1, 0.0925) 0.3398 low: (1, 0.0754) 0.2137\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0673) 0.1433\n",
            "high: (3, 0.0536) 0.172 low: (3, 0.0518) 0.0805\n",
            "high: (4, -0.0355) 0.099 low: (4, 0.0166) 0.0321\n",
            "high: (5, -0.0001) 0.1474 low: (5, -0.0099) 0.0166\n",
            "high: (6, 0.1083) 0.1475 low: (6, 0.0277) 0.0259\n",
            "3 actual high: 0 actual low: -1\n",
            "high: (-6, 0.0273) 0.032 low: (-6, 0.1258) 0.1571\n",
            "high: (-5, 0.0) 0.032 low: (-5, 0.0199) 0.1819\n",
            "high: (-4, 0.0181) 0.0533 low: (-4, 0.0369) 0.228\n",
            "high: (-3, 0.0461) 0.1074 low: (-3, 0.0389) 0.2766\n",
            "high: (-2, 0.0594) 0.1771 low: (-2, 0.0703) 0.3644\n",
            "high: (-1, 0.0894) 0.282 low: (-1, 0.0455) 0.4212\n",
            "high: (0, 0.28) 0.3286 low: (0, 0.2607) 0.3255\n",
            "high: (1, 0.0596) 0.3895 low: (1, 0.0696) 0.2532\n",
            "high: (2, 0.1148) 0.3195 low: (2, 0.0686) 0.1663\n",
            "high: (3, 0.0703) 0.1848 low: (3, 0.0008) 0.0807\n",
            "high: (4, 0.0395) 0.1023 low: (4, 0.0267) 0.0797\n",
            "high: (5, 0.0001) 0.056 low: (5, -0.0038) 0.0463\n",
            "high: (6, 0.0476) 0.0559 low: (6, 0.0409) 0.0511\n",
            "4 actual high: 0 actual low: -3\n",
            "high: (-6, 0.057) 0.0757 low: (-6, 0.1694) 0.1613\n",
            "high: (-5, 0.0025) 0.079 low: (-5, -0.0007) 0.1607\n",
            "high: (-4, -0.0645) -0.0066 low: (-4, 0.0085) 0.1688\n",
            "high: (-3, 0.038) 0.0438 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.0182) 0.068 low: (-2, 0.0569) 0.3104\n",
            "high: (-1, 0.0489) 0.133 low: (-1, 0.0911) 0.3972\n",
            "high: (0, 0.2472) 0.3284 low: (0, 0.3005) 0.2862\n",
            "high: (1, 0.0756) 0.5387 low: (1, 0.1179) 0.3166\n",
            "high: (2, 0.0537) 0.4382 low: (2, 0.0664) 0.2043\n",
            "high: (3, 0.0913) 0.3669 low: (3, 0.0575) 0.1411\n",
            "high: (4, 0.0568) 0.2456 low: (4, 0.0192) 0.0863\n",
            "high: (5, 0.0021) 0.1702 low: (5, 0.0154) 0.068\n",
            "high: (6, 0.126) 0.1674 low: (6, 0.056) 0.0533\n",
            "------\n",
            "0 actual high: -1 actual low: -3\n",
            "high: (-6, -0.0064) -0.0079 low: (-6, 0.0098) 0.0099\n",
            "high: (-5, 0.0382) 0.0393 low: (-5, 0.0215) 0.0316\n",
            "high: (-4, -0.0198) 0.0148 low: (-4, -0.0225) 0.0089\n",
            "high: (-3, -0.0234) -0.0141 low: (-3, 0.0477) 0.0571\n",
            "high: (-2, 0.0069) -0.0056 low: (-2, 0.0802) 0.1381\n",
            "high: (-1, 0.0214) 0.0209 low: (-1, 0.3268) 0.4681\n",
            "high: (0, 0.5212) 0.6447 low: (0, 0.5393) 0.5447\n",
            "high: (1, 0.1765) 0.3344 low: (1, 0.0066) -0.0128\n",
            "high: (2, 0.0836) 0.1161 low: (2, 0.0012) -0.0195\n",
            "high: (3, 0.0017) 0.0127 low: (3, 0.021) -0.0207\n",
            "high: (4, -0.0348) 0.0106 low: (4, -0.0372) -0.0419\n",
            "high: (5, -0.0069) 0.0537 low: (5, -0.0004) -0.0043\n",
            "high: (6, 0.0503) 0.0622 low: (6, -0.0039) -0.0039\n",
            "1 actual high: -1 actual low: -3\n",
            "high: (-6, 0.0251) 0.0261 low: (-6, 0.0377) 0.0437\n",
            "high: (-5, 0.0116) 0.0381 low: (-5, -0.0341) 0.0042\n",
            "high: (-4, 0.0245) 0.0636 low: (-4, 0.0367) 0.0467\n",
            "high: (-3, 0.0144) 0.0785 low: (-3, 0.06) 0.1163\n",
            "high: (-2, 0.026) 0.1055 low: (-2, 0.1028) 0.2355\n",
            "high: (-1, 0.0537) 0.1613 low: (-1, 0.1679) 0.4301\n",
            "high: (0, 0.3927) 0.408 low: (0, 0.4187) 0.4854\n",
            "high: (1, 0.1242) 0.4307 low: (1, 0.0492) 0.0845\n",
            "high: (2, 0.0904) 0.3017 low: (2, 0.0348) 0.0275\n",
            "high: (3, 0.0887) 0.2078 low: (3, 0.0182) -0.0129\n",
            "high: (4, 0.0415) 0.1156 low: (4, -0.011) -0.034\n",
            "high: (5, -0.0041) 0.0725 low: (5, -0.003) -0.0212\n",
            "high: (6, 0.0739) 0.0768 low: (6, -0.0153) -0.0177\n",
            "2 actual high: -2 actual low: -3\n",
            "high: (-6, 0.0162) 0.022 low: (-6, 0.1043) 0.0971\n",
            "high: (-5, 0.0163) 0.0441 low: (-5, -0.0013) 0.0959\n",
            "high: (-4, 0.0108) 0.0588 low: (-4, 0.0419) 0.1349\n",
            "high: (-3, 0.0116) 0.0745 low: (-3, 0.0668) 0.1971\n",
            "high: (-2, 0.0291) 0.114 low: (-2, 0.0423) 0.2364\n",
            "high: (-1, 0.0841) 0.2281 low: (-1, 0.1983) 0.421\n",
            "high: (0, 0.3186) 0.4323 low: (0, 0.3929) 0.3657\n",
            "high: (1, 0.0928) 0.3396 low: (1, 0.0755) 0.2133\n",
            "high: (2, 0.0307) 0.2137 low: (2, 0.0672) 0.143\n",
            "high: (3, 0.0541) 0.172 low: (3, 0.0519) 0.0804\n",
            "high: (4, -0.0359) 0.0986 low: (4, 0.0166) 0.0321\n",
            "high: (5, -0.0) 0.1474 low: (5, -0.0098) 0.0167\n",
            "high: (6, 0.1086) 0.1474 low: (6, 0.0277) 0.0258\n",
            "3 actual high: -2 actual low: -6\n",
            "high: (-6, 0.0275) 0.0322 low: (-6, 0.1259) 0.1568\n",
            "high: (-5, 0.0001) 0.0323 low: (-5, 0.02) 0.1817\n",
            "high: (-4, 0.0181) 0.0536 low: (-4, 0.0369) 0.2277\n",
            "high: (-3, 0.046) 0.1075 low: (-3, 0.039) 0.2763\n",
            "high: (-2, 0.0594) 0.1771 low: (-2, 0.0707) 0.3643\n",
            "high: (-1, 0.0895) 0.2819 low: (-1, 0.0455) 0.421\n",
            "high: (0, 0.2807) 0.3289 low: (0, 0.2614) 0.3256\n",
            "high: (1, 0.0597) 0.3891 low: (1, 0.0697) 0.2534\n",
            "high: (2, 0.1148) 0.3192 low: (2, 0.0688) 0.1665\n",
            "high: (3, 0.0705) 0.1847 low: (3, 0.001) 0.0808\n",
            "high: (4, 0.0398) 0.1021 low: (4, 0.0267) 0.0796\n",
            "high: (5, -0.0002) 0.0554 low: (5, -0.0035) 0.0463\n",
            "high: (6, 0.0475) 0.0557 low: (6, 0.0407) 0.0507\n",
            "4 actual high: -3 actual low: -6\n",
            "high: (-6, 0.0571) 0.0758 low: (-6, 0.17) 0.1615\n",
            "high: (-5, 0.0025) 0.0791 low: (-5, -0.0007) 0.1608\n",
            "high: (-4, -0.0649) -0.007 low: (-4, 0.0086) 0.169\n",
            "high: (-3, 0.0378) 0.0431 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.0182) 0.0673 low: (-2, 0.0573) 0.3107\n",
            "high: (-1, 0.0488) 0.132 low: (-1, 0.0913) 0.3974\n",
            "high: (0, 0.248) 0.329 low: (0, 0.3014) 0.2863\n",
            "high: (1, 0.0756) 0.5389 low: (1, 0.1182) 0.3163\n",
            "high: (2, 0.0541) 0.4386 low: (2, 0.0666) 0.204\n",
            "high: (3, 0.0915) 0.3669 low: (3, 0.0576) 0.1407\n",
            "high: (4, 0.0567) 0.2455 low: (4, 0.0194) 0.086\n",
            "high: (5, 0.0021) 0.1702 low: (5, 0.0153) 0.0675\n",
            "high: (6, 0.1262) 0.1674 low: (6, 0.0558) 0.053\n",
            "------\n",
            "0 actual high: 2 actual low: 0\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.0099) 0.01\n",
            "high: (-5, 0.0383) 0.0392 low: (-5, 0.0216) 0.0319\n",
            "high: (-4, -0.0197) 0.0147 low: (-4, -0.0222) 0.0094\n",
            "high: (-3, -0.0234) -0.0144 low: (-3, 0.0479) 0.058\n",
            "high: (-2, 0.0065) -0.0063 low: (-2, 0.0798) 0.1389\n",
            "high: (-1, 0.0214) 0.0203 low: (-1, 0.3248) 0.4683\n",
            "high: (0, 0.5184) 0.6452 low: (0, 0.5362) 0.5438\n",
            "high: (1, 0.1754) 0.3345 low: (1, 0.0065) -0.0121\n",
            "high: (2, 0.0835) 0.1162 low: (2, 0.0017) -0.0187\n",
            "high: (3, 0.002) 0.0123 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.035) 0.0098 low: (4, -0.0375) -0.0418\n",
            "high: (5, -0.0071) 0.0534 low: (5, 0.0001) -0.0038\n",
            "high: (6, 0.05) 0.0622 low: (6, -0.0038) -0.0039\n",
            "1 actual high: 1 actual low: 0\n",
            "high: (-6, 0.025) 0.0261 low: (-6, 0.0371) 0.0433\n",
            "high: (-5, 0.0114) 0.0381 low: (-5, -0.0338) 0.0039\n",
            "high: (-4, 0.0239) 0.063 low: (-4, 0.0366) 0.0466\n",
            "high: (-3, 0.0146) 0.0783 low: (-3, 0.0594) 0.1159\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1026) 0.2357\n",
            "high: (-1, 0.0529) 0.1609 low: (-1, 0.167) 0.4307\n",
            "high: (0, 0.3902) 0.4079 low: (0, 0.4158) 0.4854\n",
            "high: (1, 0.1239) 0.4312 low: (1, 0.0484) 0.0839\n",
            "high: (2, 0.0903) 0.3017 low: (2, 0.0348) 0.0274\n",
            "high: (3, 0.0878) 0.2073 low: (3, 0.0183) -0.0132\n",
            "high: (4, 0.0414) 0.1155 low: (4, -0.0114) -0.0346\n",
            "high: (5, -0.0042) 0.0722 low: (5, -0.0029) -0.0212\n",
            "high: (6, 0.0733) 0.0766 low: (6, -0.0153) -0.0179\n",
            "2 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1036) 0.097\n",
            "high: (-5, 0.0164) 0.0447 low: (-5, -0.0016) 0.0955\n",
            "high: (-4, 0.0108) 0.0594 low: (-4, 0.0417) 0.1345\n",
            "high: (-3, 0.0109) 0.0743 low: (-3, 0.0668) 0.1971\n",
            "high: (-2, 0.0288) 0.1137 low: (-2, 0.0418) 0.2362\n",
            "high: (-1, 0.0833) 0.2275 low: (-1, 0.1973) 0.4209\n",
            "high: (0, 0.3168) 0.4328 low: (0, 0.39) 0.3651\n",
            "high: (1, 0.0923) 0.3398 low: (1, 0.0753) 0.2139\n",
            "high: (2, 0.0306) 0.2137 low: (2, 0.0672) 0.1434\n",
            "high: (3, 0.0533) 0.1719 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0353) 0.099 low: (4, 0.0165) 0.0321\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.0099) 0.0167\n",
            "high: (6, 0.108) 0.1475 low: (6, 0.0277) 0.0259\n",
            "3 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0271) 0.0318 low: (-6, 0.1258) 0.1573\n",
            "high: (-5, 0.0) 0.0318 low: (-5, 0.0199) 0.1822\n",
            "high: (-4, 0.0181) 0.0531 low: (-4, 0.0369) 0.2284\n",
            "high: (-3, 0.0462) 0.1074 low: (-3, 0.0389) 0.277\n",
            "high: (-2, 0.0594) 0.1771 low: (-2, 0.07) 0.3646\n",
            "high: (-1, 0.0892) 0.2819 low: (-1, 0.0456) 0.4216\n",
            "high: (0, 0.2795) 0.3283 low: (0, 0.2601) 0.3253\n",
            "high: (1, 0.0595) 0.3898 low: (1, 0.0695) 0.2531\n",
            "high: (2, 0.1148) 0.3199 low: (2, 0.0685) 0.1662\n",
            "high: (3, 0.0701) 0.185 low: (3, 0.0007) 0.0805\n",
            "high: (4, 0.0394) 0.1027 low: (4, 0.0267) 0.0797\n",
            "high: (5, 0.0004) 0.0564 low: (5, -0.0041) 0.0463\n",
            "high: (6, 0.0476) 0.0559 low: (6, 0.0411) 0.0514\n",
            "4 actual high: 2 actual low: -1\n",
            "high: (-6, 0.0569) 0.0757 low: (-6, 0.169) 0.1613\n",
            "high: (-5, 0.0024) 0.0789 low: (-5, -0.0007) 0.1606\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0084) 0.1686\n",
            "high: (-3, 0.038) 0.044 low: (-3, 0.0918) 0.2562\n",
            "high: (-2, 0.0181) 0.0681 low: (-2, 0.0566) 0.3102\n",
            "high: (-1, 0.049) 0.1333 low: (-1, 0.091) 0.3971\n",
            "high: (0, 0.2465) 0.3279 low: (0, 0.2998) 0.2861\n",
            "high: (1, 0.0757) 0.5388 low: (1, 0.1176) 0.3168\n",
            "high: (2, 0.0534) 0.4381 low: (2, 0.0662) 0.2046\n",
            "high: (3, 0.0911) 0.3671 low: (3, 0.0575) 0.1414\n",
            "high: (4, 0.0569) 0.2459 low: (4, 0.0191) 0.0866\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0154) 0.0683\n",
            "high: (6, 0.1259) 0.1675 low: (6, 0.0562) 0.0536\n",
            "------\n",
            "0 actual high: 1 actual low: 0\n",
            "high: (-6, -0.0068) -0.0085 low: (-6, 0.01) 0.0101\n",
            "high: (-5, 0.0382) 0.0391 low: (-5, 0.0216) 0.032\n",
            "high: (-4, -0.0197) 0.0146 low: (-4, -0.0222) 0.0095\n",
            "high: (-3, -0.0234) -0.0146 low: (-3, 0.0479) 0.0581\n",
            "high: (-2, 0.0066) -0.0063 low: (-2, 0.0798) 0.139\n",
            "high: (-1, 0.0213) 0.0202 low: (-1, 0.3248) 0.4684\n",
            "high: (0, 0.5183) 0.6452 low: (0, 0.5361) 0.5437\n",
            "high: (1, 0.1754) 0.3346 low: (1, 0.0065) -0.0121\n",
            "high: (2, 0.0835) 0.1163 low: (2, 0.0017) -0.0187\n",
            "high: (3, 0.002) 0.0123 low: (3, 0.0211) -0.0204\n",
            "high: (4, -0.035) 0.0098 low: (4, -0.0375) -0.0418\n",
            "high: (5, -0.0071) 0.0534 low: (5, 0.0001) -0.0038\n",
            "high: (6, 0.05) 0.0622 low: (6, -0.0038) -0.0039\n",
            "1 actual high: 1 actual low: -2\n",
            "high: (-6, 0.025) 0.0261 low: (-6, 0.0371) 0.0433\n",
            "high: (-5, 0.0114) 0.0381 low: (-5, -0.0337) 0.004\n",
            "high: (-4, 0.0239) 0.063 low: (-4, 0.0366) 0.0467\n",
            "high: (-3, 0.0146) 0.0783 low: (-3, 0.0594) 0.116\n",
            "high: (-2, 0.0261) 0.1056 low: (-2, 0.1026) 0.2358\n",
            "high: (-1, 0.0529) 0.1609 low: (-1, 0.167) 0.4308\n",
            "high: (0, 0.3901) 0.4078 low: (0, 0.4158) 0.4854\n",
            "high: (1, 0.1239) 0.4313 low: (1, 0.0484) 0.0838\n",
            "high: (2, 0.0903) 0.3017 low: (2, 0.0348) 0.0273\n",
            "high: (3, 0.0878) 0.2073 low: (3, 0.0183) -0.0133\n",
            "high: (4, 0.0414) 0.1155 low: (4, -0.0114) -0.0347\n",
            "high: (5, -0.0042) 0.0722 low: (5, -0.0029) -0.0214\n",
            "high: (6, 0.0733) 0.0766 low: (6, -0.0154) -0.018\n",
            "2 actual high: 0 actual low: -2\n",
            "high: (-6, 0.0163) 0.0223 low: (-6, 0.1036) 0.097\n",
            "high: (-5, 0.0163) 0.0445 low: (-5, -0.0016) 0.0955\n",
            "high: (-4, 0.0108) 0.0593 low: (-4, 0.0417) 0.1345\n",
            "high: (-3, 0.0109) 0.0742 low: (-3, 0.0667) 0.197\n",
            "high: (-2, 0.0288) 0.1135 low: (-2, 0.0419) 0.2362\n",
            "high: (-1, 0.0833) 0.2273 low: (-1, 0.1972) 0.4208\n",
            "high: (0, 0.3168) 0.4328 low: (0, 0.39) 0.3651\n",
            "high: (1, 0.0923) 0.3399 low: (1, 0.0754) 0.214\n",
            "high: (2, 0.0307) 0.2138 low: (2, 0.0672) 0.1434\n",
            "high: (3, 0.0533) 0.1719 low: (3, 0.0517) 0.0805\n",
            "high: (4, -0.0353) 0.099 low: (4, 0.0165) 0.0321\n",
            "high: (5, -0.0002) 0.1473 low: (5, -0.0099) 0.0167\n",
            "high: (6, 0.108) 0.1475 low: (6, 0.0277) 0.0259\n",
            "3 actual high: 2 actual low: -1\n",
            "high: (-6, 0.0271) 0.0318 low: (-6, 0.1258) 0.1574\n",
            "high: (-5, 0.0001) 0.0319 low: (-5, 0.0199) 0.1823\n",
            "high: (-4, 0.0181) 0.0532 low: (-4, 0.0369) 0.2284\n",
            "high: (-3, 0.0462) 0.1075 low: (-3, 0.0389) 0.2771\n",
            "high: (-2, 0.0595) 0.1773 low: (-2, 0.07) 0.3646\n",
            "high: (-1, 0.0892) 0.2821 low: (-1, 0.0455) 0.4216\n",
            "high: (0, 0.2794) 0.3281 low: (0, 0.2601) 0.3254\n",
            "high: (1, 0.0595) 0.3898 low: (1, 0.0695) 0.2531\n",
            "high: (2, 0.1148) 0.3199 low: (2, 0.0685) 0.1661\n",
            "high: (3, 0.0702) 0.1851 low: (3, 0.0006) 0.0804\n",
            "high: (4, 0.0394) 0.1026 low: (4, 0.0267) 0.0797\n",
            "high: (5, 0.0004) 0.0564 low: (5, -0.0041) 0.0463\n",
            "high: (6, 0.0476) 0.0559 low: (6, 0.0411) 0.0514\n",
            "4 actual high: 1 actual low: -2\n",
            "high: (-6, 0.0569) 0.0757 low: (-6, 0.1689) 0.1612\n",
            "high: (-5, 0.0024) 0.0789 low: (-5, -0.0008) 0.1605\n",
            "high: (-4, -0.0642) -0.0065 low: (-4, 0.0084) 0.1685\n",
            "high: (-3, 0.038) 0.044 low: (-3, 0.0918) 0.2561\n",
            "high: (-2, 0.0181) 0.0681 low: (-2, 0.0566) 0.3102\n",
            "high: (-1, 0.0489) 0.1332 low: (-1, 0.091) 0.397\n",
            "high: (0, 0.2465) 0.328 low: (0, 0.2997) 0.2861\n",
            "high: (1, 0.0756) 0.5389 low: (1, 0.1176) 0.3168\n",
            "high: (2, 0.0534) 0.4383 low: (2, 0.0662) 0.2046\n",
            "high: (3, 0.0911) 0.3672 low: (3, 0.0575) 0.1414\n",
            "high: (4, 0.0569) 0.246 low: (4, 0.0191) 0.0865\n",
            "high: (5, 0.0021) 0.1703 low: (5, 0.0154) 0.0683\n",
            "high: (6, 0.1259) 0.1675 low: (6, 0.0561) 0.0536\n",
            "------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8bdff6649e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mlb_tensor_flat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlb_list_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0mrnn_out\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft_list_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m   \u001b[0mcur_gain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_pred1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactual_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandard_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0mgain_loss_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_gain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8bdff6649e55>\u001b[0m in \u001b[0;36meval_pred1\u001b[0;34m(rnn_out0, actual_outcome0, standard_labels0, gain_threshold0)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mgain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mrnn_out_flat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_out0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout2labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out_flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandard_labels0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0meval_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mac0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_outcome0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-5a91fc6549fc>\u001b[0m in \u001b[0;36mout2labels\u001b[0;34m(rnn_flat_out, label_list)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtmp_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlb0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcs0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcur_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtmp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcs0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtmp_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mfinal_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch, random\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "def eval_pred1(rnn_out0,actual_outcome0,standard_labels0,gain_threshold0=5):\n",
        "  gain_loss=0\n",
        "  rnn_out_flat=rnn_out0.ravel()\n",
        "  preds=out2labels(rnn_out_flat,standard_labels0)\n",
        "  eval_list=[]\n",
        "  for ac0,pred0 in zip(actual_outcome0,preds):\n",
        "    cur_pred=[(v[0],round(v[1].item(),4)) for v in pred0]\n",
        "    eval_list.append((ac0,cur_pred))\n",
        "  for i0 in range(0,len(eval_list),2):\n",
        "    cur_high=eval_list[i0]\n",
        "    cur_low=eval_list[i0+1]\n",
        "    high_ac,high_preds=cur_high\n",
        "    low_ac,low_preds=cur_low\n",
        "    sorted_high_preds=[(int(v[0]),v[1]) for v in high_preds]\n",
        "    sorted_high_preds=sorted(sorted_high_preds,key=lambda x:int(x[0]))\n",
        "    sum_high_pred_vals=sum([v[1] for v in sorted_high_preds])\n",
        "    sorted_low_preds=[(int(v[0]),v[1]) for v in low_preds]\n",
        "    sorted_low_preds=sorted(sorted_low_preds,key=lambda x:int(x[0]))\n",
        "    sum_low_pred_vals=sum([v[1] for v in sorted_low_preds])\n",
        "\n",
        "    #print(\"actual high:\",high_ac,\"sorted_high_preds\",sorted_high_preds)\n",
        "    print(int(i0/2), \"actual high:\",high_ac, \"actual low:\",low_ac)\n",
        "    for hi,hi_lb_wt in enumerate(sorted_high_preds):\n",
        "      hi_lb0,hi_wt0=hi_lb_wt\n",
        "      low_lb0,low_wt0=low_lb_wt=sorted_low_preds[hi]\n",
        "      \n",
        "      if hi_lb0<0: hi_cum_wt=sum([v[1] for v in sorted_high_preds[:hi+1]])\n",
        "      elif hi_lb0>0: hi_cum_wt=sum([v[1] for v in sorted_high_preds[hi:]])\n",
        "      else: hi_cum_wt=hi_wt0\n",
        "      avg_hi_cum_wt=hi_cum_wt/sum_high_pred_vals\n",
        "\n",
        "      if low_lb0<0: low_cum_wt=sum([v[1] for v in sorted_low_preds[:hi+1]])\n",
        "      elif low_lb0>0: low_cum_wt=sum([v[1] for v in sorted_low_preds[hi:]])\n",
        "      else: low_cum_wt=low_wt0\n",
        "      avg_low_cum_wt=low_cum_wt/sum_low_pred_vals\n",
        "\n",
        "      #print(hi,hi_lb_wt,round(avg_cum_wt,4))\n",
        "      print(\"high:\", hi_lb_wt,round(avg_hi_cum_wt,4),\"low:\", low_lb_wt,round(avg_low_cum_wt,4))\n",
        "    \n",
        "    \n",
        "    #print(\"cur_high\",cur_high,\"cur_low\",cur_low)\n",
        "    #print()\n",
        "\n",
        "\n",
        "    # ac_high,pred_high_wt=cur_high\n",
        "    # pred_high,pred_wt=pred_high_wt\n",
        "    # print(i0/2,cur_high)\n",
        "    # if i0>0 and int(pred_high)>=gain_threshold0: \n",
        "    #   #print(i0/2, \"Sell Decision - predicted: %s - actual: %s\"%(pred_high,ac_high))\n",
        "    #   gain_loss=int(ac_high)\n",
        "    #   break\n",
        "  return gain_loss\n",
        "\n",
        "\n",
        "#epoch_i=5\n",
        "file_i=126 #48\n",
        "\n",
        "\n",
        "#exp_name=\"new-stock-pred-test2\"\n",
        "\n",
        "# epoch_i=4\n",
        "# exp_name=\"new-stock-pred-test29-64-L2-LR1e-6\"\n",
        "\n",
        "# epoch_i=4\n",
        "# exp_name=\"new-stock-pred-test39-64-L2-LR1e-6\"\n",
        "\n",
        "#epoch_i=40\n",
        "# epoch_i=20#22\n",
        "# exp_name=\"new-stock-unflattened-000001\"\n",
        "\n",
        "epoch_i=3\n",
        "#epoch_i=2\n",
        "#exp_name=\"new-stock-unflattened-128-000001\"\n",
        "#exp_name=\"new-stock-unflattened-sample-30-10-128-0000001\"\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-0000001\"\n",
        "\n",
        "epoch_i=0\n",
        "exp_name=\"test-batches-1layer-000001-256\"\n",
        "\n",
        "model_dir=\"models\"\n",
        "exp_dir_path=os.path.join(model_dir,exp_name)\n",
        "tmp_path=os.path.join(exp_dir_path,\"model-%s.model\"%epoch_i)\n",
        "\n",
        "tmp_path='models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-25.model'\n",
        "tmp_path='models/batches-2layer-0000001-256-5day-pred-39stocks/tmp/model-batch-70.model'\n",
        "try: checkpoint = torch.load(tmp_path)\n",
        "except: checkpoint = dill_unpickle(tmp_path)\n",
        "rnn = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , checkpoint[\"matching_in_out\"]).to(device)\n",
        "n_input=checkpoint[\"n_input\"]\n",
        "standard_labels=checkpoint[\"output_labels\"]\n",
        "cur_parameters=checkpoint[\"feature_extraction_parameters\"]\n",
        "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn.eval()\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "files=['ABC', 'ADP', 'A', 'ABT', 'ABMD', 'ADI', 'ABBV', 'AAPL', 'ADSK', 'ADM', 'ACN', 'AAP', 'AAL', 'ALGN', 'APH', 'AOS', 'AWK', 'ALLE', 'AME', 'APD', 'ARE', 'AIZ', 'ALB', 'APA', 'ALK', 'AEE', 'AMGN', 'ANTM', 'AEP', 'AON', 'AKAM', 'AXP', 'AMD', 'AMAT', 'AMP', 'ANET', 'AJG', 'AZO', 'ATVI', 'AMZN', 'AMT', 'AVB', 'ALTR', 'AVY', 'CAH', 'CDNS', 'BIO', 'CDE', 'BXP', 'BK', 'BEN', 'C', 'BMRA', 'BAX', 'BLK', 'BF-A', 'BDX', 'BR', 'BSHI', 'CB', 'CAG', 'BIIB', 'BAC', 'BMY', 'CCI', 'BSX', 'CAT', 'BRK-A', 'BBY', 'BA', 'BWA', 'CME', 'CNWT', 'CF', 'CTXS', 'D', 'CTSH', 'CHD', 'CFG', 'DFS', 'CPICQ', 'DG', 'CRM', 'CHRW', 'CLX', 'DGX', 'CPB', 'COTY', 'CHTR', 'COP', 'CNC', 'CNP', 'DE', 'COO', 'CUK', 'CPRT', 'COST', 'CINF', 'CMG', 'CL', 'CTQ', 'CTAS', 'CMI', 'CSCO', 'COWN', 'DAL', 'DTE', 'ENS', 'EQIX', 'DRE', 'DOV', 'DHI', 'EW', 'ES', 'EQR', 'DIS', 'DPZ', 'FANG', 'EXR', 'EMR', 'DLTR', 'EMN', 'FAST', 'DVA', 'EBAY', 'EA', 'DRI', 'EOG', 'EL', 'ESS', 'EIX', 'DXCM', 'EFX', 'F', 'ECL', 'ED', 'GS-PJ', 'GILD', 'GIS', 'FMBM', 'FPLPF', 'GM', 'FBHS', 'HBAN', 'FLS', 'FIS', 'FE', 'FRT', 'FRMC', 'FFIV', 'GWW', 'GRMN', 'GGG', 'FN', 'GOOG', 'GPC', 'FLT', 'FITB', 'FCX', 'FISV', 'GPN', 'FMC', 'FRC', 'HAL', 'FDX', 'FCGN', 'FB', 'GE', 'FTI', 'GD', 'HAS', 'HD', 'INTU', 'IFF', 'IRM', 'ICE', 'HLT', 'IDXX', 'HII', 'ILMN', 'HTLF', 'HPQ', 'HON', 'IBM', 'IPGP', 'HCA', 'HRL', 'IR', 'HSY', 'HOLX', 'ISRG', 'HPE', 'HRB', 'HSIC', 'INTH', 'HFC', 'HBI', 'HUM', 'IP', 'HST', 'IEX', 'HES', 'KSU', 'LNT', 'KRA', 'KHC', 'KR', 'KMB', 'JKHY', 'JNJ', 'IT', 'LEG', 'ITW', 'KSS', 'KEY', 'JNPR', 'LKQ', 'KIM', 'IVZ', 'KO', 'LNC', 'JBHT', 'LDOS', 'KMX', 'LMT', 'K', 'JPM', 'KGNR', 'KACPF', 'JCI', 'LH', 'KEYS', 'LBTYA', 'MSFT', 'MKTX', 'LYB', 'MCO', 'MRO', 'MDLZ', 'MLM', 'LVS', 'LRCX', 'MSCI', 'MOS', 'MRK', 'MET', 'MGM', 'MNST', 'MMC', 'MO', 'MCHP', 'LYV', 'MHK', 'MDT', 'LUV', 'MCK', 'MS-PF', 'MMM', 'MAA', 'MCD', 'MPC', 'MAR', 'LOW', 'MRCR', 'NOV', 'PEG', 'NVRO', 'NTRR', 'MU', 'NTRA', 'PAYX', 'NSC', 'NRG', 'ODFL', 'NTAP', 'PBCT', 'NFLX', 'ORLY', 'OMC', 'NTRS', 'NCTKF', 'NOXL', 'OKE', 'NI', 'NVR', 'NOC', 'O', 'NOW', 'PCAR', 'NEE', 'NLSN', 'NWL', 'MSI', 'NDAQ', 'NMHLY', 'OXY', 'NOK', 'NEOG', 'NCLH', 'RF', 'PSX', 'RE', 'PPG', 'ROK', 'PXD', 'RIBT', 'RCL', 'REGN', 'RMD', 'PKI', 'RL', 'RJF', 'PG', 'QRVO', 'REG', 'PHM', 'PNWRF', 'PKG', 'PNW', 'PLD', 'PVH', 'PM', 'PNR', 'PWR', 'PH', 'RLI', 'PEP', 'PRU', 'PFE', 'RHI', 'ROST', 'TAP', 'TEL', 'SRG', 'SLG', 'RSG', 'SYK', 'SNPS', 'SCHW', 'SHW', 'RXMD', 'SEGXF', 'SWKS', 'SBUX', 'RSNHF', 'SWK', 'SONC', 'ROP', 'STZ-B', 'TCYSF', 'STT', 'SPG', 'SYF', 'T', 'STX', 'SIVB', 'SO', 'ROL', 'TJX', 'SEE', 'SLB', 'SRE', 'VZ', 'UNP', 'TMUS', 'TRAUF', 'V', 'TW', 'VRSK', 'TWTR', 'URI', 'ULTA', 'UPS', 'UDR', 'TSN', 'UAL', 'TSCO', 'TTWO', 'VRSN', 'UA', 'TMO', 'WBA', 'TXN', 'UNM', 'USB', 'TXT', 'VMC', 'WAT', 'UHS', 'UEEC', 'VTR', 'TYL', 'TROW', 'TRV', 'VFC', 'WYNN', 'WSPOF', 'WU', 'YUM', 'XYL', 'WST', 'WRK', 'WEC', 'WM', 'ZTS', 'ZBH', 'XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "\n",
        "fname=\"%s.csv\"%files[file_i]\n",
        "cur_fpath=os.path.join(root_dir,fname)\n",
        "tmp_data=get_data_full(cur_fpath,cur_parameters)\n",
        "\n",
        "arbitrary_i=3\n",
        "gain_loss_items=[]\n",
        "arbitrary_items=[]\n",
        "random_items=[]\n",
        "#for item_i in range(len(tmp_data)):\n",
        "for item_i in range(100):  \n",
        "  if item_i%500==0:print(\"item_i\",item_i, \"out of:\", len(tmp_data))\n",
        "  #print(\"item_i\",item_i)\n",
        "  item0=tmp_data[item_i]\n",
        "  ft_list,lb_list=item0[:2]\n",
        "  actual_labels=item0[2]\n",
        "  actual_pairs=get_pairs(actual_labels)\n",
        "  cur_pair=actual_pairs[arbitrary_i]\n",
        "  arbitrary_items.append(int(cur_pair[0]))\n",
        "  random_i=random.randint(2,4)\n",
        "  random_pair=actual_pairs[random_i]\n",
        "  random_items.append(int(random_pair[0]))  \n",
        "  #print(cur_pair)\n",
        "  ft_list_tensor=torch.tensor(ft_list,dtype=torch.float32)\n",
        "  #ft_list_tensor=ft_list_tensor.reshape([1,n_input])\n",
        "  lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "  lb_tensor_flat=lb_list_tensor.ravel()\n",
        "  rnn_out=rnn(ft_list_tensor)\n",
        "  cur_gain_loss=eval_pred1(rnn_out,actual_labels,standard_labels)\n",
        "  gain_loss_items.append(cur_gain_loss)\n",
        "  print(\"------\")\n",
        "\n",
        "\n",
        "overall_gain_loss=sum(gain_loss_items)\n",
        "avg_gain_loss=sum(gain_loss_items)/len(gain_loss_items)\n",
        "n_gain_items=len([v for v in gain_loss_items if v>0])\n",
        "n_loss_items=len([v for v in gain_loss_items if v<0])\n",
        "percent_gain_items=round(100*n_gain_items/len(gain_loss_items))\n",
        "percent_loss_items=round(100*n_loss_items/len(gain_loss_items))\n",
        "\n",
        "avg_arbitrary=sum(arbitrary_items)/len(arbitrary_items)\n",
        "avg_random=sum(random_items)/len(random_items)\n",
        "\n",
        "print(fname, \"epoch\",epoch_i)\n",
        "print(\"overall_gain_loss\",overall_gain_loss)\n",
        "print(\"avg_gain_loss\",round(avg_gain_loss,2))\n",
        "print(\"avg_arbitrary\",round(avg_arbitrary,2))\n",
        "print(\"avg_random\",round(avg_random,2))\n",
        "print(\"n_gain_items\",n_gain_items)\n",
        "print(\"n_loss_items\",n_loss_items)\n",
        "print(\"percent_gain_items\",percent_gain_items)\n",
        "print(\"percent_loss_items\",percent_loss_items)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN0HAV-npeAU"
      },
      "source": [
        "#New processing - Aug 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Vz5HwVOgk8",
        "outputId": "75dead50-1676-46c3-e5c3-3343a0371aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8165 [0.53125, 0.515625, 0.0, 0.515625]\n",
            "[0.0152, -0.0227, 0.0076, 0.0076]\n",
            "[0.053, 0.0152, 0.0303, 0.0492]\n",
            "[0.0682, 0.0455, 0.0455, 0.0606]\n",
            "[-0.0076, -0.053, -0.0152, -0.0076]\n",
            "[0.0152, -0.0379, 0.0, -0.0227]\n",
            "day:0;low:<0 1.0\n",
            "day:0;low:<-2 1.0\n",
            "day:0;low:<2 1.0\n",
            "day:1234;high:>0 1.0\n",
            "day:1234;high:>2 1.0\n",
            "day:1234;high:>5 1.0\n",
            "day:4;close:<0 1.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cur_params={}\n",
        "cur_params[\"prev_n\"]=30#30\n",
        "cur_params[\"next_n\"]=5#10\n",
        "cur_params[\"max_percent\"]=5\n",
        "cur_params[\"columns\"]=[\"High\",\"Low\",\"Open\",\"Close\"]\n",
        "#cur_params[\"labels\"]=[\"day:0;low:<0\",\"day:0;low:<-2\",\"day:1234;high:>2\",\"day:1234;high:>5\",\"day:34;high:>2\",\"day:34;high:>5\",\"day:4;close:<0\",\"day:4;close:>0\",\"day:4;close:<-3\",\"day:4;close:>3\"]\n",
        "#cur_params[\"labels\"]=[\"day:0;low:<0\",\"day:0;low:<-2\",\"day:0;low:<2\",\"day:1234;high:>2\",\"day:1234;high:>5\",\"day:4;close:<0\",\"day:4;close:>0\",\"day:4;close:<-3\",\"day:4;close:>3\"]\n",
        "cur_params[\"pred_labels\"]=[\"day:0;low:<0\",\"day:0;low:<-2\",\"day:0;low:<2\",\"day:1234;high:>0\",\"day:1234;high:>2\",\"day:1234;high:>5\",\"day:4;close:<0\"]\n",
        "cur_params[\"gain_threshold\"]=5\n",
        "cur_params[\"flatten\"]=False\n",
        "cur_params[\"standard_labels\"]=gen_labels(cur_params[\"max_percent\"])\n",
        "\n",
        "\n",
        "def get_label_spec_dict(label0): # e.g. \"day:0;low:<0\"\n",
        "  tmp_dict0={}\n",
        "  lb_split=label0.split(\";\")\n",
        "  for sp0 in lb_split:\n",
        "    colon_split=sp0.split(\":\")\n",
        "    tmp_dict0[colon_split[0]]=colon_split[1]\n",
        "  return tmp_dict0\n",
        "\n",
        "def get_specs_from_next_data(normalized_next0,params0={}):\n",
        "  pred_labels0=params0.get(\"pred_labels\",[])\n",
        "  label_val_dict0={}\n",
        "  label_spec_dict0={}\n",
        "  for a in pred_labels0:\n",
        "    label_val_dict0[a]=0.\n",
        "    spec0=get_label_spec_dict(a)\n",
        "    label_spec_dict0[a]=spec0\n",
        "  for day0,item0 in enumerate(normalized_next0):\n",
        "    high0,low0,open0,close0=item0\n",
        "    local_dict={}\n",
        "    local_dict[\"high\"]=high0\n",
        "    local_dict[\"low\"]=low0\n",
        "    local_dict[\"open\"]=open0\n",
        "    local_dict[\"close\"]=close0\n",
        "    for lb0 in pred_labels0:\n",
        "      corr_tmp_dict=label_spec_dict0[lb0]\n",
        "      if not str(day0) in corr_tmp_dict.get(\"day\",\"\"): continue\n",
        "      for ld_key,ld_val in local_dict.items():\n",
        "        corr_rule=corr_tmp_dict.get(ld_key)\n",
        "        if corr_rule==None: continue\n",
        "        comparator=corr_rule[0]\n",
        "        compared_to=int(corr_rule[1:])\n",
        "        ld_val_percent=100*ld_val\n",
        "        outcome=0.\n",
        "        if comparator==\">\" and ld_val_percent>=compared_to: outcome=1.\n",
        "        if comparator==\"<\" and ld_val_percent<=compared_to: outcome=1.\n",
        "        if comparator==\"=\" and int(ld_val_percent)==compared_to: outcome=1.\n",
        "        if outcome==1: label_val_dict0[lb0]=1.\n",
        "  return label_val_dict0\n",
        "\n",
        "\n",
        "file_i=125 #48\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "files=['ABC', 'ADP', 'A', 'ABT', 'ABMD', 'ADI', 'ABBV', 'AAPL', 'ADSK', 'ADM', 'ACN', 'AAP', 'AAL', 'ALGN', 'APH', 'AOS', 'AWK', 'ALLE', 'AME', 'APD', 'ARE', 'AIZ', 'ALB', 'APA', 'ALK', 'AEE', 'AMGN', 'ANTM', 'AEP', 'AON', 'AKAM', 'AXP', 'AMD', 'AMAT', 'AMP', 'ANET', 'AJG', 'AZO', 'ATVI', 'AMZN', 'AMT', 'AVB', 'ALTR', 'AVY', 'CAH', 'CDNS', 'BIO', 'CDE', 'BXP', 'BK', 'BEN', 'C', 'BMRA', 'BAX', 'BLK', 'BF-A', 'BDX', 'BR', 'BSHI', 'CB', 'CAG', 'BIIB', 'BAC', 'BMY', 'CCI', 'BSX', 'CAT', 'BRK-A', 'BBY', 'BA', 'BWA', 'CME', 'CNWT', 'CF', 'CTXS', 'D', 'CTSH', 'CHD', 'CFG', 'DFS', 'CPICQ', 'DG', 'CRM', 'CHRW', 'CLX', 'DGX', 'CPB', 'COTY', 'CHTR', 'COP', 'CNC', 'CNP', 'DE', 'COO', 'CUK', 'CPRT', 'COST', 'CINF', 'CMG', 'CL', 'CTQ', 'CTAS', 'CMI', 'CSCO', 'COWN', 'DAL', 'DTE', 'ENS', 'EQIX', 'DRE', 'DOV', 'DHI', 'EW', 'ES', 'EQR', 'DIS', 'DPZ', 'FANG', 'EXR', 'EMR', 'DLTR', 'EMN', 'FAST', 'DVA', 'EBAY', 'EA', 'DRI', 'EOG', 'EL', 'ESS', 'EIX', 'DXCM', 'EFX', 'F', 'ECL', 'ED', 'GS-PJ', 'GILD', 'GIS', 'FMBM', 'FPLPF', 'GM', 'FBHS', 'HBAN', 'FLS', 'FIS', 'FE', 'FRT', 'FRMC', 'FFIV', 'GWW', 'GRMN', 'GGG', 'FN', 'GOOG', 'GPC', 'FLT', 'FITB', 'FCX', 'FISV', 'GPN', 'FMC', 'FRC', 'HAL', 'FDX', 'FCGN', 'FB', 'GE', 'FTI', 'GD', 'HAS', 'HD', 'INTU', 'IFF', 'IRM', 'ICE', 'HLT', 'IDXX', 'HII', 'ILMN', 'HTLF', 'HPQ', 'HON', 'IBM', 'IPGP', 'HCA', 'HRL', 'IR', 'HSY', 'HOLX', 'ISRG', 'HPE', 'HRB', 'HSIC', 'INTH', 'HFC', 'HBI', 'HUM', 'IP', 'HST', 'IEX', 'HES', 'KSU', 'LNT', 'KRA', 'KHC', 'KR', 'KMB', 'JKHY', 'JNJ', 'IT', 'LEG', 'ITW', 'KSS', 'KEY', 'JNPR', 'LKQ', 'KIM', 'IVZ', 'KO', 'LNC', 'JBHT', 'LDOS', 'KMX', 'LMT', 'K', 'JPM', 'KGNR', 'KACPF', 'JCI', 'LH', 'KEYS', 'LBTYA', 'MSFT', 'MKTX', 'LYB', 'MCO', 'MRO', 'MDLZ', 'MLM', 'LVS', 'LRCX', 'MSCI', 'MOS', 'MRK', 'MET', 'MGM', 'MNST', 'MMC', 'MO', 'MCHP', 'LYV', 'MHK', 'MDT', 'LUV', 'MCK', 'MS-PF', 'MMM', 'MAA', 'MCD', 'MPC', 'MAR', 'LOW', 'MRCR', 'NOV', 'PEG', 'NVRO', 'NTRR', 'MU', 'NTRA', 'PAYX', 'NSC', 'NRG', 'ODFL', 'NTAP', 'PBCT', 'NFLX', 'ORLY', 'OMC', 'NTRS', 'NCTKF', 'NOXL', 'OKE', 'NI', 'NVR', 'NOC', 'O', 'NOW', 'PCAR', 'NEE', 'NLSN', 'NWL', 'MSI', 'NDAQ', 'NMHLY', 'OXY', 'NOK', 'NEOG', 'NCLH', 'RF', 'PSX', 'RE', 'PPG', 'ROK', 'PXD', 'RIBT', 'RCL', 'REGN', 'RMD', 'PKI', 'RL', 'RJF', 'PG', 'QRVO', 'REG', 'PHM', 'PNWRF', 'PKG', 'PNW', 'PLD', 'PVH', 'PM', 'PNR', 'PWR', 'PH', 'RLI', 'PEP', 'PRU', 'PFE', 'RHI', 'ROST', 'TAP', 'TEL', 'SRG', 'SLG', 'RSG', 'SYK', 'SNPS', 'SCHW', 'SHW', 'RXMD', 'SEGXF', 'SWKS', 'SBUX', 'RSNHF', 'SWK', 'SONC', 'ROP', 'STZ-B', 'TCYSF', 'STT', 'SPG', 'SYF', 'T', 'STX', 'SIVB', 'SO', 'ROL', 'TJX', 'SEE', 'SLB', 'SRE', 'VZ', 'UNP', 'TMUS', 'TRAUF', 'V', 'TW', 'VRSK', 'TWTR', 'URI', 'ULTA', 'UPS', 'UDR', 'TSN', 'UAL', 'TSCO', 'TTWO', 'VRSN', 'UA', 'TMO', 'WBA', 'TXN', 'UNM', 'USB', 'TXT', 'VMC', 'WAT', 'UHS', 'UEEC', 'VTR', 'TYL', 'TROW', 'TRV', 'VFC', 'WYNN', 'WSPOF', 'WU', 'YUM', 'XYL', 'WST', 'WRK', 'WEC', 'WM', 'ZTS', 'ZBH', 'XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "fname=\"%s.csv\"%files[file_i]\n",
        "cur_fpath=os.path.join(root_dir,fname)\n",
        "\n",
        "start_i=751\n",
        "n_prev=30\n",
        "n_next=5\n",
        "\n",
        "cur_data0=get_csv_data_new(cur_fpath)\n",
        "print(len(cur_data0), cur_data0[0])\n",
        "prev0=cur_data0[start_i:start_i+n_prev]\n",
        "next0=cur_data0[start_i+n_prev:start_i+n_prev+n_next]\n",
        "pv,nx=normalize_ft_labels_new(prev0,next0)\n",
        "\n",
        "for n0 in nx:\n",
        "  print([round(v,4) for v in n0])\n",
        "cur_label_val_dict=get_specs_from_next_data(nx,cur_params)\n",
        "for a in cur_params[\"pred_labels\"]:\n",
        "  print(a, cur_label_val_dict[a])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECuYO0UAQc2l",
        "outputId": "029d1f1c-d37b-4f74-fab6-6247a261be14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6725\n",
            "([0.09195402298850575, 0.034482758620689655, 0.08045977011494253, 0.08620689655172414, 0.09195402298850575, 0.04597701149425287, 0.06896551724137931, 0.04597701149425287, 0.08045977011494253, 0.04597701149425287, 0.04597701149425287, 0.06321839080459771, 0.06896551724137931, 0.04597701149425287, 0.06896551724137931, 0.04597701149425287, 0.06896551724137931, 0.04597701149425287, 0.04597701149425287, 0.05747126436781609, 0.06896551724137931, 0.04597701149425287, 0.04597701149425287, 0.05172413793103448, 0.05747126436781609, 0.04597701149425287, 0.04597701149425287, 0.05172413793103448, 0.05747126436781609, 0.04597701149425287, 0.04597701149425287, 0.05747126436781609, 0.08620689655172414, 0.04597701149425287, 0.04597701149425287, 0.08620689655172414, 0.08620689655172414, 0.05747126436781609, 0.06321839080459771, 0.08045977011494253, 0.06896551724137931, 0.034482758620689655, 0.05747126436781609, 0.04597701149425287, 0.034482758620689655, 0.0, 0.034482758620689655, 0.005747126436781609, 0.034482758620689655, 0.0, 0.0, 0.028735632183908046, 0.034482758620689655, 0.011494252873563218, 0.011494252873563218, 0.011494252873563218, 0.034482758620689655, 0.011494252873563218, 0.011494252873563218, 0.034482758620689655, 0.034482758620689655, 0.011494252873563218, 0.011494252873563218, 0.011494252873563218, 0.034482758620689655, 0.011494252873563218, 0.011494252873563218, 0.022988505747126436, 0.034482758620689655, 0.017241379310344827, 0.022988505747126436, 0.017241379310344827, 0.034482758620689655, 0.017241379310344827, 0.034482758620689655, 0.017241379310344827, 0.06321839080459771, 0.017241379310344827, 0.017241379310344827, 0.04597701149425287, 0.06321839080459771, 0.022988505747126436, 0.034482758620689655, 0.05172413793103448, 0.06321839080459771, 0.034482758620689655, 0.04597701149425287, 0.034482758620689655, 0.05747126436781609, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.06321839080459771, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.04597701149425287, 0.005747126436781609, 0.034482758620689655, 0.011494252873563218, 0.034482758620689655, 0.005747126436781609, 0.034482758620689655, 0.011494252873563218, 0.034482758620689655, 0.005747126436781609, 0.011494252873563218, 0.011494252873563218, 0.034482758620689655, 0.005747126436781609, 0.034482758620689655, 0.011494252873563218, 0.028735632183908046, -0.005747126436781609, 0.028735632183908046, -0.005747126436781609, 0.005747126436781609, -0.005747126436781609, -0.005747126436781609, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], ['0', '0', '0', '-4', '-2', '-4', '-2', '-4', '1', '-4', '3', '0', '3', '0', '3', '0', '4', '0', '4', '2'], 2.71875, 'ABC.csv')\n"
          ]
        }
      ],
      "source": [
        "root_dir='stock_market_data/sp500/csv'\n",
        "files=['ABC', 'ADP', 'A', 'ABT', 'ABMD', 'ADI', 'ABBV', 'AAPL', 'ADSK', 'ADM', 'ACN', 'AAP', 'AAL', 'ALGN', 'APH', 'AOS', 'AWK', 'ALLE', 'AME', 'APD', 'ARE', 'AIZ', 'ALB', 'APA', 'ALK', 'AEE', 'AMGN', 'ANTM', 'AEP', 'AON', 'AKAM', 'AXP', 'AMD', 'AMAT', 'AMP', 'ANET', 'AJG', 'AZO', 'ATVI', 'AMZN', 'AMT', 'AVB', 'ALTR', 'AVY', 'CAH', 'CDNS', 'BIO', 'CDE', 'BXP', 'BK', 'BEN', 'C', 'BMRA', 'BAX', 'BLK', 'BF-A', 'BDX', 'BR', 'BSHI', 'CB', 'CAG', 'BIIB', 'BAC', 'BMY', 'CCI', 'BSX', 'CAT', 'BRK-A', 'BBY', 'BA', 'BWA', 'CME', 'CNWT', 'CF', 'CTXS', 'D', 'CTSH', 'CHD', 'CFG', 'DFS', 'CPICQ', 'DG', 'CRM', 'CHRW', 'CLX', 'DGX', 'CPB', 'COTY', 'CHTR', 'COP', 'CNC', 'CNP', 'DE', 'COO', 'CUK', 'CPRT', 'COST', 'CINF', 'CMG', 'CL', 'CTQ', 'CTAS', 'CMI', 'CSCO', 'COWN', 'DAL', 'DTE', 'ENS', 'EQIX', 'DRE', 'DOV', 'DHI', 'EW', 'ES', 'EQR', 'DIS', 'DPZ', 'FANG', 'EXR', 'EMR', 'DLTR', 'EMN', 'FAST', 'DVA', 'EBAY', 'EA', 'DRI', 'EOG', 'EL', 'ESS', 'EIX', 'DXCM', 'EFX', 'F', 'ECL', 'ED', 'GS-PJ', 'GILD', 'GIS', 'FMBM', 'FPLPF', 'GM', 'FBHS', 'HBAN', 'FLS', 'FIS', 'FE', 'FRT', 'FRMC', 'FFIV', 'GWW', 'GRMN', 'GGG', 'FN', 'GOOG', 'GPC', 'FLT', 'FITB', 'FCX', 'FISV', 'GPN', 'FMC', 'FRC', 'HAL', 'FDX', 'FCGN', 'FB', 'GE', 'FTI', 'GD', 'HAS', 'HD', 'INTU', 'IFF', 'IRM', 'ICE', 'HLT', 'IDXX', 'HII', 'ILMN', 'HTLF', 'HPQ', 'HON', 'IBM', 'IPGP', 'HCA', 'HRL', 'IR', 'HSY', 'HOLX', 'ISRG', 'HPE', 'HRB', 'HSIC', 'INTH', 'HFC', 'HBI', 'HUM', 'IP', 'HST', 'IEX', 'HES', 'KSU', 'LNT', 'KRA', 'KHC', 'KR', 'KMB', 'JKHY', 'JNJ', 'IT', 'LEG', 'ITW', 'KSS', 'KEY', 'JNPR', 'LKQ', 'KIM', 'IVZ', 'KO', 'LNC', 'JBHT', 'LDOS', 'KMX', 'LMT', 'K', 'JPM', 'KGNR', 'KACPF', 'JCI', 'LH', 'KEYS', 'LBTYA', 'MSFT', 'MKTX', 'LYB', 'MCO', 'MRO', 'MDLZ', 'MLM', 'LVS', 'LRCX', 'MSCI', 'MOS', 'MRK', 'MET', 'MGM', 'MNST', 'MMC', 'MO', 'MCHP', 'LYV', 'MHK', 'MDT', 'LUV', 'MCK', 'MS-PF', 'MMM', 'MAA', 'MCD', 'MPC', 'MAR', 'LOW', 'MRCR', 'NOV', 'PEG', 'NVRO', 'NTRR', 'MU', 'NTRA', 'PAYX', 'NSC', 'NRG', 'ODFL', 'NTAP', 'PBCT', 'NFLX', 'ORLY', 'OMC', 'NTRS', 'NCTKF', 'NOXL', 'OKE', 'NI', 'NVR', 'NOC', 'O', 'NOW', 'PCAR', 'NEE', 'NLSN', 'NWL', 'MSI', 'NDAQ', 'NMHLY', 'OXY', 'NOK', 'NEOG', 'NCLH', 'RF', 'PSX', 'RE', 'PPG', 'ROK', 'PXD', 'RIBT', 'RCL', 'REGN', 'RMD', 'PKI', 'RL', 'RJF', 'PG', 'QRVO', 'REG', 'PHM', 'PNWRF', 'PKG', 'PNW', 'PLD', 'PVH', 'PM', 'PNR', 'PWR', 'PH', 'RLI', 'PEP', 'PRU', 'PFE', 'RHI', 'ROST', 'TAP', 'TEL', 'SRG', 'SLG', 'RSG', 'SYK', 'SNPS', 'SCHW', 'SHW', 'RXMD', 'SEGXF', 'SWKS', 'SBUX', 'RSNHF', 'SWK', 'SONC', 'ROP', 'STZ-B', 'TCYSF', 'STT', 'SPG', 'SYF', 'T', 'STX', 'SIVB', 'SO', 'ROL', 'TJX', 'SEE', 'SLB', 'SRE', 'VZ', 'UNP', 'TMUS', 'TRAUF', 'V', 'TW', 'VRSK', 'TWTR', 'URI', 'ULTA', 'UPS', 'UDR', 'TSN', 'UAL', 'TSCO', 'TTWO', 'VRSN', 'UA', 'TMO', 'WBA', 'TXN', 'UNM', 'USB', 'TXT', 'VMC', 'WAT', 'UHS', 'UEEC', 'VTR', 'TYL', 'TROW', 'TRV', 'VFC', 'WYNN', 'WSPOF', 'WU', 'YUM', 'XYL', 'WST', 'WRK', 'WEC', 'WM', 'ZTS', 'ZBH', 'XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "file_i=0\n",
        "fname=\"%s.csv\"%files[file_i]\n",
        "cur_fpath=os.path.join(root_dir,fname)\n",
        "tmp_data=get_data_full(cur_fpath)\n",
        "print(len(tmp_data))\n",
        "print(tmp_data[0])\n",
        "# for tmp0 in tmp_data[:5]:\n",
        "#   for t_ in tmp_data:\n",
        "#     print(t_)\n",
        "#   print(\"----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCaEBjC3Ik_Q",
        "outputId": "39cf69b2-acab-4780-c961-4c251254a170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.randint(2,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKebxqH08UpT"
      },
      "outputs": [],
      "source": [
        "ANET.csv\n",
        "overall_gain_loss 3022\n",
        "avg_gain_loss 1.59\n",
        "n_gain_items 1151\n",
        "n_loss_items 471\n",
        "percent_gain_items 61\n",
        "percent_loss_items 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abvz5p3x9sz-",
        "outputId": "bb80f53e-8f52-4080-bc5c-d1700a6ec177"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 'b'), ('c', 'd')]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_pairs(list1):\n",
        "  out=[]\n",
        "  for i0 in range(0,len(list1),2): out.append((list1[i0],list1[i0+1]))\n",
        "  return out\n",
        "\n",
        "list0=[\"a\",\"b\",\"c\",\"d\"]\n",
        "get_pairs(list0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rOkTJsuisXlM",
        "outputId": "602078a6-7e52-463e-fb5c-fdfc23c4a4e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'11'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_labels(3)\n",
        "percent_bin_vals(0.174,10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDZnMVcWI0Tl",
        "outputId": "0350710c-cd65-4898-e3fc-d7cb13b4ddae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "prev_vals: [1.72, 1.73, 1.72, 1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72]\n",
            "next_vals: [1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81]\n",
            "Next percent differences: [0.58, 0.0, 1.74, 1.74, 2.33, 3.49, 4.65, 4.07, 4.65, 5.23]\n",
            "min val: 0.0 @ index: 1\n",
            "max val: 5.23 @ index: 9\n",
            "max rebound val: 5.23\n",
            "rebound_diff: 5.23\n",
            "---------\n",
            "1\n",
            "prev_vals: [1.73, 1.72, 1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73]\n",
            "next_vals: [1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79]\n",
            "Next percent differences: [-0.58, 1.16, 1.16, 1.73, 2.89, 4.05, 3.47, 4.05, 4.62, 3.47]\n",
            "min val: -0.58 @ index: 0\n",
            "max val: 4.62 @ index: 8\n",
            "max rebound val: 4.62\n",
            "rebound_diff: 5.2\n",
            "---------\n",
            "2\n",
            "prev_vals: [1.72, 1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72]\n",
            "next_vals: [1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79]\n",
            "Next percent differences: [1.74, 1.74, 2.33, 3.49, 4.65, 4.07, 4.65, 5.23, 4.07, 4.07]\n",
            "min val: 1.74 @ index: 0\n",
            "max val: 5.23 @ index: 7\n",
            "max rebound val: 5.23\n",
            "rebound_diff: 3.49\n",
            "---------\n",
            "3\n",
            "prev_vals: [1.75, 1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75]\n",
            "next_vals: [1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73]\n",
            "Next percent differences: [0.0, 0.57, 1.71, 2.86, 2.29, 2.86, 3.43, 2.29, 2.29, -1.14]\n",
            "min val: -1.14 @ index: 9\n",
            "max val: 3.43 @ index: 6\n",
            "max rebound val: -1.14\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "4\n",
            "prev_vals: [1.77, 1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75]\n",
            "next_vals: [1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69]\n",
            "Next percent differences: [0.57, 1.71, 2.86, 2.29, 2.86, 3.43, 2.29, 2.29, -1.14, -3.43]\n",
            "min val: -3.43 @ index: 9\n",
            "max val: 3.43 @ index: 5\n",
            "max rebound val: -3.43\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "5\n",
            "prev_vals: [1.76, 1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76]\n",
            "next_vals: [1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7]\n",
            "Next percent differences: [1.14, 2.27, 1.7, 2.27, 2.84, 1.7, 1.7, -1.7, -3.98, -3.41]\n",
            "min val: -3.98 @ index: 8\n",
            "max val: 2.84 @ index: 4\n",
            "max rebound val: -3.41\n",
            "rebound_diff: 0.57\n",
            "---------\n",
            "6\n",
            "prev_vals: [1.75, 1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78]\n",
            "next_vals: [1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69]\n",
            "Next percent differences: [1.12, 0.56, 1.12, 1.69, 0.56, 0.56, -2.81, -5.06, -4.49, -5.06]\n",
            "min val: -5.06 @ index: 7\n",
            "max val: 1.69 @ index: 3\n",
            "max rebound val: -4.49\n",
            "rebound_diff: 0.57\n",
            "---------\n",
            "7\n",
            "prev_vals: [1.74, 1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8]\n",
            "next_vals: [1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7]\n",
            "Next percent differences: [-0.56, 0.0, 0.56, -0.56, -0.56, -3.89, -6.11, -5.56, -6.11, -5.56]\n",
            "min val: -6.11 @ index: 6\n",
            "max val: 0.56 @ index: 2\n",
            "max rebound val: -5.56\n",
            "rebound_diff: 0.55\n",
            "---------\n",
            "8\n",
            "prev_vals: [1.75, 1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79]\n",
            "next_vals: [1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69]\n",
            "Next percent differences: [0.56, 1.12, 0.0, 0.0, -3.35, -5.59, -5.03, -5.59, -5.03, -5.59]\n",
            "min val: -5.59 @ index: 5\n",
            "max val: 1.12 @ index: 1\n",
            "max rebound val: -5.03\n",
            "rebound_diff: 0.56\n",
            "---------\n",
            "9\n",
            "prev_vals: [1.75, 1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8]\n",
            "next_vals: [1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65]\n",
            "Next percent differences: [0.56, -0.56, -0.56, -3.89, -6.11, -5.56, -6.11, -5.56, -6.11, -8.33]\n",
            "min val: -8.33 @ index: 9\n",
            "max val: 0.56 @ index: 0\n",
            "max rebound val: -8.33\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "10\n",
            "prev_vals: [1.77, 1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81]\n",
            "next_vals: [1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71]\n",
            "Next percent differences: [-1.1, -1.1, -4.42, -6.63, -6.08, -6.63, -6.08, -6.63, -8.84, -5.52]\n",
            "min val: -8.84 @ index: 8\n",
            "max val: -1.1 @ index: 0\n",
            "max rebound val: -5.52\n",
            "rebound_diff: 3.32\n",
            "---------\n",
            "11\n",
            "prev_vals: [1.75, 1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79]\n",
            "next_vals: [1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71]\n",
            "Next percent differences: [0.0, -3.35, -5.59, -5.03, -5.59, -5.03, -5.59, -7.82, -4.47, -4.47]\n",
            "min val: -7.82 @ index: 7\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -4.47\n",
            "rebound_diff: 3.35\n",
            "---------\n",
            "12\n",
            "prev_vals: [1.76, 1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79]\n",
            "next_vals: [1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73]\n",
            "Next percent differences: [-3.35, -5.59, -5.03, -5.59, -5.03, -5.59, -7.82, -4.47, -4.47, -3.35]\n",
            "min val: -7.82 @ index: 6\n",
            "max val: -3.35 @ index: 0\n",
            "max rebound val: -3.35\n",
            "rebound_diff: 4.47\n",
            "---------\n",
            "13\n",
            "prev_vals: [1.77, 1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73]\n",
            "next_vals: [1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72]\n",
            "Next percent differences: [-2.31, -1.73, -2.31, -1.73, -2.31, -4.62, -1.16, -1.16, 0.0, -0.58]\n",
            "min val: -4.62 @ index: 5\n",
            "max val: 0.0 @ index: 8\n",
            "max rebound val: 0.0\n",
            "rebound_diff: 4.62\n",
            "---------\n",
            "14\n",
            "prev_vals: [1.81, 1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69]\n",
            "next_vals: [1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71]\n",
            "Next percent differences: [0.59, 0.0, 0.59, 0.0, -2.37, 1.18, 1.18, 2.37, 1.78, 1.18]\n",
            "min val: -2.37 @ index: 4\n",
            "max val: 2.37 @ index: 7\n",
            "max rebound val: 2.37\n",
            "rebound_diff: 4.74\n",
            "---------\n",
            "15\n",
            "prev_vals: [1.76, 1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7]\n",
            "next_vals: [1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7]\n",
            "Next percent differences: [-0.59, 0.0, -0.59, -2.94, 0.59, 0.59, 1.76, 1.18, 0.59, 0.0]\n",
            "min val: -2.94 @ index: 3\n",
            "max val: 1.76 @ index: 6\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 4.7\n",
            "---------\n",
            "16\n",
            "prev_vals: [1.73, 1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69]\n",
            "next_vals: [1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71]\n",
            "Next percent differences: [0.59, 0.0, -2.37, 1.18, 1.18, 2.37, 1.78, 1.18, 0.59, 1.18]\n",
            "min val: -2.37 @ index: 2\n",
            "max val: 2.37 @ index: 5\n",
            "max rebound val: 2.37\n",
            "rebound_diff: 4.74\n",
            "---------\n",
            "17\n",
            "prev_vals: [1.74, 1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7]\n",
            "next_vals: [1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72]\n",
            "Next percent differences: [-0.59, -2.94, 0.59, 0.59, 1.76, 1.18, 0.59, 0.0, 0.59, 1.18]\n",
            "min val: -2.94 @ index: 1\n",
            "max val: 1.76 @ index: 4\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 4.7\n",
            "---------\n",
            "18\n",
            "prev_vals: [1.73, 1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69]\n",
            "next_vals: [1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72]\n",
            "Next percent differences: [-2.37, 1.18, 1.18, 2.37, 1.78, 1.18, 0.59, 1.18, 1.78, 1.78]\n",
            "min val: -2.37 @ index: 0\n",
            "max val: 2.37 @ index: 3\n",
            "max rebound val: 2.37\n",
            "rebound_diff: 4.74\n",
            "---------\n",
            "19\n",
            "prev_vals: [1.72, 1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65]\n",
            "next_vals: [1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72]\n",
            "Next percent differences: [3.64, 3.64, 4.85, 4.24, 3.64, 3.03, 3.64, 4.24, 4.24, 4.24]\n",
            "min val: 3.03 @ index: 5\n",
            "max val: 4.85 @ index: 2\n",
            "max rebound val: 4.24\n",
            "rebound_diff: 1.21\n",
            "---------\n",
            "20\n",
            "prev_vals: [1.73, 1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71]\n",
            "next_vals: [1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7]\n",
            "Next percent differences: [0.0, 1.17, 0.58, 0.0, -0.58, 0.0, 0.58, 0.58, 0.58, -0.58]\n",
            "min val: -0.58 @ index: 4\n",
            "max val: 1.17 @ index: 1\n",
            "max rebound val: 0.58\n",
            "rebound_diff: 1.16\n",
            "---------\n",
            "21\n",
            "prev_vals: [1.72, 1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71]\n",
            "next_vals: [1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66]\n",
            "Next percent differences: [1.17, 0.58, 0.0, -0.58, 0.0, 0.58, 0.58, 0.58, -0.58, -2.92]\n",
            "min val: -2.92 @ index: 9\n",
            "max val: 1.17 @ index: 0\n",
            "max rebound val: -2.92\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "22\n",
            "prev_vals: [1.75, 1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73]\n",
            "next_vals: [1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66]\n",
            "Next percent differences: [-0.58, -1.16, -1.73, -1.16, -0.58, -0.58, -0.58, -1.73, -4.05, -4.05]\n",
            "min val: -4.05 @ index: 8\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -4.05\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "23\n",
            "prev_vals: [1.75, 1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72]\n",
            "next_vals: [1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68]\n",
            "Next percent differences: [-0.58, -1.16, -0.58, 0.0, 0.0, 0.0, -1.16, -3.49, -3.49, -2.33]\n",
            "min val: -3.49 @ index: 7\n",
            "max val: 0.0 @ index: 3\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 1.16\n",
            "---------\n",
            "24\n",
            "prev_vals: [1.76, 1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71]\n",
            "next_vals: [1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66]\n",
            "Next percent differences: [-0.58, 0.0, 0.58, 0.58, 0.58, -0.58, -2.92, -2.92, -1.75, -2.92]\n",
            "min val: -2.92 @ index: 6\n",
            "max val: 0.58 @ index: 2\n",
            "max rebound val: -1.75\n",
            "rebound_diff: 1.17\n",
            "---------\n",
            "25\n",
            "prev_vals: [1.78, 1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7]\n",
            "next_vals: [1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68]\n",
            "Next percent differences: [0.59, 1.18, 1.18, 1.18, 0.0, -2.35, -2.35, -1.18, -2.35, -1.18]\n",
            "min val: -2.35 @ index: 5\n",
            "max val: 1.18 @ index: 1\n",
            "max rebound val: -1.18\n",
            "rebound_diff: 1.17\n",
            "---------\n",
            "26\n",
            "prev_vals: [1.8, 1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71]\n",
            "next_vals: [1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66]\n",
            "Next percent differences: [0.58, 0.58, 0.58, -0.58, -2.92, -2.92, -1.75, -2.92, -1.75, -2.92]\n",
            "min val: -2.92 @ index: 4\n",
            "max val: 0.58 @ index: 0\n",
            "max rebound val: -1.75\n",
            "rebound_diff: 1.17\n",
            "---------\n",
            "27\n",
            "prev_vals: [1.79, 1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72]\n",
            "next_vals: [1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68]\n",
            "Next percent differences: [0.0, 0.0, -1.16, -3.49, -3.49, -2.33, -3.49, -2.33, -3.49, -2.33]\n",
            "min val: -3.49 @ index: 3\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 1.16\n",
            "---------\n",
            "28\n",
            "prev_vals: [1.8, 1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72]\n",
            "next_vals: [1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7]\n",
            "Next percent differences: [0.0, -1.16, -3.49, -3.49, -2.33, -3.49, -2.33, -3.49, -2.33, -1.16]\n",
            "min val: -3.49 @ index: 2\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -1.16\n",
            "rebound_diff: 2.33\n",
            "---------\n",
            "29\n",
            "prev_vals: [1.81, 1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72]\n",
            "next_vals: [1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7]\n",
            "Next percent differences: [-1.16, -3.49, -3.49, -2.33, -3.49, -2.33, -3.49, -2.33, -1.16, -1.16]\n",
            "min val: -3.49 @ index: 1\n",
            "max val: -1.16 @ index: 0\n",
            "max rebound val: -1.16\n",
            "rebound_diff: 2.33\n",
            "---------\n",
            "30\n",
            "prev_vals: [1.79, 1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7]\n",
            "next_vals: [1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68]\n",
            "Next percent differences: [-2.35, -2.35, -1.18, -2.35, -1.18, -2.35, -1.18, 0.0, 0.0, -1.18]\n",
            "min val: -2.35 @ index: 0\n",
            "max val: 0.0 @ index: 7\n",
            "max rebound val: 0.0\n",
            "rebound_diff: 2.35\n",
            "---------\n",
            "31\n",
            "prev_vals: [1.79, 1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66]\n",
            "next_vals: [1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7]\n",
            "Next percent differences: [0.0, 1.2, 0.0, 1.2, 0.0, 1.2, 2.41, 2.41, 1.2, 2.41]\n",
            "min val: 0.0 @ index: 0\n",
            "max val: 2.41 @ index: 6\n",
            "max rebound val: 2.41\n",
            "rebound_diff: 2.41\n",
            "---------\n",
            "32\n",
            "prev_vals: [1.73, 1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66]\n",
            "next_vals: [1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73]\n",
            "Next percent differences: [1.2, 0.0, 1.2, 0.0, 1.2, 2.41, 2.41, 1.2, 2.41, 4.22]\n",
            "min val: 0.0 @ index: 1\n",
            "max val: 4.22 @ index: 9\n",
            "max rebound val: 4.22\n",
            "rebound_diff: 4.22\n",
            "---------\n",
            "33\n",
            "prev_vals: [1.69, 1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68]\n",
            "next_vals: [1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72]\n",
            "Next percent differences: [-1.19, 0.0, -1.19, 0.0, 1.19, 1.19, 0.0, 1.19, 2.98, 2.38]\n",
            "min val: -1.19 @ index: 0\n",
            "max val: 2.98 @ index: 8\n",
            "max rebound val: 2.98\n",
            "rebound_diff: 4.17\n",
            "---------\n",
            "34\n",
            "prev_vals: [1.7, 1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66]\n",
            "next_vals: [1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72]\n",
            "Next percent differences: [1.2, 0.0, 1.2, 2.41, 2.41, 1.2, 2.41, 4.22, 3.61, 3.61]\n",
            "min val: 0.0 @ index: 1\n",
            "max val: 4.22 @ index: 7\n",
            "max rebound val: 4.22\n",
            "rebound_diff: 4.22\n",
            "---------\n",
            "35\n",
            "prev_vals: [1.69, 1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68]\n",
            "next_vals: [1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71]\n",
            "Next percent differences: [-1.19, 0.0, 1.19, 1.19, 0.0, 1.19, 2.98, 2.38, 2.38, 1.79]\n",
            "min val: -1.19 @ index: 0\n",
            "max val: 2.98 @ index: 6\n",
            "max rebound val: 2.98\n",
            "rebound_diff: 4.17\n",
            "---------\n",
            "36\n",
            "prev_vals: [1.7, 1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66]\n",
            "next_vals: [1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72]\n",
            "Next percent differences: [1.2, 2.41, 2.41, 1.2, 2.41, 4.22, 3.61, 3.61, 3.01, 3.61]\n",
            "min val: 1.2 @ index: 0\n",
            "max val: 4.22 @ index: 5\n",
            "max rebound val: 4.22\n",
            "rebound_diff: 3.02\n",
            "---------\n",
            "37\n",
            "prev_vals: [1.69, 1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68]\n",
            "next_vals: [1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71]\n",
            "Next percent differences: [1.19, 1.19, 0.0, 1.19, 2.98, 2.38, 2.38, 1.79, 2.38, 1.79]\n",
            "min val: 0.0 @ index: 2\n",
            "max val: 2.98 @ index: 4\n",
            "max rebound val: 2.98\n",
            "rebound_diff: 2.98\n",
            "---------\n",
            "38\n",
            "prev_vals: [1.65, 1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7]\n",
            "next_vals: [1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7]\n",
            "Next percent differences: [0.0, -1.18, 0.0, 1.76, 1.18, 1.18, 0.59, 1.18, 0.59, 0.0]\n",
            "min val: -1.18 @ index: 1\n",
            "max val: 1.76 @ index: 3\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 2.94\n",
            "---------\n",
            "39\n",
            "prev_vals: [1.71, 1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7]\n",
            "next_vals: [1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69]\n",
            "Next percent differences: [-1.18, 0.0, 1.76, 1.18, 1.18, 0.59, 1.18, 0.59, 0.0, -0.59]\n",
            "min val: -1.18 @ index: 0\n",
            "max val: 1.76 @ index: 2\n",
            "max rebound val: 1.76\n",
            "rebound_diff: 2.94\n",
            "---------\n",
            "40\n",
            "prev_vals: [1.71, 1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68]\n",
            "next_vals: [1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67]\n",
            "Next percent differences: [1.19, 2.98, 2.38, 2.38, 1.79, 2.38, 1.79, 1.19, 0.6, -0.6]\n",
            "min val: -0.6 @ index: 9\n",
            "max val: 2.98 @ index: 1\n",
            "max rebound val: -0.6\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "41\n",
            "prev_vals: [1.73, 1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7]\n",
            "next_vals: [1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67]\n",
            "Next percent differences: [1.76, 1.18, 1.18, 0.59, 1.18, 0.59, 0.0, -0.59, -1.76, -1.76]\n",
            "min val: -1.76 @ index: 8\n",
            "max val: 1.76 @ index: 0\n",
            "max rebound val: -1.76\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "42\n",
            "prev_vals: [1.72, 1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73]\n",
            "next_vals: [1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66]\n",
            "Next percent differences: [-0.58, -0.58, -1.16, -0.58, -1.16, -1.73, -2.31, -3.47, -3.47, -4.05]\n",
            "min val: -4.05 @ index: 9\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -4.05\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "43\n",
            "prev_vals: [1.71, 1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72]\n",
            "next_vals: [1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63]\n",
            "Next percent differences: [0.0, -0.58, 0.0, -0.58, -1.16, -1.74, -2.91, -2.91, -3.49, -5.23]\n",
            "min val: -5.23 @ index: 9\n",
            "max val: 0.0 @ index: 0\n",
            "max rebound val: -5.23\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "44\n",
            "prev_vals: [1.7, 1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72]\n",
            "next_vals: [1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68]\n",
            "Next percent differences: [-0.58, 0.0, -0.58, -1.16, -1.74, -2.91, -2.91, -3.49, -5.23, -2.33]\n",
            "min val: -5.23 @ index: 8\n",
            "max val: 0.0 @ index: 1\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 2.9\n",
            "---------\n",
            "45\n",
            "prev_vals: [1.71, 1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71]\n",
            "next_vals: [1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66]\n",
            "Next percent differences: [0.58, 0.0, -0.58, -1.17, -2.34, -2.34, -2.92, -4.68, -1.75, -2.92]\n",
            "min val: -4.68 @ index: 7\n",
            "max val: 0.58 @ index: 0\n",
            "max rebound val: -1.75\n",
            "rebound_diff: 2.93\n",
            "---------\n",
            "46\n",
            "prev_vals: [1.72, 1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72]\n",
            "next_vals: [1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64]\n",
            "Next percent differences: [-0.58, -1.16, -1.74, -2.91, -2.91, -3.49, -5.23, -2.33, -3.49, -4.65]\n",
            "min val: -5.23 @ index: 6\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -2.33\n",
            "rebound_diff: 2.9\n",
            "---------\n",
            "47\n",
            "prev_vals: [1.72, 1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71]\n",
            "next_vals: [1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62]\n",
            "Next percent differences: [-0.58, -1.17, -2.34, -2.34, -2.92, -4.68, -1.75, -2.92, -4.09, -5.26]\n",
            "min val: -5.26 @ index: 9\n",
            "max val: -0.58 @ index: 0\n",
            "max rebound val: -5.26\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "48\n",
            "prev_vals: [1.72, 1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7]\n",
            "next_vals: [1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59]\n",
            "Next percent differences: [-0.59, -1.76, -1.76, -2.35, -4.12, -1.18, -2.35, -3.53, -4.71, -6.47]\n",
            "min val: -6.47 @ index: 9\n",
            "max val: -0.59 @ index: 0\n",
            "max rebound val: -6.47\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "49\n",
            "prev_vals: [1.7, 1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69]\n",
            "next_vals: [1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6]\n",
            "Next percent differences: [-1.18, -1.18, -1.78, -3.55, -0.59, -1.78, -2.96, -4.14, -5.92, -5.33]\n",
            "min val: -5.92 @ index: 8\n",
            "max val: -0.59 @ index: 4\n",
            "max rebound val: -5.33\n",
            "rebound_diff: 0.59\n",
            "---------\n",
            "50\n",
            "prev_vals: [1.66, 1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67]\n",
            "next_vals: [1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55]\n",
            "Next percent differences: [0.0, -0.6, -2.4, 0.6, -0.6, -1.8, -2.99, -4.79, -4.19, -7.19]\n",
            "min val: -7.19 @ index: 9\n",
            "max val: 0.6 @ index: 3\n",
            "max rebound val: -7.19\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "51\n",
            "prev_vals: [1.66, 1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67]\n",
            "next_vals: [1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56]\n",
            "Next percent differences: [-0.6, -2.4, 0.6, -0.6, -1.8, -2.99, -4.79, -4.19, -7.19, -6.59]\n",
            "min val: -7.19 @ index: 8\n",
            "max val: 0.6 @ index: 2\n",
            "max rebound val: -6.59\n",
            "rebound_diff: 0.6\n",
            "---------\n",
            "52\n",
            "prev_vals: [1.68, 1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66]\n",
            "next_vals: [1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58]\n",
            "Next percent differences: [-1.81, 1.2, 0.0, -1.2, -2.41, -4.22, -3.61, -6.63, -6.02, -4.82]\n",
            "min val: -6.63 @ index: 7\n",
            "max val: 1.2 @ index: 1\n",
            "max rebound val: -4.82\n",
            "rebound_diff: 1.81\n",
            "---------\n",
            "53\n",
            "prev_vals: [1.66, 1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63]\n",
            "next_vals: [1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56]\n",
            "Next percent differences: [3.07, 1.84, 0.61, -0.61, -2.45, -1.84, -4.91, -4.29, -3.07, -4.29]\n",
            "min val: -4.91 @ index: 6\n",
            "max val: 3.07 @ index: 0\n",
            "max rebound val: -3.07\n",
            "rebound_diff: 1.84\n",
            "---------\n",
            "54\n",
            "prev_vals: [1.68, 1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68]\n",
            "next_vals: [1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54]\n",
            "Next percent differences: [-1.19, -2.38, -3.57, -5.36, -4.76, -7.74, -7.14, -5.95, -7.14, -8.33]\n",
            "min val: -8.33 @ index: 9\n",
            "max val: -1.19 @ index: 0\n",
            "max rebound val: -8.33\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "55\n",
            "prev_vals: [1.66, 1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66]\n",
            "next_vals: [1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5]\n",
            "Next percent differences: [-1.2, -2.41, -4.22, -3.61, -6.63, -6.02, -4.82, -6.02, -7.23, -9.64]\n",
            "min val: -9.64 @ index: 9\n",
            "max val: -1.2 @ index: 0\n",
            "max rebound val: -9.64\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "56\n",
            "prev_vals: [1.68, 1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64]\n",
            "next_vals: [1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47]\n",
            "Next percent differences: [-1.22, -3.05, -2.44, -5.49, -4.88, -3.66, -4.88, -6.1, -8.54, -10.37]\n",
            "min val: -10.37 @ index: 9\n",
            "max val: -1.22 @ index: 0\n",
            "max rebound val: -10.37\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "57\n",
            "prev_vals: [1.7, 1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62]\n",
            "next_vals: [1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48]\n",
            "Next percent differences: [-1.85, -1.23, -4.32, -3.7, -2.47, -3.7, -4.94, -7.41, -9.26, -8.64]\n",
            "min val: -9.26 @ index: 8\n",
            "max val: -1.23 @ index: 1\n",
            "max rebound val: -8.64\n",
            "rebound_diff: 0.62\n",
            "---------\n",
            "58\n",
            "prev_vals: [1.7, 1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59]\n",
            "next_vals: [1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52]\n",
            "Next percent differences: [0.63, -2.52, -1.89, -0.63, -1.89, -3.14, -5.66, -7.55, -6.92, -4.4]\n",
            "min val: -7.55 @ index: 7\n",
            "max val: 0.63 @ index: 0\n",
            "max rebound val: -4.4\n",
            "rebound_diff: 3.15\n",
            "---------\n",
            "59\n",
            "prev_vals: [1.68, 1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6]\n",
            "next_vals: [1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49]\n",
            "Next percent differences: [-3.13, -2.5, -1.25, -2.5, -3.75, -6.25, -8.13, -7.5, -5.0, -6.88]\n",
            "min val: -8.13 @ index: 6\n",
            "max val: -1.25 @ index: 2\n",
            "max rebound val: -5.0\n",
            "rebound_diff: 3.13\n",
            "---------\n",
            "60\n",
            "prev_vals: [1.7, 1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55]\n",
            "next_vals: [1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47]\n",
            "Next percent differences: [0.65, 1.94, 0.65, -0.65, -3.23, -5.16, -4.52, -1.94, -3.87, -5.16]\n",
            "min val: -5.16 @ index: 5\n",
            "max val: 1.94 @ index: 1\n",
            "max rebound val: -1.94\n",
            "rebound_diff: 3.22\n",
            "---------\n",
            "61\n",
            "prev_vals: [1.73, 1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56]\n",
            "next_vals: [1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45]\n",
            "Next percent differences: [1.28, 0.0, -1.28, -3.85, -5.77, -5.13, -2.56, -4.49, -5.77, -7.05]\n",
            "min val: -7.05 @ index: 9\n",
            "max val: 1.28 @ index: 0\n",
            "max rebound val: -7.05\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "62\n",
            "prev_vals: [1.72, 1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58]\n",
            "next_vals: [1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44]\n",
            "Next percent differences: [-1.27, -2.53, -5.06, -6.96, -6.33, -3.8, -5.7, -6.96, -8.23, -8.86]\n",
            "min val: -8.86 @ index: 9\n",
            "max val: -1.27 @ index: 0\n",
            "max rebound val: -8.86\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "63\n",
            "prev_vals: [1.72, 1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56]\n",
            "next_vals: [1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44]\n",
            "Next percent differences: [-1.28, -3.85, -5.77, -5.13, -2.56, -4.49, -5.77, -7.05, -7.69, -7.69]\n",
            "min val: -7.69 @ index: 8\n",
            "max val: -1.28 @ index: 0\n",
            "max rebound val: -7.69\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "64\n",
            "prev_vals: [1.71, 1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54]\n",
            "next_vals: [1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51]\n",
            "Next percent differences: [-2.6, -4.55, -3.9, -1.3, -3.25, -4.55, -5.84, -6.49, -6.49, -1.95]\n",
            "min val: -6.49 @ index: 7\n",
            "max val: -1.3 @ index: 3\n",
            "max rebound val: -1.95\n",
            "rebound_diff: 4.54\n",
            "---------\n",
            "65\n",
            "prev_vals: [1.72, 1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5]\n",
            "next_vals: [1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5]\n",
            "Next percent differences: [-2.0, -1.33, 1.33, -0.67, -2.0, -3.33, -4.0, -4.0, 0.67, 0.0]\n",
            "min val: -4.0 @ index: 6\n",
            "max val: 1.33 @ index: 2\n",
            "max rebound val: 0.67\n",
            "rebound_diff: 4.67\n",
            "---------\n",
            "66\n",
            "prev_vals: [1.71, 1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47]\n",
            "next_vals: [1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45]\n",
            "Next percent differences: [0.68, 3.4, 1.36, 0.0, -1.36, -2.04, -2.04, 2.72, 2.04, -1.36]\n",
            "min val: -2.04 @ index: 5\n",
            "max val: 3.4 @ index: 1\n",
            "max rebound val: 2.72\n",
            "rebound_diff: 4.76\n",
            "---------\n",
            "67\n",
            "prev_vals: [1.7, 1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48]\n",
            "next_vals: [1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41]\n",
            "Next percent differences: [2.7, 0.68, -0.68, -2.03, -2.7, -2.7, 2.03, 1.35, -2.03, -4.73]\n",
            "min val: -4.73 @ index: 9\n",
            "max val: 2.7 @ index: 0\n",
            "max rebound val: -4.73\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "68\n",
            "prev_vals: [1.69, 1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52]\n",
            "next_vals: [1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39]\n",
            "Next percent differences: [-1.97, -3.29, -4.61, -5.26, -5.26, -0.66, -1.32, -4.61, -7.24, -8.55]\n",
            "min val: -8.55 @ index: 9\n",
            "max val: -0.66 @ index: 5\n",
            "max rebound val: -8.55\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "69\n",
            "prev_vals: [1.67, 1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49]\n",
            "next_vals: [1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34]\n",
            "Next percent differences: [-1.34, -2.68, -3.36, -3.36, 1.34, 0.67, -2.68, -5.37, -6.71, -10.07]\n",
            "min val: -10.07 @ index: 9\n",
            "max val: 1.34 @ index: 4\n",
            "max rebound val: -10.07\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "70\n",
            "prev_vals: [1.67, 1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47]\n",
            "next_vals: [1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3]\n",
            "Next percent differences: [-1.36, -2.04, -2.04, 2.72, 2.04, -1.36, -4.08, -5.44, -8.84, -11.56]\n",
            "min val: -11.56 @ index: 9\n",
            "max val: 2.72 @ index: 3\n",
            "max rebound val: -11.56\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "71\n",
            "prev_vals: [1.66, 1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45]\n",
            "next_vals: [1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26]\n",
            "Next percent differences: [-0.69, -0.69, 4.14, 3.45, 0.0, -2.76, -4.14, -7.59, -10.34, -13.1]\n",
            "min val: -13.1 @ index: 9\n",
            "max val: 4.14 @ index: 2\n",
            "max rebound val: -13.1\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "72\n",
            "prev_vals: [1.63, 1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44]\n",
            "next_vals: [1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38]\n",
            "Next percent differences: [0.0, 4.86, 4.17, 0.69, -2.08, -3.47, -6.94, -9.72, -12.5, -4.17]\n",
            "min val: -12.5 @ index: 8\n",
            "max val: 4.86 @ index: 1\n",
            "max rebound val: -4.17\n",
            "rebound_diff: 8.33\n",
            "---------\n",
            "73\n",
            "prev_vals: [1.68, 1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44]\n",
            "next_vals: [1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46]\n",
            "Next percent differences: [4.86, 4.17, 0.69, -2.08, -3.47, -6.94, -9.72, -12.5, -4.17, 1.39]\n",
            "min val: -12.5 @ index: 7\n",
            "max val: 4.86 @ index: 0\n",
            "max rebound val: 1.39\n",
            "rebound_diff: 13.89\n",
            "---------\n",
            "74\n",
            "prev_vals: [1.66, 1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51]\n",
            "next_vals: [1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53]\n",
            "Next percent differences: [-0.66, -3.97, -6.62, -7.95, -11.26, -13.91, -16.56, -8.61, -3.31, 1.32]\n",
            "min val: -16.56 @ index: 6\n",
            "max val: 1.32 @ index: 9\n",
            "max rebound val: 1.32\n",
            "rebound_diff: 17.88\n",
            "---------\n",
            "75\n",
            "prev_vals: [1.64, 1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5]\n",
            "next_vals: [1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52]\n",
            "Next percent differences: [-3.33, -6.0, -7.33, -10.67, -13.33, -16.0, -8.0, -2.67, 2.0, 1.33]\n",
            "min val: -16.0 @ index: 5\n",
            "max val: 2.0 @ index: 8\n",
            "max rebound val: 2.0\n",
            "rebound_diff: 18.0\n",
            "---------\n",
            "76\n",
            "prev_vals: [1.62, 1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45]\n",
            "next_vals: [1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51]\n",
            "Next percent differences: [-2.76, -4.14, -7.59, -10.34, -13.1, -4.83, 0.69, 5.52, 4.83, 4.14]\n",
            "min val: -13.1 @ index: 4\n",
            "max val: 5.52 @ index: 7\n",
            "max rebound val: 5.52\n",
            "rebound_diff: 18.62\n",
            "---------\n",
            "77\n",
            "prev_vals: [1.59, 1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41]\n",
            "next_vals: [1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51]\n",
            "Next percent differences: [-1.42, -4.96, -7.8, -10.64, -2.13, 3.55, 8.51, 7.8, 7.09, 7.09]\n",
            "min val: -10.64 @ index: 3\n",
            "max val: 8.51 @ index: 6\n",
            "max rebound val: 8.51\n",
            "rebound_diff: 19.15\n",
            "---------\n",
            "78\n",
            "prev_vals: [1.6, 1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39]\n",
            "next_vals: [1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52]\n",
            "Next percent differences: [-3.6, -6.47, -9.35, -0.72, 5.04, 10.07, 9.35, 8.63, 8.63, 9.35]\n",
            "min val: -9.35 @ index: 2\n",
            "max val: 10.07 @ index: 5\n",
            "max rebound val: 10.07\n",
            "rebound_diff: 19.42\n",
            "---------\n",
            "79\n",
            "prev_vals: [1.55, 1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34]\n",
            "next_vals: [1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48]\n",
            "Next percent differences: [-2.99, -5.97, 2.99, 8.96, 14.18, 13.43, 12.69, 12.69, 13.43, 10.45]\n",
            "min val: -5.97 @ index: 1\n",
            "max val: 14.18 @ index: 4\n",
            "max rebound val: 14.18\n",
            "rebound_diff: 20.15\n",
            "---------\n",
            "80\n",
            "prev_vals: [1.56, 1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3]\n",
            "next_vals: [1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49]\n",
            "Next percent differences: [-3.08, 6.15, 12.31, 17.69, 16.92, 16.15, 16.15, 16.92, 13.85, 14.62]\n",
            "min val: -3.08 @ index: 0\n",
            "max val: 17.69 @ index: 3\n",
            "max rebound val: 17.69\n",
            "rebound_diff: 20.77\n",
            "---------\n",
            "81\n",
            "prev_vals: [1.58, 1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26]\n",
            "next_vals: [1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48]\n",
            "Next percent differences: [9.52, 15.87, 21.43, 20.63, 19.84, 19.84, 20.63, 17.46, 18.25, 17.46]\n",
            "min val: 9.52 @ index: 0\n",
            "max val: 21.43 @ index: 2\n",
            "max rebound val: 21.43\n",
            "rebound_diff: 11.91\n",
            "---------\n",
            "82\n",
            "prev_vals: [1.56, 1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38]\n",
            "next_vals: [1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47]\n",
            "Next percent differences: [5.8, 10.87, 10.14, 9.42, 9.42, 10.14, 7.25, 7.97, 7.25, 6.52]\n",
            "min val: 5.8 @ index: 0\n",
            "max val: 10.87 @ index: 1\n",
            "max rebound val: 10.87\n",
            "rebound_diff: 5.07\n",
            "---------\n",
            "83\n",
            "prev_vals: [1.54, 1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46]\n",
            "next_vals: [1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42]\n",
            "Next percent differences: [4.79, 4.11, 3.42, 3.42, 4.11, 1.37, 2.05, 1.37, 0.68, -2.74]\n",
            "min val: -2.74 @ index: 9\n",
            "max val: 4.79 @ index: 0\n",
            "max rebound val: -2.74\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "84\n",
            "prev_vals: [1.5, 1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53]\n",
            "next_vals: [1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46]\n",
            "Next percent differences: [-0.65, -1.31, -1.31, -0.65, -3.27, -2.61, -3.27, -3.92, -7.19, -4.58]\n",
            "min val: -7.19 @ index: 8\n",
            "max val: -0.65 @ index: 0\n",
            "max rebound val: -4.58\n",
            "rebound_diff: 2.61\n",
            "---------\n",
            "85\n",
            "prev_vals: [1.47, 1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52]\n",
            "next_vals: [1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48]\n",
            "Next percent differences: [-0.66, -0.66, 0.0, -2.63, -1.97, -2.63, -3.29, -6.58, -3.95, -2.63]\n",
            "min val: -6.58 @ index: 7\n",
            "max val: 0.0 @ index: 2\n",
            "max rebound val: -2.63\n",
            "rebound_diff: 3.95\n",
            "---------\n",
            "86\n",
            "prev_vals: [1.48, 1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51]\n",
            "next_vals: [1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53]\n",
            "Next percent differences: [0.0, 0.66, -1.99, -1.32, -1.99, -2.65, -5.96, -3.31, -1.99, 1.32]\n",
            "min val: -5.96 @ index: 6\n",
            "max val: 1.32 @ index: 9\n",
            "max rebound val: 1.32\n",
            "rebound_diff: 7.28\n",
            "---------\n",
            "87\n",
            "prev_vals: [1.52, 1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51]\n",
            "next_vals: [1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52]\n",
            "Next percent differences: [0.66, -1.99, -1.32, -1.99, -2.65, -5.96, -3.31, -1.99, 1.32, 0.66]\n",
            "min val: -5.96 @ index: 5\n",
            "max val: 1.32 @ index: 8\n",
            "max rebound val: 1.32\n",
            "rebound_diff: 7.28\n",
            "---------\n",
            "88\n",
            "prev_vals: [1.49, 1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52]\n",
            "next_vals: [1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55]\n",
            "Next percent differences: [-2.63, -1.97, -2.63, -3.29, -6.58, -3.95, -2.63, 0.66, 0.0, 1.97]\n",
            "min val: -6.58 @ index: 4\n",
            "max val: 1.97 @ index: 9\n",
            "max rebound val: 1.97\n",
            "rebound_diff: 8.55\n",
            "---------\n",
            "89\n",
            "prev_vals: [1.47, 1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48]\n",
            "next_vals: [1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59]\n",
            "Next percent differences: [0.68, 0.0, -0.68, -4.05, -1.35, 0.0, 3.38, 2.7, 4.73, 7.43]\n",
            "min val: -4.05 @ index: 3\n",
            "max val: 7.43 @ index: 9\n",
            "max rebound val: 7.43\n",
            "rebound_diff: 11.48\n",
            "---------\n",
            "90\n",
            "prev_vals: [1.45, 1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49]\n",
            "next_vals: [1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55]\n",
            "Next percent differences: [-0.67, -1.34, -4.7, -2.01, -0.67, 2.68, 2.01, 4.03, 6.71, 4.03]\n",
            "min val: -4.7 @ index: 2\n",
            "max val: 6.71 @ index: 8\n",
            "max rebound val: 6.71\n",
            "rebound_diff: 11.41\n",
            "---------\n",
            "91\n",
            "prev_vals: [1.44, 1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48]\n",
            "next_vals: [1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55]\n",
            "Next percent differences: [-0.68, -4.05, -1.35, 0.0, 3.38, 2.7, 4.73, 7.43, 4.73, 4.73]\n",
            "min val: -4.05 @ index: 1\n",
            "max val: 7.43 @ index: 7\n",
            "max rebound val: 7.43\n",
            "rebound_diff: 11.48\n",
            "---------\n",
            "92\n",
            "prev_vals: [1.44, 1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47]\n",
            "next_vals: [1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55]\n",
            "Next percent differences: [-3.4, -0.68, 0.68, 4.08, 3.4, 5.44, 8.16, 5.44, 5.44, 5.44]\n",
            "min val: -3.4 @ index: 0\n",
            "max val: 8.16 @ index: 6\n",
            "max rebound val: 8.16\n",
            "rebound_diff: 11.56\n",
            "---------\n",
            "93\n",
            "prev_vals: [1.51, 1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42]\n",
            "next_vals: [1.46, 1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54]\n",
            "Next percent differences: [2.82, 4.23, 7.75, 7.04, 9.15, 11.97, 9.15, 9.15, 9.15, 8.45]\n",
            "min val: 2.82 @ index: 0\n",
            "max val: 11.97 @ index: 5\n",
            "max rebound val: 11.97\n",
            "rebound_diff: 9.15\n",
            "---------\n",
            "94\n",
            "prev_vals: [1.5, 1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46]\n",
            "next_vals: [1.48, 1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49]\n",
            "Next percent differences: [1.37, 4.79, 4.11, 6.16, 8.9, 6.16, 6.16, 6.16, 5.48, 2.05]\n",
            "min val: 1.37 @ index: 0\n",
            "max val: 8.9 @ index: 4\n",
            "max rebound val: 8.9\n",
            "rebound_diff: 7.53\n",
            "---------\n",
            "95\n",
            "prev_vals: [1.45, 1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48]\n",
            "next_vals: [1.53, 1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47]\n",
            "Next percent differences: [3.38, 2.7, 4.73, 7.43, 4.73, 4.73, 4.73, 4.05, 0.68, -0.68]\n",
            "min val: -0.68 @ index: 9\n",
            "max val: 7.43 @ index: 3\n",
            "max rebound val: -0.68\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "96\n",
            "prev_vals: [1.41, 1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53]\n",
            "next_vals: [1.52, 1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47]\n",
            "Next percent differences: [-0.65, 1.31, 3.92, 1.31, 1.31, 1.31, 0.65, -2.61, -3.92, -3.92]\n",
            "min val: -3.92 @ index: 8\n",
            "max val: 3.92 @ index: 2\n",
            "max rebound val: -3.92\n",
            "rebound_diff: 0.0\n",
            "---------\n",
            "97\n",
            "prev_vals: [1.39, 1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52]\n",
            "next_vals: [1.55, 1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47, 1.5]\n",
            "Next percent differences: [1.97, 4.61, 1.97, 1.97, 1.97, 1.32, -1.97, -3.29, -3.29, -1.32]\n",
            "min val: -3.29 @ index: 7\n",
            "max val: 4.61 @ index: 1\n",
            "max rebound val: -1.32\n",
            "rebound_diff: 1.97\n",
            "---------\n",
            "98\n",
            "prev_vals: [1.34, 1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55]\n",
            "next_vals: [1.59, 1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47, 1.5, 1.5]\n",
            "Next percent differences: [2.58, 0.0, 0.0, 0.0, -0.65, -3.87, -5.16, -5.16, -3.23, -3.23]\n",
            "min val: -5.16 @ index: 6\n",
            "max val: 2.58 @ index: 0\n",
            "max rebound val: -3.23\n",
            "rebound_diff: 1.93\n",
            "---------\n",
            "99\n",
            "prev_vals: [1.3, 1.26, 1.38, 1.46, 1.53, 1.52, 1.51, 1.51, 1.52, 1.48, 1.49, 1.48, 1.47, 1.42, 1.46, 1.48, 1.53, 1.52, 1.55, 1.59]\n",
            "next_vals: [1.55, 1.55, 1.55, 1.54, 1.49, 1.47, 1.47, 1.5, 1.5, 1.52]\n",
            "Next percent differences: [-2.52, -2.52, -2.52, -3.14, -6.29, -7.55, -7.55, -5.66, -5.66, -4.4]\n",
            "min val: -7.55 @ index: 5\n",
            "max val: -2.52 @ index: 0\n",
            "max rebound val: -4.4\n",
            "rebound_diff: 3.15\n",
            "---------\n"
          ]
        }
      ],
      "source": [
        "train_i=130\n",
        "for train_i in range(100):\n",
        "  print(train_i)\n",
        "  prev_vals,next_vals=cur_train0[train_i]\n",
        "  prev_vals=[round(v,2) for v in prev_vals]\n",
        "  next_vals=[round(v,2) for v in next_vals]\n",
        "  prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "  min_val=min(next_percents)\n",
        "  min_val_index=next_percents.index(min_val)\n",
        "  max_val=max(next_percents)\n",
        "  max_val_index=next_percents.index(max_val)\n",
        "  rebound_max_val=max(next_percents[min_val_index:])\n",
        "  rebound_diff=round(rebound_max_val-min_val,2)\n",
        "\n",
        "  print(\"prev_vals:\", prev_vals)\n",
        "  print(\"next_vals:\", next_vals)\n",
        "  #print(prev_percents)\n",
        "  print(\"Next percent differences:\", next_percents)\n",
        "  print(\"min val:\",min_val, \"@ index:\",min_val_index)\n",
        "  print(\"max val:\",max_val, \"@ index:\",max_val_index)\n",
        "  print(\"max rebound val:\",rebound_max_val)\n",
        "  print(\"rebound_diff:\",rebound_diff)\n",
        "  print(\"---------\")\n",
        "\n",
        "#print(\"min val:\",min_next_diff_val, \"@ index:\",min_val_index, \"max val:\",max_next_diff_val, \"rebound_max_val:\",rebound_max_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wJ2YZ2nF0NI"
      },
      "source": [
        "#Main Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDoSxjfW7rq0",
        "outputId": "3fde7c72-d1a0-4ae0-948c-3a054c1ca1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0, -25.0, 25.0, -16.67, 8.33, 16.67, 0.0]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#define functions to extract features and process labels\n",
        "def get_diff_percent(val_list,ref_val0): #ref_val is the present val\n",
        "  out_vals=[]\n",
        "  for val0 in val_list:\n",
        "    try:\n",
        "      diff0=val0-ref_val0\n",
        "      precent0=100*(diff0/ref_val0)\n",
        "      out_vals.append(round(precent0,2))\n",
        "    except: pass\n",
        "  return out_vals\n",
        "\n",
        "class io_cls: #input to output: category >< onehot\n",
        "  def __init__(self,spacing=2,max_val=10): #for a general purpose, this can be where we define the labels\n",
        "    self.spacing=spacing\n",
        "    self.max_val=max_val\n",
        "    self.all_labels=[]\n",
        "    for mv_val0 in range(-max_val,max_val+1,spacing):self.all_labels.append(str(mv_val0))\n",
        "    self.n_labels=len(self.all_labels)\n",
        "  def one_hot(self,val_list,ref_val): #and this is when we convert from categorical to one hot\n",
        "    self.diff_list=[]\n",
        "    self.one_hot_list=[]\n",
        "    for val0 in val_list:\n",
        "      diff0=val0-ref_val\n",
        "      precent0=100*(diff0/ref_val)\n",
        "      precent0_norm=int(round(precent0/self.spacing)*self.spacing) #int(round(spacing*precent0)/spacing)\n",
        "      if precent0_norm<=-self.max_val: diff_str=str(-self.max_val)\n",
        "      elif precent0_norm>=self.max_val: diff_str=str(self.max_val)\n",
        "      else: diff_str=str(precent0_norm)\n",
        "      self.diff_list.append(diff_str)\n",
        "      tmp_one_hot_vals=[0.]*len(self.all_labels)\n",
        "      if diff_str in self.all_labels: \n",
        "        tmp_i=self.all_labels.index(diff_str)\n",
        "        tmp_one_hot_vals[tmp_i]=1.\n",
        "      self.one_hot_list.append(tmp_one_hot_vals)\n",
        "    return self.one_hot_list\n",
        "  def out2labels(self,rnn_flat_out): #a flat rnn output to split into slices, and get the label weights for each slice - and then from one hot to categorical\n",
        "    final_list=[]\n",
        "    n_slices=int(len(rnn_flat_out)/len(self.all_labels))\n",
        "    for i0 in range(n_slices):\n",
        "      i1=i0+1\n",
        "      cur_slice=rnn_flat_out[i0*len(self.all_labels):i1*len(self.all_labels)]\n",
        "      tmp_list=[]\n",
        "      for lb0,cs0 in zip(self.all_labels,cur_slice): tmp_list.append((lb0,cs0))\n",
        "      tmp_list.sort(key=lambda x:-x[-1])\n",
        "      final_list.append(tmp_list)\n",
        "    return final_list\n",
        "\n",
        "#Getting the input\n",
        "def get_norm_close(fpath,prev_n0=20,next_n0=10,train_ratio=0.75):\n",
        "  pd_df=pd.read_csv(fpath)\n",
        "  close_col=pd_df[\"Close\"].fillna(0)\n",
        "  #close_col=pd_df.dropna(subset=['Close'], how='all', inplace=True)\n",
        "  #close_col = pd_df[pd_df['Close'].notna()]\n",
        "  data_len=len(close_col)\n",
        "  all_data=[]\n",
        "  for test_i in range(prev_n0,len(close_col)-next_n0):\n",
        "    prev_items=close_col[test_i-prev_n0:test_i].to_list() #[0,1,2,3,4,5,6,7,8,9] predict the closing today and the following next_n-1 days\n",
        "    next_items=close_col[test_i:test_i+next_n0].to_list()\n",
        "    all_data.append((prev_items,next_items))    \n",
        "  train_size=int(train_ratio*data_len)\n",
        "  train_data=all_data[:train_size]\n",
        "  test_data=all_data[train_size:]\n",
        "  return train_data,test_data\n",
        "\n",
        "def get_prev_next_vals(list_vals,prev_n0=20,next_n0=10):\n",
        "  all_data=[]\n",
        "  for test_i in range(prev_n0,len(list_vals)-next_n0):\n",
        "    prev_vals=list_vals[test_i-prev_n0:test_i] #[0,1,2,3,4,5,6,7,8,9] predict the closing today and the following next_n-1 days\n",
        "    next_vals=list_vals[test_i:test_i+next_n0]\n",
        "    all_data.append((prev_vals,next_vals))  \n",
        "    #prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])  \n",
        "  return all_data\n",
        "\n",
        "def get_prev_next_percent(list_vals0,prev_n0=20,next_n0=10):\n",
        "  new_data=[]\n",
        "  tmp_data=get_prev_next_vals(list_vals0,prev_n0,next_n0)\n",
        "  for prev0,next0 in tmp_data:\n",
        "    prev_percents,next_percents=get_diff_percent(prev0,prev0[-1]),get_diff_percent(next0,prev0[-1]) \n",
        "    new_data.append((prev_percents,next_percents)) \n",
        "  return new_data\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_labels(next_percents0):\n",
        "  if next_percents0==[]: return [0.,0.]\n",
        "  min_val0=min(next_percents0)\n",
        "  #print(\"min_val0\",min_val0)\n",
        "  min_val_index=next_percents0.index(min_val0)\n",
        "  max_val=max(next_percents0[1:])\n",
        "  max_val_index=next_percents0.index(max_val)\n",
        "  rebound_max_val=max(next_percents0[min_val_index:])\n",
        "  rebound_diff=round(rebound_max_val-min_val0,2)\n",
        "  max_greater_than_5=0.\n",
        "  found_minus_5=False\n",
        "  found_rebound_greater_than_5=False\n",
        "  cur_min_val=None\n",
        "  for i0, percent_val in enumerate(next_percents0):\n",
        "    if percent_val<-5 and found_minus_5==False: \n",
        "      found_minus_5=True\n",
        "      if cur_min_val==None or percent_val<cur_min_val: cur_min_val=percent_val\n",
        "      #print(\"percent_val\",percent_val,\"found_minus_5\",found_minus_5)\n",
        "      continue\n",
        "    if cur_min_val!=None and percent_val-cur_min_val>5: \n",
        "      found_rebound_greater_than_5=True\n",
        "      break\n",
        "  min_5_rebound_greater_than_5=0.\n",
        "  if max_val>5: max_greater_than_5=1.\n",
        "  if found_rebound_greater_than_5: min_5_rebound_greater_than_5=1.\n",
        "  actual_out0=[max_greater_than_5,min_5_rebound_greater_than_5]\n",
        "  return actual_out0\n",
        "\n",
        "cur_test_list=[12,9,15,10,13,14,12]\n",
        "cur_ref_val=12\n",
        "test_out=get_diff_percent(cur_test_list,cur_ref_val)\n",
        "print(test_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP349k1sESqr"
      },
      "source": [
        "#Starting Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "2Z0dOUrLJAAs",
        "outputId": "fba95c4c-a89a-4574-9b02-0c4ee617222e"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3cac8bfc963f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtest_cutoff_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "import time, math, random\n",
        "from random import shuffle\n",
        "\n",
        "model_name=\"exp6-pred1-combined-stocks\"\n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-full\"\n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-4layer-batches2\"\n",
        "n_input=1\n",
        "n_output=1\n",
        "n_output=2 #>5, min <5 & rebound >5\n",
        "n_hidden =64#64\n",
        "n_layers=4#3\n",
        "n_epochs=100\n",
        "LR=0.0000001\n",
        "prev_n,next_n=20,10\n",
        "n_train,n_test=None,None\n",
        "#n_train,n_test=1000,50\n",
        "train_batch_size=10000\n",
        "\n",
        "test_cutoff_val=0.5\n",
        "\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "initial_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "#cur_path=os.path.join(root_dir,\"AAPL.csv\")\n",
        "all_files=[v.split(\".\")[0] for v in os.listdir(root_dir) if v.endswith(\".csv\")]\n",
        "additional_files=[v for v in all_files if not v in initial_files]\n",
        "sample_files=initial_files+additional_files[:90]\n",
        "\n",
        "#cur_train0,cur_test0=get_norm_close(\"stock_market_data/sp500/csv/AAPL.csv\",prev_n,next_n) #stock_market_data/sp500/csv/AAPL.csv\n",
        "\n",
        "\n",
        "\n",
        "#rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "rnn = RNN(n_input, n_hidden, n_output,n_layers,matching_in_out=False).to(device)\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
        "\n",
        "\n",
        "model_dir=os.path.join(cwd,\"models\", model_name) \n",
        "tmp_model_dir=os.path.join(cwd,\"models\", model_name,\"tmp\") \n",
        "if not os.path.exists(tmp_model_dir): os.makedirs(tmp_model_dir)\n",
        "log_fpath=os.path.join(model_dir,\"log.txt\")\n",
        "log_fopen=open(log_fpath,\"a\")\n",
        "log_fopen.write(str(rnn)+\"\\n\")\n",
        "\n",
        "print(\"loading data\")\n",
        "all_training,all_testing=[],[]\n",
        "for fname in sample_files:\n",
        "  cur_path=os.path.join(root_dir,fname+\".csv\")\n",
        "  cur_train0,cur_test0=get_norm_close(cur_path,prev_n,next_n,train_ratio=0.8)\n",
        "  if n_train!=None: cur_train0=cur_train0[:n_train]\n",
        "  if n_test!=None: cur_test0=cur_test0[:n_test]\n",
        "  all_training.extend(cur_train0)\n",
        "  all_testing.extend(cur_test0)\n",
        "shuffle(all_training)\n",
        "shuffle(all_testing)\n",
        "print(\"all_training\", len(all_training),\"all_testing\",len(all_testing))\n",
        "n_batches=math.floor(len(all_training)/train_batch_size)\n",
        "test_batch_size=math.floor(len(all_testing)/n_batches)\n",
        "\n",
        "\n",
        "for epoch0 in range(n_epochs):\n",
        "  PATH=os.path.join(model_dir, \"model-%s.model\"%epoch0)\n",
        "  if os.path.exists(PATH):\n",
        "    checkpoint = torch.load(PATH)\n",
        "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print(\"loaded model for this epoch\",PATH)\n",
        "    for a,b in  checkpoint.items():\n",
        "      if \"loss\" in a.lower(): print(a,round(b,6))\n",
        "    continue  \n",
        "  print(\"epoch0\",epoch0)\n",
        "  for batch_i0 in range(n_batches+1):\n",
        "    t0=time.time()\n",
        "    pred_count,correct_count=0,0\n",
        "    batch_i1=batch_i0+1\n",
        "    cur_train_items=all_training[batch_i0*train_batch_size:batch_i1*train_batch_size]\n",
        "    cur_test_items=all_testing[batch_i0*test_batch_size:batch_i1*test_batch_size]\n",
        "    print(\"batch_i0\",batch_i0, \"cur_train_items\",len(cur_train_items),\"cur_test_items\",len(cur_test_items))\n",
        "    tmp_path=os.path.join(tmp_model_dir, \"model-batch-%s.model\"%batch_i0)\n",
        "    if os.path.exists(tmp_path):\n",
        "      checkpoint = torch.load(tmp_path)\n",
        "      rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      print(\"loaded model for this epoch\",tmp_path)\n",
        "      continue  \n",
        "\n",
        "    total_train_loss,total_test_loss=0,0\n",
        "    train_counter,test_counter=0,0\n",
        "    test_pred_counter,test_correct_counter=0,0 #how many test items reach the cutoff val for prediction, how many are correctly predicted\n",
        "    #for train_i in range(1500):\n",
        "    for train_i, train_item in enumerate(cur_train_items):\n",
        "      if train_i%2000==0: print(\"train_i\",train_i)\n",
        "      #print(train_i)\n",
        "      prev_vals,next_vals=train_item# cur_train0[train_i]\n",
        "      prev_vals=[round(v,2) for v in prev_vals]\n",
        "      next_vals=[round(v,2) for v in next_vals]\n",
        "      prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "      if prev_percents==[] or next_percents==[]: continue\n",
        "      actual_out=extract_labels(next_percents)\n",
        "      # print(\"cur_actual\", cur_actual, next_percents)\n",
        "      # print(\"--------\")\n",
        "      # continue\n",
        "      # min_val=min(next_percents)\n",
        "      # min_val_index=next_percents.index(min_val)\n",
        "      # max_val=max(next_percents)\n",
        "      # max_val_index=next_percents.index(max_val)\n",
        "      # rebound_max_val=max(next_percents[min_val_index:])\n",
        "      # rebound_diff=round(rebound_max_val-min_val,2)\n",
        "      \n",
        "      # max_gr_5=0.\n",
        "      # if max_val>5: max_gr_5=1.\n",
        "      # actual_out=[max_gr_5]\n",
        "      #prev_percents=[math.log(v) for v in prev_percents] #testing\n",
        "      #if prev_percents==[]: continue\n",
        "      input_tensor=torch.tensor(prev_percents)\n",
        "      actual_out_tensor=torch.tensor(actual_out).to(device)\n",
        "      rnn_output = rnn(input_tensor).to(device)\n",
        "      rnn_output_list=rnn_output.tolist()\n",
        "      loss = loss_func(actual_out_tensor.ravel(), rnn_output.ravel()) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      # if epoch0>3 and train_i<50:\n",
        "      #   print(\"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "      #print(loss)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_train_loss+=loss.item()\n",
        "      train_counter+=1\n",
        "\n",
        "    for test_i, test_item in enumerate(cur_test_items):\n",
        "      #print(train_i)\n",
        "      if test_i%1000==0: print(\"test_i\",test_i)\n",
        "      rnn.zero_grad()\n",
        "      prev_vals,next_vals=test_item# cur_train0[train_i]\n",
        "      prev_vals=[round(v,2) for v in prev_vals]\n",
        "      next_vals=[round(v,2) for v in next_vals]\n",
        "      prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "      if prev_percents==[] or next_percents==[]: continue\n",
        "      actual_out=extract_labels(next_percents)\n",
        "\n",
        "      # continue\n",
        "      # min_val=min(next_percents)\n",
        "      # min_val_index=next_percents.index(min_val)\n",
        "      # max_val=max(next_percents)\n",
        "      # max_val_index=next_percents.index(max_val)\n",
        "      # rebound_max_val=max(next_percents[min_val_index:])\n",
        "      # rebound_diff=round(rebound_max_val-min_val,2)\n",
        "      \n",
        "      # max_gr_5=0.\n",
        "      # if max_val>5: max_gr_5=1.\n",
        "      # actual_out=[max_gr_5]\n",
        "      input_tensor=torch.tensor(prev_percents)\n",
        "      actual_out_tensor=torch.tensor(actual_out).to(device)\n",
        "      rnn_output = rnn(input_tensor).to(device)\n",
        "      rnn_output_list=rnn_output.ravel().tolist()\n",
        "      loss = loss_func(actual_out_tensor.ravel(), rnn_output.ravel()) #calculate the loss, difference between the output and the desired outcome tensors\n",
        "      predicted_increase,predicted_rebound=rnn_output_list\n",
        "      actual_increase,actual_rebound=actual_out\n",
        "      if predicted_increase>0.6 or predicted_rebound>0.4:\n",
        "        pred_count+=1\n",
        "        if actual_increase>0.5 or actual_rebound>0.5: correct_count+=1\n",
        "        tmp_rnn_output_list=[round(v,2) for v in rnn_output_list]\n",
        "        print(test_i, \"rnn out:\",tmp_rnn_output_list, \"actual:\", actual_out, \"loss:\", round(loss.item(),6))\n",
        "\n",
        "\n",
        "      # if test_i<50:\n",
        "      #   print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "      #print(loss)\n",
        "      pred_val=sum(rnn_output_list)/len(rnn_output_list)\n",
        "      if pred_val>=test_cutoff_val and False:\n",
        "        test_pred_counter+=1\n",
        "        #if pred_val>0.5\n",
        "        print(\"pred:\", round(pred_val,2), \"actual:\", sum(actual_out))\n",
        "        print(\"prev_percents\",prev_percents)\n",
        "        print(\"next_percents\",next_percents)\n",
        "        print(\"-------------\")\n",
        "      total_test_loss+=loss.item()\n",
        "      test_counter+=1\n",
        "\n",
        "\n",
        "    avg_train_loss=round(total_train_loss/train_counter,6)\n",
        "    avg_test_loss=round(total_test_loss/test_counter,6)\n",
        "    correct_ratio=0\n",
        "    if pred_count>0: correct_ratio=round(correct_count/pred_count,2)\n",
        "    print(\"pred_count\",pred_count,\"correct_count\",correct_count,\"correct_ratio\",correct_ratio)\n",
        "    # print(\"epoch0\",epoch0, fname, \"avg_train_loss\",avg_train_loss, \"avg_test_loss\",avg_test_loss)\n",
        "    # print(\"-------\") pred_count, correct_count, correct_ratio\n",
        "    \n",
        "    t1=time.time()\n",
        "    elapsed=round(t1-t0,2) \n",
        "    t0=time.time()    \n",
        "    line=\"Epoch # %s - Batch: %s -  train loss: %s - test loss: %s - Correctness :%s/%s (ratio: %s) - elapsed: %s\"%(epoch0, batch_i0, avg_train_loss,avg_test_loss,  correct_count, pred_count, correct_ratio, elapsed)\n",
        "    #line=\"Epoch # %s  -  train loss: %s - test loss: %s - elapsed: %s\"%(epoch0, avg_train_loss,avg_test_loss, elapsed)\n",
        "    print(line)\n",
        "    log_fopen=open(log_fpath,\"a\")\n",
        "    log_fopen.write(line+\"\\n\")\n",
        "    log_fopen.close() \n",
        "    cur_checkpoint={\n",
        "            'epoch': epoch0,\n",
        "            'n_input': n_input,\n",
        "            'n_hidden': n_hidden,\n",
        "            'n_layers': n_layers,\n",
        "            'n_output': n_output,\n",
        "            'LR': LR,\n",
        "            'model_state_dict': rnn.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'test_loss': avg_test_loss\n",
        "            }\n",
        "    torch.save(cur_checkpoint, tmp_path)\n",
        "  \n",
        "  torch.save(cur_checkpoint, PATH)  \n",
        "  print(\"model saved\")\n",
        "  for f in os.listdir(tmp_model_dir):\n",
        "    tmp_fpath=os.path.join(tmp_model_dir,f)\n",
        "    os.remove(tmp_fpath)\n",
        "  print(\"deleted temporary files\")\n",
        "  print(\"-----------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1byMZUiRCP"
      },
      "source": [
        "#Testing on actual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoFyl9DsfLl_",
        "outputId": "7f4a47e9-550b-4043-bb00-4a8f2a57d4ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XOM\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 rnn out: [0.6026967167854309, 0.21063825488090515] actual: [0.0, 0.0]\n",
            "2771 rnn out: [0.6120272278785706, 0.22972239553928375] actual: [1.0, 0.0]\n",
            "2777 rnn out: [0.6584183573722839, 0.2904338240623474] actual: [0.0, 0.0]\n",
            "2778 rnn out: [0.7015856504440308, 0.3120870292186737] actual: [0.0, 0.0]\n",
            "2779 rnn out: [0.6679953932762146, 0.3043203353881836] actual: [0.0, 0.0]\n",
            "2780 rnn out: [0.6524690389633179, 0.28235986828804016] actual: [0.0, 0.0]\n",
            "2781 rnn out: [0.7352421879768372, 0.3719955384731293] actual: [0.0, 1.0]\n",
            "2782 rnn out: [0.7012118697166443, 0.3452860116958618] actual: [0.0, 1.0]\n",
            "2783 rnn out: [0.7353832721710205, 0.37432312965393066] actual: [1.0, 1.0]\n",
            "2784 rnn out: [0.6801297664642334, 0.3489380180835724] actual: [1.0, 1.0]\n",
            "2785 rnn out: [0.7323333621025085, 0.37931200861930847] actual: [1.0, 1.0]\n",
            "2786 rnn out: [0.6868031620979309, 0.35476288199424744] actual: [1.0, 1.0]\n",
            "2787 rnn out: [0.7067278623580933, 0.3570493459701538] actual: [1.0, 0.0]\n",
            "2788 rnn out: [0.7088838219642639, 0.3492518961429596] actual: [1.0, 0.0]\n",
            "2789 rnn out: [0.628419816493988, 0.4476689100265503] actual: [1.0, 0.0]\n",
            "2790 rnn out: [0.5664526224136353, 0.4559811055660248] actual: [1.0, 0.0]\n",
            "2791 rnn out: [0.5304440259933472, 0.40348759293556213] actual: [1.0, 0.0]\n",
            "2792 rnn out: [0.577810525894165, 0.4078640341758728] actual: [1.0, 0.0]\n",
            "2793 rnn out: [0.5520337820053101, 0.369689017534256] actual: [1.0, 0.0]\n",
            "2805 rnn out: [0.6099442839622498, 0.21222428977489471] actual: [1.0, 0.0]\n",
            "2824 rnn out: [0.6185969710350037, 0.21900220215320587] actual: [1.0, 0.0]\n",
            ">>>> XOM pred_count 21 correct_count 16 correct_ratio 0.76\n",
            "=========\n",
            "XEL\n",
            "2583 rnn out: [0.6462969779968262, 0.24353580176830292] actual: [1.0, 0.0]\n",
            "2585 rnn out: [0.6709126830101013, 0.294965922832489] actual: [1.0, 1.0]\n",
            "2587 rnn out: [0.6692085266113281, 0.3784424364566803] actual: [0.0, 1.0]\n",
            "2588 rnn out: [0.6796320676803589, 0.3370046019554138] actual: [1.0, 1.0]\n",
            "2589 rnn out: [0.6670498847961426, 0.28091710805892944] actual: [1.0, 0.0]\n",
            "2590 rnn out: [0.6627568602561951, 0.2834048569202423] actual: [1.0, 0.0]\n",
            "2591 rnn out: [0.6191782355308533, 0.31803494691848755] actual: [1.0, 0.0]\n",
            "2593 rnn out: [0.568427562713623, 0.38753631711006165] actual: [1.0, 0.0]\n",
            "2594 rnn out: [0.5419813394546509, 0.37172695994377136] actual: [1.0, 0.0]\n",
            "2595 rnn out: [0.48406264185905457, 0.439274400472641] actual: [1.0, 1.0]\n",
            "2596 rnn out: [0.5420311093330383, 0.3637309670448303] actual: [1.0, 0.0]\n",
            "2597 rnn out: [0.6036814451217651, 0.3089621365070343] actual: [1.0, 0.0]\n",
            "2599 rnn out: [0.609135091304779, 0.28424423933029175] actual: [1.0, 0.0]\n",
            "2600 rnn out: [0.5153542757034302, 0.3742338716983795] actual: [1.0, 0.0]\n",
            "2602 rnn out: [0.4534062147140503, 0.41918954253196716] actual: [1.0, 0.0]\n",
            ">>>> XEL pred_count 15 correct_count 15 correct_ratio 1.0\n",
            "=========\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "e0=8\n",
        "model_name=\"exp5-pred1-3-test\"\n",
        "model_name=\"exp6-pred1-combined-stocks\"\n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-full\" \n",
        "model_name=\"exp6-pred1-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-3layer-batches1\"\n",
        "model_name=\"exp6-pred2-combined-stocks1-4layer-batches2\"\n",
        "\n",
        "pred_cutoff_val=0.6\n",
        "torch.manual_seed(1)\n",
        "random.seed(1)\n",
        "\n",
        "# e0=8\n",
        "# model_name=\"exp6-pred2-combined-stocks1-4layer-batches2\"\n",
        "# model_dir=os.path.join(cwd,\"models\", model_name) \n",
        "# PATH=os.path.join(model_dir, \"model-%s.model\"%e0)\n",
        "def load_model(model_fpath0):\n",
        "  checkpoint = torch.load(model_fpath0)\n",
        "  rnn0 = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , matching_in_out=False).to(device)\n",
        "  rnn0.load_state_dict(checkpoint['model_state_dict'])\n",
        "  rnn0.eval()\n",
        "  return rnn0\n",
        "\n",
        "def predict(input_list,rnn_obj):\n",
        "  input_tensor=torch.tensor(input_list)\n",
        "  rnn_output = rnn_obj(input_tensor)\n",
        "  rnn_output_list=rnn_output.ravel().tolist()\n",
        "  return rnn_output_list\n",
        "\n",
        "model_dir=os.path.join(cwd,\"models\", model_name) \n",
        "PATH=os.path.join(model_dir, \"model-%s.model\"%e0)\n",
        "checkpoint = torch.load(PATH)\n",
        "rnn = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , matching_in_out=False).to(device)\n",
        "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn.eval()\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "sample_test_files=['XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "#sample_test_files=[\"AAPL\",\"GOOG\",\"FB\",\"AMZN\",\"EA\",\"IBM\",\"MSFT\",\"GM\",\"UPS\",\"PG\"]\n",
        "for fname in sample_test_files[:2]:\n",
        "  print(fname)\n",
        "  pred_count,correct_count=0,0\n",
        "  cur_fpath=os.path.join(root_dir,fname+\".csv\")\n",
        "  #cur_train0,cur_test0=get_norm_close(\"stock_market_data/sp500/csv/AAPL.csv\") #stock_market_data/sp500/csv/AAPL.csv\n",
        "  cur_train0,cur_test0=get_norm_close(cur_fpath) #stock_market_data/sp500/csv/AAPL.csv\n",
        "  for test_i, test_item in enumerate(cur_test0):\n",
        "    #print(train_i)\n",
        "    rnn.zero_grad()\n",
        "    prev_vals,next_vals=test_item# cur_train0[train_i]\n",
        "    prev_vals=[round(v,2) for v in prev_vals]\n",
        "    next_vals=[round(v,2) for v in next_vals]\n",
        "    prev_percents,next_percents=get_diff_percent(prev_vals,prev_vals[-1]),get_diff_percent(next_vals,prev_vals[-1])\n",
        "    actual_out=extract_labels(next_percents)\n",
        "    # min_val=min(next_percents)\n",
        "    # min_val_index=next_percents.index(min_val)\n",
        "    # max_val=max(next_percents)\n",
        "    # max_val_index=next_percents.index(max_val)\n",
        "    # rebound_max_val=max(next_percents[min_val_index:])\n",
        "    # rebound_diff=round(rebound_max_val-min_val,2)\n",
        "    \n",
        "    # max_gr_5=0.\n",
        "    # if max_val>5: max_gr_5=1.\n",
        "    # actual_out=[max_gr_5]\n",
        "    \n",
        "    input_tensor=torch.tensor(prev_percents)\n",
        "    actual_out_tensor=torch.tensor(actual_out)\n",
        "    rnn_output = rnn(input_tensor)\n",
        "    rnn_output_list=rnn_output.ravel().tolist()\n",
        "    predicted_increase,predicted_rebound=rnn_output_list\n",
        "    actual_increase,actual_rebound=actual_out\n",
        "    # predicted_val=sum(rnn_output_list)\n",
        "    # actual_val=sum(actual_out)\n",
        "    # if test_i<50:\n",
        "    #   print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "    #   #print(loss)\n",
        "    #   print(\"--------\")\n",
        "    # continue\n",
        "\n",
        "    if predicted_increase>pred_cutoff_val or predicted_rebound>0.35:\n",
        "      pred_count+=1\n",
        "      if actual_increase>0.5 or actual_rebound>0.5: correct_count+=1\n",
        "      #print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out, \"loss:\", loss.item())\n",
        "      print(test_i, \"rnn out:\",rnn_output_list, \"actual:\", actual_out)\n",
        "      #if actual_increase>0.5: correct_count+=1\n",
        "      # print(fname, \"predicted_val:\",round(predicted_val,2), \"actual_out:\",round(actual_val,2))\n",
        "      # print(\"next_percents\",next_percents)\n",
        "      # #print(rnn_output_list)\n",
        "      # #print(sum(rnn_output_list))\n",
        "      # print(\"------\")\n",
        "  correct_ratio=0    \n",
        "  if pred_count>0: correct_ratio=round(correct_count/pred_count,2)\n",
        "  print(\">>>>\", fname, \"pred_count\",pred_count,\"correct_count\",correct_count,\"correct_ratio\",correct_ratio)\n",
        "  print(\"=========\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viF5xes1g1K2",
        "outputId": "b9c79c52-1800-423f-f301-8359896980cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_batch_size 3000 test_batch_size 736 n_batches 101\n"
          ]
        }
      ],
      "source": [
        "train_batch_size=3000\n",
        "n_batches=math.floor(len(all_training)/train_batch_size)\n",
        "test_batch_size=math.floor(len(all_testing)/n_batches)\n",
        "print(\"train_batch_size\",train_batch_size, \"test_batch_size\",test_batch_size, \"n_batches\",n_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GStrVn9_4oPE"
      },
      "outputs": [],
      "source": [
        "all_files=[v.split(\".\")[0] for v in os.listdir(root_dir) if v.endswith(\".csv\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpOpfx4z4pz-",
        "outputId": "255360fc-6039-400d-f113-000fd37d6552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_files[-10:]\n",
        "#print(test_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOfB_GZFv8uw"
      },
      "source": [
        "#Robinhood - Sep 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ohZUrMbv_iG",
        "outputId": "63ca30c1-c7f7-4a5e-f6e1-9b7a722db51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: robin_stocks in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from robin_stocks) (0.21.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from robin_stocks) (37.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from robin_stocks) (2.23.0)\n",
            "Requirement already satisfied: pyotp in /usr/local/lib/python3.7/dist-packages (from robin_stocks) (2.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->robin_stocks) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->robin_stocks) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->robin_stocks) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->robin_stocks) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->robin_stocks) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->robin_stocks) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install robin_stocks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Robinhood login - Sep 22"
      ],
      "metadata": {
        "id": "R9v1SpiVSvDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhP3dFfywNa_",
        "outputId": "626647b1-f22a-49a3-9292-2c0de659fe03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robinhood username: hmghaly@gmail.com\n",
            "Robinhood password: ··········\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'access_token': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJleHAiOjE2NjMxMDk4NjMsInRva2VuIjoidndicDhtMjdidldqTkZUOWxacWVQNjd0ZkRpUkpMIiwidXNlcl9pZCI6IjY1NDcxZTU3LWE3NmYtNGU3Yy1hMWM3LWY5Nzc2NzMyNjkwNCIsImRldmljZV9oYXNoIjoiMzYzN2I5NDA0NjI4MmViNTU0ZTgwMGI3MzJlMWRiNTAiLCJzY29wZSI6ImludGVybmFsIiwiZGN0IjoxNjQ3OTU4NDEzLCJzZXJ2aWNlX3JlY29yZHMiOlt7ImhhbHRlZCI6ZmFsc2UsInNlcnZpY2UiOiJudW1tdXNfdXMiLCJzaGFyZF9pZCI6MSwic3RhdGUiOiJhdmFpbGFibGUifSx7ImhhbHRlZCI6ZmFsc2UsInNlcnZpY2UiOiJicm9rZWJhY2tfdXMiLCJzaGFyZF9pZCI6OSwic3RhdGUiOiJhdmFpbGFibGUifV0sInVzZXJfb3JpZ2luIjoiVVMiLCJvcHRpb25zIjpmYWxzZSwibGV2ZWwyX2FjY2VzcyI6ZmFsc2V9.GGB3RshsZl0F_PYRihnP8ViwewZlps2FfsMVZCWzAJOlkPcCG7U7PBnBBYXxahIhp7QwAF2aYHiUBFlS5mTf-2iBl_yfinQ_cIKkCqlkb_qk15a0vpNAARjsu1wkUOy9dahP6UpK7d5t8aG0tyVY1x2Po0xjL4HqSP5ZfRMjIvaDKgHdiVuPhR0bYjfbx9YGvBLRa88tV8L347qn_kwhZeo9xAJQaLyFse4ASf0Rmkg8aWz-FmAHbvlKAPqgdFCss3NR7rWmoFDlVqr2qosZw71V7W_1a7i1C1Aoac8wtj5QWf7n7G-a2jV7om2ePpZoFWNzI63d_anyn5FTI5w4Nw',\n",
              " 'expires_in': 635718,\n",
              " 'token_type': 'Bearer',\n",
              " 'scope': 'internal',\n",
              " 'refresh_token': 'bqK2fgBkvEpDs3fwuOb6fr2Fxwk5OW',\n",
              " 'mfa_code': None,\n",
              " 'backup_code': None,\n",
              " 'detail': 'logged in with brand new authentication code.'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#we should set the environ variables first\n",
        "!export robinhood_username=\"\"\n",
        "!export robinhood_password=\"\"\n",
        "\n",
        "import robin_stocks as rs\n",
        "import os \n",
        "\n",
        "robin_user = os.environ.get(\"robinhood_username\") #email!\n",
        "robin_pass = os.environ.get(\"robinhood_password\") #ss!\n",
        "#rs.robinhood.l\n",
        "rs.robinhood.login(username=robin_user,\n",
        "         password=robin_pass,\n",
        "         expiresIn=86400,\n",
        "         by_sms=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3p4_SQLd61T7"
      },
      "source": [
        "#Collecting Stock info - Aug 22\n",
        "using our initial lists and robinhood queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB3NOGA0yfDj",
        "outputId": "fc71205b-9b50-4204-ae43-20940fe91cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ELS\n",
            "Found 1 results\n",
            "ELTK\n",
            "Found 1 results\n",
            "ELUXY\n",
            "No results found for that keyword\n",
            "ELVAF\n",
            "Found 3 results\n",
            "ELY\n",
            "Found 1 results\n",
            "EMCF\n",
            "Found 9 results\n",
            "EME\n",
            "Found 3 results\n",
            "EMF\n",
            "No results found for that keyword\n",
            "EMITF\n",
            "Found 1 results\n",
            "EMKR\n",
            "Found 10 results\n",
            "EML\n",
            "No results found for that keyword\n",
            "EMMS\n",
            "Found 2 results\n",
            "EMN\n",
            "Found 1 results\n",
            "EMO\n",
            "Found 2 results\n",
            "EMR\n",
            "No results found for that keyword\n",
            "EMSHF\n",
            "Found 2 results\n",
            "ENB\n",
            "No results found for that keyword\n",
            "ENBP\n",
            "Found 1 results\n",
            "ENDP\n",
            "No results found for that keyword\n",
            "ENGIY\n",
            "No results found for that keyword\n",
            "ENLAY\n",
            "Found 4 results\n",
            "ENPH\n",
            "Found 7 results\n",
            "ENS\n",
            "Found 1 results\n",
            "ENSG\n",
            "Found 2 results\n",
            "ENTA\n",
            "Found 1 results\n",
            "ENTG\n",
            "Found 3 results\n",
            "ENTR\n",
            "Found 1 results\n",
            "ENVA\n",
            "Found 3 results\n",
            "ENVI\n",
            "No results found for that keyword\n",
            "ENZN\n",
            "Found 1 results\n",
            "EOD\n",
            "Found 1 results\n",
            "EOG\n",
            "Found 1 results\n",
            "EONGY\n",
            "Found 3 results\n",
            "EOS\n",
            "Found 1 results\n",
            "EPAM\n",
            "No results found for that keyword\n",
            "EPAY\n",
            "Found 1 results\n",
            "EPD\n",
            "Found 6 results\n",
            "EPR\n",
            "No results found for that keyword\n",
            "EPR-PC\n",
            "No results found for that keyword\n",
            "EPR-PE\n",
            "No results found for that keyword\n",
            "EPZM\n",
            "Found 1 results\n",
            "EQIX\n",
            "Found 4 results\n",
            "EQR\n",
            "Found 1 results\n",
            "EQS\n",
            "Found 1 results\n",
            "ERIE\n",
            "Found 1 results\n",
            "ERII\n",
            "Found 10 results\n",
            "ES\n",
            "No results found for that keyword\n",
            "ESBK\n",
            "Found 1 results\n",
            "ESCA\n",
            "Found 1 results\n",
            "ESCR\n",
            "Found 1 results\n",
            "ESEA\n",
            "Found 3 results\n",
            "ESGR\n",
            "Found 2 results\n",
            "ESI\n",
            "No results found for that keyword\n",
            "ESIO\n",
            "Found 1 results\n",
            "ESLT\n",
            "No results found for that keyword\n",
            "ESMC\n",
            "Found 1 results\n",
            "ESNT\n",
            "Found 2 results\n",
            "ESPR\n",
            "Found 1 results\n",
            "ESRT\n",
            "No results found for that keyword\n",
            "ESRX\n",
            "Found 10 results\n",
            "ESS\n",
            "Found 2 results\n",
            "ESSA\n",
            "Found 1 results\n",
            "ESYJY\n",
            "Found 2 results\n",
            "ETN\n",
            "No results found for that keyword\n",
            "ETOLF\n",
            "Found 2 results\n",
            "ETP\n",
            "Found 1 results\n",
            "ETX\n",
            "No results found for that keyword\n",
            "EUZOF\n",
            "Found 2 results\n",
            "EVC\n",
            "Found 10 results\n",
            "EVER\n",
            "Found 1 results\n",
            "EVF\n",
            "Found 2 results\n",
            "EVLV\n",
            "Found 3 results\n",
            "EVN\n",
            "Found 1 results\n",
            "EVOK\n",
            "Found 7 results\n",
            "EVOL\n",
            "Found 3 results\n",
            "EVR\n",
            "Found 5 results\n",
            "EVT\n",
            "Found 10 results\n",
            "EW\n",
            "Found 1 results\n",
            "EWBC\n",
            "Found 1 results\n",
            "EXAC\n",
            "Found 1 results\n",
            "EXAS\n",
            "Found 6 results\n",
            "EXC\n",
            "Found 4 results\n",
            "EXEL\n",
            "Found 1 results\n",
            "EXLS\n",
            "Found 10 results\n",
            "EXP\n",
            "Found 1 results\n",
            "EXPD\n",
            "Found 4 results\n",
            "EXPE\n",
            "Found 1 results\n",
            "EXPGY\n",
            "Found 1 results\n",
            "EXPO\n",
            "Found 1 results\n",
            "EXR\n",
            "Found 2 results\n",
            "EXTR\n",
            "Found 2 results\n",
            "EYES\n",
            "Found 1 results\n",
            "EZPW\n",
            "Found 10 results\n",
            "F\n",
            "No results found for that keyword\n",
            "FALC\n",
            "Found 2 results\n",
            "FAM\n",
            "Found 2 results\n",
            "FANG\n",
            "Found 5 results\n",
            "FARM\n",
            "Found 1 results\n",
            "FARO\n",
            "Found 10 results\n",
            "FAST\n",
            "Found 1 results\n",
            "FATE\n",
            "Found 10 results\n",
            "FB\n",
            "Found 3 results\n",
            "FBC\n",
            "Found 1 results\n",
            "FBHS\n",
            "Found 1 results\n",
            "FBIZ\n",
            "Found 1 results\n",
            "FBMS\n",
            "Found 1 results\n",
            "FBP\n",
            "Found 2 results\n",
            "FBR\n",
            "Found 1 results\n",
            "FCAP\n",
            "Found 1 results\n",
            "FCB\n",
            "Found 1 results\n",
            "FCBC\n",
            "Found 1 results\n",
            "FCCO\n",
            "No results found for that keyword\n",
            "FCCY\n",
            "Found 1 results\n",
            "FCEL\n",
            "Found 4 results\n",
            "FCF\n",
            "No results found for that keyword\n",
            "FCGN\n",
            "No results found for that keyword\n",
            "FCMGF\n",
            "Found 4 results\n",
            "FCN\n",
            "Found 1 results\n",
            "FCNCA\n",
            "No results found for that keyword\n",
            "FCODF\n",
            "Found 2 results\n",
            "FCT\n",
            "Found 1 results\n",
            "FCX\n",
            "Found 1 results\n",
            "FDP\n",
            "Found 1 results\n",
            "FDS\n",
            "Found 1 results\n",
            "FDUS\n",
            "Found 1 results\n",
            "FDX\n",
            "Found 10 results\n",
            "FE\n",
            "Found 3 results\n",
            "FEI\n",
            "Found 1 results\n",
            "FEIM\n",
            "Found 1 results\n",
            "FELE\n",
            "Found 1 results\n",
            "FEO\n",
            "Found 1 results\n",
            "FET\n",
            "Found 10 results\n",
            "FF\n",
            "Found 1 results\n",
            "FFA\n",
            "Found 1 results\n",
            "FFBC\n",
            "Found 1 results\n",
            "FFC\n",
            "Found 1 results\n",
            "FFHL\n",
            "Found 1 results\n",
            "FFIC\n",
            "Found 1 results\n",
            "FFIN\n",
            "Found 1 results\n",
            "FFIV\n",
            "No results found for that keyword\n",
            "FFKT\n",
            "No results found for that keyword\n",
            "FFNM\n",
            "Found 1 results\n",
            "FFNW\n",
            "Found 1 results\n",
            "FFWM\n",
            "Found 3 results\n",
            "FGB\n",
            "Found 1 results\n",
            "FGEN\n",
            "Found 3 results\n",
            "FHN\n",
            "Found 1 results\n",
            "FIBK\n",
            "Found 6 results\n",
            "FIF\n",
            "No results found for that keyword\n",
            "FINL\n",
            "No results found for that keyword\n",
            "FINMY\n",
            "Found 10 results\n",
            "FIS\n",
            "Found 1 results\n",
            "FISI\n",
            "Found 1 results\n",
            "FISV\n",
            "Found 4 results\n",
            "FITB\n",
            "Found 5 results\n",
            "FIVE\n",
            "Found 1 results\n",
            "FIVN\n",
            "Found 3 results\n",
            "FIX\n",
            "Found 1 results\n",
            "FIZZ\n",
            "Found 1 results\n",
            "FJTSY\n",
            "No results found for that keyword\n",
            "FKKFY\n",
            "Found 10 results\n",
            "FL\n",
            "Found 5 results\n",
            "FLC\n",
            "No results found for that keyword\n",
            "FLDM\n",
            "Found 1 results\n",
            "FLIC\n",
            "Found 3 results\n",
            "FLL\n",
            "Found 6 results\n",
            "FLO\n",
            "Found 4 results\n",
            "FLS\n",
            "Found 5 results\n",
            "FLT\n",
            "Found 1 results\n",
            "FLXS\n",
            "No results found for that keyword\n",
            "FMBI\n",
            "No results found for that keyword\n",
            "FMBM\n",
            "Found 2 results\n",
            "FMC\n",
            "Found 4 results\n",
            "FMI\n",
            "Found 3 results\n",
            "FMN\n",
            "Found 1 results\n",
            "FMNB\n",
            "Found 1 results\n",
            "FMS\n",
            "Found 1 results\n",
            "FMX\n",
            "Found 1 results\n",
            "FMY\n",
            "Found 10 results\n",
            "FN\n",
            "Found 1 results\n",
            "FNF\n",
            "Found 1 results\n",
            "FNHC\n",
            "Found 1 results\n",
            "FNLC\n",
            "No results found for that keyword\n",
            "FNLPF\n",
            "No results found for that keyword\n",
            "FNRG\n",
            "No results found for that keyword\n",
            "FOJCY\n",
            "Found 1 results\n",
            "FOLD\n",
            "Found 1 results\n",
            "FONR\n",
            "Found 10 results\n",
            "FOR\n",
            "Found 2 results\n",
            "FORD\n",
            "Found 9 results\n",
            "FORM\n",
            "Found 1 results\n",
            "FORR\n",
            "Found 3 results\n",
            "FORTY\n",
            "Found 2 results\n",
            "FOSL\n",
            "No results found for that keyword\n",
            "FOSUF\n",
            "Found 1 results\n",
            "FOXA\n",
            "Found 1 results\n",
            "FOXF\n",
            "Found 1 results\n",
            "FPAFY\n",
            "Found 2 results\n",
            "FPF\n",
            "Found 2 results\n",
            "FPL\n",
            "No results found for that keyword\n",
            "FPLPF\n",
            "No results found for that keyword\n",
            "FPRUF\n",
            "Found 1 results\n",
            "FQVLF\n",
            "Found 10 results\n",
            "FR\n",
            "Found 10 results\n",
            "FRA\n",
            "Found 1 results\n",
            "FRBA\n",
            "Found 1 results\n",
            "FRBK\n",
            "Found 4 results\n",
            "FRC\n",
            "Found 1 results\n",
            "FRCOY\n",
            "Found 10 results\n",
            "FREE\n",
            "Found 1 results\n",
            "FRGI\n",
            "No results found for that keyword\n",
            "FRMC\n",
            "Found 2 results\n",
            "FRME\n",
            "Found 10 results\n",
            "FRO\n",
            "Found 1 results\n",
            "FRPH\n",
            "Found 1 results\n",
            "FRPT\n",
            "No results found for that keyword\n",
            "FRRVF\n",
            "Found 1 results\n",
            "FRSH\n",
            "Found 3 results\n",
            "FRT\n",
            "No results found for that keyword\n",
            "FRZCF\n",
            "Found 1 results\n",
            "FSBW\n",
            "Found 2 results\n",
            "FSD\n",
            "No results found for that keyword\n",
            "FSFF\n",
            "Found 1 results\n",
            "FSFG\n",
            "Found 1 results\n",
            "FSLR\n",
            "Found 4 results\n",
            "FSM\n",
            "Found 5 results\n",
            "FSS\n",
            "Found 1 results\n",
            "FSTR\n",
            "No results found for that keyword\n",
            "FSUMF\n",
            "Found 10 results\n",
            "FT\n",
            "Found 1 results\n",
            "FTEK\n",
            "Found 5 results\n",
            "FTI\n",
            "Found 1 results\n",
            "FTK\n",
            "Found 1 results\n",
            "FTNT\n",
            "Found 7 results\n",
            "FUL\n",
            "Found 2 results\n",
            "FULT\n",
            "Found 5 results\n",
            "FUN\n",
            "Found 1 results\n",
            "FUNC\n",
            "Found 1 results\n",
            "FUND\n",
            "Found 1 results\n",
            "FWP\n",
            "Found 1 results\n",
            "FWRD\n",
            "Found 10 results\n",
            "G\n",
            "Found 10 results\n",
            "GAB\n",
            "Found 1 results\n",
            "GABC\n",
            "Found 1 results\n",
            "GAIA\n",
            "No results found for that keyword\n",
            "GAILF\n",
            "Found 4 results\n",
            "GAIN\n",
            "Found 1 results\n",
            "GALT\n",
            "Found 10 results\n",
            "GAM\n",
            "Found 5 results\n",
            "GAME\n",
            "No results found for that keyword\n",
            "GARPF\n",
            "Found 1 results\n",
            "GASS\n",
            "Found 1 results\n",
            "GBCI\n",
            "Found 1 results\n",
            "GBDC\n",
            "Found 1 results\n",
            "GBERY\n",
            "No results found for that keyword\n",
            "GBIM\n",
            "Found 1 results\n",
            "GBLI\n",
            "No results found for that keyword\n",
            "GBLX\n",
            "No results found for that keyword\n",
            "GBNK\n",
            "Found 1 results\n",
            "GBX\n",
            "Found 1 results\n",
            "GCBC\n",
            "No results found for that keyword\n",
            "GCH\n",
            "No results found for that keyword\n",
            "GCHOY\n",
            "No results found for that keyword\n",
            "GCLMF\n",
            "No results found for that keyword\n",
            "GCRIF\n",
            "Found 1 results\n",
            "GCV\n",
            "Found 10 results\n",
            "GD\n",
            "Found 3 results\n",
            "GDO\n",
            "Found 2 results\n",
            "GDV\n",
            "Found 10 results\n",
            "GE\n",
            "No results found for that keyword\n",
            "GECFF\n",
            "Found 4 results\n",
            "GEL\n",
            "Found 1 results\n",
            "GELYY\n",
            "Found 2 results\n",
            "GENC\n",
            "Found 10 results\n",
            "GENE\n",
            "Found 1 results\n",
            "GEOS\n",
            "Found 5 results\n",
            "GER\n",
            "Found 1 results\n",
            "GERN\n",
            "Found 1 results\n",
            "GEVO\n",
            "Found 10 results\n",
            "GF\n",
            "No results found for that keyword\n",
            "GFED\n",
            "Found 1 results\n",
            "GGAL\n",
            "No results found for that keyword\n",
            "GGDVF\n",
            "Found 2 results\n",
            "GGG\n",
            "No results found for that keyword\n",
            "GGNPF\n",
            "Found 1 results\n",
            "GGZ\n",
            "Found 1 results\n",
            "GHC\n",
            "Found 3 results\n",
            "GHY\n",
            "Found 2 results\n",
            "GIB\n",
            "Found 1 results\n",
            "GIFI\n",
            "Found 1 results\n",
            "GIGM\n",
            "Found 2 results\n",
            "GILD\n",
            "Found 1 results\n",
            "GILT\n",
            "Found 1 results\n",
            "GIM\n",
            "Found 1 results\n",
            "GIS\n",
            "No results found for that keyword\n",
            "GJNSF\n",
            "Found 1 results\n",
            "GJS\n",
            "Found 2 results\n",
            "GK\n",
            "No results found for that keyword\n",
            "GKNLY\n",
            "Found 10 results\n",
            "GLAD\n",
            "No results found for that keyword\n",
            "GLBD\n",
            "Found 1 results\n",
            "GLBS\n",
            "Found 1 results\n",
            "GLBZ\n",
            "Found 1 results\n",
            "GLDD\n",
            "No results found for that keyword\n",
            "GLF\n",
            "No results found for that keyword\n",
            "GLHD\n",
            "Found 1 results\n",
            "GLMD\n",
            "Found 1 results\n",
            "GLNG\n",
            "Found 10 results\n",
            "GLOB\n",
            "No results found for that keyword\n",
            "GLPEF\n",
            "Found 1 results\n",
            "GLPI\n",
            "No results found for that keyword\n",
            "GLRI\n",
            "Found 1 results\n",
            "GLW\n",
            "Found 1 results\n",
            "GLYC\n",
            "Found 10 results\n",
            "GM\n",
            "No results found for that keyword\n",
            "GMAN\n",
            "Found 3 results\n",
            "GME\n",
            "Found 1 results\n",
            "GMED\n",
            "No results found for that keyword\n",
            "GMELF\n",
            "No results found for that keyword\n",
            "GMGSF\n",
            "No results found for that keyword\n",
            "GNBC\n",
            "No results found for that keyword\n",
            "GNCA\n",
            "No results found for that keyword\n",
            "GNCMA\n",
            "No results found for that keyword\n",
            "GNCNF\n",
            "Found 2 results\n",
            "GNE\n",
            "No results found for that keyword\n",
            "GNGBY\n",
            "No results found for that keyword\n",
            "GNOW\n",
            "Found 1 results\n",
            "GNRC\n",
            "No results found for that keyword\n",
            "GNSG\n",
            "Found 4 results\n",
            "GNT\n",
            "Found 1 results\n",
            "GNTX\n",
            "Found 1 results\n",
            "GNW\n",
            "No results found for that keyword\n",
            "GNZUF\n",
            "Found 1 results\n",
            "GOF\n",
            "Found 3 results\n",
            "GOGO\n",
            "Found 10 results\n",
            "GOLD\n",
            "Found 7 results\n",
            "GOOD\n",
            "Found 2 results\n",
            "GOOG\n",
            "Found 5 results\n",
            "GPC\n",
            "Found 1 results\n",
            "GPI\n",
            "Found 1 results\n",
            "GPK\n",
            "Found 1 results\n",
            "GPN\n",
            "Found 1 results\n",
            "GPOR\n",
            "Found 1 results\n",
            "GPRE\n",
            "Found 1 results\n",
            "GPRO\n",
            "Found 1 results\n",
            "GPS\n",
            "No results found for that keyword\n",
            "GPT\n",
            "No results found for that keyword\n",
            "GPTGF\n",
            "Found 1 results\n",
            "GRBK\n",
            "Found 1 results\n",
            "GRFS\n",
            "Found 1 results\n",
            "GRMN\n",
            "Found 2 results\n",
            "GROW\n",
            "Found 1 results\n",
            "GRPN\n",
            "Found 2 results\n",
            "GRR\n",
            "No results found for that keyword\n",
            "GRSFF\n",
            "No results found for that keyword\n",
            "GRSXY\n",
            "Found 1 results\n",
            "GRX\n",
            "No results found for that keyword\n",
            "GS-PJ\n",
            "Found 1 results\n",
            "GSBC\n",
            "No results found for that keyword\n",
            "GSEFF\n",
            "Found 1 results\n",
            "GSIG\n",
            "Found 1 results\n",
            "GSIT\n",
            "Found 1 results\n",
            "GSK\n",
            "Found 2 results\n",
            "GSL\n",
            "Found 10 results\n",
            "GT\n",
            "Found 1 results\n",
            "GTIM\n",
            "Found 1 results\n",
            "GTLS\n",
            "No results found for that keyword\n",
            "GTN-A\n",
            "No results found for that keyword\n",
            "GTWCF\n",
            "No results found for that keyword\n",
            "GTXO\n",
            "No results found for that keyword\n",
            "GULTU\n",
            "Found 1 results\n",
            "GURE\n",
            "Found 2 results\n",
            "GUT\n",
            "Found 2 results\n",
            "GVA\n",
            "Found 1 results\n",
            "GVDNY\n",
            "No results found for that keyword\n",
            "GWB\n",
            "No results found for that keyword\n",
            "GWGH\n",
            "Found 1 results\n",
            "GWLLY\n",
            "Found 1 results\n",
            "GWRE\n",
            "Found 1 results\n",
            "GWW\n",
            "No results found for that keyword\n",
            "GXP\n",
            "Found 10 results\n",
            "HA\n",
            "No results found for that keyword\n",
            "HACBY\n",
            "Found 1 results\n",
            "HAFC\n",
            "Found 2 results\n",
            "HAIN\n",
            "Found 8 results\n",
            "HAL\n",
            "No results found for that keyword\n",
            "HALFF\n",
            "Found 5 results\n",
            "HALL\n",
            "Found 1 results\n",
            "HALO\n",
            "Found 10 results\n",
            "HART\n",
            "Found 3 results\n",
            "HAS\n",
            "Found 1 results\n",
            "HASI\n",
            "Found 4 results\n",
            "HAWK\n",
            "Found 1 results\n",
            "HAYN\n",
            "Found 3 results\n",
            "HBAN\n",
            "Found 1 results\n",
            "HBCP\n",
            "Found 2 results\n",
            "HBI\n",
            "Found 1 results\n",
            "HBIO\n",
            "No results found for that keyword\n",
            "HBMD\n",
            "Found 1 results\n",
            "HBNC\n",
            "No results found for that keyword\n",
            "HBP\n",
            "Found 5 results\n",
            "HCA\n",
            "Found 1 results\n",
            "HCKT\n",
            "Found 1 results\n",
            "HCMLY\n",
            "Found 1 results\n",
            "HCOM\n",
            "Found 1 results\n",
            "HCSG\n",
            "Found 10 results\n",
            "HD\n",
            "Found 1 results\n",
            "HDB\n",
            "Found 1 results\n",
            "HDELY\n",
            "No results found for that keyword\n",
            "HDNG\n",
            "No results found for that keyword\n",
            "HDP\n",
            "Found 1 results\n",
            "HDSN\n",
            "Found 10 results\n",
            "HE\n",
            "Found 10 results\n",
            "HEAR\n",
            "Found 1 results\n",
            "HEES\n",
            "Found 1 results\n",
            "HELE\n",
            "No results found for that keyword\n",
            "HEOFF\n",
            "Found 3 results\n",
            "HEP\n",
            "Found 2 results\n",
            "HEQ\n",
            "Found 1 results\n",
            "HERB\n",
            "Found 2 results\n",
            "HERO\n",
            "Found 3 results\n",
            "HES\n",
            "No results found for that keyword\n",
            "HFBK\n",
            "Found 1 results\n",
            "HFBL\n",
            "No results found for that keyword\n",
            "HFC\n",
            "Found 1 results\n",
            "HFWA\n",
            "No results found for that keyword\n",
            "HGKGF\n",
            "No results found for that keyword\n",
            "HGSH\n",
            "Found 1 results\n",
            "HIBB\n",
            "Found 1 results\n",
            "HIE\n",
            "Found 1 results\n",
            "HIFS\n",
            "Found 10 results\n",
            "HIG\n",
            "Found 1 results\n",
            "HIHO\n",
            "Found 4 results\n",
            "HII\n",
            "Found 8 results\n",
            "HIL\n",
            "Found 1 results\n",
            "HIMX\n",
            "Found 1 results\n",
            "HIO\n",
            "Found 1 results\n",
            "HIVE\n",
            "Found 1 results\n",
            "HIW\n",
            "Found 1 results\n",
            "HIX\n",
            "Found 1 results\n",
            "HKHHY\n",
            "No results found for that keyword\n",
            "HKUOF\n",
            "No results found for that keyword\n",
            "HKXCF\n",
            "Found 10 results\n",
            "HL\n",
            "Found 2 results\n",
            "HLF\n",
            "No results found for that keyword\n",
            "HLFDF\n",
            "Found 1 results\n",
            "HLIT\n",
            "No results found for that keyword\n",
            "HLRTF\n",
            "Found 3 results\n",
            "HLT\n",
            "Found 1 results\n",
            "HLX\n",
            "Found 4 results\n",
            "HMC\n",
            "No results found for that keyword\n",
            "HMHC\n",
            "Found 2 results\n",
            "HMN\n",
            "Found 1 results\n",
            "HMNF\n",
            "No results found for that keyword\n",
            "HMNY\n",
            "Found 1 results\n",
            "HMST\n",
            "Found 1 results\n",
            "HMTV\n",
            "Found 1 results\n",
            "HMY\n",
            "No results found for that keyword\n",
            "HNHPF\n",
            "No results found for that keyword\n",
            "HNLGF\n",
            "Found 2 results\n",
            "HNNA\n",
            "No results found for that keyword\n",
            "HNP\n",
            "No results found for that keyword\n",
            "HNRDF\n",
            "Found 1 results\n",
            "HNRG\n",
            "No results found for that keyword\n",
            "HNTIF\n",
            "Found 1 results\n",
            "HOFT\n",
            "Found 1 results\n",
            "HOG\n",
            "Found 1 results\n",
            "HOLI\n",
            "Found 1 results\n",
            "HOLX\n",
            "Found 7 results\n",
            "HON\n",
            "Found 2 results\n",
            "HOT\n",
            "Found 2 results\n",
            "HOV\n",
            "Found 10 results\n",
            "HP\n",
            "Found 1 results\n",
            "HPE\n",
            "Found 1 results\n",
            "HPF\n",
            "No results found for that keyword\n",
            "HPIL\n",
            "Found 1 results\n",
            "HPP\n",
            "Found 1 results\n",
            "HPQ\n",
            "Found 1 results\n",
            "HPS\n",
            "No results found for that keyword\n",
            "HQCL\n",
            "Found 1 results\n",
            "HQH\n",
            "Found 1 results\n",
            "HQL\n",
            "Found 1 results\n",
            "HQY\n",
            "Found 10 results\n",
            "HR\n",
            "Found 1 results\n",
            "HRB\n",
            "Found 1 results\n",
            "HRL\n",
            "No results found for that keyword\n",
            "HRNNF\n",
            "Found 1 results\n",
            "HRTG\n",
            "Found 1 results\n",
            "HRTX\n",
            "Found 1 results\n",
            "HRZN\n",
            "Found 1 results\n",
            "HSBC\n",
            "Found 1 results\n",
            "HSIC\n",
            "Found 1 results\n",
            "HSII\n",
            "Found 1 results\n",
            "HSKA\n",
            "Found 1 results\n",
            "HSON\n",
            "Found 3 results\n",
            "HST\n",
            "Found 1 results\n",
            "HSTM\n",
            "Found 1 results\n",
            "HSY\n",
            "Found 4 results\n",
            "HTA\n",
            "Found 1 results\n",
            "HTBI\n",
            "Found 1 results\n",
            "HTBK\n",
            "No results found for that keyword\n",
            "HTBX\n",
            "Found 1 results\n",
            "HTD\n",
            "Found 3 results\n",
            "HTH\n",
            "Found 1 results\n",
            "HTHIY\n",
            "Found 1 results\n",
            "HTHT\n",
            "Found 1 results\n",
            "HTLD\n",
            "Found 2 results\n",
            "HTLF\n",
            "No results found for that keyword\n",
            "HTNGF\n",
            "Found 2 results\n",
            "HTZ\n",
            "Found 1 results\n",
            "HUBG\n",
            "Found 8 results\n",
            "HUM\n",
            "Found 1 results\n",
            "HURC\n",
            "Found 1 results\n",
            "HURN\n",
            "Found 2 results\n",
            "HVT\n",
            "Found 1 results\n",
            "HWBK\n",
            "Found 1 results\n",
            "HWKN\n",
            "Found 1 results\n",
            "HXGBY\n",
            "Found 3 results\n",
            "HYB\n",
            "Found 2 results\n",
            "HYI\n",
            "No results found for that keyword\n",
            "HYKUF\n",
            "No results found for that keyword\n",
            "HYMTF\n",
            "Found 2 results\n",
            "HYT\n",
            "No results found for that keyword\n",
            "HYUHF\n",
            "Found 2 results\n",
            "IAC\n",
            "Found 1 results\n",
            "IART\n",
            "No results found for that keyword\n",
            "IAUGY\n",
            "Found 1 results\n",
            "IBA\n",
            "Found 1 results\n",
            "IBCP\n",
            "No results found for that keyword\n",
            "IBDRY\n",
            "Found 1 results\n",
            "IBKR\n",
            "Found 8 results\n",
            "IBM\n",
            "Found 2 results\n",
            "IBN\n",
            "Found 1 results\n",
            "IBOC\n",
            "Found 1 results\n",
            "IBP\n",
            "Found 1 results\n",
            "IBTX\n",
            "No results found for that keyword\n",
            "ICB\n",
            "Found 1 results\n",
            "ICCC\n",
            "Found 1 results\n",
            "ICD\n",
            "Found 2 results\n",
            "ICE\n",
            "Found 1 results\n",
            "ICFI\n",
            "Found 4 results\n",
            "ICL\n",
            "No results found for that keyword\n",
            "ICLD\n",
            "Found 1 results\n",
            "ICPT\n",
            "Found 1 results\n",
            "ICUI\n",
            "Found 1 results\n",
            "IDCC\n",
            "Found 10 results\n",
            "IDE\n",
            "No results found for that keyword\n",
            "IDKOF\n",
            "No results found for that keyword\n",
            "IDLLF\n",
            "Found 1 results\n",
            "IDRA\n",
            "Found 1 results\n",
            "IDXX\n",
            "Found 1 results\n",
            "IEP\n",
            "Found 1 results\n",
            "IESFY\n",
            "Found 1 results\n",
            "IEX\n",
            "No results found for that keyword\n",
            "IFCZF\n",
            "Found 1 results\n",
            "IFF\n",
            "Found 1 results\n",
            "IFNNY\n",
            "No results found for that keyword\n",
            "IFSB\n",
            "Found 4 results\n",
            "IGA\n",
            "Found 1 results\n",
            "IGD\n",
            "Found 1 results\n",
            "IGGHY\n",
            "Found 4 results\n",
            "IGI\n",
            "Found 1 results\n",
            "IGLD\n",
            "No results found for that keyword\n",
            "IGOI\n",
            "No results found for that keyword\n",
            "IGPPF\n",
            "Found 2 results\n",
            "IGR\n",
            "Found 5 results\n",
            "IGT\n",
            "No results found for that keyword\n",
            "IHC\n",
            "No results found for that keyword\n",
            "IHCPF\n",
            "Found 2 results\n",
            "IHD\n",
            "Found 1 results\n",
            "IHG\n",
            "Found 1 results\n",
            "IIF\n",
            "Found 6 results\n",
            "III\n",
            "Found 1 results\n",
            "IIIN\n",
            "Found 1 results\n",
            "IIM\n",
            "Found 2 results\n",
            "IIN\n",
            "No results found for that keyword\n",
            "IITSF\n",
            "Found 1 results\n",
            "ILMN\n",
            "Found 1 results\n",
            "IMGN\n",
            "No results found for that keyword\n",
            "IMHDF\n",
            "Found 1 results\n",
            "IMIAY\n",
            "Found 1 results\n",
            "IMKTA\n",
            "Found 1 results\n",
            "IMMR\n",
            "Found 1 results\n",
            "IMOS\n",
            "Found 2 results\n",
            "INBK\n",
            "Found 1 results\n",
            "INCR\n",
            "Found 1 results\n",
            "INCY\n",
            "Found 1 results\n",
            "INDB\n",
            "Found 1 results\n",
            "INFA\n",
            "Found 9 results\n",
            "INFI\n",
            "Found 1 results\n",
            "INFN\n",
            "Found 1 results\n",
            "INFY\n",
            "Found 1 results\n",
            "INGN\n",
            "Found 1 results\n",
            "INGR\n",
            "Found 10 results\n",
            "INN\n",
            "Found 5 results\n",
            "INO\n",
            "Found 1 results\n",
            "INOD\n",
            "Found 10 results\n",
            "INT\n",
            "Found 1 results\n",
            "INTC\n",
            "Found 1 results\n",
            "INTG\n",
            "No results found for that keyword\n",
            "INTH\n",
            "No results found for that keyword\n",
            "INTK\n",
            "Found 2 results\n",
            "INTU\n",
            "No results found for that keyword\n",
            "INTX\n",
            "Found 10 results\n",
            "INVE\n",
            "No results found for that keyword\n",
            "INVT\n",
            "Found 10 results\n",
            "IO\n",
            "Found 1 results\n",
            "IOSP\n",
            "Found 10 results\n",
            "IP\n",
            "Found 1 results\n",
            "IPAR\n",
            "No results found for that keyword\n",
            "IPAS\n",
            "No results found for that keyword\n",
            "IPCC\n",
            "Found 1 results\n",
            "IPDN\n",
            "Found 2 results\n",
            "IPG\n",
            "Found 1 results\n",
            "IPGP\n",
            "Found 1 results\n",
            "IPI\n",
            "No results found for that keyword\n",
            "IPOAF\n",
            "No results found for that keyword\n",
            "IPSBF\n",
            "Found 1 results\n",
            "IPWR\n",
            "No results found for that keyword\n",
            "IPXL\n",
            "Found 3 results\n",
            "IQI\n",
            "Found 10 results\n",
            "IR\n",
            "Found 1 results\n",
            "IRBT\n",
            "Found 1 results\n",
            "IRDM\n",
            "Found 1 results\n",
            "IRIX\n",
            "Found 1 results\n",
            "IRL\n",
            "Found 2 results\n",
            "IRM\n",
            "Found 1 results\n",
            "IRMD\n",
            "Found 1 results\n",
            "IROQ\n",
            "Found 2 results\n",
            "IRS\n",
            "Found 1 results\n",
            "IRWD\n",
            "No results found for that keyword\n",
            "ISBC\n",
            "Found 3 results\n",
            "ISD\n",
            "Found 1 results\n",
            "ISIG\n",
            "Found 2 results\n",
            "ISLE\n",
            "Found 1 results\n",
            "ISRG\n",
            "Found 1 results\n",
            "ISSC\n",
            "Found 1 results\n",
            "ISTR\n",
            "No results found for that keyword\n",
            "ISUZY\n",
            "Found 10 results\n",
            "IT\n",
            "Found 5 results\n",
            "ITI\n",
            "Found 1 results\n",
            "ITIC\n",
            "Found 1 results\n",
            "ITRI\n",
            "Found 1 results\n",
            "ITRN\n",
            "No results found for that keyword\n",
            "ITSXF\n",
            "Found 1 results\n",
            "ITUB\n",
            "Found 1 results\n",
            "ITW\n",
            "No results found for that keyword\n",
            "IUSPF\n",
            "No results found for that keyword\n",
            "IUTCF\n",
            "No results found for that keyword\n",
            "IUTSF\n",
            "Found 1 results\n",
            "IVAC\n",
            "Found 4 results\n",
            "IVR\n",
            "No results found for that keyword\n",
            "IVSXF\n",
            "No results found for that keyword\n",
            "IVTJF\n",
            "Found 1 results\n",
            "IVZ\n",
            "Found 9 results\n",
            "JACK\n",
            "Found 1 results\n",
            "JAKK\n",
            "No results found for that keyword\n",
            "JAPAF\n",
            "Found 1 results\n",
            "JAPSY\n",
            "Found 1 results\n",
            "JAZZ\n",
            "Found 1 results\n",
            "JBAXY\n",
            "Found 1 results\n",
            "JBHT\n",
            "Found 2 results\n",
            "JBL\n",
            "Found 1 results\n",
            "JBLU\n",
            "Found 1 results\n",
            "JBSS\n",
            "Found 1 results\n",
            "JBT\n",
            "Found 4 results\n",
            "JCI\n",
            "Found 1 results\n",
            "JCS\n",
            "Found 4 results\n",
            "JD\n",
            "Found 10 results\n",
            "JE\n",
            "Found 1 results\n",
            "JEQ\n",
            "No results found for that keyword\n",
            "JEXYF\n",
            "No results found for that keyword\n",
            "JFEEF\n",
            "Found 2 results\n",
            "JFR\n",
            "Found 1 results\n",
            "JGH\n",
            "No results found for that keyword\n",
            "JGSMY\n",
            "Found 1 results\n",
            "JHI\n",
            "Found 2 results\n",
            "JHS\n",
            "No results found for that keyword\n",
            "JIXAY\n",
            "Found 1 results\n",
            "JJSF\n",
            "Found 1 results\n",
            "JKHY\n",
            "Found 1 results\n",
            "JLL\n",
            "No results found for that keyword\n",
            "JMBA\n",
            "Found 1 results\n",
            "JMHLY\n",
            "No results found for that keyword\n",
            "JMPLY\n",
            "Found 1 results\n",
            "JNJ\n",
            "Found 1 results\n",
            "JNPR\n",
            "Found 3 results\n",
            "JOB\n",
            "No results found for that keyword\n",
            "JOBS\n",
            "Found 4 results\n",
            "JOF\n",
            "Found 1 results\n",
            "JOUT\n",
            "Found 1 results\n",
            "JPC\n",
            "No results found for that keyword\n",
            "JPHLF\n",
            "Found 4 results\n",
            "JPI\n",
            "Found 10 results\n",
            "JPM\n",
            "No results found for that keyword\n",
            "JPM-PC\n",
            "Found 1 results\n",
            "JQC\n",
            "Found 1 results\n",
            "JRI\n",
            "No results found for that keyword\n",
            "JRJC\n",
            "Found 1 results\n",
            "JRO\n",
            "No results found for that keyword\n",
            "JRONY\n",
            "Found 1 results\n",
            "JRVR\n",
            "Found 1 results\n",
            "JSD\n",
            "No results found for that keyword\n",
            "JSNSF\n",
            "No results found for that keyword\n",
            "JTPY\n",
            "No results found for that keyword\n",
            "JUNO\n",
            "Found 2 results\n",
            "JVA\n",
            "No results found for that keyword\n",
            "JW-A\n",
            "Found 1 results\n",
            "JWN\n",
            "No results found for that keyword\n",
            "JXHLY\n",
            "No results found for that keyword\n",
            "JXSB\n",
            "Found 1 results\n",
            "JYNT\n",
            "No results found for that keyword\n",
            "JYSKY\n",
            "Found 10 results\n",
            "K\n",
            "No results found for that keyword\n",
            "KACPF\n",
            "Found 1 results\n",
            "KALU\n",
            "No results found for that keyword\n",
            "KANG\n",
            "Found 7 results\n",
            "KAR\n",
            "Found 10 results\n",
            "KB\n",
            "Found 1 results\n",
            "KBAL\n",
            "Found 1 results\n",
            "KBCSY\n",
            "No results found for that keyword\n",
            "KBSTF\n",
            "No results found for that keyword\n",
            "KCLI\n",
            "Found 10 results\n",
            "KE\n",
            "No results found for that keyword\n",
            "KED\n",
            "Found 1 results\n",
            "KELYA\n",
            "Found 2 results\n",
            "KEP\n",
            "Found 1 results\n",
            "KEQU\n",
            "No results found for that keyword\n",
            "KERX\n",
            "Found 9 results\n",
            "KEY\n",
            "Found 1 results\n",
            "KEYS\n",
            "No results found for that keyword\n",
            "KEYW\n",
            "Found 1 results\n",
            "KFFB\n",
            "Found 1 results\n",
            "KFRC\n",
            "Found 1 results\n",
            "KFS\n",
            "Found 2 results\n",
            "KFY\n",
            "Found 1 results\n",
            "KGFHY\n",
            "No results found for that keyword\n",
            "KGJI\n",
            "No results found for that keyword\n",
            "KGNR\n",
            "Found 1 results\n",
            "KHC\n",
            "Found 1 results\n",
            "KHNGY\n",
            "Found 10 results\n",
            "KIM\n",
            "Found 5 results\n",
            "KINS\n",
            "Found 3 results\n",
            "KIO\n",
            "Found 1 results\n",
            "KIRK\n",
            "No results found for that keyword\n",
            "KKOYF\n",
            "Found 1 results\n",
            "KLAC\n",
            "Found 1 results\n",
            "KLIC\n",
            "No results found for that keyword\n",
            "KLMR\n",
            "No results found for that keyword\n",
            "KLPEF\n",
            "No results found for that keyword\n",
            "KLXI\n",
            "No results found for that keyword\n",
            "KMAAF\n",
            "Found 1 results\n",
            "KMB\n",
            "Found 1 results\n",
            "KMDA\n",
            "Found 1 results\n",
            "KMF\n",
            "No results found for that keyword\n",
            "KMG\n",
            "Found 1 results\n",
            "KMPR\n",
            "Found 1 results\n",
            "KMTUY\n",
            "Found 1 results\n",
            "KMX\n",
            "Found 1 results\n",
            "KND\n",
            "Found 1 results\n",
            "KNDI\n",
            "Found 1 results\n",
            "KNX\n",
            "Found 10 results\n",
            "KO\n",
            "Found 1 results\n",
            "KODK\n",
            "Found 1 results\n",
            "KOF\n",
            "Found 1 results\n",
            "KOPN\n",
            "Found 3 results\n",
            "KOS\n",
            "Found 1 results\n",
            "KOSS\n",
            "Found 1 results\n",
            "KOTMY\n",
            "Found 1 results\n",
            "KPTI\n",
            "Found 10 results\n",
            "KR\n",
            "Found 10 results\n",
            "KRA\n",
            "Found 1 results\n",
            "KRC\n",
            "No results found for that keyword\n",
            "KREVF\n",
            "Found 1 results\n",
            "KRG\n",
            "No results found for that keyword\n",
            "KRNNF\n",
            "Found 1 results\n",
            "KRNY\n",
            "Found 5 results\n",
            "KRO\n",
            "No results found for that keyword\n",
            "KRYAF\n",
            "No results found for that keyword\n",
            "KRYPF\n",
            "Found 10 results\n",
            "KS\n",
            "Found 1 results\n",
            "KSM\n",
            "Found 1 results\n",
            "KSS\n",
            "Found 1 results\n",
            "KST\n",
            "No results found for that keyword\n",
            "KSU\n",
            "Found 1 results\n",
            "KTCC\n",
            "Found 1 results\n",
            "KTEC\n",
            "Found 1 results\n",
            "KTF\n",
            "Found 1 results\n",
            "KTOS\n",
            "No results found for that keyword\n",
            "KTWO\n",
            "Found 1 results\n",
            "KVHI\n",
            "No results found for that keyword\n",
            "KWGPF\n",
            "Found 1 results\n",
            "KWHIY\n",
            "Found 1 results\n",
            "KWR\n",
            "No results found for that keyword\n",
            "KYE\n",
            "Found 2 results\n",
            "KYN\n",
            "No results found for that keyword\n",
            "KYSEF\n",
            "Found 2 results\n",
            "LAD\n",
            "Found 1 results\n",
            "LADR\n",
            "Found 10 results\n",
            "LAKE\n",
            "Found 1 results\n",
            "LAMR\n",
            "Found 1 results\n",
            "LANC\n",
            "Found 10 results\n",
            "LAND\n",
            "Found 4 results\n",
            "LARK\n",
            "No results found for that keyword\n",
            "LAWS\n",
            "No results found for that keyword\n",
            "LAYN\n",
            "Found 1 results\n",
            "LBAI\n",
            "Found 4 results\n",
            "LBBB\n",
            "Found 1 results\n",
            "LBRDA\n",
            "Found 1 results\n",
            "LBTYA\n",
            "Found 10 results\n",
            "LC\n",
            "Found 1 results\n",
            "LCUT\n",
            "No results found for that keyword\n",
            "LDNXF\n",
            "Found 1 results\n",
            "LDOS\n",
            "Found 1 results\n",
            "LDP\n",
            "Found 10 results\n",
            "LE\n",
            "Found 1 results\n",
            "LECO\n",
            "Found 1 results\n",
            "LEDS\n",
            "Found 2 results\n",
            "LEE\n",
            "Found 10 results\n",
            "LEG\n",
            "No results found for that keyword\n",
            "LEN-B\n",
            "Found 4 results\n",
            "LEO\n",
            "Found 2 results\n",
            "LEU\n",
            "Found 1 results\n",
            "LFC\n",
            "Found 1 results\n",
            "LFUS\n",
            "Found 1 results\n",
            "LFVN\n",
            "No results found for that keyword\n",
            "LGCY\n",
            "Found 1 results\n",
            "LGFRY\n",
            "Found 1 results\n",
            "LGGNY\n",
            "Found 2 results\n",
            "LGI\n",
            "Found 1 results\n",
            "LGIH\n",
            "Found 1 results\n",
            "LGND\n",
            "No results found for that keyword\n",
            "LGRDY\n",
            "Found 10 results\n",
            "LH\n",
            "No results found for that keyword\n",
            "LHO\n",
            "Found 1 results\n",
            "LII\n",
            "Found 2 results\n",
            "LILA\n",
            "Found 1 results\n",
            "LIME\n",
            "Found 3 results\n",
            "LINC\n",
            "Found 7 results\n",
            "LION\n",
            "No results found for that keyword\n",
            "LJPC\n",
            "Found 1 results\n",
            "LKFN\n",
            "Found 1 results\n",
            "LKQ\n",
            "Found 7 results\n",
            "LL\n",
            "No results found for that keyword\n",
            "LLESF\n",
            "Found 1 results\n",
            "LLL\n",
            "No results found for that keyword\n",
            "LLNW\n",
            "Found 1 results\n",
            "LLY\n",
            "Found 1 results\n",
            "LMAT\n",
            "No results found for that keyword\n",
            "LMDCF\n",
            "Found 1 results\n",
            "LMNR\n",
            "No results found for that keyword\n",
            "LMRK\n",
            "Found 1 results\n",
            "LMT\n",
            "Found 1 results\n",
            "LNC\n",
            "Found 1 results\n",
            "LNDC\n",
            "No results found for that keyword\n",
            "LNDZF\n",
            "Found 1 results\n",
            "LNG\n",
            "Found 2 results\n",
            "LNT\n",
            "Found 1 results\n",
            "LNVGY\n",
            "Found 2 results\n",
            "LOAN\n",
            "Found 1 results\n",
            "LOCO\n",
            "Found 6 results\n",
            "LOGI\n",
            "Found 1 results\n",
            "LOPE\n",
            "Found 1 results\n",
            "LOW\n",
            "Found 1 results\n",
            "LPCN\n",
            "Found 1 results\n",
            "LPG\n",
            "Found 1 results\n",
            "LPI\n",
            "Found 2 results\n",
            "LPL\n",
            "Found 1 results\n",
            "LPLA\n",
            "No results found for that keyword\n",
            "LPNT\n",
            "Found 1 results\n",
            "LPSN\n",
            "Found 1 results\n",
            "LPTH\n",
            "Found 8 results\n",
            "LQ\n",
            "Found 1 results\n",
            "LQDT\n",
            "No results found for that keyword\n",
            "LRCDF\n",
            "Found 1 results\n",
            "LRCX\n",
            "Found 1 results\n",
            "LSBK\n",
            "Found 1 results\n",
            "LSCC\n",
            "No results found for that keyword\n",
            "LSGOF\n",
            "Found 1 results\n",
            "LSTR\n",
            "Found 1 results\n",
            "LTBR\n",
            "Found 3 results\n",
            "LTC\n",
            "No results found for that keyword\n",
            "LTOUF\n",
            "No results found for that keyword\n",
            "LTRE\n",
            "Found 1 results\n",
            "LTRPA\n",
            "Found 1 results\n",
            "LTRX\n",
            "Found 2 results\n",
            "LULU\n",
            "Found 1 results\n",
            "LUNA\n",
            "Found 1 results\n",
            "LUV\n",
            "Found 1 results\n",
            "LVS\n",
            "No results found for that keyword\n",
            "LVWD\n",
            "Found 1 results\n",
            "LWAY\n",
            "Found 2 results\n",
            "LXP\n",
            "Found 1 results\n",
            "LXRX\n",
            "Found 1 results\n",
            "LXU\n",
            "Found 1 results\n",
            "LYB\n",
            "Found 1 results\n",
            "LYG\n",
            "Found 1 results\n",
            "LYTS\n",
            "Found 1 results\n",
            "LYV\n",
            "No results found for that keyword\n",
            "LZAGF\n",
            "Found 10 results\n",
            "M\n",
            "Found 10 results\n",
            "MA\n",
            "Found 7 results\n",
            "MAA\n",
            "No results found for that keyword\n",
            "MAANF\n",
            "Found 1 results\n",
            "MACK\n",
            "No results found for that keyword\n",
            "MAEOY\n",
            "No results found for that keyword\n",
            "MAHMF\n",
            "Found 8 results\n",
            "MAIN\n",
            "Found 1 results\n",
            "MAKSY\n",
            "Found 10 results\n",
            "MAN\n",
            "Found 2 results\n",
            "MANH\n",
            "Found 1 results\n",
            "MANT\n",
            "Found 10 results\n",
            "MAR\n",
            "Found 4 results\n",
            "MARA\n",
            "Found 1 results\n",
            "MARPS\n",
            "Found 1 results\n",
            "MASI\n",
            "Found 10 results\n",
            "MAT\n",
            "Found 1 results\n",
            "MATR\n",
            "Found 1 results\n",
            "MATW\n",
            "Found 2 results\n",
            "MAV\n",
            "Found 1 results\n",
            "MAYS\n",
            "Found 1 results\n",
            "MBCN\n",
            "No results found for that keyword\n",
            "MBII\n",
            "Found 1 results\n",
            "MBUU\n",
            "Found 1 results\n",
            "MBWM\n",
            "Found 10 results\n",
            "MCA\n",
            "Found 1 results\n",
            "MCBC\n",
            "No results found for that keyword\n",
            "MCBK\n",
            "No results found for that keyword\n",
            "MCCK\n",
            "Found 1 results\n",
            "MCD\n",
            "No results found for that keyword\n",
            "MCFUF\n",
            "Found 1 results\n",
            "MCHP\n",
            "Found 1 results\n",
            "MCHVY\n",
            "Found 1 results\n",
            "MCHX\n",
            "Found 1 results\n",
            "MCK\n",
            "Found 1 results\n",
            "MCN\n",
            "Found 1 results\n",
            "MCO\n",
            "No results found for that keyword\n",
            "MCOA\n",
            "Found 4 results\n",
            "MCR\n",
            "Found 1 results\n",
            "MCRI\n",
            "Found 1 results\n",
            "MCY\n",
            "Found 2 results\n",
            "MDC\n",
            "No results found for that keyword\n",
            "MDEVF\n",
            "Found 1 results\n",
            "MDLZ\n",
            "Found 1 results\n",
            "MDRX\n",
            "Found 1 results\n",
            "MDT\n",
            "Found 1 results\n",
            "MDU\n",
            "Found 1 results\n",
            "MDWD\n",
            "Found 1 results\n",
            "MDXG\n",
            "Found 6 results\n",
            "MEG\n",
            "Found 6 results\n",
            "MEI\n",
            "No results found for that keyword\n",
            "MEIL\n",
            "Found 1 results\n",
            "MEIP\n",
            "No results found for that keyword\n",
            "MEJHF\n",
            "Found 2 results\n",
            "MELI\n",
            "Found 1 results\n",
            "MEOH\n",
            "Found 10 results\n",
            "MERC\n",
            "Found 10 results\n",
            "MET\n",
            "Found 2 results\n",
            "MFA\n",
            "Found 1 results\n",
            "MFC\n",
            "Found 2 results\n",
            "MFG\n",
            "Found 1 results\n",
            "MFL\n",
            "Found 1 results\n",
            "MFM\n",
            "Found 1 results\n",
            "MFV\n",
            "Found 10 results\n",
            "MG\n",
            "Found 2 results\n",
            "MGA\n",
            "Found 1 results\n",
            "MGEE\n",
            "Found 1 results\n",
            "MGF\n",
            "Found 3 results\n",
            "MGI\n",
            "Found 2 results\n",
            "MGIC\n",
            "No results found for that keyword\n",
            "MGLN\n",
            "Found 3 results\n",
            "MGM\n",
            "Found 1 results\n",
            "MGNX\n",
            "Found 1 results\n",
            "MGPI\n",
            "Found 1 results\n",
            "MGRC\n",
            "Found 1 results\n",
            "MGU\n",
            "Found 1 results\n",
            "MGYR\n",
            "Found 1 results\n",
            "MHD\n",
            "Found 1 results\n",
            "MHF\n",
            "Found 1 results\n",
            "MHI\n",
            "Found 1 results\n",
            "MHK\n",
            "Found 1 results\n",
            "MHLD\n",
            "Found 2 results\n",
            "MHN\n",
            "Found 1 results\n",
            "MHNC\n",
            "No results found for that keyword\n",
            "MHVYF\n",
            "Found 1 results\n",
            "MICT\n",
            "Found 3 results\n",
            "MIDD\n",
            "Found 1 results\n",
            "MIELY\n",
            "Found 2 results\n",
            "MIG\n",
            "No results found for that keyword\n",
            "MIMTF\n",
            "Found 10 results\n",
            "MIN\n",
            "Found 5 results\n",
            "MIND\n",
            "Found 1 results\n",
            "MITEY\n",
            "Found 1 results\n",
            "MITK\n",
            "No results found for that keyword\n",
            "MITL\n",
            "Found 3 results\n",
            "MITT\n",
            "No results found for that keyword\n",
            "MITUF\n",
            "No results found for that keyword\n",
            "MIUFY\n",
            "Found 1 results\n",
            "MIY\n",
            "Found 2 results\n",
            "MKC\n",
            "Found 1 results\n",
            "MKSI\n",
            "Found 1 results\n",
            "MKTX\n",
            "Found 1 results\n",
            "MLAB\n",
            "Found 1 results\n",
            "MLI\n",
            "Found 1 results\n",
            "MLLUY\n",
            "Found 1 results\n",
            "MLM\n",
            "Found 1 results\n",
            "MLNK\n",
            "Found 6 results\n",
            "MLP\n",
            "Found 1 results\n",
            "MLR\n",
            "Found 1 results\n",
            "MLVF\n",
            "Found 3 results\n",
            "MMC\n",
            "Found 1 results\n",
            "MMD\n",
            "Found 3 results\n",
            "MMI\n",
            "Found 1 results\n",
            "MMLP\n",
            "Found 2 results\n",
            "MMM\n",
            "No results found for that keyword\n",
            "MMMKF\n",
            "Found 1 results\n",
            "MMP\n",
            "Found 4 results\n",
            "MMS\n",
            "Found 1 results\n",
            "MMSI\n",
            "Found 3 results\n",
            "MMT\n",
            "No results found for that keyword\n",
            "MMTOF\n",
            "Found 1 results\n",
            "MMU\n",
            "Found 1 results\n",
            "MMYT\n",
            "Found 10 results\n",
            "MN\n",
            "Found 1 results\n",
            "MNDO\n",
            "Found 1 results\n",
            "MNKD\n",
            "Found 1 results\n",
            "MNOV\n",
            "Found 2 results\n",
            "MNP\n",
            "Found 2 results\n",
            "MNR\n",
            "Found 1 results\n",
            "MNRO\n",
            "Found 1 results\n",
            "MNST\n",
            "Found 1 results\n",
            "MNTX\n",
            "Found 10 results\n",
            "MO\n",
            "No results found for that keyword\n",
            "MOBL\n",
            "Found 10 results\n",
            "MOD\n",
            "Found 1 results\n",
            "MOFG\n",
            "No results found for that keyword\n",
            "MOG-A\n",
            "Found 4 results\n",
            "MOH\n",
            "Found 1 results\n",
            "MOMO\n",
            "Found 10 results\n",
            "MON\n",
            "Found 1 results\n",
            "MONDY\n",
            "Found 1 results\n",
            "MORN\n",
            "Found 1 results\n",
            "MOS\n",
            "No results found for that keyword\n",
            "MOSY\n",
            "Found 3 results\n",
            "MOV\n",
            "Found 7 results\n",
            "MPA\n",
            "Found 1 results\n",
            "MPAA\n",
            "Found 1 results\n",
            "MPB\n",
            "Found 1 results\n",
            "MPC\n",
            "No results found for that keyword\n",
            "MPFRF\n",
            "No results found for that keyword\n",
            "MPRWP\n",
            "Found 1 results\n",
            "MPV\n",
            "Found 2 results\n",
            "MPW\n",
            "Found 1 results\n",
            "MPWR\n",
            "Found 1 results\n",
            "MPX\n",
            "Found 1 results\n",
            "MQBKY\n",
            "Found 1 results\n",
            "MQT\n",
            "Found 1 results\n",
            "MQY\n",
            "Found 1 results\n",
            "MRAAY\n",
            "Found 1 results\n",
            "MRCC\n",
            "No results found for that keyword\n",
            "MRCR\n",
            "Found 1 results\n",
            "MRCY\n",
            "Found 1 results\n",
            "MRIN\n",
            "Found 2 results\n",
            "MRK\n",
            "No results found for that keyword\n",
            "MRLN\n",
            "Found 1 results\n",
            "MRNS\n",
            "Found 1 results\n",
            "MRO\n",
            "No results found for that keyword\n",
            "MRPRF\n",
            "Found 1 results\n",
            "MRTN\n",
            "Found 1 results\n",
            "MRTX\n",
            "No results found for that keyword\n",
            "MRVGF\n",
            "Found 1 results\n",
            "MRVL\n",
            "No results found for that keyword\n",
            "MS-PF\n",
            "Found 4 results\n",
            "MSA\n",
            "Found 2 results\n",
            "MSB\n",
            "No results found for that keyword\n",
            "MSCC\n",
            "Found 1 results\n",
            "MSCI\n",
            "Found 4 results\n",
            "MSD\n",
            "Found 1 results\n",
            "MSEX\n",
            "No results found for that keyword\n",
            "MSFG\n",
            "Found 1 results\n",
            "MSFT\n",
            "Found 1 results\n",
            "MSI\n",
            "No results found for that keyword\n",
            "MSLOF\n",
            "Found 2 results\n",
            "MSM\n",
            "Found 1 results\n",
            "MSTR\n",
            "Found 10 results\n",
            "MT\n",
            "Found 3 results\n",
            "MTBC\n",
            "Found 2 results\n",
            "MTD\n",
            "Found 1 results\n",
            "MTDR\n",
            "Found 1 results\n",
            "MTEX\n",
            "Found 3 results\n",
            "MTG\n",
            "Found 1 results\n",
            "MTLS\n",
            "Found 3 results\n",
            "MTN\n",
            "No results found for that keyword\n",
            "MTPOF\n",
            "Found 6 results\n",
            "MTR\n",
            "No results found for that keyword\n",
            "MTRAF\n",
            "Found 1 results\n",
            "MTRN\n",
            "Found 1 results\n",
            "MTRX\n",
            "No results found for that keyword\n",
            "MTSFF\n",
            "Found 1 results\n",
            "MTX\n",
            "Found 10 results\n",
            "MU\n",
            "Found 1 results\n",
            "MUA\n",
            "Found 1 results\n",
            "MUC\n",
            "Found 4 results\n",
            "MUE\n",
            "Found 1 results\n",
            "MUI\n",
            "Found 1 results\n",
            "MUJ\n",
            "Found 1 results\n",
            "MUSA\n",
            "No results found for that keyword\n",
            "MUTM\n",
            "Found 1 results\n",
            "MUX\n",
            "Found 1 results\n",
            "MVIS\n",
            "Found 1 results\n",
            "MVT\n",
            "No results found for that keyword\n",
            "MXCHF\n",
            "Found 1 results\n",
            "MXE\n",
            "No results found for that keyword\n",
            "MYC\n",
            "Found 1 results\n",
            "MYD\n",
            "Found 1 results\n",
            "MYE\n",
            "Found 1 results\n",
            "MYGN\n",
            "Found 1 results\n",
            "MYI\n",
            "No results found for that keyword\n",
            "MYJ\n",
            "Found 3 results\n",
            "MYN\n",
            "Found 1 results\n",
            "MYRG\n",
            "Found 1 results\n",
            "MZDAY\n",
            "No results found for that keyword\n",
            "MZF\n",
            "No results found for that keyword\n",
            "MZOR\n",
            "Found 1 results\n",
            "NABZY\n",
            "Found 1 results\n",
            "NAII\n",
            "Found 10 results\n",
            "NAT\n",
            "Found 1 results\n",
            "NATH\n",
            "Found 10 results\n",
            "NATI\n",
            "Found 1 results\n",
            "NATR\n",
            "No results found for that keyword\n",
            "NAUH\n",
            "Found 1 results\n",
            "NBB\n",
            "Found 1 results\n",
            "NBD\n",
            "No results found for that keyword\n",
            "NBGGY\n",
            "Found 1 results\n",
            "NBHC\n",
            "Found 1 results\n",
            "NBIX\n",
            "Found 1 results\n",
            "NBN\n",
            "Found 2 results\n",
            "NBR\n",
            "Found 1 results\n",
            "NBTB\n",
            "Found 10 results\n",
            "NC\n",
            "Found 4 results\n",
            "NCA\n",
            "Found 1 results\n",
            "NCLH\n",
            "No results found for that keyword\n",
            "NCLIP\n",
            "No results found for that keyword\n",
            "NCLTF\n",
            "Found 1 results\n",
            "NCMGY\n",
            "Found 1 results\n",
            "NCMI\n",
            "No results found for that keyword\n",
            "NCTKF\n",
            "Found 1 results\n",
            "NCTY\n",
            "Found 1 results\n",
            "NCV\n",
            "Found 1 results\n",
            "NCZ\n",
            "Found 1 results\n",
            "NDAQ\n",
            "Found 1 results\n",
            "NDEKY\n",
            "Found 1 results\n",
            "NDLS\n",
            "Found 1 results\n",
            "NDP\n",
            "Found 1 results\n",
            "NDSN\n",
            "Found 10 results\n",
            "NE\n",
            "Found 1 results\n",
            "NECB\n",
            "Found 5 results\n",
            "NEE\n",
            "No results found for that keyword\n",
            "NEE-PP\n",
            "Found 2 results\n",
            "NEM\n",
            "Found 9 results\n",
            "NEO\n",
            "Found 3 results\n",
            "NEOG\n",
            "Found 1 results\n",
            "NEON\n",
            "Found 1 results\n",
            "NEPT\n",
            "Found 1 results\n",
            "NERV\n",
            "Found 10 results\n",
            "NEU\n",
            "Found 1 results\n",
            "NEV\n",
            "Found 2 results\n",
            "NEWP\n",
            "Found 3 results\n",
            "NEWT\n",
            "Found 2 results\n",
            "NFG\n",
            "Found 1 results\n",
            "NFJ\n",
            "Found 1 results\n",
            "NFLX\n",
            "No results found for that keyword\n",
            "NFX\n",
            "Found 1 results\n",
            "NGG\n",
            "Found 1 results\n",
            "NGLOY\n",
            "Found 1 results\n",
            "NGS\n",
            "Found 1 results\n",
            "NGVC\n",
            "Found 4 results\n",
            "NHI\n",
            "Found 1 results\n",
            "NHYDY\n",
            "Found 10 results\n",
            "NI\n",
            "Found 1 results\n",
            "NICE\n",
            "Found 1 results\n",
            "NICK\n",
            "Found 2 results\n",
            "NID\n",
            "Found 2 results\n",
            "NIE\n",
            "Found 1 results\n",
            "NILE\n",
            "Found 2 results\n",
            "NIM\n",
            "Found 1 results\n",
            "NIO\n",
            "No results found for that keyword\n",
            "NIPMY\n",
            "Found 1 results\n",
            "NIQ\n",
            "No results found for that keyword\n",
            "NISTF\n",
            "Found 1 results\n",
            "NJR\n",
            "Found 1 results\n",
            "NKSH\n",
            "Found 1 results\n",
            "NKTR\n",
            "Found 10 results\n",
            "NL\n",
            "Found 1 results\n",
            "NLSN\n",
            "No results found for that keyword\n",
            "NLST\n",
            "Found 3 results\n",
            "NLY\n",
            "Found 10 results\n",
            "NM\n",
            "Found 1 results\n",
            "NMFC\n",
            "No results found for that keyword\n",
            "NMHLY\n",
            "Found 2 results\n",
            "NMI\n",
            "Found 1 results\n",
            "NMIH\n",
            "Found 1 results\n",
            "NMS\n",
            "Found 1 results\n",
            "NNBR\n",
            "No results found for that keyword\n",
            "NNGPF\n",
            "Found 1 results\n",
            "NNY\n",
            "Found 5 results\n",
            "NOA\n",
            "Found 1 results\n",
            "NOAH\n",
            "No results found for that keyword\n",
            "NOBGF\n",
            "Found 6 results\n",
            "NOC\n",
            "Found 1 results\n",
            "NOK\n",
            "No results found for that keyword\n",
            "NORNQ\n",
            "Found 10 results\n",
            "NOV\n",
            "Found 2 results\n",
            "NOW\n",
            "No results found for that keyword\n",
            "NOXL\n",
            "Found 10 results\n",
            "NP\n",
            "No results found for that keyword\n",
            "NPEXY\n",
            "Found 1 results\n",
            "NPK\n",
            "Found 1 results\n",
            "NPO\n",
            "Found 1 results\n",
            "NPSNY\n",
            "No results found for that keyword\n",
            "NPTN\n",
            "Found 10 results\n",
            "NR\n",
            "No results found for that keyword\n",
            "NRCIB\n",
            "Found 5 results\n",
            "NRG\n",
            "No results found for that keyword\n",
            "NRILY\n",
            "Found 1 results\n",
            "NRIM\n",
            "Found 1 results\n",
            "NRP\n",
            "Found 1 results\n",
            "NRT\n",
            "No results found for that keyword\n",
            "NRTHF\n",
            "No results found for that keyword\n",
            "NRVTF\n",
            "No results found for that keyword\n",
            "NRZ\n",
            "Found 10 results\n",
            "NS\n",
            "Found 1 results\n",
            "NSANY\n",
            "Found 2 results\n",
            "NSC\n",
            "No results found for that keyword\n",
            "NSEC\n",
            "Found 1 results\n",
            "NSIT\n",
            "Found 1 results\n",
            "NSL\n",
            "No results found for that keyword\n",
            "NSM\n",
            "Found 2 results\n",
            "NSS\n",
            "Found 1 results\n",
            "NSSC\n",
            "Found 1 results\n",
            "NSTG\n",
            "Found 1 results\n",
            "NSYS\n",
            "Found 1 results\n",
            "NTAP\n",
            "Found 1 results\n",
            "NTCT\n",
            "No results found for that keyword\n",
            "NTCXF\n",
            "Found 1 results\n",
            "NTDOY\n",
            "Found 1 results\n",
            "NTES\n",
            "Found 1 results\n",
            "NTGR\n",
            "Found 1 results\n",
            "NTIC\n",
            "Found 1 results\n",
            "NTIOF\n",
            "Found 1 results\n",
            "NTOIY\n",
            "Found 1 results\n",
            "NTP\n",
            "Found 1 results\n",
            "NTRA\n",
            "No results found for that keyword\n",
            "NTRR\n",
            "Found 2 results\n",
            "NTRS\n",
            "Found 1 results\n",
            "NTWK\n",
            "Found 2 results\n",
            "NTZ\n",
            "No results found for that keyword\n",
            "NUAN\n",
            "Found 1 results\n",
            "NURO\n",
            "Found 10 results\n",
            "NUS\n",
            "Found 10 results\n",
            "NUV\n",
            "Found 4 results\n",
            "NUVA\n",
            "Found 1 results\n",
            "NVAX\n",
            "Found 1 results\n",
            "NVCN\n",
            "Found 1 results\n",
            "NVDA\n",
            "Found 1 results\n",
            "NVEC\n",
            "Found 1 results\n",
            "NVFY\n",
            "Found 1 results\n",
            "NVMI\n",
            "Found 2 results\n",
            "NVO\n",
            "Found 2 results\n",
            "NVR\n",
            "Found 1 results\n",
            "NVRO\n",
            "Found 5 results\n",
            "NVS\n",
            "Found 1 results\n",
            "NVZMY\n",
            "Found 1 results\n",
            "NWBI\n",
            "No results found for that keyword\n",
            "NWBO\n",
            "Found 1 results\n",
            "NWFL\n",
            "Found 3 results\n",
            "NWL\n",
            "Found 1 results\n",
            "NWN\n",
            "Found 1 results\n",
            "NWPX\n",
            "Found 1 results\n",
            "NWSA\n",
            "No results found for that keyword\n",
            "NWYF\n",
            "Found 10 results\n",
            "NX\n",
            "Found 1 results\n",
            "NXC\n",
            "No results found for that keyword\n",
            "NXGPY\n",
            "Found 1 results\n",
            "NXN\n",
            "Found 1 results\n",
            "NXPI\n",
            "Found 1 results\n",
            "NXRT\n",
            "Found 3 results\n",
            "NYCB\n",
            "Found 5 results\n",
            "NYMT\n",
            "Found 1 results\n",
            "NYMX\n",
            "No results found for that keyword\n",
            "NYRT\n",
            "Found 10 results\n",
            "O\n",
            "No results found for that keyword\n",
            "OAOFY\n",
            "No results found for that keyword\n",
            "OAS\n",
            "Found 8 results\n",
            "OB\n",
            "No results found for that keyword\n",
            "OBAS\n",
            "No results found for that keyword\n",
            "OBCI\n",
            "Found 10 results\n",
            "OC\n",
            "Found 6 results\n",
            "OCC\n",
            "Found 2 results\n",
            "OCFC\n",
            "No results found for that keyword\n",
            "OCLR\n",
            "Found 1 results\n",
            "OCN\n",
            "Found 1 results\n",
            "OCUL\n",
            "Found 1 results\n",
            "ODFL\n",
            "No results found for that keyword\n",
            "ODMUF\n",
            "Found 1 results\n",
            "ODP\n",
            "Found 1 results\n",
            "OEC\n",
            "Found 1 results\n",
            "OFC\n",
            "Found 1 results\n",
            "OFED\n",
            "Found 1 results\n",
            "OFIX\n",
            "Found 1 results\n",
            "OFLX\n",
            "Found 5 results\n",
            "OFS\n",
            "No results found for that keyword\n",
            "OGFGF\n",
            "Found 2 results\n",
            "OHI\n",
            "Found 1 results\n",
            "OIA\n",
            "Found 2 results\n",
            "OII\n",
            "Found 1 results\n",
            "OIIM\n",
            "Found 1 results\n",
            "OIS\n",
            "No results found for that keyword\n",
            "OJOC\n",
            "Found 1 results\n",
            "OKE\n",
            "No results found for that keyword\n",
            "OLCLY\n",
            "Found 1 results\n",
            "OLED\n",
            "No results found for that keyword\n",
            "OLMIY\n",
            "Found 2 results\n",
            "OLP\n",
            "Found 1 results\n",
            "OMAB\n",
            "Found 2 results\n",
            "OMC\n",
            "Found 1 results\n",
            "OMCL\n",
            "Found 1 results\n",
            "OMER\n",
            "Found 1 results\n",
            "OMEX\n",
            "Found 3 results\n",
            "OMF\n",
            "Found 2 results\n",
            "OMI\n",
            "Found 1 results\n",
            "OMRNY\n",
            "Found 10 results\n",
            "ON\n",
            "Found 3 results\n",
            "ONB\n",
            "Found 1 results\n",
            "ONCY\n",
            "Found 10 results\n",
            "ONE\n",
            "Found 1 results\n",
            "ONEQ\n",
            "Found 1 results\n",
            "ONTX\n",
            "Found 1 results\n",
            "OPHC\n",
            "Found 1 results\n",
            "OPOF\n",
            "Found 1 results\n",
            "OPTT\n",
            "Found 2 results\n",
            "ORAN\n",
            "Found 4 results\n",
            "ORC\n",
            "No results found for that keyword\n",
            "ORENF\n",
            "Found 10 results\n",
            "ORI\n",
            "Found 5 results\n",
            "ORIG\n",
            "Found 1 results\n",
            "ORKLY\n",
            "Found 1 results\n",
            "ORLY\n",
            "Found 1 results\n",
            "ORMP\n",
            "Found 1 results\n",
            "ORRF\n",
            "No results found for that keyword\n",
            "ORYX\n",
            "Found 1 results\n",
            "OSBC\n",
            "No results found for that keyword\n",
            "OSCUF\n",
            "No results found for that keyword\n",
            "OSGSF\n",
            "Found 3 results\n",
            "OSIS\n",
            "Found 1 results\n",
            "OSTK\n",
            "Found 1 results\n",
            "OSUR\n",
            "Found 1 results\n",
            "OTEX\n",
            "Found 1 results\n",
            "OTIC\n",
            "No results found for that keyword\n",
            "OTPGF\n",
            "Found 1 results\n",
            "OTSKY\n",
            "Found 1 results\n",
            "OTTR\n",
            "Found 1 results\n",
            "OVBC\n",
            "No results found for that keyword\n",
            "OVCHF\n",
            "Found 1 results\n",
            "OVLY\n",
            "Found 5 results\n",
            "OXBR\n",
            "Found 7 results\n",
            "OXLC\n",
            "Found 1 results\n",
            "OXM\n",
            "Found 2 results\n",
            "OXY\n",
            "Found 10 results\n",
            "P\n",
            "Found 2 results\n",
            "PAA\n",
            "Found 1 results\n",
            "PAAS\n",
            "Found 1 results\n",
            "PACB\n",
            "Found 2 results\n",
            "PACW\n",
            "Found 6 results\n",
            "PAG\n",
            "Found 1 results\n",
            "PAHC\n",
            "Found 2 results\n",
            "PAI\n",
            "Found 2 results\n",
            "PAM\n",
            "Found 1 results\n",
            "PANL\n",
            "Found 1 results\n",
            "PANW\n",
            "No results found for that keyword\n",
            "PAOTF\n",
            "Found 10 results\n",
            "PAR\n",
            "Found 1 results\n",
            "PATI\n",
            "Found 1 results\n",
            "PATK\n",
            "Found 10 results\n",
            "PAY\n",
            "Found 3 results\n",
            "PAYC\n",
            "Found 1 results\n",
            "PAYX\n",
            "Found 10 results\n",
            "PB\n",
            "Found 4 results\n",
            "PBA\n",
            "No results found for that keyword\n",
            "PBCO\n",
            "No results found for that keyword\n",
            "PBCRY\n",
            "No results found for that keyword\n",
            "PBCT\n",
            "Found 3 results\n",
            "PBF\n",
            "Found 2 results\n",
            "PBH\n",
            "Found 1 results\n",
            "PBHC\n",
            "Found 2 results\n",
            "PBI\n",
            "No results found for that keyword\n",
            "PBIP\n",
            "No results found for that keyword\n",
            "PBLOF\n",
            "No results found for that keyword\n",
            "PBNK\n",
            "Found 1 results\n",
            "PBPB\n",
            "Found 2 results\n",
            "PBR\n",
            "No results found for that keyword\n",
            "PBSFF\n",
            "No results found for that keyword\n",
            "PBSK\n",
            "Found 3 results\n",
            "PBT\n",
            "Found 1 results\n",
            "PBYI\n",
            "Found 1 results\n",
            "PCAR\n",
            "Found 2 results\n",
            "PCF\n",
            "Found 1 results\n",
            "PCH\n",
            "Found 1 results\n",
            "PCN\n",
            "No results found for that keyword\n",
            "PCOM\n",
            "Found 1 results\n",
            "PCQ\n",
            "Found 1 results\n",
            "PCRX\n",
            "Found 1 results\n",
            "PCTY\n",
            "Found 1 results\n",
            "PCYG\n",
            "Found 1 results\n",
            "PCYO\n",
            "Found 1 results\n",
            "PDCE\n",
            "Found 1 results\n",
            "PDCO\n",
            "Found 1 results\n",
            "PDFS\n",
            "Found 1 results\n",
            "PDI\n",
            "Found 1 results\n",
            "PDM\n",
            "Found 2 results\n",
            "PDS\n",
            "Found 1 results\n",
            "PDT\n",
            "Found 3 results\n",
            "PEB\n",
            "Found 1 results\n",
            "PEBK\n",
            "Found 1 results\n",
            "PEBO\n",
            "Found 10 results\n",
            "PEG\n",
            "Found 4 results\n",
            "PEGA\n",
            "Found 4 results\n",
            "PEI\n",
            "Found 10 results\n",
            "PENN\n",
            "Found 5 results\n",
            "PEP\n",
            "Found 8 results\n",
            "PERI\n",
            "No results found for that keyword\n",
            "PERY\n",
            "Found 1 results\n",
            "PETS\n",
            "No results found for that keyword\n",
            "PEXUF\n",
            "Found 1 results\n",
            "PFBC\n",
            "No results found for that keyword\n",
            "PFBX\n",
            "Found 4 results\n",
            "PFD\n",
            "Found 4 results\n",
            "PFE\n",
            "Found 2 results\n",
            "PFG\n",
            "Found 1 results\n",
            "PFGC\n",
            "Found 1 results\n",
            "PFIE\n",
            "Found 1 results\n",
            "PFIN\n",
            "Found 1 results\n",
            "PFIS\n",
            "Found 1 results\n",
            "PFLT\n",
            "Found 1 results\n",
            "PFMT\n",
            "Found 1 results\n",
            "PFN\n",
            "Found 1 results\n",
            "PFO\n",
            "Found 3 results\n",
            "PFS\n",
            "Found 1 results\n",
            "PFSI\n",
            "Found 1 results\n",
            "PFSW\n",
            "Found 10 results\n",
            "PG\n",
            "No results found for that keyword\n",
            "PGEM\n",
            "Found 1 results\n",
            "PGH\n",
            "No results found for that keyword\n",
            "PGPEF\n",
            "No results found for that keyword\n",
            "PGPHF\n",
            "Found 1 results\n",
            "PGRE\n",
            "Found 1 results\n",
            "PGZ\n",
            "Found 10 results\n",
            "PH\n",
            "Found 4 results\n",
            "PHG\n",
            "Found 9 results\n",
            "PHI\n",
            "Found 1 results\n",
            "PHM\n",
            "No results found for that keyword\n",
            "PHOJY\n",
            "Found 1 results\n",
            "PHX\n",
            "Found 3 results\n",
            "PII\n",
            "Found 10 results\n",
            "PIM\n",
            "Found 1 results\n",
            "PINC\n",
            "Found 1 results\n",
            "PKBK\n",
            "Found 1 results\n",
            "PKG\n",
            "Found 1 results\n",
            "PKI\n",
            "No results found for that keyword\n",
            "PKIN\n",
            "No results found for that keyword\n",
            "PKWY\n",
            "Found 1 results\n",
            "PLAB\n",
            "Found 6 results\n",
            "PLAY\n",
            "Found 1 results\n",
            "PLBC\n",
            "Found 1 results\n",
            "PLCE\n",
            "Found 3 results\n",
            "PLD\n",
            "Found 1 results\n",
            "PLOW\n",
            "Found 1 results\n",
            "PLPC\n",
            "No results found for that keyword\n",
            "PLPL\n",
            "Found 1 results\n",
            "PLUG\n",
            "Found 2 results\n",
            "PLUS\n",
            "Found 1 results\n",
            "PLXS\n",
            "Found 10 results\n",
            "PM\n",
            "Found 1 results\n",
            "PMD\n",
            "Found 1 results\n",
            "PME\n",
            "Found 1 results\n",
            "PMF\n",
            "No results found for that keyword\n",
            "PMHV\n",
            "Found 1 results\n",
            "PMM\n",
            "Found 1 results\n",
            "PMO\n",
            "Found 4 results\n",
            "PMT\n",
            "Found 1 results\n",
            "PMX\n",
            "Found 1 results\n",
            "PNBK\n",
            "Found 2 results\n",
            "PNC\n",
            "Found 3 results\n",
            "PNF\n",
            "Found 2 results\n",
            "PNFP\n",
            "No results found for that keyword\n",
            "PNK\n",
            "Found 1 results\n",
            "PNNT\n",
            "Found 2 results\n",
            "PNR\n",
            "Found 1 results\n",
            "PNRG\n",
            "Found 1 results\n",
            "PNW\n",
            "No results found for that keyword\n",
            "PNWRF\n",
            "Found 1 results\n",
            "POAHY\n",
            "Found 1 results\n",
            "PODD\n",
            "Found 1 results\n",
            "POOL\n",
            "Found 10 results\n",
            "POR\n",
            "Found 5 results\n",
            "POST\n",
            "Found 1 results\n",
            "POWI\n",
            "Found 1 results\n",
            "POWL\n",
            "Found 1 results\n",
            "PPBI\n",
            "Found 1 results\n",
            "PPC\n",
            "Found 1 results\n",
            "PPG\n",
            "No results found for that keyword\n",
            "PPRUY\n",
            "Found 1 results\n",
            "PPSI\n",
            "Found 3 results\n",
            "PPT\n",
            "Found 1 results\n",
            "PRAA\n",
            "Found 1 results\n",
            "PRAY\n",
            "No results found for that keyword\n",
            "PRDSF\n",
            "Found 4 results\n",
            "PREC\n",
            "Found 1 results\n",
            "PRFT\n",
            "Found 1 results\n",
            "PRGO\n",
            "Found 1 results\n",
            "PRGS\n",
            "Found 10 results\n",
            "PRIM\n",
            "No results found for that keyword\n",
            "PRKR\n",
            "Found 1 results\n",
            "PRLB\n",
            "No results found for that keyword\n",
            "PRMRF\n",
            "Found 1 results\n",
            "PRMW\n",
            "Found 10 results\n",
            "PRO\n",
            "Found 8 results\n",
            "PROV\n",
            "Found 1 results\n",
            "PRPH\n",
            "Found 1 results\n",
            "PRQR\n",
            "No results found for that keyword\n",
            "PRSS\n",
            "Found 1 results\n",
            "PRTA\n",
            "Found 1 results\n",
            "PRTK\n",
            "Found 1 results\n",
            "PRTS\n",
            "Found 4 results\n",
            "PRU\n",
            "No results found for that keyword\n",
            "PSA-PQ\n",
            "No results found for that keyword\n",
            "PSA-PR\n",
            "Found 1 results\n",
            "PSB\n",
            "No results found for that keyword\n",
            "PSBQ\n",
            "Found 1 results\n",
            "PSEC\n",
            "Found 8 results\n",
            "PSF\n",
            "No results found for that keyword\n",
            "PSIX\n",
            "Found 1 results\n",
            "PSMMY\n",
            "Found 1 results\n",
            "PSMT\n",
            "Found 1 results\n",
            "PSO\n",
            "No results found for that keyword\n",
            "PSTI\n",
            "Found 1 results\n",
            "PSX\n",
            "Found 10 results\n",
            "PT\n",
            "No results found for that keyword\n",
            "PTBRY\n",
            "Found 2 results\n",
            "PTC\n",
            "Found 1 results\n",
            "PTCT\n",
            "Found 1 results\n",
            "PTNR\n",
            "Found 4 results\n",
            "PTR\n",
            "Found 1 results\n",
            "PTSI\n",
            "No results found for that keyword\n",
            "PTTTS\n",
            "Found 1 results\n",
            "PTY\n",
            "Found 1 results\n",
            "PUK\n",
            "Found 2 results\n",
            "PULS\n",
            "No results found for that keyword\n",
            "PVG\n",
            "Found 1 results\n",
            "PVH\n",
            "No results found for that keyword\n",
            "PVNC\n",
            "Found 1 results\n",
            "PWCDF\n",
            "Found 1 results\n",
            "PWOD\n",
            "Found 1 results\n",
            "PWR\n",
            "Found 10 results\n",
            "PX\n",
            "Found 1 results\n",
            "PXD\n",
            "Found 1 results\n",
            "PXLW\n",
            "Found 1 results\n",
            "PYN\n",
            "Found 1 results\n",
            "PYS\n",
            "Found 2 results\n",
            "PZC\n",
            "Found 1 results\n",
            "PZE\n",
            "Found 1 results\n",
            "PZN\n",
            "Found 1 results\n",
            "PZZA\n",
            "No results found for that keyword\n",
            "QBAK\n",
            "No results found for that keyword\n",
            "QBEIF\n",
            "No results found for that keyword\n",
            "QCCO\n",
            "Found 1 results\n",
            "QCOM\n",
            "Found 1 results\n",
            "QCRH\n",
            "Found 1 results\n",
            "QDEL\n",
            "No results found for that keyword\n",
            "QIFTF\n",
            "No results found for that keyword\n",
            "QKLS\n",
            "Found 1 results\n",
            "QLYS\n",
            "Found 1 results\n",
            "QNST\n",
            "Found 1 results\n",
            "QRHC\n",
            "Found 1 results\n",
            "QRVO\n",
            "Found 1 results\n",
            "QSR\n",
            "Found 1 results\n",
            "QTNT\n",
            "Found 1 results\n",
            "QTWO\n",
            "No results found for that keyword\n",
            "QUBSF\n",
            "Found 1 results\n",
            "QUIK\n",
            "Found 1 results\n",
            "QUMU\n",
            "Found 2 results\n",
            "QUOT\n",
            "Found 1 results\n",
            "QURE\n",
            "Found 10 results\n",
            "R\n",
            "Found 1 results\n",
            "RACE\n",
            "Found 1 results\n",
            "RADA\n",
            "Found 1 results\n",
            "RAIFF\n",
            "Found 3 results\n",
            "RAIL\n",
            "Found 2 results\n",
            "RAND\n",
            "No results found for that keyword\n",
            "RANJY\n",
            "No results found for that keyword\n",
            "RANKF\n",
            "Found 5 results\n",
            "RARE\n",
            "Found 1 results\n",
            "RAVE\n",
            "Found 1 results\n",
            "RBA\n",
            "Found 1 results\n",
            "RBCAA\n",
            "Found 1 results\n",
            "RBCN\n",
            "Found 1 results\n",
            "RBGLY\n",
            "Found 3 results\n",
            "RCI\n",
            "Found 1 results\n",
            "RCKY\n",
            "Found 4 results\n",
            "RCL\n",
            "Found 1 results\n",
            "RCMT\n",
            "Found 1 results\n",
            "RCON\n",
            "No results found for that keyword\n",
            "RCRRF\n",
            "Found 1 results\n",
            "RCS\n",
            "Found 1 results\n",
            "RDCM\n",
            "Found 1 results\n",
            "RDHL\n",
            "Found 3 results\n",
            "RDI\n",
            "Found 1 results\n",
            "RDNT\n",
            "No results found for that keyword\n",
            "RDS-A\n",
            "No results found for that keyword\n",
            "RDUS\n",
            "Found 1 results\n",
            "RDWR\n",
            "Found 1 results\n",
            "RDY\n",
            "Found 10 results\n",
            "RE\n",
            "Found 1 results\n",
            "REFR\n",
            "Found 10 results\n",
            "REG\n",
            "Found 8 results\n",
            "REGI\n",
            "Found 1 results\n",
            "REGN\n",
            "No results found for that keyword\n",
            "REIS\n",
            "Found 1 results\n",
            "RELL\n",
            "No results found for that keyword\n",
            "RELV\n",
            "Found 3 results\n",
            "RENT\n",
            "No results found for that keyword\n",
            "REPH\n",
            "No results found for that keyword\n",
            "REPYY\n",
            "Found 2 results\n",
            "RESI\n",
            "No results found for that keyword\n",
            "RESN\n",
            "Found 3 results\n",
            "REVB\n",
            "Found 2 results\n",
            "REX\n",
            "Found 1 results\n",
            "REXR\n",
            "Found 10 results\n",
            "RF\n",
            "Found 2 results\n",
            "RFI\n",
            "Found 1 results\n",
            "RFIL\n",
            "Found 1 results\n",
            "RFP\n",
            "Found 1 results\n",
            "RGA\n",
            "Found 2 results\n",
            "RGC\n",
            "Found 1 results\n",
            "RGCO\n",
            "Found 1 results\n",
            "RGEN\n",
            "Found 1 results\n",
            "RGLD\n",
            "Found 1 results\n",
            "RGLS\n",
            "Found 1 results\n",
            "RGR\n",
            "Found 1 results\n",
            "RGS\n",
            "Found 3 results\n",
            "RGT\n",
            "Found 10 results\n",
            "RH\n",
            "No results found for that keyword\n",
            "RHDGF\n",
            "Found 1 results\n",
            "RHHBY\n",
            "Found 2 results\n",
            "RHI\n",
            "Found 1 results\n",
            "RHP\n",
            "Found 1 results\n",
            "RIBT\n",
            "Found 1 results\n",
            "RICK\n",
            "Found 10 results\n",
            "RIG\n",
            "Found 1 results\n",
            "RIGL\n",
            "Found 3 results\n",
            "RIO\n",
            "Found 1 results\n",
            "RIOCF\n",
            "No results found for that keyword\n",
            "RITT\n",
            "Found 3 results\n",
            "RJF\n",
            "No results found for that keyword\n",
            "RKUNF\n",
            "Found 10 results\n",
            "RL\n",
            "No results found for that keyword\n",
            "RLGY\n",
            "Found 1 results\n",
            "RLI\n",
            "No results found for that keyword\n",
            "RLJE\n",
            "Found 10 results\n",
            "RM\n",
            "Found 1 results\n",
            "RMAX\n",
            "Found 1 results\n",
            "RMBS\n",
            "Found 1 results\n",
            "RMCF\n",
            "Found 1 results\n",
            "RMD\n",
            "No results found for that keyword\n",
            "RMGN\n",
            "No results found for that keyword\n",
            "RMGOF\n",
            "Found 1 results\n",
            "RMTI\n",
            "No results found for that keyword\n",
            "RMYHY\n",
            "Found 2 results\n",
            "RNA\n",
            "No results found for that keyword\n",
            "RNECY\n",
            "No results found for that keyword\n",
            "RNLSY\n",
            "Found 1 results\n",
            "RNP\n",
            "Found 3 results\n",
            "RNR\n",
            "Found 1 results\n",
            "RNST\n",
            "Found 1 results\n",
            "RNWK\n",
            "Found 10 results\n",
            "ROC\n",
            "Found 10 results\n",
            "ROCK\n",
            "Found 1 results\n",
            "ROIC\n",
            "Found 3 results\n",
            "ROK\n",
            "Found 4 results\n",
            "ROL\n",
            "Found 4 results\n",
            "ROLL\n",
            "Found 1 results\n",
            "ROP\n",
            "Found 1 results\n",
            "ROST\n",
            "No results found for that keyword\n",
            "ROYL\n",
            "Found 1 results\n",
            "ROYMY\n",
            "Found 1 results\n",
            "RPM\n",
            "Found 1 results\n",
            "RPRX\n",
            "Found 1 results\n",
            "RQI\n",
            "Found 1 results\n",
            "RRC\n",
            "No results found for that keyword\n",
            "RRD\n",
            "Found 1 results\n",
            "RRGB\n",
            "No results found for that keyword\n",
            "RRTS\n",
            "Found 10 results\n",
            "RS\n",
            "Found 1 results\n",
            "RSG\n",
            "No results found for that keyword\n",
            "RSNHF\n",
            "No results found for that keyword\n",
            "RSPP\n",
            "No results found for that keyword\n",
            "RSYS\n",
            "Found 1 results\n",
            "RUSHA\n",
            "Found 1 results\n",
            "RUTH\n",
            "Found 1 results\n",
            "RVNC\n",
            "Found 1 results\n",
            "RVSB\n",
            "Found 1 results\n",
            "RVT\n",
            "Found 1 results\n",
            "RWLK\n",
            "No results found for that keyword\n",
            "RWMI\n",
            "Found 1 results\n",
            "RWT\n",
            "Found 1 results\n",
            "RXDX\n",
            "No results found for that keyword\n",
            "RXEEY\n",
            "No results found for that keyword\n",
            "RXMD\n",
            "Found 10 results\n",
            "RY\n",
            "Found 1 results\n",
            "RYAAY\n",
            "Found 1 results\n",
            "RYAM\n",
            "Found 1 results\n",
            "RYCEY\n",
            "Found 1 results\n",
            "RYI\n",
            "Found 10 results\n",
            "S\n",
            "Found 3 results\n",
            "SABR\n",
            "No results found for that keyword\n",
            "SAFM\n",
            "No results found for that keyword\n",
            "SAFRY\n",
            "Found 1 results\n",
            "SAFT\n",
            "Found 2 results\n",
            "SAGE\n",
            "Found 1 results\n",
            "SAH\n",
            "No results found for that keyword\n",
            "SAHN\n",
            "Found 1 results\n",
            "SAIA\n",
            "Found 7 results\n",
            "SAL\n",
            "Found 9 results\n",
            "SAM\n",
            "Found 1 results\n",
            "SAMG\n",
            "Found 10 results\n",
            "SAN\n",
            "Found 1 results\n",
            "SANM\n",
            "Found 1 results\n",
            "SANW\n",
            "Found 2 results\n",
            "SAP\n",
            "No results found for that keyword\n",
            "SAPIF\n",
            "No results found for that keyword\n",
            "SAPMY\n",
            "Found 10 results\n",
            "SAR\n",
            "Found 1 results\n",
            "SASR\n",
            "Found 2 results\n",
            "SATS\n",
            "No results found for that keyword\n",
            "SAUHF\n",
            "Found 4 results\n",
            "SAVE\n",
            "No results found for that keyword\n",
            "SAXPF\n",
            "Found 1 results\n",
            "SBAC\n",
            "Found 1 results\n",
            "SBCF\n",
            "Found 1 results\n",
            "SBFG\n",
            "Found 1 results\n",
            "SBGI\n",
            "No results found for that keyword\n",
            "SBGSF\n",
            "No results found for that keyword\n",
            "SBHGF\n",
            "No results found for that keyword\n",
            "SBKFF\n",
            "No results found for that keyword\n",
            "SBKO\n",
            "Found 1 results\n",
            "SBLK\n",
            "Found 2 results\n",
            "SBNY\n",
            "Found 2 results\n",
            "SBR\n",
            "Found 1 results\n",
            "SBRA\n",
            "No results found for that keyword\n",
            "SBRCY\n",
            "Found 3 results\n",
            "SBS\n",
            "Found 1 results\n",
            "SBSI\n",
            "Found 1 results\n",
            "SBUX\n",
            "Found 10 results\n",
            "SC\n",
            "No results found for that keyword\n",
            "SCBFF\n",
            "Found 1 results\n",
            "SCCO\n",
            "Found 2 results\n",
            "SCD\n",
            "No results found for that keyword\n",
            "SCFLF\n",
            "No results found for that keyword\n",
            "SCGLY\n",
            "Found 2 results\n",
            "SCHL\n",
            "Found 2 results\n",
            "SCHN\n",
            "Found 10 results\n",
            "SCHW\n",
            "Found 10 results\n",
            "SCI\n",
            "Found 1 results\n",
            "SCMWY\n",
            "Found 3 results\n",
            "SCOR\n",
            "No results found for that keyword\n",
            "SCPJ\n",
            "Found 1 results\n",
            "SCSC\n",
            "No results found for that keyword\n",
            "SCTBF\n",
            "No results found for that keyword\n",
            "SCTY\n",
            "Found 1 results\n",
            "SCVL\n",
            "No results found for that keyword\n",
            "SCYT\n",
            "Found 1 results\n",
            "SCYX\n",
            "No results found for that keyword\n",
            "SDAD\n",
            "Found 1 results\n",
            "SDVKY\n",
            "No results found for that keyword\n",
            "SDXAY\n",
            "Found 10 results\n",
            "SE\n",
            "Found 3 results\n",
            "SEAC\n",
            "Found 2 results\n",
            "SEAS\n",
            "No results found for that keyword\n",
            "SEBYF\n",
            "Found 5 results\n",
            "SEE\n",
            "Found 1 results\n",
            "SEED\n",
            "No results found for that keyword\n",
            "SEGXF\n",
            "Found 1 results\n",
            "SEIC\n",
            "Found 2 results\n",
            "SEKEY\n",
            "Found 10 results\n",
            "SEM\n",
            "Found 1 results\n",
            "SENEB\n",
            "Found 1 results\n",
            "SEOAY\n",
            "Found 4 results\n",
            "SEV\n",
            "Found 10 results\n",
            "SF\n",
            "Found 1 results\n",
            "SFBC\n",
            "Found 1 results\n",
            "SFBS\n",
            "Found 2 results\n",
            "SFE\n",
            "Found 1 results\n",
            "SFL\n",
            "Found 1 results\n",
            "SFM\n",
            "Found 1 results\n",
            "SFNC\n",
            "No results found for that keyword\n",
            "SFOSF\n",
            "Found 1 results\n",
            "SFST\n",
            "No results found for that keyword\n",
            "SGBI\n",
            "Found 1 results\n",
            "SGBLY\n",
            "Found 1 results\n",
            "SGC\n",
            "No results found for that keyword\n",
            "SGDBF\n",
            "Found 1 results\n",
            "SGEN\n",
            "Found 1 results\n",
            "SGF\n",
            "No results found for that keyword\n",
            "SGFEF\n",
            "Found 1 results\n",
            "SGHIY\n",
            "Found 1 results\n",
            "SGMA\n",
            "No results found for that keyword\n",
            "SGMS\n",
            "Found 1 results\n",
            "SGRP\n",
            "No results found for that keyword\n",
            "SGSOF\n",
            "No results found for that keyword\n",
            "SGTI\n",
            "Found 1 results\n",
            "SGU\n",
            "No results found for that keyword\n",
            "SHASF\n",
            "Found 1 results\n",
            "SHBI\n",
            "Found 1 results\n",
            "SHECY\n",
            "Found 3 results\n",
            "SHEN\n",
            "No results found for that keyword\n",
            "SHERF\n",
            "Found 2 results\n",
            "SHG\n",
            "Found 10 results\n",
            "SHI\n",
            "Found 1 results\n",
            "SHIP\n",
            "No results found for that keyword\n",
            "SHLAF\n",
            "No results found for that keyword\n",
            "SHLD\n",
            "No results found for that keyword\n",
            "SHLM\n",
            "No results found for that keyword\n",
            "SHMDF\n",
            "Found 1 results\n",
            "SHOO\n",
            "No results found for that keyword\n",
            "SHPG\n",
            "No results found for that keyword\n",
            "SHTDF\n",
            "Found 1 results\n",
            "SHW\n",
            "No results found for that keyword\n",
            "SHZUY\n",
            "No results found for that keyword\n",
            "SIBE\n",
            "Found 2 results\n",
            "SID\n",
            "Found 1 results\n",
            "SIEB\n",
            "Found 1 results\n",
            "SIELY\n",
            "Found 1 results\n",
            "SIEN\n",
            "Found 1 results\n",
            "SIFI\n",
            "Found 1 results\n",
            "SIFY\n",
            "Found 10 results\n",
            "SIG\n",
            "Found 3 results\n",
            "SIGI\n",
            "Found 1 results\n",
            "SILC\n",
            "Found 6 results\n",
            "SIMO\n",
            "Found 1 results\n",
            "SINGY\n",
            "No results found for that keyword\n",
            "SIOLF\n",
            "No results found for that keyword\n",
            "SIOPF\n",
            "Found 2 results\n",
            "SIRI\n",
            "Found 2 results\n",
            "SIVB\n",
            "Found 8 results\n",
            "SIX\n",
            "No results found for that keyword\n",
            "SIXD\n",
            "Found 3 results\n",
            "SJI\n",
            "Found 1 results\n",
            "SJR\n",
            "Found 1 results\n",
            "SJT\n",
            "Found 1 results\n",
            "SJW\n",
            "No results found for that keyword\n",
            "SKFOF\n",
            "No results found for that keyword\n",
            "SKHSF\n",
            "No results found for that keyword\n",
            "SKLKF\n",
            "Found 1 results\n",
            "SKM\n",
            "No results found for that keyword\n",
            "SKPI\n",
            "No results found for that keyword\n",
            "SKSUF\n",
            "Found 1 results\n",
            "SKT\n",
            "Found 1 results\n",
            "SKX\n",
            "No results found for that keyword\n",
            "SKYAY\n",
            "No results found for that keyword\n",
            "SKYS\n",
            "Found 4 results\n",
            "SKYW\n",
            "Found 1 results\n",
            "SLAB\n",
            "Found 1 results\n",
            "SLB\n",
            "Found 1 results\n",
            "SLCA\n",
            "Found 1 results\n",
            "SLF\n",
            "Found 7 results\n",
            "SLG\n",
            "Found 1 results\n",
            "SLGN\n",
            "Found 1 results\n",
            "SLLDY\n",
            "Found 2 results\n",
            "SLM\n",
            "Found 1 results\n",
            "SLP\n",
            "Found 1 results\n",
            "SLRC\n",
            "Found 1 results\n",
            "SLVM\n",
            "Found 10 results\n",
            "SM\n",
            "Found 1 results\n",
            "SMBC\n",
            "Found 1 results\n",
            "SMCI\n",
            "No results found for that keyword\n",
            "SMEBF\n",
            "Found 1 results\n",
            "SMED\n",
            "Found 1 results\n",
            "SMFG\n",
            "Found 1 results\n",
            "SMFKY\n",
            "Found 3 results\n",
            "SMG\n",
            "Found 1 results\n",
            "SMGZY\n",
            "Found 10 results\n",
            "SMI\n",
            "Found 6 results\n",
            "SMIT\n",
            "Found 1 results\n",
            "SMLR\n",
            "Found 1 results\n",
            "SMMF\n",
            "No results found for that keyword\n",
            "SMMYY\n",
            "Found 2 results\n",
            "SMP\n",
            "Found 1 results\n",
            "SMRT\n",
            "Found 1 results\n",
            "SMSI\n",
            "Found 1 results\n",
            "SMTC\n",
            "No results found for that keyword\n",
            "SMTOY\n",
            "No results found for that keyword\n",
            "SMTUF\n",
            "Found 2 results\n",
            "SNCR\n",
            "Found 1 results\n",
            "SNFCA\n",
            "No results found for that keyword\n",
            "SNI\n",
            "No results found for that keyword\n",
            "SNMCY\n",
            "No results found for that keyword\n",
            "SNMX\n",
            "Found 1 results\n",
            "SNN\n",
            "Found 2 results\n",
            "SNOW\n",
            "Found 6 results\n",
            "SNP\n",
            "Found 1 results\n",
            "SNPS\n",
            "No results found for that keyword\n",
            "SNSGF\n",
            "Found 1 results\n",
            "SNV\n",
            "Found 10 results\n",
            "SO\n",
            "No results found for that keyword\n",
            "SOCGM\n",
            "No results found for that keyword\n",
            "SODA\n",
            "Found 1 results\n",
            "SOFO\n",
            "Found 4 results\n",
            "SOHO\n",
            "Found 1 results\n",
            "SOHU\n",
            "No results found for that keyword\n",
            "SOHVF\n",
            "Found 1 results\n",
            "SOMMY\n",
            "Found 10 results\n",
            "SON\n",
            "No results found for that keyword\n",
            "SONC\n",
            "Found 2 results\n",
            "SOR\n",
            "Found 10 results\n",
            "SP\n",
            "Found 3 results\n",
            "SPB\n",
            "Found 1 results\n",
            "SPCB\n",
            "No results found for that keyword\n",
            "SPDC\n",
            "Found 7 results\n",
            "SPE\n",
            "Found 8 results\n",
            "SPG\n",
            "Found 6 results\n",
            "SPH\n",
            "No results found for that keyword\n",
            "SPHS\n",
            "No results found for that keyword\n",
            "SPIL\n",
            "Found 1 results\n",
            "SPLK\n",
            "Found 2 results\n",
            "SPLP\n",
            "Found 1 results\n",
            "SPNS\n",
            "Found 1 results\n",
            "SPOK\n",
            "Found 1 results\n",
            "SPPI\n",
            "Found 10 results\n",
            "SPR\n",
            "Found 10 results\n",
            "SPRO\n",
            "Found 1 results\n",
            "SPSC\n",
            "Found 1 results\n",
            "SPTN\n",
            "Found 1 results\n",
            "SPWH\n",
            "Found 1 results\n",
            "SPWR\n",
            "Found 1 results\n",
            "SPXX\n",
            "Found 1 results\n",
            "SQM\n",
            "Found 3 results\n",
            "SRC\n",
            "Found 1 results\n",
            "SRCE\n",
            "Found 1 results\n",
            "SRCL\n",
            "Found 1 results\n",
            "SRDX\n",
            "Found 3 results\n",
            "SRE\n",
            "No results found for that keyword\n",
            "SREDY\n",
            "No results found for that keyword\n",
            "SREV\n",
            "Found 3 results\n",
            "SRG\n",
            "Found 1 results\n",
            "SRGHY\n",
            "Found 1 results\n",
            "SRNE\n",
            "Found 1 results\n",
            "SRPT\n",
            "No results found for that keyword\n",
            "SSAAY\n",
            "Found 3 results\n",
            "SSB\n",
            "Found 1 results\n",
            "SSBI\n",
            "Found 2 results\n",
            "SSD\n",
            "Found 1 results\n",
            "SSNC\n",
            "No results found for that keyword\n",
            "SSNLF\n",
            "Found 1 results\n",
            "SSREY\n",
            "Found 1 results\n",
            "SSTK\n",
            "Found 1 results\n",
            "SSUMY\n",
            "Found 1 results\n",
            "SSYS\n",
            "Found 10 results\n",
            "ST\n",
            "Found 1 results\n",
            "STAA\n",
            "Found 2 results\n",
            "STAG\n",
            "Found 1 results\n",
            "STBA\n",
            "No results found for that keyword\n",
            "STBFF\n",
            "No results found for that keyword\n",
            "STBZ\n",
            "Found 3 results\n",
            "STC\n",
            "Found 1 results\n",
            "STEM\n",
            "No results found for that keyword\n",
            "STFC\n",
            "No results found for that keyword\n",
            "STGPF\n",
            "No results found for that keyword\n",
            "STJPF\n",
            "Found 1 results\n",
            "STKL\n",
            "Found 4 results\n",
            "STL\n",
            "Found 1 results\n",
            "STLD\n",
            "No results found for that keyword\n",
            "STLY\n",
            "Found 1 results\n",
            "STM\n",
            "Found 1 results\n",
            "STNG\n",
            "Found 2 results\n",
            "STOR\n",
            "Found 10 results\n",
            "STRA\n",
            "Found 2 results\n",
            "STRI\n",
            "Found 1 results\n",
            "STRL\n",
            "Found 1 results\n",
            "STRM\n",
            "Found 3 results\n",
            "STRN\n",
            "Found 1 results\n",
            "STRS\n",
            "Found 4 results\n",
            "STT\n",
            "Found 1 results\n",
            "STWD\n",
            "Found 2 results\n",
            "STX\n",
            "Found 1 results\n",
            "STXS\n",
            "No results found for that keyword\n",
            "STZ-B\n",
            "Found 10 results\n",
            "SU\n",
            "No results found for that keyword\n",
            "SUEZY\n",
            "No results found for that keyword\n",
            "SUGBY\n",
            "Found 1 results\n",
            "SUHJY\n",
            "Found 1 results\n",
            "SUI\n",
            "No results found for that keyword\n",
            "SUMR\n",
            "Found 3 results\n",
            "SUNS\n",
            "Found 10 results\n",
            "SUP\n",
            "Found 1 results\n",
            "SUPN\n",
            "Found 5 results\n",
            "SURG\n",
            "Found 1 results\n",
            "SURRY\n",
            "No results found for that keyword\n",
            "SURVF\n",
            "Found 2 results\n",
            "SVA\n",
            "No results found for that keyword\n",
            "SVJTY\n",
            "Found 1 results\n",
            "SVNDY\n",
            "No results found for that keyword\n",
            "SVNLF\n",
            "No results found for that keyword\n",
            "SVTRF\n",
            "Found 1 results\n",
            "SVVC\n",
            "No results found for that keyword\n",
            "SVYSF\n",
            "Found 1 results\n",
            "SWGAY\n",
            "Found 3 results\n",
            "SWIR\n",
            "Found 3 results\n",
            "SWK\n",
            "Found 1 results\n",
            "SWKS\n",
            "Found 1 results\n",
            "SWN\n",
            "Found 1 results\n",
            "SWRBY\n",
            "No results found for that keyword\n",
            "SWSH\n",
            "No results found for that keyword\n",
            "SWSKF\n",
            "Found 1 results\n",
            "SWZ\n",
            "No results found for that keyword\n",
            "SWZNF\n",
            "Found 1 results\n",
            "SXC\n",
            "Found 1 results\n",
            "SXI\n",
            "Found 2 results\n",
            "SXT\n",
            "Found 1 results\n",
            "SYBT\n",
            "Found 1 results\n",
            "SYF\n",
            "Found 1 results\n",
            "SYK\n",
            "Found 2 results\n",
            "SYNA\n",
            "Found 1 results\n",
            "SYNL\n",
            "Found 10 results\n",
            "SYNT\n",
            "Found 1 results\n",
            "SYPR\n",
            "No results found for that keyword\n",
            "SZHIF\n",
            "Found 1 results\n",
            "SZKMY\n",
            "No results found for that keyword\n",
            "SZLMY\n",
            "Found 10 results\n",
            "T\n",
            "Found 10 results\n",
            "TA\n",
            "Found 5 results\n",
            "TAC\n",
            "Found 2 results\n",
            "TACT\n",
            "Found 1 results\n",
            "TAIT\n",
            "Found 3 results\n",
            "TAP\n",
            "Found 1 results\n",
            "TARO\n",
            "Found 4 results\n",
            "TAST\n",
            "Found 2 results\n",
            "TATT\n",
            "Found 1 results\n",
            "TAYD\n",
            "Found 1 results\n",
            "TBBK\n",
            "Found 2 results\n",
            "TBK\n",
            "Found 1 results\n",
            "TBNK\n",
            "Found 1 results\n",
            "TBPH\n",
            "Found 2 results\n",
            "TCBI\n",
            "Found 1 results\n",
            "TCBK\n",
            "No results found for that keyword\n",
            "TCCO\n",
            "Found 1 results\n",
            "TCFC\n",
            "Found 1 results\n",
            "TCI\n",
            "No results found for that keyword\n",
            "TCKGY\n",
            "No results found for that keyword\n",
            "TCLCF\n",
            "Found 1 results\n",
            "TCPC\n",
            "No results found for that keyword\n",
            "TCPTF\n",
            "No results found for that keyword\n",
            "TCTZF\n",
            "Found 1 results\n",
            "TCX\n",
            "No results found for that keyword\n",
            "TCYSF\n",
            "No results found for that keyword\n",
            "TDBOF\n",
            "Found 1 results\n",
            "TDF\n",
            "Found 1 results\n",
            "TDG\n",
            "No results found for that keyword\n",
            "TDHOY\n",
            "Found 7 results\n",
            "TDS\n",
            "Found 1 results\n",
            "TDY\n",
            "Found 1 results\n",
            "TEDU\n",
            "Found 1 results\n",
            "TEF\n",
            "Found 1 results\n",
            "TEI\n",
            "Found 10 results\n",
            "TEL\n",
            "Found 1 results\n",
            "TELNY\n",
            "Found 10 results\n",
            "TEN\n",
            "Found 1 results\n",
            "TENX\n",
            "Found 1 results\n",
            "TEO\n",
            "No results found for that keyword\n",
            "TEPCY\n",
            "Found 1 results\n",
            "TESS\n",
            "No results found for that keyword\n",
            "TEUFF\n",
            "Found 1 results\n",
            "TEVA\n",
            "No results found for that keyword\n",
            "TFIV\n",
            "Found 5 results\n",
            "TGA\n",
            "No results found for that keyword\n",
            "TGEN\n",
            "Found 1 results\n",
            "TGH\n",
            "Found 2 results\n",
            "TGI\n",
            "Found 1 results\n",
            "TGLS\n",
            "Found 1 results\n",
            "TGOPY\n",
            "No results found for that keyword\n",
            "TGP\n",
            "Found 2 results\n",
            "TGS\n",
            "Found 1 results\n",
            "TGTX\n",
            "Found 8 results\n",
            "THC\n",
            "Found 1 results\n",
            "THFF\n",
            "Found 1 results\n",
            "THG\n",
            "No results found for that keyword\n",
            "THLEF\n",
            "Found 5 results\n",
            "THO\n",
            "Found 1 results\n",
            "THQ\n",
            "Found 9 results\n",
            "THR\n",
            "Found 1 results\n",
            "THRX\n",
            "Found 1 results\n",
            "THS\n",
            "No results found for that keyword\n",
            "THYCF\n",
            "No results found for that keyword\n",
            "TIACF\n",
            "Found 1 results\n",
            "TIGR\n",
            "Found 2 results\n",
            "TILE\n",
            "Found 1 results\n",
            "TITN\n",
            "Found 1 results\n",
            "TJX\n",
            "No results found for that keyword\n",
            "TKAMY\n",
            "Found 1 results\n",
            "TKC\n",
            "No results found for that keyword\n",
            "TKGSF\n",
            "Found 1 results\n",
            "TKOMY\n",
            "Found 2 results\n",
            "TLK\n",
            "No results found for that keyword\n",
            "TLOG\n",
            "No results found for that keyword\n",
            "TLPC\n",
            "Found 10 results\n",
            "TM\n",
            "Found 1 results\n",
            "TMHC\n",
            "No results found for that keyword\n",
            "TMIX\n",
            "Found 1 results\n",
            "TMO\n",
            "Found 1 results\n",
            "TMST\n",
            "Found 1 results\n",
            "TMUS\n",
            "No results found for that keyword\n",
            "TNABF\n",
            "Found 1 results\n",
            "TNC\n",
            "Found 1 results\n",
            "TNDM\n",
            "No results found for that keyword\n",
            "TNH\n",
            "Found 1 results\n",
            "TNK\n",
            "Found 1 results\n",
            "TNP\n",
            "Found 1 results\n",
            "TNXP\n",
            "Found 1 results\n",
            "TOELY\n",
            "No results found for that keyword\n",
            "TOKUF\n",
            "Found 2 results\n",
            "TOL\n",
            "No results found for that keyword\n",
            "TONPF\n",
            "Found 1 results\n",
            "TOPS\n",
            "Found 1 results\n",
            "TORM\n",
            "Found 1 results\n",
            "TOUR\n",
            "Found 2 results\n",
            "TOWN\n",
            "Found 1 results\n",
            "TPC\n",
            "No results found for that keyword\n",
            "TPHIF\n",
            "No results found for that keyword\n",
            "TPIL\n",
            "Found 3 results\n",
            "TPL\n",
            "No results found for that keyword\n",
            "TPPPF\n",
            "Found 1 results\n",
            "TPVG\n",
            "Found 1 results\n",
            "TPX\n",
            "Found 1 results\n",
            "TPZ\n",
            "Found 10 results\n",
            "TR\n",
            "No results found for that keyword\n",
            "TRAUF\n",
            "Found 4 results\n",
            "TRC\n",
            "No results found for that keyword\n",
            "TREC\n",
            "Found 2 results\n",
            "TREE\n",
            "No results found for that keyword\n",
            "TRGNF\n",
            "Found 1 results\n",
            "TRGP\n",
            "Found 10 results\n",
            "TRI\n",
            "Found 1 results\n",
            "TRIB\n",
            "Found 4 results\n",
            "TRIP\n",
            "Found 1 results\n",
            "TRMK\n",
            "Found 1 results\n",
            "TRMR\n",
            "Found 4 results\n",
            "TRN\n",
            "Found 1 results\n",
            "TRNO\n",
            "Found 1 results\n",
            "TRNS\n",
            "Found 1 results\n",
            "TROW\n",
            "Found 1 results\n",
            "TRQ\n",
            "Found 2 results\n",
            "TRS\n",
            "Found 1 results\n",
            "TRST\n",
            "Found 10 results\n",
            "TRUE\n",
            "Found 4 results\n",
            "TRV\n",
            "Found 1 results\n",
            "TRVN\n",
            "Found 1 results\n",
            "TRYIY\n",
            "Found 10 results\n",
            "TS\n",
            "Found 1 results\n",
            "TSBK\n",
            "Found 3 results\n",
            "TSC\n",
            "Found 1 results\n",
            "TSCDY\n",
            "Found 1 results\n",
            "TSCO\n",
            "Found 2 results\n",
            "TSE\n",
            "Found 1 results\n",
            "TSEM\n",
            "Found 5 results\n",
            "TSI\n",
            "Found 1 results\n",
            "TSLA\n",
            "Found 1 results\n",
            "TSLX\n",
            "Found 1 results\n",
            "TSM\n",
            "Found 1 results\n",
            "TSN\n",
            "Found 1 results\n",
            "TSQ\n",
            "Found 1 results\n",
            "TSRI\n",
            "No results found for that keyword\n",
            "TSSI\n",
            "Found 1 results\n",
            "TTEC\n",
            "Found 1 results\n",
            "TTEK\n",
            "No results found for that keyword\n",
            "TTGPF\n",
            "Found 1 results\n",
            "TTGT\n",
            "Found 2 results\n",
            "TTM\n",
            "Found 1 results\n",
            "TTMI\n",
            "Found 1 results\n",
            "TTOO\n",
            "Found 1 results\n",
            "TTP\n",
            "Found 1 results\n",
            "TTWO\n",
            "No results found for that keyword\n",
            "TUIFF\n",
            "Found 1 results\n",
            "TUP\n",
            "Found 4 results\n",
            "TV\n",
            "Found 1 results\n",
            "TVE\n",
            "No results found for that keyword\n",
            "TVFCF\n",
            "No results found for that keyword\n",
            "TVPT\n",
            "Found 10 results\n",
            "TW\n",
            "No results found for that keyword\n",
            "TWER\n",
            "Found 10 results\n",
            "TWI\n",
            "Found 5 results\n",
            "TWIN\n",
            "Found 7 results\n",
            "TWO\n",
            "Found 1 results\n",
            "TWODY\n",
            "Found 1 results\n",
            "TWOU\n",
            "Found 1 results\n",
            "TWTR\n",
            "No results found for that keyword\n",
            "TWX\n",
            "Found 6 results\n",
            "TX\n",
            "Found 1 results\n",
            "TXN\n",
            "Found 1 results\n",
            "TXRH\n",
            "Found 1 results\n",
            "TXT\n",
            "Found 10 results\n",
            "TY\n",
            "Found 1 results\n",
            "TYG\n",
            "No results found for that keyword\n",
            "TYHOF\n",
            "No results found for that keyword\n",
            "TYIDY\n",
            "Found 1 results\n",
            "TYL\n",
            "Found 1 results\n",
            "TZOO\n",
            "Found 10 results\n",
            "UA\n",
            "No results found for that keyword\n",
            "UABK\n",
            "Found 1 results\n",
            "UAL\n",
            "Found 1 results\n",
            "UBA\n",
            "Found 1 results\n",
            "UBCP\n",
            "Found 1 results\n",
            "UBFO\n",
            "Found 1 results\n",
            "UBOH\n",
            "Found 1 results\n",
            "UBSI\n",
            "No results found for that keyword\n",
            "UCBA\n",
            "Found 2 results\n",
            "UCBI\n",
            "Found 1 results\n",
            "UCBJY\n",
            "Found 1 results\n",
            "UCTT\n",
            "Found 1 results\n",
            "UDR\n",
            "No results found for that keyword\n",
            "UEEC\n",
            "Found 1 results\n",
            "UEIC\n",
            "No results found for that keyword\n",
            "UEPS\n",
            "Found 1 results\n",
            "UFCS\n",
            "Found 1 results\n",
            "UFPI\n",
            "Found 1 results\n",
            "UFPT\n",
            "Found 1 results\n",
            "UGP\n",
            "Found 1 results\n",
            "UHS\n",
            "Found 1 results\n",
            "UHT\n",
            "Found 1 results\n",
            "UIHC\n",
            "Found 10 results\n",
            "UL\n",
            "Found 1 results\n",
            "ULBI\n",
            "Found 1 results\n",
            "ULTA\n",
            "Found 6 results\n",
            "ULTR\n",
            "Found 1 results\n",
            "UMBF\n",
            "Found 1 results\n",
            "UMC\n",
            "Found 1 results\n",
            "UMH\n",
            "Found 1 results\n",
            "UMICY\n",
            "Found 1 results\n",
            "UMPQ\n",
            "Found 1 results\n",
            "UNAM\n",
            "Found 1 results\n",
            "UNB\n",
            "No results found for that keyword\n",
            "UNBK\n",
            "No results found for that keyword\n",
            "UNCFF\n",
            "Found 1 results\n",
            "UNFI\n",
            "Found 1 results\n",
            "UNH\n",
            "Found 2 results\n",
            "UNM\n",
            "Found 1 results\n",
            "UNP\n",
            "No results found for that keyword\n",
            "UNPSF\n",
            "Found 1 results\n",
            "UNTY\n",
            "Found 1 results\n",
            "UOVEY\n",
            "Found 1 results\n",
            "UPLD\n",
            "Found 2 results\n",
            "UPS\n",
            "Found 1 results\n",
            "URBN\n",
            "Found 1 results\n",
            "URI\n",
            "Found 1 results\n",
            "USAK\n",
            "Found 1 results\n",
            "USAP\n",
            "Found 4 results\n",
            "USB\n",
            "No results found for that keyword\n",
            "USCS\n",
            "Found 1 results\n",
            "USEG\n",
            "Found 1 results\n",
            "USLM\n",
            "Found 5 results\n",
            "USM\n",
            "Found 1 results\n",
            "USNA\n",
            "Found 1 results\n",
            "USPH\n",
            "Found 1 results\n",
            "UTF\n",
            "Found 1 results\n",
            "UTHR\n",
            "Found 3 results\n",
            "UTI\n",
            "Found 1 results\n",
            "UTMD\n",
            "Found 1 results\n",
            "UTSI\n",
            "Found 1 results\n",
            "UUGRY\n",
            "Found 1 results\n",
            "UVSP\n",
            "Found 10 results\n",
            "V\n",
            "Found 4 results\n",
            "VAC\n",
            "Found 4 results\n",
            "VALU\n",
            "Found 2 results\n",
            "VBF\n",
            "Found 1 results\n",
            "VBFC\n",
            "Found 1 results\n",
            "VBIV\n",
            "Found 1 results\n",
            "VBLT\n",
            "Found 1 results\n",
            "VBTX\n",
            "Found 1 results\n",
            "VCEL\n",
            "No results found for that keyword\n",
            "VCISY\n",
            "No results found for that keyword\n",
            "VCRA\n",
            "Found 1 results\n",
            "VCV\n",
            "Found 1 results\n",
            "VCYT\n",
            "No results found for that keyword\n",
            "VDNRF\n",
            "Found 1 results\n",
            "VECO\n",
            "Found 1 results\n",
            "VEEV\n",
            "No results found for that keyword\n",
            "VEOEY\n",
            "Found 1 results\n",
            "VET\n",
            "Found 1 results\n",
            "VFC\n",
            "Found 10 results\n",
            "VG\n",
            "Found 2 results\n",
            "VGI\n",
            "Found 1 results\n",
            "VGM\n",
            "Found 1 results\n",
            "VGR\n",
            "Found 1 results\n",
            "VICR\n",
            "Found 3 results\n",
            "VIDE\n",
            "No results found for that keyword\n",
            "VIEWF\n",
            "Found 1 results\n",
            "VIPS\n",
            "Found 10 results\n",
            "VIV\n",
            "No results found for that keyword\n",
            "VIVEF\n",
            "Found 3 results\n",
            "VIVO\n",
            "Found 1 results\n",
            "VKQ\n",
            "No results found for that keyword\n",
            "VLEEY\n",
            "Found 1 results\n",
            "VLGEA\n",
            "No results found for that keyword\n",
            "VLNSF\n",
            "No results found for that keyword\n",
            "VLPNF\n",
            "Found 3 results\n",
            "VLT\n",
            "Found 3 results\n",
            "VLY\n",
            "Found 4 results\n",
            "VMC\n",
            "Found 1 results\n",
            "VMI\n",
            "Found 2 results\n",
            "VMO\n",
            "Found 1 results\n",
            "VMW\n",
            "Found 1 results\n",
            "VNCE\n",
            "Found 1 results\n",
            "VNDA\n",
            "Found 1 results\n",
            "VNET\n",
            "Found 1 results\n",
            "VNOM\n",
            "Found 1 results\n",
            "VNRFY\n",
            "Found 2 results\n",
            "VOD\n",
            "Found 1 results\n",
            "VOXX\n",
            "Found 7 results\n",
            "VOYA\n",
            "Found 1 results\n",
            "VPG\n",
            "No results found for that keyword\n",
            "VPRIF\n",
            "Found 1 results\n",
            "VPV\n",
            "Found 10 results\n",
            "VR\n",
            "Found 5 results\n",
            "VRA\n",
            "Found 1 results\n",
            "VRNS\n",
            "Found 1 results\n",
            "VRNT\n",
            "Found 1 results\n",
            "VRSK\n",
            "Found 1 results\n",
            "VRSN\n",
            "No results found for that keyword\n",
            "VRTB\n",
            "Found 1 results\n",
            "VRTS\n",
            "Found 1 results\n",
            "VRTX\n",
            "Found 1 results\n",
            "VSAT\n",
            "Found 1 results\n",
            "VSEC\n",
            "Found 1 results\n",
            "VSH\n",
            "Found 1 results\n",
            "VSTM\n",
            "Found 2 results\n",
            "VTN\n",
            "Found 1 results\n",
            "VTNR\n",
            "Found 3 results\n",
            "VTR\n",
            "Found 1 results\n",
            "VULC\n",
            "No results found for that keyword\n",
            "VUPPF\n",
            "No results found for that keyword\n",
            "VVDB\n",
            "Found 1 results\n",
            "VVR\n",
            "No results found for that keyword\n",
            "VWSYF\n",
            "Found 3 results\n",
            "VZ\n",
            "Found 3 results\n",
            "WAB\n",
            "Found 1 results\n",
            "WABC\n",
            "Found 2 results\n",
            "WAFD\n",
            "Found 10 results\n",
            "WAL\n",
            "Found 4 results\n",
            "WASH\n",
            "Found 7 results\n",
            "WAT\n",
            "Found 2 results\n",
            "WATT\n",
            "No results found for that keyword\n",
            "WAWL\n",
            "No results found for that keyword\n",
            "WAYN\n",
            "Found 10 results\n",
            "WB\n",
            "Found 2 results\n",
            "WBA\n",
            "No results found for that keyword\n",
            "WBK\n",
            "Found 1 results\n",
            "WBS\n",
            "Found 2 results\n",
            "WCC\n",
            "Found 1 results\n",
            "WCN\n",
            "Found 9 results\n",
            "WD\n",
            "Found 1 results\n",
            "WDAY\n",
            "Found 1 results\n",
            "WDC\n",
            "Found 4 results\n",
            "WEB\n",
            "Found 1 results\n",
            "WEC\n",
            "No results found for that keyword\n",
            "WEFIF\n",
            "Found 1 results\n",
            "WEICY\n",
            "No results found for that keyword\n",
            "WELPP\n",
            "Found 1 results\n",
            "WEN\n",
            "Found 1 results\n",
            "WERN\n",
            "Found 10 results\n",
            "WES\n",
            "Found 1 results\n",
            "WETF\n",
            "Found 1 results\n",
            "WEYS\n",
            "Found 1 results\n",
            "WFAFY\n",
            "Found 6 results\n",
            "WFC\n",
            "No results found for that keyword\n",
            "WGL\n",
            "Found 1 results\n",
            "WGO\n",
            "Found 1 results\n",
            "WHF\n",
            "Found 2 results\n",
            "WHG\n",
            "Found 1 results\n",
            "WHLM\n",
            "Found 4 results\n",
            "WHLR\n",
            "Found 1 results\n",
            "WHR\n",
            "No results found for that keyword\n",
            "WHZT\n",
            "Found 1 results\n",
            "WIA\n",
            "Found 1 results\n",
            "WINA\n",
            "Found 1 results\n",
            "WING\n",
            "Found 2 results\n",
            "WIRE\n",
            "Found 1 results\n",
            "WIT\n",
            "Found 1 results\n",
            "WIX\n",
            "Found 1 results\n",
            "WJRYY\n",
            "Found 1 results\n",
            "WLDN\n",
            "Found 1 results\n",
            "WLFC\n",
            "Found 2 results\n",
            "WLK\n",
            "Found 1 results\n",
            "WLKP\n",
            "No results found for that keyword\n",
            "WLKR\n",
            "No results found for that keyword\n",
            "WLL\n",
            "No results found for that keyword\n",
            "WLMIF\n",
            "Found 10 results\n",
            "WM\n",
            "Found 1 results\n",
            "WMB\n",
            "Found 1 results\n",
            "WMC\n",
            "Found 1 results\n",
            "WMK\n",
            "Found 1 results\n",
            "WMS\n",
            "Found 1 results\n",
            "WMT\n",
            "Found 1 results\n",
            "WNC\n",
            "No results found for that keyword\n",
            "WNDLF\n",
            "Found 1 results\n",
            "WNGRF\n",
            "Found 1 results\n",
            "WNS\n",
            "No results found for that keyword\n",
            "WOLWF\n",
            "Found 1 results\n",
            "WOOF\n",
            "No results found for that keyword\n",
            "WOPEY\n",
            "Found 10 results\n",
            "WOR\n",
            "Found 7 results\n",
            "WPC\n",
            "No results found for that keyword\n",
            "WPZ\n",
            "Found 2 results\n",
            "WRB\n",
            "Found 1 results\n",
            "WRE\n",
            "Found 1 results\n",
            "WRK\n",
            "Found 1 results\n",
            "WRLD\n",
            "No results found for that keyword\n",
            "WRTBF\n",
            "Found 2 results\n",
            "WSBC\n",
            "Found 1 results\n",
            "WSBF\n",
            "No results found for that keyword\n",
            "WSCI\n",
            "Found 1 results\n",
            "WSFS\n",
            "No results found for that keyword\n",
            "WSPOF\n",
            "Found 1 results\n",
            "WSR\n",
            "Found 2 results\n",
            "WST\n",
            "Found 1 results\n",
            "WSTG\n",
            "No results found for that keyword\n",
            "WSTL\n",
            "Found 1 results\n",
            "WTBA\n",
            "No results found for that keyword\n",
            "WTBCF\n",
            "Found 3 results\n",
            "WTFC\n",
            "Found 1 results\n",
            "WTI\n",
            "Found 1 results\n",
            "WTKWY\n",
            "Found 5 results\n",
            "WTM\n",
            "Found 1 results\n",
            "WTS\n",
            "Found 1 results\n",
            "WTW\n",
            "Found 3 results\n",
            "WU\n",
            "No results found for that keyword\n",
            "WVFC\n",
            "Found 2 results\n",
            "WVVI\n",
            "Found 1 results\n",
            "WWD\n",
            "Found 1 results\n",
            "WWE\n",
            "No results found for that keyword\n",
            "WWNTF\n",
            "No results found for that keyword\n",
            "WWSG\n",
            "Found 1 results\n",
            "WWW\n",
            "Found 6 results\n",
            "WY\n",
            "Found 2 results\n",
            "WYNN\n",
            "Found 10 results\n",
            "X\n",
            "No results found for that keyword\n",
            "XCRA\n",
            "Found 4 results\n",
            "XEL\n",
            "Found 3 results\n",
            "XENE\n",
            "No results found for that keyword\n",
            "XENT\n",
            "Found 1 results\n",
            "XHR\n",
            "Found 2 results\n",
            "XIN\n",
            "Found 10 results\n",
            "XL\n",
            "No results found for that keyword\n",
            "XLEFF\n",
            "No results found for that keyword\n",
            "XLNX\n",
            "Found 1 results\n",
            "XNCR\n",
            "Found 1 results\n",
            "XNET\n",
            "Found 5 results\n",
            "XOM\n",
            "Found 3 results\n",
            "XOMA\n",
            "No results found for that keyword\n",
            "XPLR\n",
            "Found 3 results\n",
            "XPO\n",
            "Found 1 results\n",
            "XTLB\n",
            "Found 3 results\n",
            "XYL\n",
            "No results found for that keyword\n",
            "YACAF\n",
            "No results found for that keyword\n",
            "YAMHF\n",
            "No results found for that keyword\n",
            "YARIY\n",
            "No results found for that keyword\n",
            "YATRF\n",
            "No results found for that keyword\n",
            "YFGSF\n",
            "No results found for that keyword\n",
            "YMDAF\n",
            "Found 1 results\n",
            "YNDX\n",
            "Found 1 results\n",
            "YORW\n",
            "Found 1 results\n",
            "YPF\n",
            "Found 4 results\n",
            "YUM\n",
            "No results found for that keyword\n",
            "YWGRF\n",
            "Found 2 results\n",
            "YY\n",
            "Found 10 results\n",
            "Z\n",
            "No results found for that keyword\n",
            "ZAZA\n",
            "Found 1 results\n",
            "ZBH\n",
            "Found 1 results\n",
            "ZBRA\n",
            "No results found for that keyword\n",
            "ZDPY\n",
            "Found 1 results\n",
            "ZEUS\n",
            "No results found for that keyword\n",
            "ZGNX\n",
            "No results found for that keyword\n",
            "ZIJMF\n",
            "Found 4 results\n",
            "ZION\n",
            "No results found for that keyword\n",
            "ZIOP\n",
            "No results found for that keyword\n",
            "ZIXI\n",
            "No results found for that keyword\n",
            "ZNGA\n",
            "Found 1 results\n",
            "ZNH\n",
            "No results found for that keyword\n",
            "ZOES\n",
            "Found 1 results\n",
            "ZSHGY\n",
            "Found 1 results\n",
            "ZTS\n",
            "Found 1 results\n",
            "ZUMZ\n",
            "Found 1 results\n",
            "ZURVY\n",
            "total number of stocks found: 5453\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "stock_info_fname=\"stock_info-aug28.txt\"\n",
        "all_symbols=[]\n",
        "stock_groups=[\"sp500\",\"nyse\",\"nasdaq\",\"forbes2000\"]\n",
        "for stg in stock_groups:\n",
        "  cur_root_dir=os.path.join(\"stock_market_data\",stg,\"csv\")\n",
        "  cur_symbols=[v.split(\".\")[0] for v in os.listdir(cur_root_dir)]\n",
        "  all_symbols.extend(cur_symbols)\n",
        "all_symbols=sorted(list(set(all_symbols)))\n",
        "all_stock_info_items=[]\n",
        "# cur_symbols=[v.split(\".\")[0] for v in os.listdir(root_dir)]\n",
        "print(len(all_symbols),all_symbols[:10])\n",
        "for sym0 in all_symbols:\n",
        "  cur_stock_info=rs.robinhood.stocks.find_instrument_data(sym0)\n",
        "  print(sym0)\n",
        "  for info1 in cur_stock_info:\n",
        "    if info1 in all_stock_info_items: continue\n",
        "    all_stock_info_items.append(info1)\n",
        "stock_info_fopen=open(stock_info_fname,\"w\")\n",
        "print(\"total number of stocks found:\", len(all_stock_info_items))\n",
        "for a in all_stock_info_items:\n",
        "  #print(a[\"symbol\"])\n",
        "  json_content=json.dumps(a)\n",
        "  stock_info_fopen.write(json_content+\"\\n\")\n",
        "stock_info_fopen.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTT5BOT905o"
      },
      "source": [
        "#Get daily data for stocks - Aug 22\n",
        "Run every day to get the historic data for stocks of interest - identify stocks less than a certain value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a-oKmHZVy93",
        "outputId": "ab5065cd-d110-4bc5-d794-11671085d98c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n",
            "500\n",
            "550\n",
            "600\n",
            "650\n",
            "700\n",
            "750\n",
            "800\n",
            "850\n",
            "900\n",
            "950\n",
            "1000\n",
            "1050\n",
            "1100\n",
            "1150\n",
            "1200\n",
            "1250\n",
            "1300\n",
            "1350\n",
            "1400\n",
            "1450\n",
            "1500\n",
            "1550\n",
            "1600\n",
            "1650\n",
            "1700\n",
            "1750\n",
            "1800\n",
            "1850\n",
            "1900\n",
            "1950\n",
            "2000\n",
            "2050\n",
            "2100\n",
            "2150\n",
            "2200\n",
            "2250\n",
            "2300\n",
            "2350\n",
            "2400\n",
            "2450\n",
            "2500\n",
            "2550\n",
            "2600\n",
            "2650\n",
            "2700\n",
            "2750\n",
            "2800\n",
            "2850\n",
            "2900\n",
            "2950\n",
            "3000\n",
            "3050\n",
            "3100\n",
            "3150\n",
            "3200\n",
            "3250\n",
            "3300\n",
            "3350\n",
            "3400\n",
            "3450\n",
            "3500\n",
            "3550\n",
            "3600\n",
            "3650\n",
            "3700\n",
            "3750\n",
            "3800\n",
            "3850\n",
            "3900\n",
            "3950\n",
            "4000\n",
            "4050\n",
            "4100\n",
            "4150\n",
            "4200\n",
            "4250\n",
            "4300\n",
            "4350\n",
            "4400\n",
            "4450\n",
            "4500\n",
            "4550\n",
            "4600\n",
            "4650\n",
            "4700\n",
            "4750\n",
            "4800\n",
            "4850\n",
            "4900\n",
            "4950\n",
            "5000\n",
            "5050\n",
            "5100\n",
            "5150\n",
            "5200\n",
            "5250\n",
            "5300\n",
            "5350\n",
            "5400\n",
            "5450\n",
            "valid_stocks 2662\n"
          ]
        }
      ],
      "source": [
        "#identifying only valid stocks\n",
        "import json\n",
        "\n",
        "max_price=25\n",
        "stock_info_fname=\"stock_info-aug28.txt\"\n",
        "stock_info_fopen=open(stock_info_fname)\n",
        "valid_stocks=[]\n",
        "for f_i,f_line in enumerate(stock_info_fopen):\n",
        "  if f_i%50==0 and f_i>0: \n",
        "    print(f_i)\n",
        "\n",
        "  cur_info_dict=json.loads(f_line)\n",
        "  if cur_info_dict==None: continue\n",
        "  sym0=cur_info_dict.get(\"symbol\") \n",
        "  if sym0==None: continue\n",
        "  is_tradeable=cur_info_dict[\"tradeable\"] \n",
        "  if not is_tradeable: continue\n",
        "  latest_price_val=None\n",
        "  latest_price=rs.robinhood.stocks.get_latest_price(sym0)\n",
        "  if len(latest_price)>0 and latest_price!=None and latest_price[0]!=None: latest_price_val=float(latest_price[0])\n",
        "  if latest_price_val==None: continue\n",
        "  if latest_price_val>max_price: continue\n",
        "  #print(sym0, latest_price_val)\n",
        "  cur_info_dict[\"latest_price\"]=latest_price_val\n",
        "  valid_stocks.append((sym0, cur_info_dict, latest_price_val))\n",
        "\n",
        "stock_info_fopen.close()\n",
        "valid_stocks.sort(key=lambda x:x[-1])\n",
        "\n",
        "valid_stock_fname=\"valid_stock_25_less_aug_28.txt\"\n",
        "valid_stock_fopen=open(valid_stock_fname,\"w\")\n",
        "\n",
        "for a in valid_stocks: \n",
        "  valid_stock_fopen.write(json.dumps(a[1])+\"\\n\")\n",
        "valid_stock_fopen.close()\n",
        "print(\"valid_stocks\",len(valid_stocks))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting the actual daily values of valid stocks - Sep 22"
      ],
      "metadata": {
        "id": "kVKlxFdlS9kr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjLwi2Ym7IvI",
        "outputId": "6970747e-f895-41c8-e5af-19ee9d7d4a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AMPE True\n",
            "TMBR True\n",
            "ALNA True\n",
            "CSCW True\n",
            "TTOO True\n",
            "KPRX True\n",
            "RMED True\n",
            "MOHO True\n",
            "IRS+ True\n",
            "VGFC True\n",
            "ATHX True\n",
            "NBRV True\n",
            "EYESW True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=EYESW&interval=day&span=3month&bounds=regular\n",
            "ALRN True\n",
            "OXBRW True\n",
            "OBSV True\n",
            "SLRX True\n",
            "VBLT True\n",
            "QTNT True\n",
            "TRVN True\n",
            "PIXY True\n",
            "CLXT True\n",
            "NBY True\n",
            "CNSP True\n",
            "TENX True\n",
            "SPCB True\n",
            "CFRX True\n",
            "OBLG True\n",
            "GFAI True\n",
            "AUMN True\n",
            "NILE True\n",
            "TYME True\n",
            "HGEN True\n",
            "RIBT True\n",
            "MRKR True\n",
            "TOPS True\n",
            "ESPGY True\n",
            "ACRX True\n",
            "ADMP True\n",
            "CETX True\n",
            "ENDP True\n",
            "404 Client Error: Not Found for url: https://api.robinhood.com/quotes/historicals/?symbols=ENDP&interval=day&span=3month&bounds=regular\n",
            "CHEK True\n",
            "OTIC True\n",
            "AKBA True\n",
            "BTOG True\n",
            "LOGC True\n",
            "EXN True\n",
            "NRBO True\n",
            "FNHC True\n",
            "ACOR True\n",
            "GRIL True\n",
            "ENSC True\n",
            "HOTH True\n",
            "MARK True\n",
            "AGTC True\n",
            "RSLS True\n",
            "GLMD True\n",
            "REVB True\n",
            "YCBD True\n",
            "XIN True\n",
            "AGRX True\n",
            "ASTC True\n",
            "SNMP True\n",
            "GMBL True\n",
            "DXF True\n",
            "AXU True\n",
            "UAMY True\n",
            "NYMX True\n",
            "DAVE True\n",
            "ICLK True\n",
            "IDRA True\n",
            "SFET True\n",
            "MEIP True\n",
            "CLBS True\n",
            "ECOR True\n",
            "BACK True\n",
            "BRCN True\n",
            "SEAC True\n",
            "AWH True\n",
            "TMDI True\n",
            "ADIL True\n",
            "ORTX True\n",
            "ASXC True\n",
            "FSRD True\n",
            "LPCN True\n",
            "CIDM True\n",
            "CRKN True\n",
            "LCFY True\n",
            "ELYS True\n",
            "VGZ True\n",
            "UAVS True\n",
            "DYNT True\n",
            "CHNR True\n",
            "QUMU True\n",
            "SIEN True\n",
            "IDEX True\n",
            "PHGE True\n",
            "JOB True\n",
            "CRTD True\n",
            "EFTR True\n",
            "SHIP True\n",
            "CLRO True\n",
            "HEPA True\n",
            "NLSP True\n",
            "MBIO True\n",
            "SGTX True\n",
            "ANY True\n",
            "CSCMY True\n",
            "MSN True\n",
            "DRMA True\n",
            "PHIO True\n",
            "MIGI True\n",
            "RNWK True\n",
            "WNW True\n",
            "RCON True\n",
            "AIRI True\n",
            "BBLN True\n",
            "WORX True\n",
            "NBSE True\n",
            "CMCM True\n",
            "DGLY True\n",
            "GAME True\n",
            "MICT True\n",
            "BNTC True\n",
            "PRFX True\n",
            "MTNB True\n",
            "SMBMY True\n",
            "PSTV True\n",
            "IPDN True\n",
            "XBIO True\n",
            "EFOI True\n",
            "CEMI True\n",
            "MNMD True\n",
            "DRRX True\n",
            "NVFY True\n",
            "SISI True\n",
            "TOUR True\n",
            "PRQR True\n",
            "APTO True\n",
            "CNET True\n",
            "GLYC True\n",
            "MIND True\n",
            "TMC True\n",
            "HLBZ True\n",
            "OCX True\n",
            "LABP True\n",
            "PME True\n",
            "ACST True\n",
            "CTIB True\n",
            "TYDE True\n",
            "VIVE True\n",
            "BBIG True\n",
            "RDHL True\n",
            "HEPS True\n",
            "HUGE True\n",
            "ATIP True\n",
            "SDPI True\n",
            "FRLN True\n",
            "CLIR True\n",
            "SFT True\n",
            "SPRO True\n",
            "AIHS True\n",
            "HOFV True\n",
            "SLGG True\n",
            "DGHI True\n",
            "EM True\n",
            "BIOC True\n",
            "FLGC True\n",
            "RVSN True\n",
            "RYCEY True\n",
            "SYBX True\n",
            "AEHL True\n",
            "ARDX True\n",
            "ATOS True\n",
            "CNXA True\n",
            "RWLK True\n",
            "ANGN True\n",
            "FAMI True\n",
            "VBIV True\n",
            "GLS True\n",
            "PTE True\n",
            "SANW True\n",
            "UIHC True\n",
            "CETXP True\n",
            "SASI True\n",
            "VCNX True\n",
            "CRVS True\n",
            "FBIO True\n",
            "PT True\n",
            "SEEL True\n",
            "AIU True\n",
            "MTMT True\n",
            "CIOXY True\n",
            "BSGM True\n",
            "CRIS True\n",
            "FTK True\n",
            "AVYA True\n",
            "BJDX True\n",
            "BYFC True\n",
            "EDBL True\n",
            "IONM True\n",
            "WDH True\n",
            "OPTT True\n",
            "SBFM True\n",
            "AEMD True\n",
            "DARE True\n",
            "NLTX True\n",
            "XELA True\n",
            "HILS True\n",
            "LTCH True\n",
            "OCFT True\n",
            "SMFR True\n",
            "ALR True\n",
            "RNAZ True\n",
            "EVK True\n",
            "PFIE True\n",
            "XELB True\n",
            "BGXX True\n",
            "BFRI True\n",
            "DXYN True\n",
            "GTH True\n",
            "BVXV True\n",
            "LGMK True\n",
            "VZLA True\n",
            "DS True\n",
            "XL True\n",
            "TACBY True\n",
            "CEAD True\n",
            "VVPR True\n",
            "VVOS True\n",
            "ARVL True\n",
            "ZCMD True\n",
            "VSTM True\n",
            "BKEAY True\n",
            "PRPO True\n",
            "CLVS True\n",
            "FTEK True\n",
            "ONTX True\n",
            "CCLP True\n",
            "KBNT True\n",
            "GTE True\n",
            "LTRPA True\n",
            "BBGI True\n",
            "SPPI True\n",
            "BSFC True\n",
            "FBRX True\n",
            "HALL True\n",
            "CGEN True\n",
            "TNXP True\n",
            "WBEV True\n",
            "BEAT True\n",
            "RAVE True\n",
            "SGRP True\n",
            "BSQR True\n",
            "NCMI True\n",
            "NCTY True\n",
            "CTHR True\n",
            "DWSN True\n",
            "SDC True\n",
            "BLCM True\n",
            "RGS True\n",
            "CPTN True\n",
            "AMRN True\n",
            "DTEA True\n",
            "LLL True\n",
            "PBTS True\n",
            "WATT True\n",
            "XNET True\n",
            "ID True\n",
            "STCN True\n",
            "HITI True\n",
            "WULF True\n",
            "CBAT True\n",
            "ONCY True\n",
            "CNTB True\n",
            "EGLX True\n",
            "TRIB True\n",
            "CENN True\n",
            "PLSE True\n",
            "ARDS True\n",
            "ACB True\n",
            "LGHL True\n",
            "RLX True\n",
            "OESX True\n",
            "SOFO True\n",
            "WTT True\n",
            "SVVC True\n",
            "AESE True\n",
            "RKLY True\n",
            "BBLG True\n",
            "BLIN True\n",
            "BLRX True\n",
            "SXTC True\n",
            "APLT True\n",
            "KOPN True\n",
            "RGLS True\n",
            "APWC True\n",
            "INFI True\n",
            "TRVG True\n",
            "MMMB True\n",
            "APM True\n",
            "SBIG True\n",
            "GIGM True\n",
            "CTRM True\n",
            "GENE True\n",
            "CYRN True\n",
            "CYCC True\n",
            "EEIQ True\n",
            "AXDX True\n",
            "LDI True\n",
            "AMTI True\n",
            "LPTH True\n",
            "YMTX True\n",
            "MITQ True\n",
            "FORD True\n",
            "NCNA True\n",
            "STRM True\n",
            "NVOS True\n",
            "CCO True\n",
            "BHAT True\n",
            "AGFS True\n",
            "CIFR True\n",
            "UNAM True\n",
            "DTIL True\n",
            "SNCR True\n",
            "SIEB True\n",
            "UTME True\n",
            "MTC True\n",
            "AACAY True\n",
            "NXGL True\n",
            "LIDR True\n",
            "CLWT True\n",
            "RIGL True\n",
            "XTLB True\n",
            "FFIE True\n",
            "MNPR True\n",
            "GLBS True\n",
            "STRN True\n",
            "RMTI True\n",
            "ICCM True\n",
            "OCG True\n",
            "PSFE True\n",
            "ATIF True\n",
            "CIBEY True\n",
            "MRIN True\n",
            "SRNE True\n",
            "HSCS True\n",
            "VIVK True\n",
            "ASMB True\n",
            "FPAFY True\n",
            "IINN True\n",
            "APLD True\n",
            "SYPR True\n",
            "LHDX True\n",
            "SCOR True\n",
            "CBIO True\n",
            "GALT True\n",
            "JCSE True\n",
            "MDWD True\n",
            "CCM True\n",
            "CTSO True\n",
            "ENSV True\n",
            "HSTO True\n",
            "VLTA True\n",
            "DUO True\n",
            "AHPI True\n",
            "GBR True\n",
            "SOND True\n",
            "EOSE True\n",
            "NCRA True\n",
            "CIF True\n",
            "BGRY True\n",
            "BDSX True\n",
            "PXLW True\n",
            "QUOT True\n",
            "AMTD True\n",
            "BCDA True\n",
            "EQS True\n",
            "LYG True\n",
            "MCHX True\n",
            "WHLR True\n",
            "EPOW True\n",
            "AMPG True\n",
            "EXPR True\n",
            "PGEN True\n",
            "SHGKY True\n",
            "NMRD True\n",
            "NHWK True\n",
            "AWRE True\n",
            "PFMT True\n",
            "DBVT True\n",
            "CDAK True\n",
            "SIFY True\n",
            "TNON True\n",
            "AEY True\n",
            "BOSC True\n",
            "NLS True\n",
            "STXS True\n",
            "TRUE True\n",
            "XLO True\n",
            "KNDI True\n",
            "AGMH True\n",
            "MHLD True\n",
            "FPLPY True\n",
            "CORR True\n",
            "CRNT True\n",
            "MNOV True\n",
            "SMSI True\n",
            "MFG True\n",
            "NM True\n",
            "NNBR True\n",
            "WKSP True\n",
            "CMRX True\n",
            "SALM True\n",
            "HIHO True\n",
            "CLSN True\n",
            "PRCH True\n",
            "CO True\n",
            "ABIO True\n",
            "AXLA True\n",
            "BARK True\n",
            "MNDO True\n",
            "TAST True\n",
            "DHF True\n",
            "AAOI True\n",
            "GERN True\n",
            "HTCR True\n",
            "REFR True\n",
            "INO True\n",
            "WKEY True\n",
            "LITM True\n",
            "TDUP True\n",
            "FSM True\n",
            "AIMD True\n",
            "SAN True\n",
            "SVRE True\n",
            "FNCH True\n",
            "BKTI True\n",
            "CHG True\n",
            "ICAGY True\n",
            "MAPS True\n",
            "EMKR True\n",
            "WYY True\n",
            "AACG True\n",
            "AMRS True\n",
            "CPIX True\n",
            "KRNGY True\n",
            "NOVN True\n",
            "ARAY True\n",
            "CDRO True\n",
            "CYAN True\n",
            "PMCB True\n",
            "BKKT True\n",
            "SZGPY True\n",
            "ADMA True\n",
            "EVOK True\n",
            "GTBP True\n",
            "PRTK True\n",
            "CDGXY True\n",
            "FIXX True\n",
            "HRTG True\n",
            "INKT True\n",
            "DYAI True\n",
            "PPTA True\n",
            "SCYX True\n",
            "NAT True\n",
            "ASRT True\n",
            "SIDU True\n",
            "UGP True\n",
            "IRIX True\n",
            "KSPN True\n",
            "EPIX True\n",
            "JVA True\n",
            "HIL True\n",
            "AFMD True\n",
            "EVLV True\n",
            "SOHO True\n",
            "TKC True\n",
            "BNR True\n",
            "NEWP True\n",
            "CDE True\n",
            "ORC True\n",
            "GTIM True\n",
            "OXBR True\n",
            "SVFD True\n",
            "SKLKY True\n",
            "KSCP True\n",
            "LEDS True\n",
            "SURRY True\n",
            "AGEN True\n",
            "LXRX True\n",
            "NEPT True\n",
            "NUVB True\n",
            "OMIC True\n",
            "NYMT True\n",
            "ZDGE True\n",
            "MGAM True\n",
            "MIN True\n",
            "TCRX True\n",
            "FKWL True\n",
            "BBI True\n",
            "MTGRY True\n",
            "COOK True\n",
            "ABEV True\n",
            "MOVE True\n",
            "ACHL True\n",
            "GMDA True\n",
            "AAME True\n",
            "ARC True\n",
            "ELYM True\n",
            "SNOA True\n",
            "CYH True\n",
            "CMPX True\n",
            "MUX True\n",
            "BFI True\n",
            "CMAKY True\n",
            "GASS True\n",
            "PAYS True\n",
            "SEV True\n",
            "EDUC True\n",
            "ITCB True\n",
            "NR True\n",
            "MAKSY True\n",
            "MFGP True\n",
            "PEV True\n",
            "BBDO True\n",
            "PBI True\n",
            "RAIFY True\n",
            "BRFS True\n",
            "GEVO True\n",
            "HLFDY True\n",
            "HBIO True\n",
            "PBYI True\n",
            "DSWL True\n",
            "ITI True\n",
            "STTK True\n",
            "WKHS True\n",
            "KIND True\n",
            "STRC True\n",
            "SID True\n",
            "AAIC True\n",
            "HMY True\n",
            "SJ True\n",
            "INZY True\n",
            "RHE True\n",
            "BMRA True\n",
            "GAIA True\n",
            "NRDY True\n",
            "NURO True\n",
            "FRBK True\n",
            "SIHBY True\n",
            "VRCA True\n",
            "APDN True\n",
            "ICD True\n",
            "GANX True\n",
            "EERN True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=EERN&interval=day&span=3month&bounds=regular\n",
            "CREG True\n",
            "DERM True\n",
            "PIM True\n",
            "BWEN True\n",
            "CGC True\n",
            "IQ True\n",
            "OIIM True\n",
            "NCZ True\n",
            "PEGY True\n",
            "DAIO True\n",
            "MTBC True\n",
            "SMRT True\n",
            "TGA True\n",
            "MGF True\n",
            "MYPS True\n",
            "VRAY True\n",
            "CASI True\n",
            "TIGR True\n",
            "LMPMY True\n",
            "TRVI True\n",
            "FGB True\n",
            "PPSI True\n",
            "NTWK True\n",
            "IOBT True\n",
            "OMEX True\n",
            "ACXP True\n",
            "CMGMY True\n",
            "GSIT True\n",
            "GTEC True\n",
            "OCC True\n",
            "RIG True\n",
            "MMX True\n",
            "ACHR True\n",
            "FPH True\n",
            "DLNG True\n",
            "XPON True\n",
            "FWP True\n",
            "CBD True\n",
            "SLGC True\n",
            "UTSI True\n",
            "BHIL True\n",
            "MPLN True\n",
            "NXE True\n",
            "PHX True\n",
            "MLLUY True\n",
            "USEG True\n",
            "BBD True\n",
            "DUFRY True\n",
            "BRFH True\n",
            "CBAY True\n",
            "KINS True\n",
            "MGNX True\n",
            "THCX True\n",
            "PPT True\n",
            "CIA True\n",
            "API True\n",
            "BPTH True\n",
            "EEX True\n",
            "MNKD True\n",
            "NEON True\n",
            "HLTH True\n",
            "CRMD True\n",
            "VVR True\n",
            "MDXG True\n",
            "ASRV True\n",
            "CXE True\n",
            "INOD True\n",
            "GROW True\n",
            "BKCC True\n",
            "GWH True\n",
            "FCEL True\n",
            "GIFI True\n",
            "CEPU True\n",
            "MHUA True\n",
            "BCKIY True\n",
            "NCV True\n",
            "ADES True\n",
            "EYES True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=EYES&interval=day&span=3month&bounds=regular\n",
            "NEOV True\n",
            "SFE True\n",
            "BGCP True\n",
            "CALA True\n",
            "TTSH True\n",
            "TAIT True\n",
            "AERC True\n",
            "NFGC True\n",
            "ABEO True\n",
            "SMIT True\n",
            "HIO True\n",
            "LFVN True\n",
            "HL True\n",
            "OSUR True\n",
            "PXJ True\n",
            "KRON True\n",
            "MACK True\n",
            "BCLI True\n",
            "SRGA True\n",
            "OGFGY True\n",
            "BNSO True\n",
            "OPHC True\n",
            "CX True\n",
            "DAKT True\n",
            "NTP True\n",
            "Warning: \"NTP\" is not a valid stock ticker. It is being ignored\n",
            "CDZI True\n",
            "EVNVY True\n",
            "ATRA True\n",
            "MTA True\n",
            "RDI True\n",
            "HUMA True\n",
            "TEF True\n",
            "HLX True\n",
            "IRS True\n",
            "CHCI True\n",
            "DBTX True\n",
            "SPNT True\n",
            "NPCE True\n",
            "SGHC True\n",
            "ALLG True\n",
            "DCTH True\n",
            "DXLG True\n",
            "GURE True\n",
            "LLAP True\n",
            "TACT True\n",
            "MMLP True\n",
            "CUBA True\n",
            "EVTV True\n",
            "GNW True\n",
            "ESYJY True\n",
            "TME True\n",
            "VRAX True\n",
            "MZDAY True\n",
            "ATAI True\n",
            "OB True\n",
            "DSEEY True\n",
            "ELTK True\n",
            "TEO True\n",
            "VRA True\n",
            "TNYA True\n",
            "MOMO True\n",
            "BBVA True\n",
            "NKTR True\n",
            "RGTI True\n",
            "EDD True\n",
            "CAAS True\n",
            "CERS True\n",
            "XELAP True\n",
            "ANGI True\n",
            "BKD True\n",
            "ALLT True\n",
            "GRWG True\n",
            "DDL True\n",
            "MODD True\n",
            "TELL True\n",
            "GIM True\n",
            "AGIL True\n",
            "BKT True\n",
            "VNRFY True\n",
            "CGAU True\n",
            "CGA True\n",
            "HRTX True\n",
            "EGHT True\n",
            "MMT True\n",
            "ACTG True\n",
            "FOSL True\n",
            "VACC True\n",
            "ACHV True\n",
            "MVIS True\n",
            "VNET True\n",
            "BWAY True\n",
            "EDF True\n",
            "PRVB True\n",
            "AVTX True\n",
            "GNT True\n",
            "OMER True\n",
            "PDSB True\n",
            "SIELY True\n",
            "DTC True\n",
            "MFV True\n",
            "RYAM True\n",
            "GGB True\n",
            "RAIL True\n",
            "VCSA True\n",
            "CLBT True\n",
            "KIRK True\n",
            "SURG True\n",
            "THRN True\n",
            "ULBI True\n",
            "WHLM True\n",
            "NOK True\n",
            "TWO True\n",
            "WETF True\n",
            "EOD True\n",
            "TSI True\n",
            "BLDE True\n",
            "IVAC True\n",
            "FENG True\n",
            "NG True\n",
            "CPAC True\n",
            "AWP True\n",
            "GGR True\n",
            "PIII True\n",
            "BULZ True\n",
            "GEOS True\n",
            "ETNB True\n",
            "KTCC True\n",
            "KMDA True\n",
            "NSL True\n",
            "ITUB True\n",
            "PANL True\n",
            "ODV True\n",
            "RSI True\n",
            "NRGV True\n",
            "GCV True\n",
            "BGI True\n",
            "DHX True\n",
            "SUP True\n",
            "WIT True\n",
            "DRIO True\n",
            "IREN True\n",
            "YPF True\n",
            "NDLS True\n",
            "BHC True\n",
            "CARV True\n",
            "OCUL True\n",
            "CPCAY True\n",
            "CS True\n",
            "EDI True\n",
            "RSKD True\n",
            "AMSC True\n",
            "TEI True\n",
            "JOBY True\n",
            "CLRB True\n",
            "OIS True\n",
            "TESS True\n",
            "AATC True\n",
            "AEZS True\n",
            "EVC True\n",
            "HIVE True\n",
            "PEI True\n",
            "RNRTY True\n",
            "RKLB True\n",
            "CVV True\n",
            "AEF True\n",
            "TGAN True\n",
            "BVN True\n",
            "PFIN True\n",
            "ATTO True\n",
            "APRN True\n",
            "DSX True\n",
            "RCS True\n",
            "DMTK True\n",
            "INFN True\n",
            "ASUR True\n",
            "CVT True\n",
            "HIX True\n",
            "INCR True\n",
            "MCRB True\n",
            "PZCUY True\n",
            "JQC True\n",
            "LOAN True\n",
            "RENT True\n",
            "IGD True\n",
            "EQRX True\n",
            "MG True\n",
            "IHD True\n",
            "MFM True\n",
            "KODK True\n",
            "KGFHY True\n",
            "GBIO True\n",
            "LUNA True\n",
            "IMMR True\n",
            "SFIX True\n",
            "LPL True\n",
            "FARM True\n",
            "KPTI True\n",
            "EVF True\n",
            "VRAR True\n",
            "CMPO True\n",
            "MTRX True\n",
            "ELSE True\n",
            "MNTX True\n",
            "BLUE True\n",
            "VEEE True\n",
            "SWRBY True\n",
            "CAMP True\n",
            "FREE True\n",
            "JEQ True\n",
            "CDEVY True\n",
            "ATNM True\n",
            "ASX True\n",
            "ISSC True\n",
            "PACB True\n",
            "CNSL True\n",
            "NERV True\n",
            "PBPB True\n",
            "PCYG True\n",
            "DLAKY True\n",
            "STSA True\n",
            "TZOO True\n",
            "CTTAY True\n",
            "EVTL True\n",
            "SLGL True\n",
            "OXLC True\n",
            "LTBR True\n",
            "LWAY True\n",
            "SAND True\n",
            "GDRX True\n",
            "SMFG True\n",
            "EGO True\n",
            "III True\n",
            "IMGN True\n",
            "VYGR True\n",
            "YAHOY True\n",
            "CRWS True\n",
            "ORGN True\n",
            "CAAP True\n",
            "JMM True\n",
            "PTRA True\n",
            "CTIC True\n",
            "TATT True\n",
            "CRZBY True\n",
            "NMG True\n",
            "GAB True\n",
            "MCHVY True\n",
            "PAYO True\n",
            "SIRI True\n",
            "FEIM True\n",
            "GPRO True\n",
            "FAM True\n",
            "GDEV True\n",
            "Warning: \"GDEV\" is not a valid stock ticker. It is being ignored\n",
            "HLN True\n",
            "AEYE True\n",
            "JOF True\n",
            "ROYMY True\n",
            "DSM True\n",
            "IONQ True\n",
            "JCTCF True\n",
            "CRCT True\n",
            "FPL True\n",
            "PNNT True\n",
            "WVVI True\n",
            "AVDL True\n",
            "ARLO True\n",
            "LVLU True\n",
            "PAYA True\n",
            "POAHY True\n",
            "BCOV True\n",
            "CHMI True\n",
            "LINC True\n",
            "JEMD True\n",
            "NLY True\n",
            "ACCO True\n",
            "MCR True\n",
            "IAE True\n",
            "HIMX True\n",
            "PSLV True\n",
            "SPNE True\n",
            "TEDU True\n",
            "MHF True\n",
            "PMM True\n",
            "UMC True\n",
            "PMD True\n",
            "OIA True\n",
            "SLLDY True\n",
            "MITT True\n",
            "TGTX True\n",
            "GILT True\n",
            "LEO True\n",
            "STGW True\n",
            "VNCE True\n",
            "WTI True\n",
            "ELP True\n",
            "TGS True\n",
            "KOS True\n",
            "YMM True\n",
            "MSD True\n",
            "QRHC True\n",
            "CHS True\n",
            "BBCP True\n",
            "MRNS True\n",
            "PLYA True\n",
            "RMCF True\n",
            "FRGI True\n",
            "IDBA True\n",
            "SMHI True\n",
            "FCREY True\n",
            "EAD True\n",
            "TPC True\n",
            "CNCE True\n",
            "UTI True\n",
            "FLL True\n",
            "SABR True\n",
            "CVGI True\n",
            "LFC True\n",
            "404 Client Error: Not Found for url: https://api.robinhood.com/quotes/historicals/?symbols=LFC&interval=day&span=3month&bounds=regular\n",
            "TV True\n",
            "ASGLY True\n",
            "SWRAY True\n",
            "BKCH True\n",
            "SXC True\n",
            "LILAK True\n",
            "LLESY True\n",
            "RIOT True\n",
            "NHYDY True\n",
            "RFIL True\n",
            "LILA True\n",
            "RVSB True\n",
            "EWEB True\n",
            "CMCT True\n",
            "HNRG True\n",
            "SANA True\n",
            "APE True\n",
            "QUIK True\n",
            "SPOK True\n",
            "ESBA True\n",
            "HMTV True\n",
            "TTCF True\n",
            "UBFO True\n",
            "ALDX True\n",
            "EDAP True\n",
            "KFS True\n",
            "ARR True\n",
            "BNFT True\n",
            "LTRX True\n",
            "PGRE True\n",
            "EEA True\n",
            "CHW True\n",
            "LQDA True\n",
            "DCFC True\n",
            "COTY True\n",
            "FF True\n",
            "NESR True\n",
            "SNPHY True\n",
            "NVCN True\n",
            "CMAX True\n",
            "BLDP True\n",
            "HYB True\n",
            "DALN True\n",
            "ESRT True\n",
            "KWHIY True\n",
            "PRTS True\n",
            "TWOU True\n",
            "PCF True\n",
            "TGOPY True\n",
            "CLNE True\n",
            "ARCO True\n",
            "PCFBY True\n",
            "IGR True\n",
            "KCDMY True\n",
            "MCN True\n",
            "ALIM True\n",
            "EHI True\n",
            "EHTH True\n",
            "VTNR True\n",
            "CDXS True\n",
            "RLGT True\n",
            "THRX True\n",
            "IGIC True\n",
            "AGS True\n",
            "SINGY True\n",
            "ALVR True\n",
            "EPM True\n",
            "NSANY True\n",
            "SNFCA True\n",
            "CDEV True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=CDEV&interval=day&span=3month&bounds=regular\n",
            "LYTS True\n",
            "FT True\n",
            "ADCT True\n",
            "KEP True\n",
            "SWZ True\n",
            "AG True\n",
            "BVS True\n",
            "SGMA True\n",
            "DHT True\n",
            "SWN True\n",
            "ATEC True\n",
            "CNTY True\n",
            "CXH True\n",
            "GUT True\n",
            "BITS True\n",
            "AGI True\n",
            "FISK True\n",
            "CTG True\n",
            "VDMCY True\n",
            "BCS True\n",
            "PFN True\n",
            "AVDX True\n",
            "ICCC True\n",
            "IRL True\n",
            "PATI True\n",
            "SEKEY True\n",
            "AUPH True\n",
            "JPC True\n",
            "CSPI True\n",
            "KFFB True\n",
            "CD True\n",
            "CPG True\n",
            "DEX True\n",
            "SGHT True\n",
            "PSEC True\n",
            "RRGB True\n",
            "PRTG True\n",
            "UA True\n",
            "ACGBY True\n",
            "MTNOY True\n",
            "ESPR True\n",
            "GF True\n",
            "MYNA True\n",
            "MXE True\n",
            "FNCB True\n",
            "FUND True\n",
            "ISIG True\n",
            "JBLU True\n",
            "BDORY True\n",
            "RWT True\n",
            "FULC True\n",
            "GRFS True\n",
            "NTZ True\n",
            "GGGSY True\n",
            "CTRYY True\n",
            "GDL True\n",
            "AMAX True\n",
            "UMICY True\n",
            "BTF True\n",
            "KBAL True\n",
            "AOD True\n",
            "DB True\n",
            "GAMB True\n",
            "KOSS True\n",
            "ADSE True\n",
            "AMTX True\n",
            "BDN True\n",
            "SRET True\n",
            "INN True\n",
            "CEE True\n",
            "ACP True\n",
            "GGAL True\n",
            "HLTOY True\n",
            "PEXNY True\n",
            "SANG True\n",
            "SNPX True\n",
            "USAP True\n",
            "ALVO True\n",
            "PZC True\n",
            "VGI True\n",
            "EJPRY True\n",
            "FEI True\n",
            "EFL True\n",
            "EARN True\n",
            "TSRI True\n",
            "FTI True\n",
            "NNY True\n",
            "TELA True\n",
            "CUK True\n",
            "WEBR True\n",
            "CENX True\n",
            "CYD True\n",
            "FSR True\n",
            "MARPS True\n",
            "BACHY True\n",
            "EIGR True\n",
            "ORKLY True\n",
            "TRMR True\n",
            "ADFI True\n",
            "PYN True\n",
            "DDF True\n",
            "PTNR True\n",
            "IAS True\n",
            "AVEO True\n",
            "VIV True\n",
            "BBBY True\n",
            "WALD True\n",
            "JRO True\n",
            "HLMN True\n",
            "GFI True\n",
            "SMED True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=SMED&interval=day&span=3month&bounds=regular\n",
            "AHHX True\n",
            "BCAB True\n",
            "HPIFY True\n",
            "ORMP True\n",
            "NCA True\n",
            "FUJHY True\n",
            "TCMD True\n",
            "CPS True\n",
            "CIM True\n",
            "JFR True\n",
            "NICK True\n",
            "KMF True\n",
            "CSBR True\n",
            "MRCC True\n",
            "AHT True\n",
            "WBX True\n",
            "EONGY True\n",
            "AFIF True\n",
            "EVER True\n",
            "TSQ True\n",
            "UAA True\n",
            "GLUE True\n",
            "EGAN True\n",
            "EZPW True\n",
            "FEO True\n",
            "NL True\n",
            "TBPH True\n",
            "SEAT True\n",
            "BWG True\n",
            "CBH True\n",
            "NUV True\n",
            "HOOD True\n",
            "IRBA True\n",
            "HCMLY True\n",
            "AIV True\n",
            "MAV True\n",
            "SMM True\n",
            "JHAA True\n",
            "TWIN True\n",
            "SBS True\n",
            "EMD True\n",
            "IGA True\n",
            "MYNZ True\n",
            "AXTI True\n",
            "DRH True\n",
            "ACH True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=ACH&interval=day&span=3month&bounds=regular\n",
            "HDELY True\n",
            "MGTX True\n",
            "NMI True\n",
            "KSM True\n",
            "NLIT True\n",
            "LOCO True\n",
            "SBSW True\n",
            "LGF.B True\n",
            "RKT True\n",
            "SGBLY True\n",
            "TWIO True\n",
            "LL True\n",
            "RBKB True\n",
            "NIM True\n",
            "ITCFY True\n",
            "OII True\n",
            "PTRS True\n",
            "TSCDY True\n",
            "BDJ True\n",
            "AGRO True\n",
            "DALT True\n",
            "LCUT True\n",
            "RGT True\n",
            "KTF True\n",
            "HROW True\n",
            "KYN True\n",
            "SGU True\n",
            "HBI True\n",
            "BKQNY True\n",
            "PZN True\n",
            "AJX True\n",
            "SEED True\n",
            "TLKGY True\n",
            "DDI True\n",
            "ECF True\n",
            "BUR True\n",
            "AMC True\n",
            "CFFN True\n",
            "FSLY True\n",
            "HEAR True\n",
            "ADRZY True\n",
            "CMLS True\n",
            "KIGRY True\n",
            "HYT True\n",
            "WYGPY True\n",
            "PEPG True\n",
            "NFYS True\n",
            "BCX True\n",
            "HNNA True\n",
            "OBE True\n",
            "PBFS True\n",
            "DALS True\n",
            "ENX True\n",
            "ASC True\n",
            "TWCB True\n",
            "VOXX True\n",
            "LITT True\n",
            "CLBR True\n",
            "CPAA True\n",
            "PSPC True\n",
            "THCP True\n",
            "AFTR True\n",
            "DGNU True\n",
            "FICV True\n",
            "DNAB True\n",
            "DNAD True\n",
            "AXGN True\n",
            "MTAL True\n",
            "MHI True\n",
            "FWAC True\n",
            "TAC True\n",
            "PMX True\n",
            "PFTA True\n",
            "EMLD True\n",
            "AGD True\n",
            "HWKZ True\n",
            "SMIH True\n",
            "RACB True\n",
            "AAQC True\n",
            "BGSX True\n",
            "FZT True\n",
            "HPLT True\n",
            "IACC True\n",
            "NDAC True\n",
            "PFO True\n",
            "SCAQ True\n",
            "BACA True\n",
            "GFGD True\n",
            "KSI True\n",
            "LGF.A True\n",
            "SPWH True\n",
            "ACR True\n",
            "ARRW True\n",
            "CPAR True\n",
            "GAMC True\n",
            "KVHI True\n",
            "LCA True\n",
            "LHAA True\n",
            "OHPA True\n",
            "RKTA True\n",
            "TRCA True\n",
            "TWOA True\n",
            "RCLF True\n",
            "EVNT True\n",
            "AGAC True\n",
            "AMPI True\n",
            "ASAX True\n",
            "CLAA True\n",
            "HIII True\n",
            "SPGS True\n",
            "AGGR True\n",
            "EAC True\n",
            "FRON True\n",
            "GAQ True\n",
            "IRAA True\n",
            "JOFF True\n",
            "PEGR True\n",
            "PFDR True\n",
            "RMGC True\n",
            "SDAC True\n",
            "STRE True\n",
            "TMAC True\n",
            "WPCA True\n",
            "WPCB True\n",
            "MSAC True\n",
            "AAC True\n",
            "ABGI True\n",
            "CHAA True\n",
            "CPTK True\n",
            "DISA True\n",
            "FMIV True\n",
            "ITQ True\n",
            "LCAA True\n",
            "LEGA True\n",
            "MCAC True\n",
            "MLP True\n",
            "MSDA True\n",
            "RDRUY True\n",
            "OEPW True\n",
            "PMGM True\n",
            "ARIZ True\n",
            "BLUA True\n",
            "CSLM True\n",
            "FACT True\n",
            "FSSI True\n",
            "MON True\n",
            "NHIC True\n",
            "NRAC True\n",
            "PDOT True\n",
            "SCOB True\n",
            "TSIB True\n",
            "ADEX True\n",
            "CLRM True\n",
            "DSU True\n",
            "FCAX True\n",
            "HLAH True\n",
            "KYCH True\n",
            "MACA True\n",
            "AKIC True\n",
            "ENER True\n",
            "ANAC True\n",
            "IGGHY True\n",
            "RJAC True\n",
            "LHC True\n",
            "MTAC True\n",
            "NVSA True\n",
            "RFAC True\n",
            "AACI True\n",
            "AEAC True\n",
            "IIII True\n",
            "JCIC True\n",
            "FINM True\n",
            "MCAE True\n",
            "PEPL True\n",
            "HMCO True\n",
            "BHAC True\n",
            "HCAR True\n",
            "LCW True\n",
            "PAQC True\n",
            "IGTA True\n",
            "NFNT True\n",
            "MCBC True\n",
            "ASCB True\n",
            "COOL True\n",
            "HAAC True\n",
            "MEAC True\n",
            "SAMA True\n",
            "SCOA True\n",
            "KCGI True\n",
            "GATE True\n",
            "NOAC True\n",
            "PHIC True\n",
            "PORT True\n",
            "ULE True\n",
            "CIIG True\n",
            "GWII True\n",
            "SGHL True\n",
            "ACEL True\n",
            "GMFI True\n",
            "DCRD True\n",
            "HORI True\n",
            "DRAY True\n",
            "TMKR True\n",
            "WQGA True\n",
            "WTMA True\n",
            "ASAQ True\n",
            "BTWN True\n",
            "HIGA True\n",
            "AIB True\n",
            "IGAC True\n",
            "MEKA True\n",
            "PGSS True\n",
            "PNAC True\n",
            "RRAC True\n",
            "SRSA True\n",
            "SMAP True\n",
            "ASCA True\n",
            "CLRC True\n",
            "XFIN True\n",
            "HDSN True\n",
            "LSPR True\n",
            "LGTO True\n",
            "BRKH True\n",
            "HMA True\n",
            "LBBB True\n",
            "MAAQ True\n",
            "PIAI True\n",
            "SEER True\n",
            "IPOD True\n",
            "AVHI True\n",
            "ISLE True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=ISLE&interval=day&span=3month&bounds=regular\n",
            "ITAQ True\n",
            "NATR True\n",
            "NPAB True\n",
            "ASPA True\n",
            "BFIN True\n",
            "KACL True\n",
            "ADRT True\n",
            "BCSA True\n",
            "CCTS True\n",
            "GPS True\n",
            "LAAA True\n",
            "TGAA True\n",
            "ADAL True\n",
            "CPAQ True\n",
            "EVGR True\n",
            "FTII True\n",
            "GTAC True\n",
            "AMCI True\n",
            "APAC True\n",
            "APN True\n",
            "BIOS True\n",
            "BWAQ True\n",
            "DENN True\n",
            "LION True\n",
            "ROC True\n",
            "TAYD True\n",
            "AFAC True\n",
            "CENQ True\n",
            "HAIA True\n",
            "OXAC True\n",
            "PFSW True\n",
            "RACY True\n",
            "TEVA True\n",
            "ACDI True\n",
            "CHEA True\n",
            "KINZ True\n",
            "CITE True\n",
            "CMCA True\n",
            "DWIN True\n",
            "GLDD True\n",
            "INKA True\n",
            "MTRY True\n",
            "BBDC True\n",
            "GEEX True\n",
            "IFIN True\n",
            "OCAX True\n",
            "PBAX True\n",
            "IOAC True\n",
            "KPELY True\n",
            "PCX True\n",
            "PNF True\n",
            "STKL True\n",
            "HTAQ True\n",
            "TGR True\n",
            "LGST True\n",
            "VMCA True\n",
            "IDE True\n",
            "NCAC True\n",
            "THAC True\n",
            "ARCK True\n",
            "ESAC True\n",
            "GLEE True\n",
            "LPRO True\n",
            "MEOA True\n",
            "VTRS True\n",
            "SANB True\n",
            "NOVV True\n",
            "MCAA True\n",
            "CFFE True\n",
            "HGLB True\n",
            "OR True\n",
            "RIGZ True\n",
            "CSTE True\n",
            "GENC True\n",
            "DDD True\n",
            "GBRG True\n",
            "CEVIY True\n",
            "FST True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=FST&interval=day&span=3month&bounds=regular\n",
            "GPCO True\n",
            "FLYA True\n",
            "WFCF True\n",
            "ICL True\n",
            "GLAD True\n",
            "PSO True\n",
            "AXL True\n",
            "COMM True\n",
            "ADOC True\n",
            "ORAN True\n",
            "WIA True\n",
            "CECE True\n",
            "HIE True\n",
            "MBTC True\n",
            "NOTE True\n",
            "CLM True\n",
            "ESSC True\n",
            "PLTK True\n",
            "VCV True\n",
            "VHAQ True\n",
            "MGI True\n",
            "RADA True\n",
            "CLAQ True\n",
            "DKDCA True\n",
            "AWF True\n",
            "EOLS True\n",
            "CMCL True\n",
            "BWAC True\n",
            "NRDS True\n",
            "VKQ True\n",
            "BENE True\n",
            "NYCB True\n",
            "THCA True\n",
            "FFHL True\n",
            "VMO True\n",
            "VNDA True\n",
            "VGR True\n",
            "TXMD True\n",
            "MPAC True\n",
            "WSR True\n",
            "NABZY True\n",
            "EGF True\n",
            "IQI True\n",
            "NBO True\n",
            "BASFY True\n",
            "GLBZ True\n",
            "AFRI True\n",
            "BV True\n",
            "OCLDY True\n",
            "MITK True\n",
            "SEVN True\n",
            "CION True\n",
            "VIPS True\n",
            "LXP True\n",
            "FCT True\n",
            "SHZHY True\n",
            "MEGL True\n",
            "ENTR True\n",
            "OCCI True\n",
            "VGM True\n",
            "WEICY True\n",
            "MYN True\n",
            "NEO True\n",
            "WEBL True\n",
            "PGAL True\n",
            "LNDC True\n",
            "ASAZY True\n",
            "AREN True\n",
            "CEV True\n",
            "KD True\n",
            "APEI True\n",
            "HXGBY True\n",
            "ADPT True\n",
            "CTT True\n",
            "IDYA True\n",
            "EIM True\n",
            "VTN True\n",
            "GRPN True\n",
            "MMU True\n",
            "TDF True\n",
            "CARA True\n",
            "EC True\n",
            "GNE True\n",
            "SFL True\n",
            "DNPLY True\n",
            "CCU True\n",
            "MPX True\n",
            "RAM True\n",
            "AMPS True\n",
            "BBIO True\n",
            "MHN True\n",
            "BFK True\n",
            "TCN True\n",
            "OFS True\n",
            "PCYO True\n",
            "VPV True\n",
            "UPLD True\n",
            "VZIO True\n",
            "HOUS True\n",
            "ATRO True\n",
            "HLVX True\n",
            "HLIT True\n",
            "DYN True\n",
            "FRHLF True\n",
            "BNY True\n",
            "ADPXY True\n",
            "BTZ True\n",
            "HKTVY True\n",
            "DFH True\n",
            "EBKDY True\n",
            "GBNY True\n",
            "LPSN True\n",
            "HTZWW True\n",
            "VLT True\n",
            "BLBD True\n",
            "LADR True\n",
            "DNOW True\n",
            "TRYIY True\n",
            "DNP True\n",
            "PAY True\n",
            "BNKD True\n",
            "ACCD True\n",
            "PRM True\n",
            "CFRUY True\n",
            "FMN True\n",
            "METC True\n",
            "AGBA True\n",
            "CMRE True\n",
            "MYD True\n",
            "MTLS True\n",
            "CGO True\n",
            "CLPBY True\n",
            "CONN True\n",
            "GRX True\n",
            "PRPH True\n",
            "ABCL True\n",
            "BHK True\n",
            "NGS True\n",
            "BFZ True\n",
            "PPERY True\n",
            "CXM True\n",
            "MFA True\n",
            "TSAT True\n",
            "WMPN True\n",
            "EVN True\n",
            "NAII True\n",
            "PMF True\n",
            "BLFY True\n",
            "GTES True\n",
            "IRWD True\n",
            "MVT True\n",
            "TELNY True\n",
            "CMTL True\n",
            "FRO True\n",
            "SHO True\n",
            "BGT True\n",
            "ASPS True\n",
            "ECC True\n",
            "PCG True\n",
            "HFRO True\n",
            "HTBK True\n",
            "DVAX True\n",
            "AFB True\n",
            "TUP True\n",
            "CATO True\n",
            "CJPRY True\n",
            "SNPO True\n",
            "MWA True\n",
            "NBH True\n",
            "NXN True\n",
            "GGZ True\n",
            "BGB True\n",
            "FMY True\n",
            "MYI True\n",
            "RDCM True\n",
            "ESCA True\n",
            "GHY True\n",
            "MUE True\n",
            "PNBK True\n",
            "BMEA True\n",
            "NOA True\n",
            "BLKC True\n",
            "BGR True\n",
            "ALTY True\n",
            "FMTX True\n",
            "LASR True\n",
            "JHS True\n",
            "NBW True\n",
            "EMF True\n",
            "MLPO True\n",
            "NTIC True\n",
            "KRNY True\n",
            "AKRO True\n",
            "MUA True\n",
            "BYTE True\n",
            "AESR True\n",
            "FOLD True\n",
            "CHI True\n",
            "DNUT True\n",
            "PFLT True\n",
            "FSD True\n",
            "WKME True\n",
            "MQT True\n",
            "EVCM True\n",
            "MGYR True\n",
            "SUHJY True\n",
            "MUI True\n",
            "BGX True\n",
            "SWGAY True\n",
            "ATOM True\n",
            "PAA True\n",
            "GEL True\n",
            "JELD True\n",
            "FRA True\n",
            "DCT True\n",
            "BOTJ True\n",
            "BRID True\n",
            "CICHY True\n",
            "VLY True\n",
            "PAI True\n",
            "HCAT True\n",
            "AGNC True\n",
            "BGFV True\n",
            "PFD True\n",
            "SCPL True\n",
            "JGH True\n",
            "EFR True\n",
            "FFBW True\n",
            "RESI True\n",
            "CIO True\n",
            "FNB True\n",
            "EFT True\n",
            "SMSEY True\n",
            "AEO True\n",
            "CSUAY True\n",
            "TILE True\n",
            "MUC True\n",
            "ARI True\n",
            "DMO True\n",
            "HEQ True\n",
            "PDM True\n",
            "CHRS True\n",
            "CNHI True\n",
            "PMO True\n",
            "FOR True\n",
            "QNST True\n",
            "DSL True\n",
            "AMCR True\n",
            "PAGP True\n",
            "AMKBY True\n",
            "HYI True\n",
            "RHRX True\n",
            "NSTG True\n",
            "CHY True\n",
            "KIO True\n",
            "MNP True\n",
            "NECB True\n",
            "PX True\n",
            "MQY True\n",
            "ASPN True\n",
            "XPRO True\n",
            "MN True\n",
            "ULCC True\n",
            "NFJ True\n",
            "JSD True\n",
            "WBD True\n",
            "KTEC True\n",
            "MPV True\n",
            "GDIV True\n",
            "NXJ True\n",
            "RLJ True\n",
            "BYM True\n",
            "NCLH True\n",
            "ASYS True\n",
            "WRBY True\n",
            "SEMR True\n",
            "SRG True\n",
            "BAK True\n",
            "HPKEW True\n",
            "CARS True\n",
            "IPWR True\n",
            "AIF True\n",
            "HRZN True\n",
            "HFGO True\n",
            "MHD True\n",
            "ISD True\n",
            "NIQ True\n",
            "PBR.A True\n",
            "PGZ True\n",
            "PBBK True\n",
            "CMT True\n",
            "TRDA True\n",
            "CGEM True\n",
            "ARDC True\n",
            "YYY True\n",
            "CLST True\n",
            "NCMGY True\n",
            "NMCO True\n",
            "BITO True\n",
            "SPE True\n",
            "MUJ True\n",
            "JDST True\n",
            "NMFC True\n",
            "LMNR True\n",
            "TCDA True\n",
            "TWODY True\n",
            "CPSS True\n",
            "ORCC True\n",
            "MPA True\n",
            "MANU True\n",
            "GER True\n",
            "FRST True\n",
            "ADIV True\n",
            "AFT True\n",
            "CZWI True\n",
            "IIM True\n",
            "DMB True\n",
            "BNTGY True\n",
            "TPVG True\n",
            "BLW True\n",
            "TWKS True\n",
            "SCD True\n",
            "NID True\n",
            "ACRE True\n",
            "BRKL True\n",
            "MIY True\n",
            "SGC True\n",
            "AVK True\n",
            "PHYS True\n",
            "TGI True\n",
            "AHH True\n",
            "BXMX True\n",
            "FRTY True\n",
            "MARA True\n",
            "WHG True\n",
            "FGEN True\n",
            "MITEY True\n",
            "JRI True\n",
            "HBAN True\n",
            "HOTL True\n",
            "NXC True\n",
            "LC True\n",
            "MEGEF True\n",
            "AAL True\n",
            "TPZ True\n",
            "BSL True\n",
            "STEM True\n",
            "TCPC True\n",
            "CFB True\n",
            "LSBK True\n",
            "VRE True\n",
            "GBDC True\n",
            "PTY True\n",
            "GDO True\n",
            "VOD True\n",
            "AAN True\n",
            "HPP True\n",
            "FRSH True\n",
            "WMC True\n",
            "CWBC True\n",
            "BLX True\n",
            "EXTR True\n",
            "PAGS True\n",
            "STLA True\n",
            "FBRT True\n",
            "SSUMY True\n",
            "RFI True\n",
            "KTOS True\n",
            "FMNB True\n",
            "RMBI True\n",
            "DMLRY True\n",
            "SRGHY True\n",
            "MRAAY True\n",
            "BGH True\n",
            "PRMW True\n",
            "NMAI True\n",
            "HONE True\n",
            "CHSTY True\n",
            "CSTM True\n",
            "ECBK True\n",
            "GT True\n",
            "NSYS True\n",
            "PCN True\n",
            "SWBI True\n",
            "OSBC True\n",
            "FPI True\n",
            "JHI True\n",
            "EXPI True\n",
            "SJT True\n",
            "CGBD True\n",
            "BKRKY True\n",
            "TMAT True\n",
            "HCSG True\n",
            "ARL True\n",
            "CBAN True\n",
            "PROV True\n",
            "HPE True\n",
            "RTMVY True\n",
            "ASEA True\n",
            "ARCE True\n",
            "FSTR True\n",
            "CSII True\n",
            "BCRX True\n",
            "BCMXY True\n",
            "ACET True\n",
            "NWBI True\n",
            "RBCN True\n",
            "AQWA True\n",
            "RQI True\n",
            "PBR True\n",
            "FCF True\n",
            "RAND True\n",
            "IVRA True\n",
            "KRO True\n",
            "ABCM True\n",
            "KAR True\n",
            "ALKT True\n",
            "PVBC True\n",
            "AFBI True\n",
            "MRAD True\n",
            "GAIN True\n",
            "BCSF True\n",
            "SMR True\n",
            "ASA True\n",
            "AMBC True\n",
            "OLPX True\n",
            "SLCA True\n",
            "SBH True\n",
            "DGICB True\n",
            "TGSGY True\n",
            "PCQ True\n",
            "DPG True\n",
            "LXU True\n",
            "NMS True\n",
            "SLRC True\n",
            "BZH True\n",
            "CPLP True\n",
            "LAKE True\n",
            "DGICA True\n",
            "SLM True\n",
            "PMT True\n",
            "SKYT True\n",
            "MDV True\n",
            "PGY True\n",
            "CAF True\n",
            "FEAM True\n",
            "DRIP True\n",
            "DNB True\n",
            "MSADY True\n",
            "CHYHY True\n",
            "HA True\n",
            "BIOX True\n",
            "CPRX True\n",
            "AERI True\n",
            "FBP True\n",
            "DCPH True\n",
            "SGHIY True\n",
            "WHF True\n",
            "MTG True\n",
            "EDIT True\n",
            "CSQ True\n",
            "PDT True\n",
            "MPW True\n",
            "AU True\n",
            "FONR True\n",
            "EFC True\n",
            "F True\n",
            "COCO True\n",
            "IOT True\n",
            "ABR True\n",
            "BSAC True\n",
            "RNRG True\n",
            "ATEN True\n",
            "NGVC True\n",
            "FFNW True\n",
            "KEMQ True\n",
            "SSBI True\n",
            "LPG True\n",
            "HQL True\n",
            "UBCP True\n",
            "MPAA True\n",
            "SEOAY True\n",
            "TNP True\n",
            "BKN True\n",
            "CRSR True\n",
            "SBRA True\n",
            "LE True\n",
            "FRBA True\n",
            "MLVF True\n",
            "INVE True\n",
            "AMPL True\n",
            "NRGX True\n",
            "LGGNY True\n",
            "PSTL True\n",
            "JCE True\n",
            "RVT True\n",
            "MMSC True\n",
            "FIF True\n",
            "RBGLY True\n",
            "WOOF True\n",
            "TRC True\n",
            "VBF True\n",
            "CALT True\n",
            "NPSCY True\n",
            "HDLB True\n",
            "DX True\n",
            "GNK True\n",
            "NPNYY True\n",
            "EOPS True\n",
            "WU True\n",
            "BETZ True\n",
            "SHI True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=SHI&interval=day&span=3month&bounds=regular\n",
            "MERC True\n",
            "NUVL True\n",
            "PTEN True\n",
            "RIOCF True\n",
            "DIAX True\n",
            "NXDT True\n",
            "TWI True\n",
            "DBL True\n",
            "TMST True\n",
            "MOD True\n",
            "CSAN True\n",
            "IVR True\n",
            "HTLD True\n",
            "EOI True\n",
            "GOLD True\n",
            "SPXX True\n",
            "CEF True\n",
            "IMLP True\n",
            "CWCO True\n",
            "HPS True\n",
            "AVO True\n",
            "CNCR True\n",
            "TAIL True\n",
            "PAAS True\n",
            "SKT True\n",
            "HOFT True\n",
            "SWDHY True\n",
            "CRUZ True\n",
            "TTMI True\n",
            "SPH True\n",
            "UAE True\n",
            "GOGO True\n",
            "NS True\n",
            "APLE True\n",
            "AKR True\n",
            "NGMS True\n",
            "LBAI True\n",
            "MDRX True\n",
            "OEC True\n",
            "TDS True\n",
            "BMEZ True\n",
            "DBOEY True\n",
            "MLNK True\n",
            "AIRT True\n",
            "BEKE True\n",
            "IPOS True\n",
            "CARE True\n",
            "ACAD True\n",
            "FULT True\n",
            "RCKT True\n",
            "ACNT True\n",
            "SARDY True\n",
            "BPT True\n",
            "BBW True\n",
            "OTSKY True\n",
            "BASE True\n",
            "JLS True\n",
            "LGI True\n",
            "MFLX True\n",
            "XHR True\n",
            "DMRC True\n",
            "DLHC True\n",
            "SDVKY True\n",
            "DKNG True\n",
            "CMSQY True\n",
            "GOF True\n",
            "ENRG True\n",
            "KELYA True\n",
            "FLXS True\n",
            "KROP True\n",
            "MTR True\n",
            "NRT True\n",
            "AFTY True\n",
            "HKSHY True\n",
            "TYNE True\n",
            "BRDG True\n",
            "NOAH True\n",
            "RELL True\n",
            "PAHC True\n",
            "ALIZY True\n",
            "IGI True\n",
            "DOC True\n",
            "EATV True\n",
            "NWSA True\n",
            "NXGN True\n",
            "CLB True\n",
            "ROIC True\n",
            "AFK True\n",
            "AAAU True\n",
            "SEMI True\n",
            "ASAI True\n",
            "HOLI True\n",
            "ONB True\n",
            "NDGPY True\n",
            "CLPXY True\n",
            "GSL True\n",
            "CPZ True\n",
            "CIZN True\n",
            "NGLOY True\n",
            "SOHU True\n",
            "CIR True\n",
            "FFA True\n",
            "NBB True\n",
            "CMPS True\n",
            "UBA True\n",
            "EPRE True\n",
            "NJDCY True\n",
            "SBFG True\n",
            "DISH True\n",
            "FFC True\n",
            "DBI True\n",
            "CLMT True\n",
            "HHS True\n",
            "IVZ True\n",
            "CEA True\n",
            "CNX True\n",
            "BGLD True\n",
            "LEGH True\n",
            "PHG True\n",
            "FNA True\n",
            "HRT True\n",
            "AIO True\n",
            "WSBF True\n",
            "HPI True\n",
            "PBT True\n",
            "BWB True\n",
            "TWEB True\n",
            "SMGZY True\n",
            "GGRW True\n",
            "CVCY True\n",
            "SSYS True\n",
            "TYA True\n",
            "MGIC True\n",
            "MFC True\n",
            "RCMT True\n",
            "ACLX True\n",
            "DKILY True\n",
            "FMS True\n",
            "KSTR True\n",
            "VTRU True\n",
            "T True\n",
            "CRCBY True\n",
            "SWKH True\n",
            "HPF True\n",
            "WNC True\n",
            "AWAY True\n",
            "KRP True\n",
            "XOMA True\n",
            "AINC True\n",
            "FLC True\n",
            "HEP True\n",
            "FCCO True\n",
            "OVLY True\n",
            "AEHR True\n",
            "ICPT True\n",
            "AMSWA True\n",
            "ARIS True\n",
            "PXH True\n",
            "CDMO True\n",
            "LQDT True\n",
            "CEN True\n",
            "UMPQ True\n",
            "WB True\n",
            "NIO True\n",
            "FRC-K True\n",
            "Warning: \"FRC-K\" is not a valid stock ticker. It is being ignored\n",
            "NOV True\n",
            "FUNC True\n",
            "UMH True\n",
            "GVAL True\n",
            "GK True\n",
            "XPOF True\n",
            "ASBFY True\n",
            "EOS True\n",
            "HST True\n",
            "KRT True\n",
            "DEA True\n",
            "BDCZ True\n",
            "BCH True\n",
            "MFEM True\n",
            "THR True\n",
            "MMD True\n",
            "RSF True\n",
            "BUFG True\n",
            "CARG True\n",
            "ESCR True\n",
            "DTEGY True\n",
            "KEY True\n",
            "SEPJY True\n",
            "AMX True\n",
            "PEB True\n",
            "ALBO True\n",
            "PBHC True\n",
            "TEN True\n",
            "TMCI True\n",
            "YNDX True\n",
            "Warning: \"YNDX\" is not a valid stock ticker. It is being ignored\n",
            "WBIT True\n",
            "IROQ True\n",
            "WDNA True\n",
            "BUFT True\n",
            "PARR True\n",
            "SATS True\n",
            "CARCY True\n",
            "RUTH True\n",
            "BCBP True\n",
            "ATAX True\n",
            "CRK True\n",
            "SMGBY True\n",
            "M True\n",
            "SSREY True\n",
            "BANX True\n",
            "TSLX True\n",
            "ESSA True\n",
            "EXEL True\n",
            "QURE True\n",
            "CVE True\n",
            "IGLD True\n",
            "EMFM True\n",
            "EGPT True\n",
            "FSST True\n",
            "SAMG True\n",
            "AI True\n",
            "INFY True\n",
            "PGHY True\n",
            "SNDA True\n",
            "FLIC True\n",
            "CHAD True\n",
            "MLPB True\n",
            "LEE True\n",
            "SGH True\n",
            "ALEX True\n",
            "EBMT True\n",
            "FLCH True\n",
            "ASGI True\n",
            "AVTE True\n",
            "CNO True\n",
            "GARPY True\n",
            "ZEAL True\n",
            "GLDB True\n",
            "EATZ True\n",
            "KEQU True\n",
            "STBL True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=STBL&interval=day&span=3month&bounds=regular\n",
            "HTZ True\n",
            "PBFX True\n",
            "BRDCY True\n",
            "UBOH True\n",
            "FPF True\n",
            "KBUY True\n",
            "HTAB True\n",
            "CII True\n",
            "NPFD True\n",
            "AZEK True\n",
            "ASLE True\n",
            "RF-E True\n",
            "SHBI True\n",
            "ESI True\n",
            "EXFY True\n",
            "ASAN True\n",
            "NWLG True\n",
            "NBCC True\n",
            "PUNK True\n",
            "PTA True\n",
            "TENG True\n",
            "ANF True\n",
            "LMST True\n",
            "MEGGY True\n",
            "HBNC True\n",
            "FQVLF True\n",
            "EPRF True\n",
            "YUMY True\n",
            "ATMP True\n",
            "OCEN True\n",
            "GOOD True\n",
            "FSMB True\n",
            "KNYJY True\n",
            "HQH True\n",
            "CMBM True\n",
            "IGT True\n",
            "NOTV True\n",
            "CRNC True\n",
            "NXDT-A True\n",
            "DLA True\n",
            "BSET True\n",
            "THQ True\n",
            "WEN True\n",
            "BUFD True\n",
            "JWN True\n",
            "JYNT True\n",
            "TRMD True\n",
            "EVRI True\n",
            "ETX True\n",
            "KRG True\n",
            "FFWM True\n",
            "MOHR True\n",
            "UEHPY True\n",
            "400 Client Error: Bad Request for url: https://api.robinhood.com/quotes/historicals/?symbols=UEHPY&interval=day&span=3month&bounds=regular\n",
            "ARCC True\n",
            "KESG True\n",
            "PERI True\n",
            "FDUS True\n",
            "AHOY True\n",
            "WRE True\n",
            "NBDS True\n",
            "CIIHY True\n",
            "JPI True\n",
            "CRIT True\n",
            "PSFO True\n",
            "CVLY True\n",
            "CSWC True\n",
            "RFP True\n",
            "SOMMY True\n",
            "LBTYA True\n",
            "OCFC True\n",
            "PMTS True\n",
            "ARRY True\n",
            "BSTZ True\n",
            "EML True\n",
            "CALB True\n",
            "CPII True\n",
            "DEI True\n",
            "GDMN True\n",
            "NUSI True\n",
            "NEOG True\n",
            "VSH True\n",
            "MIELY True\n",
            "HDEF True\n",
            "BSCO True\n",
            "KMTUY True\n",
            "NWL True\n",
            "CVET True\n",
            "SVNDY True\n",
            "CDNA True\n",
            "SAEF True\n",
            "MCBS True\n",
            "HFBL True\n",
            "DSTX True\n",
            "BLNK True\n",
            "FPFD True\n",
            "GDOT True\n",
            "KSET True\n",
            "HYIN True\n",
            "RDWR True\n",
            "HERO True\n",
            "BSCN True\n",
            "BZQ True\n",
            "SMID True\n",
            "BKLN True\n",
            "WWW True\n",
            "FSMO True\n",
            "ASB True\n",
            "IBCP True\n",
            "APPS True\n",
            "LDP True\n",
            "BURBY True\n",
            "PTBD True\n",
            "ARKR True\n",
            "OALC True\n",
            "AGQ True\n",
            "AMKR True\n",
            "OFIX True\n",
            "CFBK True\n",
            "SKM True\n",
            "BSCM True\n",
            "TBNK True\n",
            "CAPD True\n",
            "MIG True\n",
            "FFTI True\n",
            "PETS True\n",
            "BLHY True\n",
            "SKYW True\n",
            "KOCG True\n",
            "GBL True\n",
            "MYE True\n",
            "PRIM True\n",
            "DISV True\n",
            "GAST True\n",
            "RGCO True\n",
            "URBN True\n",
            "AGL True\n",
            "AVD True\n",
            "GFGF True\n",
            "MFUL True\n",
            "EMFQ True\n",
            "HDAW True\n",
            "VCYT True\n",
            "NREF True\n",
            "PSF True\n",
            "RDNT True\n",
            "PSFJ True\n",
            "FLLA True\n",
            "CPHC True\n",
            "CTRE True\n",
            "HCKT True\n",
            "NEWT True\n",
            "BLMN True\n",
            "VR True\n",
            "EWCZ True\n",
            "FFIC True\n",
            "BCOR True\n",
            "FLSP True\n",
            "MAAX True\n",
            "BFIT True\n",
            "SPNS True\n",
            "WBII True\n",
            "BSRR True\n",
            "HRGLY True\n",
            "IBN True\n",
            "REZI True\n",
            "CLBK True\n",
            "MSMR True\n",
            "AKZOY True\n",
            "PRO True\n",
            "VREX True\n",
            "DSCF True\n",
            "TDSA True\n",
            "PDI True\n",
            "TDSB True\n",
            "PSFF True\n",
            "NDJI True\n",
            "FLCO True\n",
            "BZ True\n",
            "AGOX True\n",
            "MRTN True\n",
            "RDIB True\n",
            "DFP True\n",
            "HYTR True\n",
            "DLX True\n",
            "CRT True\n",
            "ITAN True\n",
            "BOB True\n",
            "KIM True\n",
            "CBFV True\n",
            "RVNC True\n",
            "NSCS True\n",
            "MMCA True\n",
            "VECO True\n",
            "RF True\n",
            "FIVR True\n",
            "FLCB True\n",
            "ISTR True\n",
            "PSFM True\n",
            "GDV True\n",
            "EWG True\n",
            "BUFQ True\n",
            "RSPE True\n",
            "SAXPY True\n",
            "TDSD True\n",
            "GDE True\n",
            "ACV True\n",
            "ISDR True\n",
            "CPER True\n",
            "CIVB True\n",
            "ATLO True\n",
            "ICLN True\n",
            "BRKY True\n",
            "AIQ True\n",
            "BLFS True\n",
            "INFA True\n",
            "COFS True\n",
            "NMIH True\n",
            "RNA True\n",
            "HNDL True\n",
            "EWA True\n",
            "FNKO True\n",
            "AUDC True\n",
            "MAMB True\n",
            "BRX True\n",
            "VIDI True\n",
            "SGDM True\n",
            "RTAI True\n",
            "GDVD True\n",
            "HMNF True\n",
            "SPBC True\n",
            "PUK True\n",
            "DBEM True\n",
            "FFBC True\n",
            "ANGO True\n",
            "BWX True\n",
            "NZRO True\n",
            "CMPGY True\n",
            "DYLD True\n",
            "RFCI True\n",
            "ALL-H True\n",
            "BUFR True\n",
            "PRAY True\n",
            "KE True\n",
            "AIRTP True\n",
            "VLGEA True\n",
            "PBD True\n",
            "BCHEY True\n",
            "PBEE True\n",
            "CHUY True\n",
            "FHN True\n",
            "MNDT True\n",
            "KALL True\n",
            "BMED True\n",
            "MDCP True\n",
            "TDSE True\n",
            "BNDC True\n",
            "SHEN True\n",
            "PLDR True\n",
            "HEET True\n",
            "ANIK True\n",
            "CTGO True\n",
            "FROG True\n",
            "RSPY True\n",
            "PSFD True\n",
            "RLAY True\n",
            "AFMC True\n",
            "TDSC True\n",
            "CODI True\n",
            "ELY True\n",
            "CPF True\n",
            "MURGY True\n",
            "PKBK True\n",
            "ERII True\n",
            "NIE True\n",
            "BIS True\n",
            "CBLS True\n",
            "EPRT True\n",
            "BVH True\n",
            "RIGS True\n",
            "NLOK True\n",
            "SKHHY True\n",
            "CAPE True\n",
            "NUSA True\n",
            "DVLU True\n",
            "FLDZ True\n",
            "PLAB True\n",
            "MPAY True\n",
            "RNP True\n",
            "SPHY True\n",
            "SAGP True\n",
            "SIXJ True\n",
            "TRQ True\n",
            "ESEA True\n",
            "GPK True\n",
            "X True\n",
            "BSCE True\n",
            "BFIX True\n",
            "FSFG True\n",
            "MSTQ True\n",
            "FGLD True\n",
            "PBDM True\n",
            "EBIX True\n",
            "CGDV True\n",
            "STCE True\n",
            "CGCP True\n",
            "STWD True\n",
            "IIIV True\n",
            "MAT True\n",
            "UEIC True\n",
            "JAKK True\n",
            "PWOD True\n",
            "HSTM True\n",
            "FCSH True\n",
            "AGGH True\n",
            "SAR True\n",
            "SBGI True\n",
            "OACP True\n",
            "MCH True\n",
            "IOCT True\n",
            "BUI True\n",
            "KBCSY True\n",
            "MYGN True\n",
            "KONG True\n",
            "BUSE True\n",
            "TBBK True\n",
            "MT True\n",
            "HPK True\n",
            "BBU True\n",
            "SPLP-A True\n",
            "CCD True\n",
            "MLR True\n",
            "RMBL True\n",
            "MRVI True\n",
            "RF-C True\n",
            "OPOF True\n",
            "DIEM True\n",
            "LSST True\n",
            "SMIG True\n",
            "BUYZ True\n",
            "NTGR True\n",
            "HEQT True\n",
            "FGBI True\n",
            "SAL True\n",
            "CCRD True\n",
            "BSBE True\n",
            "DIVS True\n",
            "UAV True\n",
            "SAVE True\n",
            "FMCX True\n",
            "AMID True\n",
            "RDFI True\n",
            "TACK True\n",
            "TPLE True\n",
            "DRIV True\n",
            "LAND True\n",
            "SSBK True\n",
            "IMOS True\n",
            "AFSM True\n",
            "LEXI True\n",
            "ALRS True\n",
            "ZECP True\n",
            "PEGRY True\n",
            "WDS True\n",
            "DEIF True\n",
            "MNSB True\n",
            "NX True\n",
            "BOOM True\n",
            "SFNC True\n",
            "SIX True\n",
            "EMLC True\n",
            "CNM True\n",
            "DRQ True\n",
            "ORI True\n",
            "PFS True\n",
            "SNN True\n",
            "HTBI True\n",
            "SAMT True\n",
            "JRVR True\n",
            "VRDN True\n",
            "ASHX True\n",
            "HLGE True\n",
            "FLTN True\n",
            "MMIN True\n",
            "MATV True\n",
            "MMIT True\n",
            "AFLG True\n",
            "AAA True\n",
            "ADTN True\n",
            "PARA True\n",
            "MBCC True\n",
            "CZNC True\n",
            "GERM True\n",
            "HAIN True\n",
            "SPWR True\n",
            "GTEK True\n",
            "AAPD True\n",
            "ACIW True\n",
            "PCPC True\n",
            "FJTSY True\n",
            "CCRN True\n",
            "NOVA True\n",
            "NIB True\n",
            "ALGM True\n",
            "IIF True\n",
            "SNDR True\n",
            "SCVL True\n",
            "NBCT True\n",
            "KF True\n",
            "TNK True\n",
            "JUNZ True\n",
            "HNGKY True\n",
            "CSH True\n",
            "CIHKY True\n",
            "RSEE True\n",
            "FLTR True\n",
            "ALKS True\n",
            "MINV True\n",
            "MMSB True\n",
            "HTD True\n",
            "TGLS True\n",
            "FSBC True\n",
            "PAM True\n",
            "HURC True\n"
          ]
        }
      ],
      "source": [
        "#We need to have the stock info file ready \"stock_info.txt\"\n",
        "import json\n",
        "from datetime import datetime\n",
        "date_str=datetime.today().strftime('%Y-%m-%d')\n",
        "daily_dir=\"daily\"\n",
        "daily_dir_path=os.path.join(cwd,daily_dir)\n",
        "if not os.path.exists(daily_dir_path): os.makedirs(daily_dir_path)\n",
        "daily_fpath=os.path.join(daily_dir_path,date_str+\".txt\")\n",
        "daily_fopen=open(daily_fpath,\"w\")\n",
        "#stock_info_fname=\"stock_info-aug28.txt\"\n",
        "stock_info_fname=\"valid_stock_25_less_aug_28.txt\"\n",
        "stock_info_fopen=open(stock_info_fname)\n",
        "for f_i,f_line in enumerate(stock_info_fopen):\n",
        "  #if f_i%50: print(f_i)\n",
        "  cur_info_dict=json.loads(f_line)\n",
        "  if cur_info_dict==None: continue\n",
        "  sym0=cur_info_dict.get(\"symbol\") \n",
        "  if sym0==None: continue\n",
        "  is_tradeable=cur_info_dict[\"tradeable\"] \n",
        "  print(sym0, is_tradeable)\n",
        "  if not is_tradeable: continue\n",
        "  cur_stock_data = rs.robinhood.stocks.get_stock_historicals([sym0], interval=\"day\", span=\"3month\")\n",
        "  cur_stock_data_json=json.dumps(cur_stock_data)\n",
        "  price_line=\"%s\\t%s\\n\"%(sym0,cur_stock_data_json)\n",
        "  daily_fopen.write(price_line)\n",
        "  # print(cur_stock_data)\n",
        "  # print(\"-----\")\n",
        "  #if f_i>10: break\n",
        "\n",
        "stock_info_fopen.close()\n",
        "daily_fopen.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxBM_nsLaznl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPKotP1DfTVy"
      },
      "source": [
        "#Final Robinhood testing - Aug 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVbOYYScSJi9",
        "outputId": "d04cde6d-b562-4e31-f8b7-a651d6b21c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63\n",
            "{'begins_at': '2022-05-23T00:00:00Z', 'open_price': '255.490000', 'close_price': '260.650000', 'high_price': '261.500000', 'low_price': '253.430000', 'volume': 33175379, 'session': 'reg', 'interpolated': False, 'symbol': 'MSFT'}\n",
            "{'begins_at': '2022-08-22T00:00:00Z', 'open_price': '282.080000', 'close_price': '277.750000', 'high_price': '282.460000', 'low_price': '277.220000', 'volume': 25061070, 'session': 'reg', 'interpolated': False, 'symbol': 'MSFT'}\n",
            "(261.5, 253.43, 255.49, 260.65)\n",
            "(261.33, 253.5, 257.89, 259.62)\n",
            "(264.58, 257.125, 258.14, 262.52)\n",
            "(267.11, 261.4294, 262.27, 265.9)\n",
            "(273.34, 267.56, 268.48, 273.24)\n",
            "(274.77, 268.93, 272.53, 271.87)\n",
            "(277.69, 270.04, 275.195, 272.42)\n",
            "(274.65, 261.6, 264.45, 274.58)\n",
            "(273.45, 268.41, 270.31, 270.02)\n",
            "(274.18, 267.22, 272.06, 268.75)\n",
            "(273.13, 265.94, 266.635, 272.5)\n",
            "(273.0, 269.61, 271.71, 270.41)\n",
            "(272.7081, 264.63, 267.78, 264.79)\n",
            "(260.58, 252.53, 260.58, 252.99)\n",
            "(249.0242, 241.53, 245.11, 242.26)\n",
            "(245.74, 241.51, 243.86, 244.49)\n",
            "(255.3, 246.42, 248.31, 251.76)\n",
            "(247.4174, 243.02, 245.98, 244.97)\n",
            "(250.5, 244.03, 244.7, 247.65)\n",
            "(254.75, 249.51, 250.255, 253.74)\n",
            "test_labels ['1', '0', '0', '-1', '0', '-1', '0', '-6', '-4', '-6', '-4', '-6', '-2', '-4', '-2', '-4', '-2', '-4', '0', '-2']\n",
            "tensor([[[-6.0560e-04,  3.4457e-04, -1.0643e-02,  2.2767e-03, -4.6581e-04,\n",
            "           9.1116e-03,  5.3106e-01,  2.3568e-01,  9.8993e-02,  4.1709e-02,\n",
            "           1.7481e-02, -4.8091e-04,  1.0310e-02, -2.3892e-03,  2.9890e-03,\n",
            "           1.8590e-02,  2.9303e-02,  8.2207e-02,  2.2928e-01,  5.1441e-01,\n",
            "           5.9751e-03,  4.5224e-03,  1.3467e-03, -2.4348e-04,  1.8791e-03,\n",
            "           2.0724e-03,  6.1288e-03, -9.6033e-04,  8.7439e-03,  8.4520e-03,\n",
            "           2.9370e-02,  6.7027e-02,  3.9290e-01,  2.0380e-01,  1.1153e-01,\n",
            "           7.2198e-02,  3.0098e-02,  3.6381e-03,  3.3056e-02,  4.3977e-02,\n",
            "          -8.4018e-04,  2.8854e-02,  5.0649e-02,  1.1286e-01,  1.7370e-01,\n",
            "           4.0457e-01,  6.5547e-02,  2.1490e-02,  1.8708e-02,  1.1050e-02,\n",
            "          -5.6272e-04,  1.4449e-02,  2.1270e-02,  6.6504e-04,  1.3477e-02,\n",
            "           2.1286e-02,  3.9764e-02,  7.5615e-02,  3.1789e-01,  1.6623e-01,\n",
            "           1.2226e-01,  7.4132e-02,  4.0515e-02, -5.3127e-04,  6.5080e-02,\n",
            "           5.2654e-02,  4.6299e-03,  3.6748e-02,  4.9742e-02,  1.0175e-01,\n",
            "           1.5620e-01,  3.2665e-01,  7.7654e-02,  4.1259e-02,  3.3032e-02,\n",
            "           1.0450e-02,  5.7696e-03,  7.1844e-03,  2.5648e-02, -2.1885e-03,\n",
            "           1.1499e-02,  3.0764e-02,  3.8900e-02,  8.3479e-02,  2.7896e-01,\n",
            "           1.6473e-01,  1.1680e-01,  7.6140e-02,  5.2054e-02,  7.0702e-04,\n",
            "           8.3155e-02,  1.0829e-01, -1.5808e-04,  4.7064e-02,  6.4102e-02,\n",
            "           1.0509e-01,  1.3687e-01,  2.8542e-01,  1.1090e-01,  5.8691e-02,\n",
            "           4.0410e-02,  1.3678e-02,  2.0818e-03,  1.4483e-02,  9.7353e-03,\n",
            "           2.4872e-04,  1.7682e-02,  3.2833e-02,  5.3043e-02,  9.1331e-02,\n",
            "           2.3781e-01,  1.3316e-01,  1.1466e-01,  8.6520e-02,  5.5907e-02,\n",
            "           2.7873e-03,  1.0891e-01,  9.6116e-02,  1.3008e-03,  3.4725e-02,\n",
            "           7.0931e-02,  9.6610e-02,  1.3423e-01,  2.6169e-01,  1.0657e-01,\n",
            "           6.6640e-02,  4.3648e-02,  2.0344e-02,  2.8965e-03,  3.2285e-02,\n",
            "           2.1360e-02, -1.1418e-03,  2.6555e-02,  2.7234e-02,  4.8220e-02,\n",
            "           7.7535e-02,  2.3940e-01,  1.2879e-01,  9.5011e-02,  7.1738e-02,\n",
            "           4.2777e-02, -3.7035e-03,  1.3311e-01,  1.2148e-01,  9.1651e-04,\n",
            "           3.9239e-02,  6.3593e-02,  9.9801e-02,  1.1227e-01,  2.3536e-01,\n",
            "           9.5146e-02,  6.9946e-02,  4.9192e-02,  3.3409e-02, -5.2608e-03,\n",
            "           5.3645e-02,  4.4654e-02,  1.2549e-03,  2.1136e-02,  3.9985e-02,\n",
            "           5.3566e-02,  8.9511e-02,  1.9903e-01,  1.0869e-01,  1.0472e-01,\n",
            "           8.4754e-02,  6.3662e-02, -2.3691e-03,  1.5977e-01,  1.2772e-01,\n",
            "           4.7671e-03,  5.0035e-02,  7.3497e-02,  8.6244e-02,  8.9781e-02,\n",
            "           2.3533e-01,  8.7350e-02,  6.6565e-02,  5.1411e-02,  3.3853e-02,\n",
            "          -1.9455e-03,  5.5618e-02,  4.9313e-02, -1.8696e-03,  2.4079e-02,\n",
            "           3.6751e-02,  5.1181e-02,  8.2729e-02,  2.0861e-01,  1.0956e-01,\n",
            "           9.2255e-02,  6.3885e-02,  6.7859e-02, -3.3223e-03,  1.6286e-01,\n",
            "           1.4427e-01,  6.2581e-04,  4.8805e-02,  6.4312e-02,  8.2453e-02,\n",
            "           9.4056e-02,  2.2381e-01,  8.2299e-02,  6.7170e-02,  4.9175e-02,\n",
            "           3.8495e-02,  6.4371e-04,  7.0098e-02,  5.4469e-02,  8.7136e-04,\n",
            "           2.9641e-02,  3.1900e-02,  4.0815e-02,  6.2020e-02,  1.7649e-01,\n",
            "           1.0413e-01,  8.8676e-02,  8.3652e-02,  7.4758e-02, -1.1437e-03,\n",
            "           1.9530e-01,  1.5412e-01,  4.4116e-03,  4.4863e-02,  5.8012e-02,\n",
            "           7.8571e-02,  1.0162e-01,  1.9376e-01,  8.6132e-02,  7.3057e-02,\n",
            "           6.3244e-02,  3.5631e-02,  2.3522e-05,  8.3879e-02,  6.4294e-02,\n",
            "          -3.3903e-03,  2.9650e-02,  3.9125e-02,  4.1293e-02,  6.3895e-02,\n",
            "           1.8399e-01,  1.1157e-01,  7.9177e-02,  7.5557e-02,  5.7903e-02,\n",
            "           3.1334e-03,  1.8743e-01,  1.5404e-01,  1.8959e-03,  4.5748e-02,\n",
            "           5.5485e-02,  7.2999e-02,  9.4869e-02,  2.0034e-01,  7.9358e-02,\n",
            "           7.1977e-02,  4.5906e-02,  2.6413e-02, -4.2263e-03,  9.7383e-02]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "ac_high 1 pred_high 0 pred_wt 0.5311 cur_low ('0', ('0', 0.5144))\n",
            "ac_high 0 pred_high 0 pred_wt 0.3929 cur_low ('-1', ('0', 0.4046))\n",
            "ac_high 0 pred_high 0 pred_wt 0.3179 cur_low ('-1', ('0', 0.3267))\n",
            "ac_high 0 pred_high 0 pred_wt 0.279 cur_low ('-6', ('0', 0.2854))\n",
            "ac_high -4 pred_high 0 pred_wt 0.2378 cur_low ('-6', ('0', 0.2617))\n",
            "ac_high -4 pred_high 0 pred_wt 0.2394 cur_low ('-6', ('0', 0.2354))\n",
            "ac_high -2 pred_high 0 pred_wt 0.199 cur_low ('-4', ('0', 0.2353))\n",
            "ac_high -2 pred_high 0 pred_wt 0.2086 cur_low ('-4', ('0', 0.2238))\n",
            "ac_high -2 pred_high 6 pred_wt 0.1953 cur_low ('-4', ('0', 0.1938))\n",
            "ac_high 0 pred_high 6 pred_wt 0.1874 cur_low ('-2', ('0', 0.2003))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "#cur_stock_data = rs.robinhood.stocks.get_stock_historicals([\"AAPL\"], interval=\"day\", span=\"3month\")\n",
        "cur_stock_data = rs.robinhood.stocks.get_stock_historicals([\"MSFT\"], interval=\"day\", span=\"3month\")\n",
        "print(len(cur_stock_data))\n",
        "print(cur_stock_data[0])\n",
        "print(cur_stock_data[-1])\n",
        "high_low_open_close_data=[(float(v[\"high_price\"]),float(v[\"low_price\"]),float(v[\"open_price\"]),float(v[\"close_price\"])) for v in cur_stock_data]\n",
        "for a in high_low_open_close_data[:20]:\n",
        "  print(a)\n",
        "\n",
        "\n",
        "epoch_i=3\n",
        "exp_name=\"new-stock-unflattened-sample19-30-10-128-0000001\"\n",
        "\n",
        "model_dir=\"models\"\n",
        "exp_dir_path=os.path.join(model_dir,exp_name)\n",
        "tmp_path=os.path.join(exp_dir_path,\"model-%s.model\"%epoch_i)\n",
        "try: checkpoint = torch.load(tmp_path)\n",
        "except: checkpoint = dill_unpickle(tmp_path)\n",
        "rnn = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , checkpoint[\"matching_in_out\"]).to(device)\n",
        "n_input=checkpoint[\"n_input\"]\n",
        "standard_labels=checkpoint[\"output_labels\"]\n",
        "cur_parameters=checkpoint[\"feature_extraction_parameters\"]\n",
        "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn.eval()\n",
        "\n",
        "start_i=0\n",
        "n_prev=30\n",
        "n_next=10\n",
        "prev_vals=high_low_open_close_data[start_i:start_i+n_prev]\n",
        "next_vals=high_low_open_close_data[start_i+n_prev:start_i+n_prev+n_next]\n",
        "test_ft=normalize_ft_vals(prev_vals)\n",
        "test_labels=normalize_next_labels(next_vals,prev_vals[-1][-1])\n",
        "print(\"test_labels\",test_labels)\n",
        "\n",
        "ft_list_tensor=torch.tensor(test_ft,dtype=torch.float32)\n",
        "#lb_list_tensor=torch.tensor(lb_list,dtype=torch.float32)\n",
        "#lb_tensor_flat=lb_list_tensor.ravel()\n",
        "rnn_out=rnn(ft_list_tensor)\n",
        "print(rnn_out)\n",
        "rnn_out_flat=rnn_out.ravel()\n",
        "preds=out2labels(rnn_out_flat,standard_labels)\n",
        "eval_list=[]\n",
        "for ac0,pred0 in zip(test_labels,preds):\n",
        "  cur_pred=[(v[0],round(v[1].item(),4)) for v in pred0]\n",
        "  eval_list.append((ac0,cur_pred[0]))\n",
        "for i0 in range(0,len(eval_list),2):\n",
        "  cur_high=eval_list[i0]\n",
        "  cur_low=eval_list[i0+1]\n",
        "  ac_high,pred_high_wt=cur_high\n",
        "  ac_low,pred_low_wt=cur_low\n",
        "  pred_high,pred_wt=pred_high_wt\n",
        "  print(\"ac_high\",ac_high, \"pred_high\",pred_high,\"pred_wt\", pred_wt,\"cur_low\",cur_low)\n",
        "\n",
        "  # if i0>0 and int(pred_high)>=gain_threshold0: \n",
        "  #   #print(i0/2, \"Sell Decision - predicted: %s - actual: %s\"%(pred_high,ac_high))\n",
        "  #   gain_loss=int(ac_high)\n",
        "  #   break\n",
        "\n",
        "\n",
        "\n",
        "# print(test_ft)\n",
        "# print(test_ft[-1])\n",
        "# print(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Apply model to daily data - Sep 22"
      ],
      "metadata": {
        "id": "SIHr2a19LPlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "model1='models/test2-batches-summary-sigmoid-1preds-30prev-58-64-2layer-0000001/model-8.model' \n",
        "model2='models/test2-batches-summary-sigmoid-1preds-30prev-58-64-2layer-0000001/model-8.model' \n",
        "model3='models/test2-batches-summary-sigmoid-1preds-30prev-58-128-2layer-0000001/model-13.model' \n",
        "cur_model_fpath=model3\n",
        "\n",
        "try: checkpoint = torch.load(cur_model_fpath)\n",
        "except: checkpoint = dill_unpickle(cur_model_fpath)\n",
        "rnn = RNN(checkpoint[\"n_input\"], checkpoint[\"n_hidden\"] , checkpoint[\"n_output\"] , checkpoint[\"n_layers\"] , checkpoint[\"matching_in_out\"]).to(device)\n",
        "cur_parameters=checkpoint[\"parameters\"]\n",
        "pred_labels=cur_parameters[\"pred_labels\"]\n",
        "cur_parameters=checkpoint[\"feature_extraction_parameters\"]\n",
        "label_func=checkpoint['label_extraction_function']\n",
        "feature_func=checkpoint['feature_extraction_function']\n",
        "prev_n=cur_parameters[\"prev_n\"]\n",
        "rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "rnn.eval()\n",
        "\n",
        "pred_threshold=0.75\n",
        "daily_fpath='daily/2022-09-06.txt'\n",
        "daily_fopen=open(daily_fpath)\n",
        "out_pred=\"prediction-2022-09-06.tsv\"\n",
        "out_pred_open=open(out_pred,\"w\")\n",
        "headers=[\"Stock\",\"Link\",\"Last Closing\",\"Accuracy\",\"Accuracy for prediction>%s\"%pred_threshold,\"Prediction\"]\n",
        "header_line=\"\\t\".join(headers)+\"\\n\"\n",
        "out_pred_open.write(header_line)\n",
        "for f_i,f_line in enumerate(daily_fopen):\n",
        "  total_count,correct_count=0,0\n",
        "  threshold_count=0\n",
        "  #print(f_line)\n",
        "  sym0,price_json=f_line.strip().split(\"\\t\")\n",
        "  price_obj_list=json.loads(price_json)\n",
        "  #price_list=[float(v[\"close_price\"]) for v in price_obj_list]\n",
        "  try: \n",
        "    price_list=[(float(v[\"high_price\"]),float(v[\"low_price\"]),float(v[\"close_price\"])) for v in price_obj_list]\n",
        "    last_closing_price=price_list[-1][-1]\n",
        "    normalized_items=get_data_full_from_list_new(price_list,cur_parameters)\n",
        "  except: continue\n",
        "  \n",
        "  \n",
        "  for item0 in normalized_items:\n",
        "    ft_list,lb_list=item0[:2]\n",
        "    ft_list_tensor=to_tensor(ft_list) #torch.tensor(ft_list,dtype=torch.float32)\n",
        "    extracted_out_specs=label_func(lb_list,cur_parameters)\n",
        "    lb_list_tensor=to_tensor(extracted_out_specs) #torch.tensor(extracted_out_specs,dtype=torch.float32)\n",
        "    lb_tensor_flat=lb_list_tensor.ravel()\n",
        "    rnn_out=rnn(ft_list_tensor)\n",
        "    total_count+=1\n",
        "    if extracted_out_specs[0]==1 and rnn_out.item()>pred_threshold:\n",
        "      threshold_count+=1\n",
        "    if int(round(extracted_out_specs[0]))==int(round(rnn_out.item())):\n",
        "      correct_count+=1\n",
        "    else: pass\n",
        "      #print(\"xxx incorrect:\",\"actual:\",extracted_out_specs,\"predicted:\",rnn_out)\n",
        "\n",
        "    #print(\"actual:\",extracted_out_specs,\"predicted:\",rnn_out,\"cur_eval\",cur_eval)\n",
        "  ft_for_prediction=price_list[-prev_n:]\n",
        "  normalized_ft_for_prediction=feature_func(ft_for_prediction)\n",
        "  cur_pred=rnn(normalized_ft_for_prediction)\n",
        "  cur_pred_val=cur_pred.item()\n",
        "\n",
        "  accuracy=0\n",
        "  threshold_accuracy=0\n",
        "  if total_count>0:\n",
        "    accuracy=correct_count/total_count\n",
        "    threshold_accuracy=threshold_count/total_count\n",
        "  if accuracy<0.5: continue\n",
        "  if cur_pred_val<0.5: continue\n",
        "  cur_link='https://finance.yahoo.com/quote/'+sym0\n",
        "  row_items=[sym0,cur_link,round(last_closing_price,4),round(accuracy,4),round(threshold_accuracy,4),round(cur_pred_val,4)]\n",
        "  row_items=[str(v) for v in row_items]\n",
        "  row_line=\"\\t\".join(row_items)+\"\\n\"\n",
        "  out_pred_open.write(row_line)\n",
        "\n",
        "  \n",
        "  \n",
        "  print(sym0, \"last_closing_price\",last_closing_price, \"accuracy\",round(accuracy,4),\"threshold_accuracy\",round(threshold_accuracy,4),\"pred:\",  round(cur_pred_val,4))\n",
        "  # for a in price_list[:5]:\n",
        "  #   print(a)\n",
        "  # close_prices=[float(v[\"close_price\"]) for v in price_obj_list]\n",
        "  #if f_i>20: break\n",
        "out_pred_open.close()\n",
        "daily_fopen.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIcHgUgkLTPB",
        "outputId": "ba14abf3-e684-43ad-f7f1-1cb4e0fe9ba2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALNA last_closing_price 0.1047 accuracy 0.6071 threshold_accuracy 0.0714 pred: 0.47\n",
            "CSCW last_closing_price 0.1123 accuracy 0.75 threshold_accuracy 0.0 pred: 0.5165\n",
            "TTOO last_closing_price 0.1149 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.6164\n",
            "KPRX last_closing_price 0.215 accuracy 0.6786 threshold_accuracy 0.1071 pred: 0.8235\n",
            "RMED last_closing_price 0.16 accuracy 0.6786 threshold_accuracy 0.2143 pred: 0.5164\n",
            "MOHO last_closing_price 0.146 accuracy 0.6429 threshold_accuracy 0.3571 pred: 1.0562\n",
            "IRS+ last_closing_price 3.88 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.4549\n",
            "VGFC last_closing_price 0.1508 accuracy 0.6786 threshold_accuracy 0.0714 pred: 0.8488\n",
            "NBRV last_closing_price 0.1635 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.5649\n",
            "ALRN last_closing_price 0.1749 accuracy 0.7857 threshold_accuracy 0.2857 pred: 0.6437\n",
            "OXBRW last_closing_price 0.20021 accuracy 0.6786 threshold_accuracy 0.0 pred: 0.6936\n",
            "OBSV last_closing_price 0.1661 accuracy 0.5 threshold_accuracy 0.25 pred: 1.1441\n",
            "SLRX last_closing_price 0.2196 accuracy 0.8214 threshold_accuracy 0.0714 pred: 0.4729\n",
            "VBLT last_closing_price 0.208 accuracy 0.5357 threshold_accuracy 0.1786 pred: 0.589\n",
            "QTNT last_closing_price 0.1744 accuracy 0.6429 threshold_accuracy 0.0714 pred: 0.0556\n",
            "TRVN last_closing_price 0.2059 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.5572\n",
            "PIXY last_closing_price 13.6 accuracy 0.6786 threshold_accuracy 0.25 pred: 1.4002\n",
            "CLXT last_closing_price 0.228 accuracy 0.6429 threshold_accuracy 0.1429 pred: 0.4874\n",
            "NBY last_closing_price 0.2315 accuracy 0.6786 threshold_accuracy 0.3214 pred: 0.4994\n",
            "CNSP last_closing_price 0.2478 accuracy 0.75 threshold_accuracy 0.0714 pred: 0.5962\n",
            "TENX last_closing_price 0.2064 accuracy 0.931 threshold_accuracy 0.3103 pred: 1.0922\n",
            "SPCB last_closing_price 0.365 accuracy 0.6429 threshold_accuracy 0.1786 pred: 0.6182\n",
            "OBLG last_closing_price 0.250201 accuracy 0.7931 threshold_accuracy 0.1379 pred: 0.845\n",
            "NILE last_closing_price 0.2856 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.5034\n",
            "TYME last_closing_price 0.2849 accuracy 0.6429 threshold_accuracy 0.0357 pred: 0.4685\n",
            "RIBT last_closing_price 2.58 accuracy 0.5714 threshold_accuracy 0.2857 pred: 0.8818\n",
            "MRKR last_closing_price 0.3046 accuracy 0.5714 threshold_accuracy 0.1429 pred: 0.9448\n",
            "ESPGY last_closing_price 0.2336 accuracy 0.5714 threshold_accuracy 0.4286 pred: 0.7899\n",
            "ACRX last_closing_price 0.295 accuracy 0.7143 threshold_accuracy 0.0714 pred: 0.4939\n",
            "ADMP last_closing_price 0.3278 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.3408\n",
            "CETX last_closing_price 0.31 accuracy 0.6071 threshold_accuracy 0.2143 pred: 0.4335\n",
            "CHEK last_closing_price 0.3211 accuracy 0.6429 threshold_accuracy 0.0 pred: 0.7029\n",
            "OTIC last_closing_price 0.3513 accuracy 0.5714 threshold_accuracy 0.5357 pred: 0.6806\n",
            "AKBA last_closing_price 0.38 accuracy 0.6429 threshold_accuracy 0.0357 pred: 0.3557\n",
            "BTOG last_closing_price 0.3927 accuracy 0.6071 threshold_accuracy 0.1786 pred: 0.4287\n",
            "LOGC last_closing_price 0.36 accuracy 0.7241 threshold_accuracy 0.3448 pred: 0.9313\n",
            "EXN last_closing_price 0.395 accuracy 0.6897 threshold_accuracy 0.3793 pred: 0.474\n",
            "NRBO last_closing_price 0.3058 accuracy 0.6071 threshold_accuracy 0.1786 pred: 0.7856\n",
            "FNHC last_closing_price 0.3 accuracy 0.8966 threshold_accuracy 0.7586 pred: 0.7269\n",
            "ACOR last_closing_price 0.398 accuracy 0.7143 threshold_accuracy 0.1071 pred: 0.5088\n",
            "GRIL last_closing_price 0.4197 accuracy 0.75 threshold_accuracy 0.2857 pred: 0.4823\n",
            "ENSC last_closing_price 0.41 accuracy 0.6429 threshold_accuracy 0.3929 pred: 0.4649\n",
            "HOTH last_closing_price 0.4295 accuracy 0.7143 threshold_accuracy 0.5714 pred: 0.4597\n",
            "MARK last_closing_price 0.3888 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.5149\n",
            "RSLS last_closing_price 0.381 accuracy 0.6786 threshold_accuracy 0.4286 pred: 0.6938\n",
            "REVB last_closing_price 0.39 accuracy 0.5 threshold_accuracy 0.3571 pred: 0.9611\n",
            "YCBD last_closing_price 0.3656 accuracy 0.7241 threshold_accuracy 0.5172 pred: 1.0296\n",
            "XIN last_closing_price 0.5101 accuracy 0.7241 threshold_accuracy 0.6552 pred: 0.7082\n",
            "AGRX last_closing_price 0.38 accuracy 0.5 threshold_accuracy 0.3571 pred: -1.6096\n",
            "SNMP last_closing_price 0.3415 accuracy 0.75 threshold_accuracy 0.1786 pred: -1.7651\n",
            "GMBL last_closing_price 0.4271 accuracy 0.8214 threshold_accuracy 0.3214 pred: 0.6562\n",
            "DXF last_closing_price 0.4448 accuracy 0.5 threshold_accuracy 0.2143 pred: 0.4401\n",
            "AXU last_closing_price 0.426 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.0513\n",
            "NYMX last_closing_price 0.363599 accuracy 0.75 threshold_accuracy 0.3571 pred: -1.0297\n",
            "DAVE last_closing_price 0.473 accuracy 0.7143 threshold_accuracy 0.6071 pred: 0.9769\n",
            "ICLK last_closing_price 0.4105 accuracy 0.5357 threshold_accuracy 0.2143 pred: -0.9942\n",
            "IDRA last_closing_price 0.5202 accuracy 0.7857 threshold_accuracy 0.2143 pred: 0.4615\n",
            "SFET last_closing_price 0.56 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.4668\n",
            "CLBS last_closing_price 0.5094 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.334\n",
            "BRCN last_closing_price 0.52 accuracy 0.5714 threshold_accuracy 0.2857 pred: 0.9674\n",
            "SEAC last_closing_price 0.5083 accuracy 0.5714 threshold_accuracy 0.1071 pred: 0.4778\n",
            "AWH last_closing_price 0.5051 accuracy 0.6429 threshold_accuracy 0.4643 pred: -2.748\n",
            "TMDI last_closing_price 0.4602 accuracy 0.6429 threshold_accuracy 0.0714 pred: 0.8124\n",
            "ADIL last_closing_price 0.5079 accuracy 0.6786 threshold_accuracy 0.3929 pred: 0.656\n",
            "ASXC last_closing_price 0.5238 accuracy 0.7143 threshold_accuracy 0.2143 pred: 0.6824\n",
            "FSRD last_closing_price 0.6797 accuracy 0.6071 threshold_accuracy 0.3571 pred: 0.5332\n",
            "LPCN last_closing_price 0.5699 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.8979\n",
            "CIDM last_closing_price 0.5271 accuracy 0.6786 threshold_accuracy 0.1786 pred: -1.0873\n",
            "CRKN last_closing_price 0.39 accuracy 0.6786 threshold_accuracy 0.4286 pred: -4.776\n",
            "LCFY last_closing_price 0.7 accuracy 0.8929 threshold_accuracy 0.8571 pred: 0.6365\n",
            "ELYS last_closing_price 0.5845 accuracy 0.5714 threshold_accuracy 0.3214 pred: 0.8057\n",
            "SIEN last_closing_price 0.62 accuracy 0.6071 threshold_accuracy 0.3929 pred: 0.9995\n",
            "IDEX last_closing_price 0.607 accuracy 0.6207 threshold_accuracy 0.069 pred: 0.6386\n",
            "PHGE last_closing_price 0.55 accuracy 0.5862 threshold_accuracy 0.4828 pred: 0.9663\n",
            "CRTD last_closing_price 0.3758 accuracy 0.5357 threshold_accuracy 0.2143 pred: -3.5791\n",
            "EFTR last_closing_price 0.4962 accuracy 0.5714 threshold_accuracy 0.3571 pred: 1.1164\n",
            "SHIP last_closing_price 0.5263 accuracy 0.5714 threshold_accuracy 0.0357 pred: -1.8122\n",
            "CLRO last_closing_price 0.6448 accuracy 0.7931 threshold_accuracy 0.4138 pred: 0.4577\n",
            "HEPA last_closing_price 0.6419 accuracy 0.75 threshold_accuracy 0.2143 pred: 0.5337\n",
            "NLSP last_closing_price 0.53 accuracy 0.8214 threshold_accuracy 0.5714 pred: 0.5481\n",
            "MBIO last_closing_price 0.5987 accuracy 0.7931 threshold_accuracy 0.6552 pred: 0.5806\n",
            "SGTX last_closing_price 0.6419 accuracy 0.6786 threshold_accuracy 0.2857 pred: 0.66\n",
            "ANY last_closing_price 0.5965 accuracy 0.6429 threshold_accuracy 0.25 pred: -0.8264\n",
            "DRMA last_closing_price 0.6074 accuracy 0.7241 threshold_accuracy 0.6897 pred: 0.6208\n",
            "PHIO last_closing_price 0.6888 accuracy 0.7857 threshold_accuracy 0.1429 pred: 0.3691\n",
            "MIGI last_closing_price 0.71 accuracy 0.6786 threshold_accuracy 0.5 pred: 0.6162\n",
            "RNWK last_closing_price 0.6929 accuracy 0.8214 threshold_accuracy 0.0 pred: 1.0171\n",
            "WNW last_closing_price 0.703301 accuracy 0.6786 threshold_accuracy 0.2143 pred: 0.559\n",
            "RCON last_closing_price 0.6694 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.3634\n",
            "AIRI last_closing_price 0.6977 accuracy 0.7241 threshold_accuracy 0.0 pred: 0.2159\n",
            "BBLN last_closing_price 0.685 accuracy 0.5714 threshold_accuracy 0.2143 pred: 0.9029\n",
            "WORX last_closing_price 0.6015 accuracy 0.6897 threshold_accuracy 0.6552 pred: 0.6686\n",
            "NBSE last_closing_price 0.62 accuracy 0.6429 threshold_accuracy 0.3571 pred: -4.2532\n",
            "CMCM last_closing_price 3.7 accuracy 0.7143 threshold_accuracy 0.0 pred: 0.8631\n",
            "DGLY last_closing_price 0.5613 accuracy 0.6786 threshold_accuracy 0.1071 pred: -2.8576\n",
            "BNTC last_closing_price 0.6848 accuracy 0.5357 threshold_accuracy 0.3214 pred: 0.551\n",
            "PRFX last_closing_price 0.7248 accuracy 0.6429 threshold_accuracy 0.4286 pred: 0.5004\n",
            "MTNB last_closing_price 0.743 accuracy 0.6786 threshold_accuracy 0.0357 pred: 0.8063\n",
            "PSTV last_closing_price 0.86 accuracy 0.9643 threshold_accuracy 0.6429 pred: 0.528\n",
            "IPDN last_closing_price 0.7623 accuracy 0.7586 threshold_accuracy 0.3793 pred: 0.5282\n",
            "XBIO last_closing_price 0.7399 accuracy 0.5862 threshold_accuracy 0.2069 pred: 0.8165\n",
            "MNMD last_closing_price 11.48 accuracy 0.7143 threshold_accuracy 0.1071 pred: 0.5679\n",
            "DRRX last_closing_price 0.72 accuracy 0.75 threshold_accuracy 0.1429 pred: 0.5456\n",
            "NVFY last_closing_price 0.745 accuracy 0.5172 threshold_accuracy 0.3448 pred: 0.4995\n",
            "SISI last_closing_price 0.727 accuracy 0.6429 threshold_accuracy 0.4643 pred: 0.9049\n",
            "PRQR last_closing_price 0.7701 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.4689\n",
            "APTO last_closing_price 0.7672 accuracy 0.6429 threshold_accuracy 0.0357 pred: 0.4076\n",
            "CNET last_closing_price 0.6503 accuracy 0.6897 threshold_accuracy 0.4138 pred: 0.4436\n",
            "GLYC last_closing_price 0.7354 accuracy 0.6429 threshold_accuracy 0.2857 pred: 0.6529\n",
            "TMC last_closing_price 0.9783 accuracy 0.6786 threshold_accuracy 0.2143 pred: 0.7451\n",
            "HLBZ last_closing_price 0.65 accuracy 0.6071 threshold_accuracy 0.5 pred: 1.0322\n",
            "OCX last_closing_price 0.7604 accuracy 0.6786 threshold_accuracy 0.2143 pred: 0.7478\n",
            "LABP last_closing_price 0.8901 accuracy 0.7241 threshold_accuracy 0.6552 pred: 0.5811\n",
            "PME last_closing_price 0.79 accuracy 0.6897 threshold_accuracy 0.1379 pred: 0.4537\n",
            "CTIB last_closing_price 0.88 accuracy 0.5517 threshold_accuracy 0.1034 pred: 0.3768\n",
            "TYDE last_closing_price 0.7753 accuracy 0.6786 threshold_accuracy 0.6786 pred: 0.9876\n",
            "VIVE last_closing_price 0.8201 accuracy 0.7143 threshold_accuracy 0.3214 pred: 0.6216\n",
            "RDHL last_closing_price 0.8666 accuracy 0.75 threshold_accuracy 0.2857 pred: 0.531\n",
            "HEPS last_closing_price 0.9912 accuracy 0.6071 threshold_accuracy 0.25 pred: 0.4358\n",
            "HUGE last_closing_price 0.89 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.4741\n",
            "ATIP last_closing_price 0.9355 accuracy 0.5 threshold_accuracy 0.25 pred: 0.3702\n",
            "SDPI last_closing_price 0.9011 accuracy 0.5714 threshold_accuracy 0.0714 pred: 0.2772\n",
            "FRLN last_closing_price 0.862 accuracy 0.6071 threshold_accuracy 0.5 pred: 0.4425\n",
            "CLIR last_closing_price 1.03 accuracy 0.7241 threshold_accuracy 0.3793 pred: 0.4712\n",
            "SFT last_closing_price 0.8165 accuracy 0.5 threshold_accuracy 0.4286 pred: 0.9235\n",
            "SPRO last_closing_price 0.7267 accuracy 0.7857 threshold_accuracy 0.4286 pred: -2.5499\n",
            "AIHS last_closing_price 0.828 accuracy 0.5714 threshold_accuracy 0.5357 pred: 0.6275\n",
            "HOFV last_closing_price 0.7067 accuracy 0.7857 threshold_accuracy 0.3929 pred: -0.6066\n",
            "SLGG last_closing_price 0.875 accuracy 0.6071 threshold_accuracy 0.4643 pred: 0.6077\n",
            "DGHI last_closing_price 0.6701 accuracy 0.75 threshold_accuracy 0.6429 pred: -4.7722\n",
            "BIOC last_closing_price 1.01 accuracy 0.7857 threshold_accuracy 0.1429 pred: 0.4552\n",
            "FLGC last_closing_price 0.7933 accuracy 0.6786 threshold_accuracy 0.5 pred: 0.6514\n",
            "RVSN last_closing_price 0.8781 accuracy 0.7857 threshold_accuracy 0.3929 pred: 0.4277\n",
            "RYCEY last_closing_price 0.891 accuracy 0.8214 threshold_accuracy 0.0 pred: -1.4255\n",
            "SYBX last_closing_price 1.05 accuracy 0.6429 threshold_accuracy 0.4286 pred: 0.349\n",
            "ARDX last_closing_price 1.09 accuracy 0.5714 threshold_accuracy 0.1429 pred: 1.0638\n",
            "ATOS last_closing_price 0.9128 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.4906\n",
            "CNXA last_closing_price 0.9252 accuracy 0.7143 threshold_accuracy 0.5 pred: 0.4774\n",
            "VBIV last_closing_price 0.893 accuracy 0.6429 threshold_accuracy 0.4286 pred: 0.3108\n",
            "GLS last_closing_price 1.2 accuracy 0.6897 threshold_accuracy 0.5517 pred: 0.5495\n",
            "PTE last_closing_price 1.01 accuracy 0.6071 threshold_accuracy 0.3929 pred: 0.9606\n",
            "SANW last_closing_price 1.25 accuracy 0.5714 threshold_accuracy 0.0357 pred: 1.4001\n",
            "UIHC last_closing_price 1.01 accuracy 0.5862 threshold_accuracy 0.1724 pred: 0.5004\n",
            "CETXP last_closing_price 0.99 accuracy 0.6207 threshold_accuracy 0.4828 pred: 0.1845\n",
            "CRVS last_closing_price 0.908 accuracy 0.7931 threshold_accuracy 0.6207 pred: 0.5576\n",
            "FBIO last_closing_price 1.07 accuracy 0.5714 threshold_accuracy 0.2143 pred: 0.3478\n",
            "PT last_closing_price 1.0 accuracy 0.6786 threshold_accuracy 0.5 pred: 0.5719\n",
            "SEEL last_closing_price 1.14 accuracy 0.7857 threshold_accuracy 0.3571 pred: 0.4824\n",
            "AIU last_closing_price 1.0199 accuracy 0.8214 threshold_accuracy 0.5714 pred: 0.582\n",
            "MTMT last_closing_price 1.56 accuracy 0.7143 threshold_accuracy 0.3214 pred: 1.6813\n",
            "CIOXY last_closing_price 0.9505 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.5761\n",
            "BSGM last_closing_price 1.09 accuracy 0.8571 threshold_accuracy 0.6786 pred: 0.62\n",
            "CRIS last_closing_price 0.9053 accuracy 0.75 threshold_accuracy 0.4643 pred: 0.6676\n",
            "AVYA last_closing_price 1.71 accuracy 0.7857 threshold_accuracy 0.6071 pred: 1.3661\n",
            "BJDX last_closing_price 1.01 accuracy 0.5 threshold_accuracy 0.4286 pred: 0.4951\n",
            "BYFC last_closing_price 1.11 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.9742\n",
            "EDBL last_closing_price 0.7982 accuracy 0.6552 threshold_accuracy 0.5517 pred: 1.0919\n",
            "IONM last_closing_price 0.9136 accuracy 0.5714 threshold_accuracy 0.25 pred: 0.8947\n",
            "WDH last_closing_price 1.15 accuracy 0.6071 threshold_accuracy 0.2857 pred: 0.3863\n",
            "OPTT last_closing_price 1.04 accuracy 0.6786 threshold_accuracy 0.5 pred: 0.5027\n",
            "SBFM last_closing_price 1.08 accuracy 0.5714 threshold_accuracy 0.5357 pred: 0.5067\n",
            "AEMD last_closing_price 1.0 accuracy 0.6429 threshold_accuracy 0.4286 pred: 0.749\n",
            "DARE last_closing_price 1.11 accuracy 0.6429 threshold_accuracy 0.0357 pred: 0.3593\n",
            "XELA last_closing_price 0.8679 accuracy 0.5714 threshold_accuracy 0.2857 pred: 0.6828\n",
            "HILS last_closing_price 0.9424 accuracy 0.7143 threshold_accuracy 0.6071 pred: 0.702\n",
            "LTCH last_closing_price 0.9405 accuracy 0.5714 threshold_accuracy 0.2857 pred: 0.7126\n",
            "SMFR last_closing_price 1.01 accuracy 0.7857 threshold_accuracy 0.5 pred: -4.4893\n",
            "ALR last_closing_price 1.135 accuracy 0.6071 threshold_accuracy 0.25 pred: 0.2269\n",
            "RNAZ last_closing_price 1.19 accuracy 0.5517 threshold_accuracy 0.3793 pred: 0.5191\n",
            "EVK last_closing_price 1.0901 accuracy 0.5172 threshold_accuracy 0.3103 pred: 0.6286\n",
            "PFIE last_closing_price 1.08 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.4459\n",
            "XELB last_closing_price 1.13 accuracy 0.6071 threshold_accuracy 0.0714 pred: 0.1869\n",
            "BFRI last_closing_price 1.19 accuracy 0.6071 threshold_accuracy 0.1429 pred: 0.4245\n",
            "GTH last_closing_price 1.13 accuracy 0.5172 threshold_accuracy 0.2759 pred: 0.3557\n",
            "BVXV last_closing_price 1.05 accuracy 0.6786 threshold_accuracy 0.0 pred: -2.136\n",
            "LGMK last_closing_price 1.0706 accuracy 0.6786 threshold_accuracy 0.5357 pred: 0.4768\n",
            "VZLA last_closing_price 1.17 accuracy 0.6897 threshold_accuracy 0.2414 pred: 0.5145\n",
            "DS last_closing_price 1.15 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.8102\n",
            "XL last_closing_price 1.12 accuracy 0.5714 threshold_accuracy 0.1786 pred: 0.4365\n",
            "CEAD last_closing_price 1.19 accuracy 0.75 threshold_accuracy 0.6429 pred: 0.5172\n",
            "VVPR last_closing_price 1.03 accuracy 0.75 threshold_accuracy 0.2857 pred: 0.5651\n",
            "VVOS last_closing_price 1.26 accuracy 0.75 threshold_accuracy 0.4286 pred: 0.3146\n",
            "ARVL last_closing_price 1.01 accuracy 0.5357 threshold_accuracy 0.2143 pred: -2.686\n",
            "VSTM last_closing_price 1.18 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.4743\n",
            "CLVS last_closing_price 1.13 accuracy 0.5357 threshold_accuracy 0.1429 pred: 0.9765\n",
            "CCLP last_closing_price 1.2601 accuracy 0.5172 threshold_accuracy 0.069 pred: 0.2771\n",
            "GTE last_closing_price 1.38 accuracy 0.75 threshold_accuracy 0.2143 pred: 1.0018\n",
            "LTRPA last_closing_price 1.15 accuracy 0.75 threshold_accuracy 0.4643 pred: 0.5538\n",
            "SPPI last_closing_price 1.21 accuracy 0.7143 threshold_accuracy 0.1429 pred: 0.5119\n",
            "BSFC last_closing_price 1.12 accuracy 0.7143 threshold_accuracy 0.2857 pred: 0.634\n",
            "FBRX last_closing_price 1.3 accuracy 0.6429 threshold_accuracy 0.0 pred: 0.2432\n",
            "HALL last_closing_price 1.2 accuracy 0.5517 threshold_accuracy 0.0345 pred: -4.7104\n",
            "CGEN last_closing_price 1.14 accuracy 0.75 threshold_accuracy 0.0357 pred: 0.8184\n",
            "TNXP last_closing_price 0.992 accuracy 0.5 threshold_accuracy 0.3571 pred: 1.0188\n",
            "WBEV last_closing_price 1.07 accuracy 0.6429 threshold_accuracy 0.5 pred: 0.8926\n",
            "BEAT last_closing_price 1.18 accuracy 0.8214 threshold_accuracy 0.5357 pred: 0.9111\n",
            "RAVE last_closing_price 1.19 accuracy 0.6071 threshold_accuracy 0.0357 pred: -2.191\n",
            "NCMI last_closing_price 1.12 accuracy 0.6786 threshold_accuracy 0.2857 pred: 0.2552\n",
            "NCTY last_closing_price 1.2 accuracy 0.6071 threshold_accuracy 0.25 pred: 0.8971\n",
            "CTHR last_closing_price 1.21 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.41\n",
            "DWSN last_closing_price 1.1 accuracy 0.6429 threshold_accuracy 0.4643 pred: 0.941\n",
            "SDC last_closing_price 1.15 accuracy 0.5714 threshold_accuracy 0.3571 pred: 0.8779\n",
            "BLCM last_closing_price 1.25 accuracy 0.7857 threshold_accuracy 0.1786 pred: 0.5098\n",
            "RGS last_closing_price 1.36 accuracy 0.6786 threshold_accuracy 0.4286 pred: 0.5621\n",
            "CPTN last_closing_price 1.63 accuracy 0.75 threshold_accuracy 0.5357 pred: 0.5696\n",
            "AMRN last_closing_price 1.21 accuracy 0.5 threshold_accuracy 0.1786 pred: -0.9333\n",
            "DTEA last_closing_price 1.27 accuracy 0.6071 threshold_accuracy 0.1429 pred: 0.4589\n",
            "LLL last_closing_price 1.17 accuracy 0.7931 threshold_accuracy 0.2414 pred: 0.6056\n",
            "PBTS last_closing_price 1.35 accuracy 0.8571 threshold_accuracy 0.2857 pred: 0.5038\n",
            "WATT last_closing_price 1.31 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.4703\n",
            "ID last_closing_price 1.3 accuracy 0.7241 threshold_accuracy 0.6207 pred: 0.4441\n",
            "WULF last_closing_price 1.35 accuracy 0.6429 threshold_accuracy 0.4643 pred: 0.609\n",
            "ONCY last_closing_price 1.32 accuracy 0.6552 threshold_accuracy 0.1379 pred: 0.4053\n",
            "CNTB last_closing_price 1.34 accuracy 0.6897 threshold_accuracy 0.3103 pred: 0.5798\n",
            "EGLX last_closing_price 1.27 accuracy 0.5 threshold_accuracy 0.2143 pred: 0.0342\n",
            "TRIB last_closing_price 1.19 accuracy 0.6786 threshold_accuracy 0.1071 pred: 0.6211\n",
            "ARDS last_closing_price 1.51 accuracy 0.7143 threshold_accuracy 0.3929 pred: 0.8723\n",
            "ACB last_closing_price 1.51 accuracy 0.7143 threshold_accuracy 0.4286 pred: 0.5239\n",
            "LGHL last_closing_price 1.63 accuracy 0.7143 threshold_accuracy 0.25 pred: 1.389\n",
            "RLX last_closing_price 1.34 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.5304\n",
            "SOFO last_closing_price 1.49 accuracy 0.6071 threshold_accuracy 0.25 pred: 0.3124\n",
            "WTT last_closing_price 1.48 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0026\n",
            "SVVC last_closing_price 1.51 accuracy 0.6207 threshold_accuracy 0.0 pred: 0.4709\n",
            "AESE last_closing_price 1.41 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.4795\n",
            "RKLY last_closing_price 1.3 accuracy 0.5 threshold_accuracy 0.3571 pred: -2.5274\n",
            "BBLG last_closing_price 1.24 accuracy 0.6786 threshold_accuracy 0.5357 pred: 0.7614\n",
            "BLIN last_closing_price 1.39 accuracy 0.6429 threshold_accuracy 0.2143 pred: 0.4757\n",
            "BLRX last_closing_price 1.5 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.4737\n",
            "SXTC last_closing_price 1.2 accuracy 0.5714 threshold_accuracy 0.4286 pred: 1.0682\n",
            "APLT last_closing_price 1.3 accuracy 0.7857 threshold_accuracy 0.25 pred: 0.6564\n",
            "KOPN last_closing_price 1.28 accuracy 0.6786 threshold_accuracy 0.2857 pred: 0.973\n",
            "RGLS last_closing_price 1.42 accuracy 0.6429 threshold_accuracy 0.2857 pred: 0.541\n",
            "INFI last_closing_price 1.52 accuracy 0.7857 threshold_accuracy 0.4643 pred: 0.7986\n",
            "TRVG last_closing_price 1.43 accuracy 0.5714 threshold_accuracy 0.0 pred: -0.8268\n",
            "APM last_closing_price 1.36 accuracy 0.8214 threshold_accuracy 0.3214 pred: 0.6602\n",
            "SBIG last_closing_price 1.47 accuracy 0.8571 threshold_accuracy 0.7143 pred: 0.623\n",
            "CTRM last_closing_price 1.3 accuracy 0.5357 threshold_accuracy 0.0714 pred: -1.3291\n",
            "GENE last_closing_price 1.39 accuracy 0.5714 threshold_accuracy 0.2143 pred: 0.6018\n",
            "CYRN last_closing_price 1.35 accuracy 0.5357 threshold_accuracy 0.1786 pred: 0.9162\n",
            "CYCC last_closing_price 1.69 accuracy 0.7143 threshold_accuracy 0.5714 pred: 0.4392\n",
            "EEIQ last_closing_price 1.5 accuracy 0.6429 threshold_accuracy 0.4286 pred: 0.7936\n",
            "AXDX last_closing_price 1.63 accuracy 0.8929 threshold_accuracy 0.7143 pred: 0.631\n",
            "LDI last_closing_price 1.71 accuracy 0.6786 threshold_accuracy 0.2143 pred: 0.2732\n",
            "LPTH last_closing_price 1.475 accuracy 0.75 threshold_accuracy 0.6071 pred: 0.6095\n",
            "MITQ last_closing_price 1.59 accuracy 0.6786 threshold_accuracy 0.2857 pred: 0.6887\n",
            "NCNA last_closing_price 1.3823 accuracy 0.6786 threshold_accuracy 0.3929 pred: 0.8434\n",
            "NVOS last_closing_price 1.555 accuracy 0.6552 threshold_accuracy 0.0 pred: -2.4886\n",
            "CCO last_closing_price 1.57 accuracy 0.8214 threshold_accuracy 0.6071 pred: 0.5536\n",
            "BHAT last_closing_price 1.3 accuracy 0.5 threshold_accuracy 0.3214 pred: -0.176\n",
            "CIFR last_closing_price 1.73 accuracy 0.7931 threshold_accuracy 0.5172 pred: 0.8028\n",
            "UNAM last_closing_price 1.8 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.4376\n",
            "DTIL last_closing_price 1.57 accuracy 0.6071 threshold_accuracy 0.2143 pred: 0.5185\n",
            "SNCR last_closing_price 1.55 accuracy 0.5517 threshold_accuracy 0.2759 pred: 0.4824\n",
            "UTME last_closing_price 1.7 accuracy 0.5714 threshold_accuracy 0.5357 pred: 0.5741\n",
            "MTC last_closing_price 1.81 accuracy 0.6786 threshold_accuracy 0.5 pred: 0.7039\n",
            "AACAY last_closing_price 1.87 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0521\n",
            "LIDR last_closing_price 1.5 accuracy 0.5714 threshold_accuracy 0.1429 pred: -2.8992\n",
            "CLWT last_closing_price 1.51 accuracy 0.6429 threshold_accuracy 0.3214 pred: 0.5649\n",
            "RIGL last_closing_price 1.43 accuracy 0.7857 threshold_accuracy 0.5357 pred: 0.626\n",
            "XTLB last_closing_price 1.5876 accuracy 0.5 threshold_accuracy 0.1786 pred: 0.388\n",
            "GLBS last_closing_price 1.6 accuracy 0.5714 threshold_accuracy 0.0714 pred: 0.4204\n",
            "RMTI last_closing_price 1.69 accuracy 0.7857 threshold_accuracy 0.5714 pred: 0.6513\n",
            "OCG last_closing_price 1.59 accuracy 0.6429 threshold_accuracy 0.1429 pred: 0.9957\n",
            "PSFE last_closing_price 1.58 accuracy 0.5357 threshold_accuracy 0.0714 pred: -0.2519\n",
            "ATIF last_closing_price 1.7727 accuracy 0.6071 threshold_accuracy 0.3571 pred: 0.6389\n",
            "CIBEY last_closing_price 1.8 accuracy 0.7586 threshold_accuracy 0.1034 pred: 0.3646\n",
            "MRIN last_closing_price 1.71 accuracy 0.5 threshold_accuracy 0.2143 pred: 0.5625\n",
            "SRNE last_closing_price 2.01 accuracy 0.6071 threshold_accuracy 0.3214 pred: 0.9438\n",
            "HSCS last_closing_price 1.37 accuracy 0.8276 threshold_accuracy 0.6897 pred: 1.0333\n",
            "VIVK last_closing_price 1.74 accuracy 0.8214 threshold_accuracy 0.5357 pred: 0.6597\n",
            "ASMB last_closing_price 1.98 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.3053\n",
            "IINN last_closing_price 1.6801 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.5445\n",
            "APLD last_closing_price 2.24 accuracy 0.7857 threshold_accuracy 0.5714 pred: 0.6196\n",
            "JCSE last_closing_price 1.35 accuracy 0.6071 threshold_accuracy 0.6071 pred: 1.0612\n",
            "MDWD last_closing_price 1.91 accuracy 0.6207 threshold_accuracy 0.1724 pred: 0.4642\n",
            "CTSO last_closing_price 1.85 accuracy 0.6897 threshold_accuracy 0.3103 pred: 0.5187\n",
            "ENSV last_closing_price 1.7 accuracy 0.6786 threshold_accuracy 0.3929 pred: 0.5204\n",
            "HSTO last_closing_price 1.82 accuracy 0.7857 threshold_accuracy 0.5357 pred: 0.7253\n",
            "VLTA last_closing_price 2.09 accuracy 0.8214 threshold_accuracy 0.5714 pred: 0.5974\n",
            "DUO last_closing_price 1.78 accuracy 0.7143 threshold_accuracy 0.5357 pred: 0.9789\n",
            "AHPI last_closing_price 1.83 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.4231\n",
            "GBR last_closing_price 1.51 accuracy 0.8571 threshold_accuracy 0.4286 pred: 0.5767\n",
            "SOND last_closing_price 2.04 accuracy 0.9286 threshold_accuracy 0.7143 pred: 0.5171\n",
            "EOSE last_closing_price 1.88 accuracy 0.6071 threshold_accuracy 0.5 pred: 0.9654\n",
            "BGRY last_closing_price 2.03 accuracy 0.7857 threshold_accuracy 0.2857 pred: 0.4779\n",
            "BDSX last_closing_price 1.83 accuracy 0.6897 threshold_accuracy 0.2759 pred: 0.9493\n",
            "QUOT last_closing_price 1.74 accuracy 0.5 threshold_accuracy 0.0357 pred: -3.0556\n",
            "AMTD last_closing_price 1.57 accuracy 0.6786 threshold_accuracy 0.5357 pred: 1.0424\n",
            "BCDA last_closing_price 1.9 accuracy 0.6786 threshold_accuracy 0.25 pred: 0.6255\n",
            "MCHX last_closing_price 1.96 accuracy 0.5357 threshold_accuracy 0.1429 pred: 0.4982\n",
            "WHLR last_closing_price 2.65 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.2164\n",
            "AMPG last_closing_price 2.12 accuracy 0.6071 threshold_accuracy 0.3214 pred: 0.4495\n",
            "EXPR last_closing_price 1.4 accuracy 0.5 threshold_accuracy 0.1071 pred: -2.6869\n",
            "PGEN last_closing_price 2.38 accuracy 0.6071 threshold_accuracy 0.1786 pred: 0.3925\n",
            "NHWK last_closing_price 2.06 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.9191\n",
            "AWRE last_closing_price 2.06 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.3921\n",
            "PFMT last_closing_price 2.09 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.8911\n",
            "DBVT last_closing_price 2.18 accuracy 0.8276 threshold_accuracy 0.0 pred: -0.7152\n",
            "CDAK last_closing_price 1.72 accuracy 0.7143 threshold_accuracy 0.1786 pred: 0.1018\n",
            "TNON last_closing_price 2.06 accuracy 0.75 threshold_accuracy 0.5714 pred: 0.5556\n",
            "NLS last_closing_price 2.0 accuracy 0.75 threshold_accuracy 0.1786 pred: 0.5329\n",
            "STXS last_closing_price 2.04 accuracy 0.5 threshold_accuracy 0.1786 pred: 0.5492\n",
            "TRUE last_closing_price 2.05 accuracy 0.5 threshold_accuracy 0.0357 pred: -0.3498\n",
            "XLO last_closing_price 2.07 accuracy 0.7857 threshold_accuracy 0.6786 pred: 0.92\n",
            "AGMH last_closing_price 2.3305 accuracy 0.6552 threshold_accuracy 0.4828 pred: 0.8936\n",
            "MHLD last_closing_price 2.25 accuracy 0.75 threshold_accuracy 0.0357 pred: 0.8902\n",
            "CRNT last_closing_price 2.14 accuracy 0.8214 threshold_accuracy 0.0 pred: -1.0414\n",
            "MNOV last_closing_price 2.1 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.7101\n",
            "NNBR last_closing_price 2.24 accuracy 0.5517 threshold_accuracy 0.069 pred: 0.8509\n",
            "WKSP last_closing_price 2.08 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.5451\n",
            "CMRX last_closing_price 2.22 accuracy 0.7143 threshold_accuracy 0.5 pred: 0.5212\n",
            "HIHO last_closing_price 2.41 accuracy 0.6786 threshold_accuracy 0.4643 pred: 0.8576\n",
            "CLSN last_closing_price 2.045 accuracy 0.5714 threshold_accuracy 0.1071 pred: -2.2106\n",
            "PRCH last_closing_price 2.16 accuracy 0.5357 threshold_accuracy 0.3929 pred: 0.6579\n",
            "CO last_closing_price 2.31 accuracy 0.6207 threshold_accuracy 0.1724 pred: 0.3838\n",
            "ABIO last_closing_price 2.35 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.8045\n",
            "AXLA last_closing_price 2.6 accuracy 0.5357 threshold_accuracy 0.1786 pred: 0.489\n",
            "BARK last_closing_price 2.03 accuracy 0.6786 threshold_accuracy 0.25 pred: 0.6661\n",
            "MNDO last_closing_price 2.355 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.0668\n",
            "TAST last_closing_price 1.99 accuracy 0.5517 threshold_accuracy 0.1724 pred: 0.8573\n",
            "AAOI last_closing_price 2.27 accuracy 0.5862 threshold_accuracy 0.1379 pred: 0.5372\n",
            "GERN last_closing_price 2.66 accuracy 0.5714 threshold_accuracy 0.2143 pred: 1.1367\n",
            "HTCR last_closing_price 1.8 accuracy 0.5 threshold_accuracy 0.2143 pred: 0.6319\n",
            "INO last_closing_price 2.2 accuracy 0.7857 threshold_accuracy 0.3929 pred: 0.5954\n",
            "LITM last_closing_price 2.2 accuracy 0.6429 threshold_accuracy 0.3214 pred: 0.7341\n",
            "FSM last_closing_price 2.28 accuracy 0.5 threshold_accuracy 0.0 pred: 0.2286\n",
            "SAN last_closing_price 2.35 accuracy 0.6786 threshold_accuracy 0.0 pred: -0.6849\n",
            "SVRE last_closing_price 2.24 accuracy 0.8571 threshold_accuracy 0.8571 pred: 0.5559\n",
            "FNCH last_closing_price 2.36 accuracy 0.6786 threshold_accuracy 0.2857 pred: 0.4529\n",
            "BKTI last_closing_price 2.49 accuracy 0.6207 threshold_accuracy 0.2069 pred: 0.3686\n",
            "ICAGY last_closing_price 2.44 accuracy 0.6429 threshold_accuracy 0.0 pred: -1.8415\n",
            "EMKR last_closing_price 2.19 accuracy 0.5517 threshold_accuracy 0.0345 pred: -1.3606\n",
            "WYY last_closing_price 2.315 accuracy 0.5517 threshold_accuracy 0.069 pred: 0.0946\n",
            "AACG last_closing_price 2.19 accuracy 0.5714 threshold_accuracy 0.25 pred: 0.5624\n",
            "AMRS last_closing_price 2.73 accuracy 0.75 threshold_accuracy 0.5714 pred: 0.6275\n",
            "CPIX last_closing_price 2.24 accuracy 0.5714 threshold_accuracy 0.1071 pred: 0.5354\n",
            "CDRO last_closing_price 3.0 accuracy 0.8276 threshold_accuracy 0.6897 pred: 0.4532\n",
            "CYAN last_closing_price 2.29 accuracy 0.7241 threshold_accuracy 0.0 pred: 0.7203\n",
            "BKKT last_closing_price 2.56 accuracy 0.7143 threshold_accuracy 0.2143 pred: 0.7799\n",
            "EVOK last_closing_price 2.57 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.9036\n",
            "GTBP last_closing_price 2.54 accuracy 0.5517 threshold_accuracy 0.1724 pred: 0.5104\n",
            "PRTK last_closing_price 2.56 accuracy 0.5862 threshold_accuracy 0.1034 pred: 0.3994\n",
            "FIXX last_closing_price 2.18 accuracy 0.7143 threshold_accuracy 0.3214 pred: 0.7537\n",
            "HRTG last_closing_price 2.71 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.3935\n",
            "INKT last_closing_price 2.58 accuracy 0.7586 threshold_accuracy 0.6897 pred: 0.546\n",
            "DYAI last_closing_price 2.6 accuracy 0.7586 threshold_accuracy 0.2414 pred: 0.478\n",
            "PPTA last_closing_price 2.65 accuracy 0.5862 threshold_accuracy 0.0 pred: 0.2609\n",
            "SCYX last_closing_price 2.44 accuracy 0.6552 threshold_accuracy 0.2069 pred: 0.5427\n",
            "ASRT last_closing_price 2.51 accuracy 0.5357 threshold_accuracy 0.0357 pred: -0.5732\n",
            "SIDU last_closing_price 2.8 accuracy 0.7143 threshold_accuracy 0.3929 pred: 0.4197\n",
            "IRIX last_closing_price 2.48 accuracy 0.5 threshold_accuracy 0.0 pred: 0.5808\n",
            "KSPN last_closing_price 2.46 accuracy 0.5714 threshold_accuracy 0.3929 pred: 0.6635\n",
            "EPIX last_closing_price 2.46 accuracy 0.5172 threshold_accuracy 0.1034 pred: -0.4921\n",
            "JVA last_closing_price 2.43 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.3789\n",
            "AFMD last_closing_price 2.62 accuracy 0.7143 threshold_accuracy 0.0357 pred: -0.8507\n",
            "EVLV last_closing_price 2.12 accuracy 0.5714 threshold_accuracy 0.0714 pred: -1.0189\n",
            "BNR last_closing_price 3.11 accuracy 0.7143 threshold_accuracy 0.25 pred: 0.732\n",
            "NEWP last_closing_price 2.55 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.2517\n",
            "OXBR last_closing_price 2.7 accuracy 0.5172 threshold_accuracy 0.1379 pred: 0.7561\n",
            "KSCP last_closing_price 2.66 accuracy 0.6071 threshold_accuracy 0.0714 pred: 0.3591\n",
            "LEDS last_closing_price 2.655 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.4728\n",
            "AGEN last_closing_price 2.57 accuracy 0.6786 threshold_accuracy 0.25 pred: 0.5354\n",
            "LXRX last_closing_price 2.72 accuracy 0.6429 threshold_accuracy 0.1429 pred: 0.4651\n",
            "NEPT last_closing_price 2.04 accuracy 0.7143 threshold_accuracy 0.5714 pred: 0.6267\n",
            "OMIC last_closing_price 2.51 accuracy 0.5714 threshold_accuracy 0.1786 pred: -2.0606\n",
            "NYMT last_closing_price 2.76 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.2877\n",
            "ZDGE last_closing_price 2.79 accuracy 0.5517 threshold_accuracy 0.1724 pred: 0.4271\n",
            "TCRX last_closing_price 3.27 accuracy 0.6071 threshold_accuracy 0.2857 pred: 0.5462\n",
            "FKWL last_closing_price 2.92 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.1833\n",
            "COOK last_closing_price 2.6 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.7696\n",
            "MOVE last_closing_price 2.77 accuracy 0.6207 threshold_accuracy 0.1034 pred: 0.4436\n",
            "GMDA last_closing_price 2.63 accuracy 0.6429 threshold_accuracy 0.3214 pred: 0.6149\n",
            "AAME last_closing_price 2.97 accuracy 0.7857 threshold_accuracy 0.0714 pred: 0.306\n",
            "ELYM last_closing_price 3.0 accuracy 0.6897 threshold_accuracy 0.1724 pred: 0.4428\n",
            "SNOA last_closing_price 2.81 accuracy 0.5172 threshold_accuracy 0.1379 pred: 0.3948\n",
            "CYH last_closing_price 2.67 accuracy 0.6429 threshold_accuracy 0.1071 pred: 0.9714\n",
            "CMPX last_closing_price 2.9 accuracy 0.7586 threshold_accuracy 0.069 pred: 0.4923\n",
            "MUX last_closing_price 3.09 accuracy 0.7241 threshold_accuracy 0.069 pred: 0.3784\n",
            "GASS last_closing_price 3.03 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.5753\n",
            "EDUC last_closing_price 3.09 accuracy 0.5862 threshold_accuracy 0.0 pred: 0.3966\n",
            "MAKSY last_closing_price 2.76 accuracy 0.6207 threshold_accuracy 0.0 pred: -1.4102\n",
            "PEV last_closing_price 2.45 accuracy 0.5 threshold_accuracy 0.1786 pred: 0.096\n",
            "PBI last_closing_price 2.81 accuracy 0.5 threshold_accuracy 0.0 pred: -0.6496\n",
            "GEVO last_closing_price 2.82 accuracy 0.75 threshold_accuracy 0.0357 pred: -2.4462\n",
            "HBIO last_closing_price 3.22 accuracy 0.5862 threshold_accuracy 0.0 pred: 0.314\n",
            "DSWL last_closing_price 3.0817 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.9019\n",
            "WKHS last_closing_price 2.99 accuracy 0.5714 threshold_accuracy 0.1071 pred: 0.6095\n",
            "KIND last_closing_price 3.09 accuracy 0.75 threshold_accuracy 0.1429 pred: 0.4098\n",
            "AAIC last_closing_price 3.21 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.1017\n",
            "HMY last_closing_price 2.5 accuracy 0.5 threshold_accuracy 0.0 pred: -2.4537\n",
            "SJ last_closing_price 3.18 accuracy 0.6552 threshold_accuracy 0.1034 pred: 0.9141\n",
            "RHE last_closing_price 3.4 accuracy 0.6786 threshold_accuracy 0.3571 pred: 0.6256\n",
            "BMRA last_closing_price 3.49 accuracy 0.6897 threshold_accuracy 0.0345 pred: 1.1737\n",
            "NRDY last_closing_price 2.95 accuracy 0.7857 threshold_accuracy 0.3929 pred: 0.5665\n",
            "NURO last_closing_price 3.26 accuracy 0.5714 threshold_accuracy 0.1071 pred: 0.4399\n",
            "FRBK last_closing_price 2.97 accuracy 0.75 threshold_accuracy 0.0 pred: -1.2625\n",
            "VRCA last_closing_price 3.94 accuracy 0.7241 threshold_accuracy 0.1379 pred: 0.6017\n",
            "APDN last_closing_price 2.77 accuracy 0.7143 threshold_accuracy 0.2857 pred: 0.8439\n",
            "GANX last_closing_price 3.65 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.3569\n",
            "BWEN last_closing_price 2.99 accuracy 0.6786 threshold_accuracy 0.3214 pred: 0.4967\n",
            "CGC last_closing_price 3.46 accuracy 0.6429 threshold_accuracy 0.3929 pred: 0.5705\n",
            "IQ last_closing_price 3.53 accuracy 0.6429 threshold_accuracy 0.1071 pred: 0.5458\n",
            "OIIM last_closing_price 3.2 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4079\n",
            "PEGY last_closing_price 2.215 accuracy 0.7857 threshold_accuracy 0.6071 pred: -0.2981\n",
            "MTBC last_closing_price 3.87 accuracy 0.5714 threshold_accuracy 0.0714 pred: 0.8566\n",
            "SMRT last_closing_price 2.92 accuracy 0.5714 threshold_accuracy 0.0714 pred: -4.1164\n",
            "MYPS last_closing_price 3.5 accuracy 0.7857 threshold_accuracy 0.0357 pred: 0.5226\n",
            "VRAY last_closing_price 3.32 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.3858\n",
            "CASI last_closing_price 3.77 accuracy 0.6786 threshold_accuracy 0.3214 pred: 0.4249\n",
            "TIGR last_closing_price 3.61 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.4393\n",
            "TRVI last_closing_price 3.21 accuracy 0.6429 threshold_accuracy 0.2143 pred: 0.5595\n",
            "FGB last_closing_price 3.425 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0299\n",
            "IOBT last_closing_price 3.22 accuracy 0.8929 threshold_accuracy 0.5357 pred: 0.6362\n",
            "OMEX last_closing_price 3.19 accuracy 0.5172 threshold_accuracy 0.069 pred: 0.4589\n",
            "ACXP last_closing_price 3.38 accuracy 0.5 threshold_accuracy 0.0714 pred: 0.0826\n",
            "CMGMY last_closing_price 3.31 accuracy 0.5 threshold_accuracy 0.1071 pred: -1.2487\n",
            "GSIT last_closing_price 3.13 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.3909\n",
            "MMX last_closing_price 3.46 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.0198\n",
            "ACHR last_closing_price 3.36 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.0727\n",
            "FPH last_closing_price 3.52 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.3176\n",
            "DLNG last_closing_price 3.78 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.8838\n",
            "XPON last_closing_price 2.52 accuracy 0.7143 threshold_accuracy 0.2143 pred: -0.2544\n",
            "SLGC last_closing_price 3.51 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.8789\n",
            "UTSI last_closing_price 4.0077 accuracy 0.6552 threshold_accuracy 0.1379 pred: 0.4375\n",
            "BHIL last_closing_price 2.98 accuracy 0.7586 threshold_accuracy 0.1034 pred: -3.4176\n",
            "MPLN last_closing_price 3.35 accuracy 0.6071 threshold_accuracy 0.0 pred: -0.2386\n",
            "DUFRY last_closing_price 3.62 accuracy 0.5357 threshold_accuracy 0.0 pred: -2.2938\n",
            "BRFH last_closing_price 3.33 accuracy 0.7586 threshold_accuracy 0.0 pred: -2.1373\n",
            "CBAY last_closing_price 3.5 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.4902\n",
            "KINS last_closing_price 3.72 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.2735\n",
            "MGNX last_closing_price 4.24 accuracy 0.6071 threshold_accuracy 0.1429 pred: 0.2567\n",
            "THCX last_closing_price 3.54 accuracy 0.6429 threshold_accuracy 0.0357 pred: 0.5271\n",
            "BPTH last_closing_price 3.57 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.4026\n",
            "HLTH last_closing_price 3.29 accuracy 0.5357 threshold_accuracy 0.1071 pred: -0.7523\n",
            "CRMD last_closing_price 3.52 accuracy 0.5714 threshold_accuracy 0.1786 pred: 0.3528\n",
            "MDXG last_closing_price 3.43 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.6512\n",
            "INOD last_closing_price 3.34 accuracy 0.5172 threshold_accuracy 0.0 pred: -4.5719\n",
            "GROW last_closing_price 3.76 accuracy 0.6897 threshold_accuracy 0.0 pred: -1.5339\n",
            "FCEL last_closing_price 3.94 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.5988\n",
            "MHUA last_closing_price 4.68 accuracy 0.75 threshold_accuracy 0.5357 pred: 1.3105\n",
            "SFE last_closing_price 3.95 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4874\n",
            "CALA last_closing_price 3.32 accuracy 0.5357 threshold_accuracy 0.1071 pred: -0.5306\n",
            "NFGC last_closing_price 3.49 accuracy 0.6786 threshold_accuracy 0.0357 pred: -2.3733\n",
            "ABEO last_closing_price 3.57 accuracy 0.6429 threshold_accuracy 0.2143 pred: 0.6475\n",
            "SMIT last_closing_price 3.929 accuracy 0.7143 threshold_accuracy 0.1071 pred: 0.3069\n",
            "KRON last_closing_price 4.22 accuracy 0.6071 threshold_accuracy 0.1429 pred: 0.4717\n",
            "MACK last_closing_price 4.16 accuracy 0.7241 threshold_accuracy 0.069 pred: 0.1712\n",
            "SRGA last_closing_price 3.96 accuracy 0.6071 threshold_accuracy 0.2143 pred: 0.513\n",
            "BNSO last_closing_price 3.21 accuracy 0.6071 threshold_accuracy 0.4286 pred: 0.6249\n",
            "CX last_closing_price 3.93 accuracy 0.5 threshold_accuracy 0.0 pred: 1.5363\n",
            "CDZI last_closing_price 3.9 accuracy 0.5172 threshold_accuracy 0.1034 pred: 0.4462\n",
            "MTA last_closing_price 4.03 accuracy 0.6429 threshold_accuracy 0.0 pred: 0.595\n",
            "TEF last_closing_price 4.05 accuracy 0.8571 threshold_accuracy 0.0 pred: 0.8349\n",
            "IRS last_closing_price 3.88 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.4549\n",
            "DBTX last_closing_price 4.44 accuracy 0.7857 threshold_accuracy 0.3214 pred: 0.6519\n",
            "NPCE last_closing_price 4.53 accuracy 0.5862 threshold_accuracy 0.1034 pred: 0.7814\n",
            "SGHC last_closing_price 3.93 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.292\n",
            "LLAP last_closing_price 4.17 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.4017\n",
            "MMLP last_closing_price 3.96 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.5151\n",
            "EVTV last_closing_price 4.15 accuracy 0.6071 threshold_accuracy 0.3571 pred: 0.7344\n",
            "GNW last_closing_price 4.13 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.0442\n",
            "ESYJY last_closing_price 4.12 accuracy 0.6429 threshold_accuracy 0.0 pred: -2.2966\n",
            "TME last_closing_price 5.06 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.9808\n",
            "MZDAY last_closing_price 4.22 accuracy 0.5 threshold_accuracy 0.0 pred: 1.1496\n",
            "TNYA last_closing_price 4.56 accuracy 0.6786 threshold_accuracy 0.1071 pred: 0.5117\n",
            "BBVA last_closing_price 4.4 accuracy 0.5 threshold_accuracy 0.0 pred: 1.2156\n",
            "NKTR last_closing_price 4.01 accuracy 0.5517 threshold_accuracy 0.0 pred: -1.325\n",
            "RGTI last_closing_price 3.64 accuracy 0.7931 threshold_accuracy 0.2069 pred: -1.5165\n",
            "CERS last_closing_price 3.92 accuracy 0.6429 threshold_accuracy 0.0 pred: -1.4743\n",
            "XELAP last_closing_price 5.0 accuracy 0.8214 threshold_accuracy 0.1786 pred: 0.8048\n",
            "ANGI last_closing_price 3.9 accuracy 0.5 threshold_accuracy 0.0714 pred: -0.4117\n",
            "BKD last_closing_price 4.45 accuracy 0.6207 threshold_accuracy 0.0 pred: 0.3897\n",
            "GRWG last_closing_price 4.39 accuracy 0.5714 threshold_accuracy 0.1071 pred: 0.6606\n",
            "DDL last_closing_price 4.06 accuracy 0.6786 threshold_accuracy 0.1429 pred: 0.6557\n",
            "TELL last_closing_price 4.08 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.4897\n",
            "AGIL last_closing_price 4.2 accuracy 0.7586 threshold_accuracy 0.069 pred: 0.8868\n",
            "HRTX last_closing_price 4.07 accuracy 0.7143 threshold_accuracy 0.25 pred: 0.6488\n",
            "EGHT last_closing_price 4.66 accuracy 0.8214 threshold_accuracy 0.1071 pred: 0.5118\n",
            "ACTG last_closing_price 4.27 accuracy 0.5357 threshold_accuracy 0.0 pred: -0.77\n",
            "FOSL last_closing_price 4.31 accuracy 0.5357 threshold_accuracy 0.0 pred: -1.935\n",
            "VACC last_closing_price 4.0 accuracy 0.5357 threshold_accuracy 0.2143 pred: 0.681\n",
            "MVIS last_closing_price 4.68 accuracy 0.7143 threshold_accuracy 0.0357 pred: 0.4694\n",
            "VNET last_closing_price 4.97 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.4904\n",
            "PRVB last_closing_price 4.64 accuracy 0.5862 threshold_accuracy 0.0345 pred: 0.5355\n",
            "AVTX last_closing_price 4.8 accuracy 0.6552 threshold_accuracy 0.2759 pred: 0.2189\n",
            "OMER last_closing_price 4.51 accuracy 0.5 threshold_accuracy 0.1071 pred: -0.0408\n",
            "DTC last_closing_price 4.03 accuracy 0.5357 threshold_accuracy 0.1786 pred: 0.9397\n",
            "SURG last_closing_price 4.27 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.6606\n",
            "WHLM last_closing_price 4.64 accuracy 0.5862 threshold_accuracy 0.1724 pred: 0.3604\n",
            "NOK last_closing_price 4.93 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.1041\n",
            "TWO last_closing_price 4.77 accuracy 0.75 threshold_accuracy 0.0 pred: 1.2206\n",
            "WETF last_closing_price 5.06 accuracy 0.75 threshold_accuracy 0.0 pred: 0.5342\n",
            "FENG last_closing_price 4.9 accuracy 0.5517 threshold_accuracy 0.0345 pred: 0.1689\n",
            "CPAC last_closing_price 4.81 accuracy 0.5 threshold_accuracy 0.0 pred: 0.9873\n",
            "GGR last_closing_price 4.52 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.527\n",
            "PIII last_closing_price 4.78 accuracy 0.5714 threshold_accuracy 0.1429 pred: 0.6163\n",
            "BULZ last_closing_price 3.94 accuracy 0.6786 threshold_accuracy 0.0357 pred: -2.7098\n",
            "KMDA last_closing_price 4.85 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.7559\n",
            "NRGV last_closing_price 5.34 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.4768\n",
            "GCV last_closing_price 5.22 accuracy 0.6429 threshold_accuracy 0.0 pred: 0.2431\n",
            "DHX last_closing_price 5.13 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.2578\n",
            "IREN last_closing_price 4.08 accuracy 0.6429 threshold_accuracy 0.25 pred: -1.8382\n",
            "NDLS last_closing_price 4.58 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.5691\n",
            "BHC last_closing_price 6.45 accuracy 0.75 threshold_accuracy 0.1429 pred: 1.1646\n",
            "AMSC last_closing_price 5.2 accuracy 0.6429 threshold_accuracy 0.1071 pred: 0.4231\n",
            "JOBY last_closing_price 5.23 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.5527\n",
            "TESS last_closing_price 4.55 accuracy 0.6071 threshold_accuracy 0.0 pred: -0.3521\n",
            "HIVE last_closing_price 4.79 accuracy 0.5357 threshold_accuracy 0.0714 pred: -1.0024\n",
            "PEI last_closing_price 4.07 accuracy 0.75 threshold_accuracy 0.2143 pred: -2.5515\n",
            "TGAN last_closing_price 5.04 accuracy 0.6786 threshold_accuracy 0.1429 pred: 0.5391\n",
            "ATTO last_closing_price 5.325 accuracy 0.6552 threshold_accuracy 0.0 pred: 0.8925\n",
            "APRN last_closing_price 5.7 accuracy 0.75 threshold_accuracy 0.5 pred: 0.5827\n",
            "DSX last_closing_price 4.82 accuracy 0.5 threshold_accuracy 0.0 pred: -0.157\n",
            "DMTK last_closing_price 5.39 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.3154\n",
            "CVT last_closing_price 5.1 accuracy 0.5357 threshold_accuracy 0.0714 pred: -0.0208\n",
            "INCR last_closing_price 4.84 accuracy 0.5357 threshold_accuracy 0.0357 pred: -1.0998\n",
            "MCRB last_closing_price 5.39 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.3341\n",
            "LOAN last_closing_price 5.5248 accuracy 0.6552 threshold_accuracy 0.0 pred: 1.1258\n",
            "RENT last_closing_price 3.87 accuracy 0.5 threshold_accuracy 0.0714 pred: -1.7783\n",
            "EQRX last_closing_price 4.73 accuracy 0.5172 threshold_accuracy 0.1034 pred: 0.5045\n",
            "MG last_closing_price 5.41 accuracy 0.5172 threshold_accuracy 0.0 pred: -1.0249\n",
            "KGFHY last_closing_price 5.36 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.7997\n",
            "GBIO last_closing_price 5.33 accuracy 0.7143 threshold_accuracy 0.0714 pred: 0.8443\n",
            "LPL last_closing_price 5.65 accuracy 0.6429 threshold_accuracy 0.0 pred: 1.2522\n",
            "KPTI last_closing_price 5.24 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.444\n",
            "VRAR last_closing_price 5.44 accuracy 0.7241 threshold_accuracy 0.2414 pred: 0.5188\n",
            "CMPO last_closing_price 5.96 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.3748\n",
            "MNTX last_closing_price 5.49 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.6619\n",
            "BLUE last_closing_price 6.83 accuracy 0.6786 threshold_accuracy 0.25 pred: 0.7075\n",
            "VEEE last_closing_price 4.24 accuracy 0.6071 threshold_accuracy 0.25 pred: -2.5996\n",
            "FREE last_closing_price 5.04 accuracy 0.6429 threshold_accuracy 0.0 pred: -2.5789\n",
            "PACB last_closing_price 5.32 accuracy 0.75 threshold_accuracy 0.1786 pred: 0.8435\n",
            "CNSL last_closing_price 5.74 accuracy 0.5357 threshold_accuracy 0.0 pred: -0.3439\n",
            "NERV last_closing_price 9.22 accuracy 0.7143 threshold_accuracy 0.4286 pred: 2.1184\n",
            "DLAKY last_closing_price 5.818 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.1241\n",
            "TZOO last_closing_price 5.62 accuracy 0.6786 threshold_accuracy 0.1071 pred: 0.3085\n",
            "CTTAY last_closing_price 5.57 accuracy 0.8276 threshold_accuracy 0.0 pred: -2.0946\n",
            "EVTL last_closing_price 5.17 accuracy 0.6786 threshold_accuracy 0.2143 pred: 0.4877\n",
            "SLGL last_closing_price 6.033 accuracy 0.5357 threshold_accuracy 0.1071 pred: 0.1688\n",
            "OXLC last_closing_price 5.87 accuracy 0.6429 threshold_accuracy 0.0 pred: 1.2285\n",
            "GDRX last_closing_price 5.77 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.4918\n",
            "YAHOY last_closing_price 5.73 accuracy 0.5172 threshold_accuracy 0.0 pred: -1.3468\n",
            "NMG last_closing_price 6.01 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.8624\n",
            "GAB last_closing_price 6.22 accuracy 0.6786 threshold_accuracy 0.0 pred: 1.0754\n",
            "FEIM last_closing_price 6.3 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.9801\n",
            "HLN last_closing_price 5.63 accuracy 0.6429 threshold_accuracy 0.0 pred: -0.824\n",
            "AEYE last_closing_price 6.62 accuracy 0.6429 threshold_accuracy 0.1071 pred: 0.3279\n",
            "ROYMY last_closing_price 6.14 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.548\n",
            "JCTCF last_closing_price 6.36 accuracy 0.5172 threshold_accuracy 0.0 pred: 1.0085\n",
            "CRCT last_closing_price 6.56 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.293\n",
            "WVVI last_closing_price 6.39 accuracy 0.5172 threshold_accuracy 0.0 pred: 1.0109\n",
            "ARLO last_closing_price 5.7 accuracy 0.5 threshold_accuracy 0.0 pred: -1.0145\n",
            "PAYA last_closing_price 6.32 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4792\n",
            "POAHY last_closing_price 7.0 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.0912\n",
            "CHMI last_closing_price 6.32 accuracy 0.7857 threshold_accuracy 0.0 pred: -0.0534\n",
            "NLY last_closing_price 6.38 accuracy 0.6786 threshold_accuracy 0.0 pred: 1.1645\n",
            "ACCO last_closing_price 5.72 accuracy 0.7241 threshold_accuracy 0.0 pred: -1.9323\n",
            "HIMX last_closing_price 5.9 accuracy 0.5714 threshold_accuracy 0.0 pred: -0.1741\n",
            "PMM last_closing_price 6.38 accuracy 0.6429 threshold_accuracy 0.0 pred: -2.249\n",
            "MITT last_closing_price 6.15 accuracy 0.6429 threshold_accuracy 0.0 pred: -0.707\n",
            "TGTX last_closing_price 6.8 accuracy 0.6429 threshold_accuracy 0.0714 pred: 0.5682\n",
            "GILT last_closing_price 6.13 accuracy 0.5357 threshold_accuracy 0.0 pred: -2.4906\n",
            "VNCE last_closing_price 7.64 accuracy 0.5862 threshold_accuracy 0.0 pred: 1.0444\n",
            "YMM last_closing_price 7.01 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.4849\n",
            "MRNS last_closing_price 6.56 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.5124\n",
            "IDBA last_closing_price 6.8 accuracy 0.5517 threshold_accuracy 0.0 pred: -2.3733\n",
            "CNCE last_closing_price 6.95 accuracy 0.6071 threshold_accuracy 0.0714 pred: 0.2719\n",
            "CVGI last_closing_price 6.17 accuracy 0.5 threshold_accuracy 0.0 pred: -1.2709\n",
            "TV last_closing_price 6.37 accuracy 0.8214 threshold_accuracy 0.0 pred: -0.9201\n",
            "ASGLY last_closing_price 6.87 accuracy 0.5 threshold_accuracy 0.0 pred: 1.1564\n",
            "BKCH last_closing_price 6.46 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.5695\n",
            "SXC last_closing_price 6.48 accuracy 0.5 threshold_accuracy 0.0 pred: 0.2717\n",
            "RIOT last_closing_price 6.63 accuracy 0.6429 threshold_accuracy 0.0714 pred: -0.0142\n",
            "RFIL last_closing_price 6.81 accuracy 0.6897 threshold_accuracy 0.0 pred: 0.9182\n",
            "RVSB last_closing_price 7.1 accuracy 0.6897 threshold_accuracy 0.0 pred: 1.0426\n",
            "EWEB last_closing_price 7.29 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0869\n",
            "CMCT last_closing_price 7.05 accuracy 0.75 threshold_accuracy 0.0 pred: 1.051\n",
            "SANA last_closing_price 6.53 accuracy 0.6786 threshold_accuracy 0.1786 pred: 0.6161\n",
            "QUIK last_closing_price 6.6 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.4875\n",
            "ESBA last_closing_price 6.79 accuracy 0.6429 threshold_accuracy 0.0 pred: -1.9105\n",
            "HMTV last_closing_price 7.26 accuracy 0.5517 threshold_accuracy 0.0345 pred: 0.5052\n",
            "KFS last_closing_price 6.78 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.9323\n",
            "ARR last_closing_price 7.05 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.2291\n",
            "BNFT last_closing_price 6.65 accuracy 0.5862 threshold_accuracy 0.0 pred: 0.0588\n",
            "PGRE last_closing_price 6.96 accuracy 0.6429 threshold_accuracy 0.0 pred: 1.377\n",
            "LQDA last_closing_price 5.86 accuracy 0.6429 threshold_accuracy 0.2143 pred: 0.6287\n",
            "FF last_closing_price 6.84 accuracy 0.5172 threshold_accuracy 0.0 pred: 1.035\n",
            "SNPHY last_closing_price 7.03 accuracy 0.6897 threshold_accuracy 0.0 pred: 1.2858\n",
            "NVCN last_closing_price 7.1 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.3306\n",
            "CMAX last_closing_price 6.58 accuracy 0.5172 threshold_accuracy 0.069 pred: 0.6069\n",
            "ESRT last_closing_price 7.18 accuracy 0.5862 threshold_accuracy 0.0 pred: 1.049\n",
            "PRTS last_closing_price 6.34 accuracy 0.6071 threshold_accuracy 0.0 pred: -0.4096\n",
            "TWOU last_closing_price 7.01 accuracy 0.5 threshold_accuracy 0.0714 pred: 0.8625\n",
            "IGR last_closing_price 7.26 accuracy 0.5 threshold_accuracy 0.0 pred: -2.172\n",
            "ALIM last_closing_price 6.5888 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.4154\n",
            "EHTH last_closing_price 5.65 accuracy 0.5 threshold_accuracy 0.0357 pred: -0.0086\n",
            "VTNR last_closing_price 8.21 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.8823\n",
            "CDXS last_closing_price 6.74 accuracy 0.5714 threshold_accuracy 0.0357 pred: -2.0237\n",
            "RLGT last_closing_price 6.87 accuracy 0.5172 threshold_accuracy 0.0 pred: 1.0539\n",
            "THRX last_closing_price 6.91 accuracy 0.5862 threshold_accuracy 0.2069 pred: 0.5769\n",
            "IGIC last_closing_price 7.73 accuracy 0.6429 threshold_accuracy 0.0 pred: 1.0165\n",
            "ALVR last_closing_price 7.65 accuracy 0.7143 threshold_accuracy 0.3214 pred: 0.5249\n",
            "NSANY last_closing_price 7.69 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0426\n",
            "SNFCA last_closing_price 7.26 accuracy 0.8929 threshold_accuracy 0.0 pred: 1.1285\n",
            "BVS last_closing_price 7.29 accuracy 0.5517 threshold_accuracy 0.0 pred: -0.564\n",
            "CTG last_closing_price 7.6 accuracy 0.7143 threshold_accuracy 0.0 pred: 0.4388\n",
            "BCS last_closing_price 7.72 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.1711\n",
            "ICCC last_closing_price 7.04 accuracy 0.5172 threshold_accuracy 0.0 pred: -0.3363\n",
            "AUPH last_closing_price 7.22 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.4934\n",
            "CSPI last_closing_price 7.86 accuracy 0.7586 threshold_accuracy 0.0 pred: 0.2491\n",
            "DEX last_closing_price 7.64 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0885\n",
            "SGHT last_closing_price 6.85 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.4009\n",
            "PRTG last_closing_price 8.69 accuracy 0.7143 threshold_accuracy 0.1429 pred: 0.4364\n",
            "ACGBY last_closing_price 7.99 accuracy 0.75 threshold_accuracy 0.0 pred: 1.0396\n",
            "GF last_closing_price 7.63 accuracy 0.5 threshold_accuracy 0.0 pred: -2.1646\n",
            "MYNA last_closing_price 7.59 accuracy 0.5862 threshold_accuracy 0.069 pred: 0.3902\n",
            "ISIG last_closing_price 7.36 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.0536\n",
            "RWT last_closing_price 7.34 accuracy 0.5357 threshold_accuracy 0.0 pred: -2.5835\n",
            "FULC last_closing_price 8.13 accuracy 0.7143 threshold_accuracy 0.1071 pred: 0.4674\n",
            "GRFS last_closing_price 7.63 accuracy 0.8214 threshold_accuracy 0.0 pred: -1.1594\n",
            "NTZ last_closing_price 8.08 accuracy 0.6786 threshold_accuracy 0.25 pred: 0.5115\n",
            "UMICY last_closing_price 7.6 accuracy 0.5517 threshold_accuracy 0.0 pred: -2.4643\n",
            "BTF last_closing_price 7.6039 accuracy 0.5714 threshold_accuracy 0.0 pred: -1.8952\n",
            "KBAL last_closing_price 7.43 accuracy 0.5517 threshold_accuracy 0.0 pred: -2.6439\n",
            "DB last_closing_price 8.27 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.2873\n",
            "KOSS last_closing_price 7.5 accuracy 0.5 threshold_accuracy 0.1429 pred: 0.6413\n",
            "AMTX last_closing_price 8.41 accuracy 0.7143 threshold_accuracy 0.1429 pred: 0.6154\n",
            "BDN last_closing_price 7.98 accuracy 0.75 threshold_accuracy 0.0 pred: 1.0215\n",
            "CEE last_closing_price 8.39 accuracy 0.7931 threshold_accuracy 0.0 pred: -0.4905\n",
            "HLTOY last_closing_price 7.67 accuracy 0.6786 threshold_accuracy 0.0 pred: -2.5684\n",
            "SNPX last_closing_price 8.3 accuracy 0.5714 threshold_accuracy 0.0714 pred: 0.8652\n",
            "ALVO last_closing_price 7.56 accuracy 0.6429 threshold_accuracy 0.0 pred: 0.5701\n",
            "TSRI last_closing_price 8.09 accuracy 0.6552 threshold_accuracy 0.0345 pred: 0.509\n",
            "TELA last_closing_price 8.33 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.2245\n",
            "WEBR last_closing_price 6.46 accuracy 0.6071 threshold_accuracy 0.25 pred: 0.8277\n",
            "FSR last_closing_price 8.41 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.5628\n",
            "MARPS last_closing_price 8.2 accuracy 0.7857 threshold_accuracy 0.25 pred: 0.5577\n",
            "BACHY last_closing_price 8.617 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.0872\n",
            "PYN last_closing_price 8.32 accuracy 0.5 threshold_accuracy 0.0 pred: 1.1389\n",
            "IAS last_closing_price 7.68 accuracy 0.6897 threshold_accuracy 0.069 pred: 0.3968\n",
            "VIV last_closing_price 8.0 accuracy 0.5517 threshold_accuracy 0.0 pred: -1.1147\n",
            "BBBY last_closing_price 8.63 accuracy 0.5517 threshold_accuracy 0.2759 pred: 0.9828\n",
            "WALD last_closing_price 8.8 accuracy 0.6786 threshold_accuracy 0.1071 pred: 0.4759\n",
            "BCAB last_closing_price 7.97 accuracy 0.6071 threshold_accuracy 0.1429 pred: 0.6165\n",
            "ORMP last_closing_price 8.17 accuracy 0.7143 threshold_accuracy 0.0357 pred: 0.4633\n",
            "TCMD last_closing_price 7.95 accuracy 0.5357 threshold_accuracy 0.0 pred: -2.6367\n",
            "CPS last_closing_price 8.07 accuracy 0.7143 threshold_accuracy 0.1071 pred: -2.0501\n",
            "CIM last_closing_price 8.11 accuracy 0.6429 threshold_accuracy 0.0 pred: -0.5126\n",
            "NICK last_closing_price 8.75 accuracy 0.6786 threshold_accuracy 0.0 pred: 1.0854\n",
            "CSBR last_closing_price 7.62 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.2041\n",
            "MRCC last_closing_price 8.45 accuracy 0.6207 threshold_accuracy 0.0 pred: 1.1525\n",
            "WBX last_closing_price 8.47 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.4397\n",
            "EVER last_closing_price 8.54 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.8743\n",
            "GLUE last_closing_price 8.07 accuracy 0.6429 threshold_accuracy 0.2857 pred: 0.6061\n",
            "SEAT last_closing_price 8.26 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.4421\n",
            "HDELY last_closing_price 8.92 accuracy 0.5714 threshold_accuracy 0.0 pred: -1.9915\n",
            "MGTX last_closing_price 8.94 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.4233\n",
            "NLIT last_closing_price 10.9 accuracy 0.6786 threshold_accuracy 0.3929 pred: 1.0794\n",
            "LOCO last_closing_price 8.9 accuracy 0.6207 threshold_accuracy 0.0 pred: 1.2262\n",
            "LCUT last_closing_price 8.59 accuracy 0.6429 threshold_accuracy 0.0 pred: -1.4488\n",
            "HBI last_closing_price 8.61 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.97\n",
            "PZN last_closing_price 9.57 accuracy 0.8214 threshold_accuracy 0.0 pred: 0.947\n",
            "AJX last_closing_price 8.94 accuracy 0.6897 threshold_accuracy 0.0 pred: -0.9581\n",
            "DDI last_closing_price 10.31 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.9346\n",
            "BUR last_closing_price 9.07 accuracy 0.7931 threshold_accuracy 0.0 pred: -1.5864\n",
            "AMC last_closing_price 8.88 accuracy 0.6071 threshold_accuracy 0.1429 pred: -0.2855\n",
            "FSLY last_closing_price 8.47 accuracy 0.6429 threshold_accuracy 0.1071 pred: -0.0453\n",
            "HEAR last_closing_price 7.51 accuracy 0.5 threshold_accuracy 0.0 pred: -3.9083\n",
            "KIGRY last_closing_price 9.52 accuracy 0.6071 threshold_accuracy 0.0 pred: -2.0311\n",
            "PEPG last_closing_price 9.78 accuracy 0.7857 threshold_accuracy 0.2857 pred: 0.3916\n",
            "HNNA last_closing_price 9.72 accuracy 0.5517 threshold_accuracy 0.0 pred: 1.0071\n",
            "VOXX last_closing_price 8.56 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4225\n",
            "TAC last_closing_price 9.1 accuracy 0.8571 threshold_accuracy 0.0 pred: -0.0417\n",
            "SPWH last_closing_price 8.72 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.22\n",
            "MLP last_closing_price 9.31 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.9654\n",
            "ULE last_closing_price 9.92 accuracy 0.6786 threshold_accuracy 0.0 pred: 1.0812\n",
            "ACEL last_closing_price 9.36 accuracy 0.6786 threshold_accuracy 0.0357 pred: 0.3118\n",
            "NATR last_closing_price 9.5 accuracy 0.6552 threshold_accuracy 0.0 pred: 0.5496\n",
            "TAYD last_closing_price 9.9358 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.0911\n",
            "PFSW last_closing_price 9.03 accuracy 0.6071 threshold_accuracy 0.0 pred: -0.9191\n",
            "TEVA last_closing_price 8.84 accuracy 0.5 threshold_accuracy 0.0 pred: -2.1977\n",
            "CHEA last_closing_price 10.03 accuracy 0.5 threshold_accuracy 0.0 pred: 0.9405\n",
            "INKA last_closing_price 10.05 accuracy 0.8571 threshold_accuracy 0.0 pred: 0.9542\n",
            "VTRS last_closing_price 9.45 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.8718\n",
            "OR last_closing_price 9.72 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.8937\n",
            "GENC last_closing_price 10.01 accuracy 0.5 threshold_accuracy 0.0 pred: 0.6581\n",
            "WFCF last_closing_price 10.22 accuracy 0.5517 threshold_accuracy 0.0345 pred: 0.4902\n",
            "GLAD last_closing_price 9.93 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.2067\n",
            "ORAN last_closing_price 10.01 accuracy 0.6897 threshold_accuracy 0.0 pred: 1.0918\n",
            "NOTE last_closing_price 8.1 accuracy 0.8571 threshold_accuracy 0.5357 pred: 0.6075\n",
            "PLTK last_closing_price 10.19 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.3247\n",
            "CMCL last_closing_price 9.4 accuracy 0.5 threshold_accuracy 0.0 pred: -0.1749\n",
            "NRDS last_closing_price 9.76 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4365\n",
            "FFHL last_closing_price 10.12 accuracy 0.6552 threshold_accuracy 0.1034 pred: 1.054\n",
            "VNDA last_closing_price 10.38 accuracy 0.5862 threshold_accuracy 0.0 pred: 0.3349\n",
            "VGR last_closing_price 9.75 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.5094\n",
            "TXMD last_closing_price 10.18 accuracy 0.7143 threshold_accuracy 0.25 pred: 0.2773\n",
            "WSR last_closing_price 9.91 accuracy 0.5862 threshold_accuracy 0.0 pred: 1.3027\n",
            "BASFY last_closing_price 10.38 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.3357\n",
            "MITK last_closing_price 9.8 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.1405\n",
            "SHZHY last_closing_price 9.6 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.9306\n",
            "WEICY last_closing_price 10.31 accuracy 0.5862 threshold_accuracy 0.0 pred: 0.8599\n",
            "NEO last_closing_price 9.8 accuracy 0.5357 threshold_accuracy 0.0 pred: -0.2433\n",
            "WEBL last_closing_price 9.16 accuracy 0.7857 threshold_accuracy 0.0714 pred: -1.8364\n",
            "LNDC last_closing_price 10.06 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.9678\n",
            "AREN last_closing_price 10.92 accuracy 0.6786 threshold_accuracy 0.0714 pred: 0.2734\n",
            "APEI last_closing_price 9.82 accuracy 0.5517 threshold_accuracy 0.069 pred: -2.8291\n",
            "ADPT last_closing_price 8.72 accuracy 0.5714 threshold_accuracy 0.0714 pred: -1.4822\n",
            "TDF last_closing_price 10.51 accuracy 0.6207 threshold_accuracy 0.0 pred: -1.373\n",
            "CARA last_closing_price 10.64 accuracy 0.5 threshold_accuracy 0.0 pred: 0.3745\n",
            "CCU last_closing_price 11.4 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.0926\n",
            "AMPS last_closing_price 10.52 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.2985\n",
            "BBIO last_closing_price 10.06 accuracy 0.6071 threshold_accuracy 0.25 pred: 0.5127\n",
            "HLVX last_closing_price 13.66 accuracy 0.7931 threshold_accuracy 0.1379 pred: 1.0737\n",
            "HKTVY last_closing_price 14.89 accuracy 0.8929 threshold_accuracy 0.0 pred: 0.882\n",
            "EBKDY last_closing_price 11.06 accuracy 0.5517 threshold_accuracy 0.0 pred: -1.8066\n",
            "LPSN last_closing_price 10.77 accuracy 0.6071 threshold_accuracy 0.0357 pred: 0.6502\n",
            "ACCD last_closing_price 10.04 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.671\n",
            "PRM last_closing_price 9.3 accuracy 0.5714 threshold_accuracy 0.0 pred: -1.3426\n",
            "METC last_closing_price 10.15 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.4216\n",
            "MTLS last_closing_price 11.4 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.7268\n",
            "PRPH last_closing_price 11.3 accuracy 0.6552 threshold_accuracy 0.2069 pred: 0.4502\n",
            "ABCL last_closing_price 10.84 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.4656\n",
            "MFA last_closing_price 10.64 accuracy 0.75 threshold_accuracy 0.0 pred: -1.2873\n",
            "IRWD last_closing_price 10.88 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.2418\n",
            "TELNY last_closing_price 10.73 accuracy 0.75 threshold_accuracy 0.0 pred: -2.551\n",
            "DVAX last_closing_price 11.21 accuracy 0.6786 threshold_accuracy 0.0 pred: -1.5884\n",
            "TUP last_closing_price 11.02 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.4561\n",
            "CATO last_closing_price 10.43 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.3426\n",
            "MWA last_closing_price 11.01 accuracy 0.5 threshold_accuracy 0.0 pred: -1.8653\n",
            "RDCM last_closing_price 11.9848 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.8605\n",
            "ESCA last_closing_price 11.18 accuracy 0.7241 threshold_accuracy 0.0 pred: 0.9418\n",
            "PNBK last_closing_price 11.81 accuracy 0.75 threshold_accuracy 0.0 pred: 1.0149\n",
            "BMEA last_closing_price 12.31 accuracy 0.8929 threshold_accuracy 0.1071 pred: 0.4374\n",
            "AKRO last_closing_price 12.67 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4084\n",
            "MUA last_closing_price 12.17 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.942\n",
            "FOLD last_closing_price 11.46 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.3701\n",
            "CHI last_closing_price 11.27 accuracy 0.5172 threshold_accuracy 0.0 pred: -2.1825\n",
            "DNUT last_closing_price 11.7 accuracy 0.6071 threshold_accuracy 0.0 pred: -1.3559\n",
            "PFLT last_closing_price 11.97 accuracy 0.6429 threshold_accuracy 0.0 pred: 1.0982\n",
            "JELD last_closing_price 10.4 accuracy 0.5 threshold_accuracy 0.0714 pred: 0.3395\n",
            "BOTJ last_closing_price 12.3 accuracy 0.5 threshold_accuracy 0.0 pred: 0.0481\n",
            "BRID last_closing_price 13.83 accuracy 0.7143 threshold_accuracy 0.0357 pred: 0.4435\n",
            "CICHY last_closing_price 12.17 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.1389\n",
            "PAI last_closing_price 12.09 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0718\n",
            "SCPL last_closing_price 11.3 accuracy 0.6552 threshold_accuracy 0.0 pred: -2.0903\n",
            "CIO last_closing_price 11.36 accuracy 0.5714 threshold_accuracy 0.0 pred: -0.7767\n",
            "HEQ last_closing_price 12.55 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.1531\n",
            "CHRS last_closing_price 10.2 accuracy 0.5517 threshold_accuracy 0.0345 pred: -3.3165\n",
            "FOR last_closing_price 12.12 accuracy 0.6897 threshold_accuracy 0.0 pred: -0.9149\n",
            "AMKBY last_closing_price 11.39 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.7062\n",
            "CHY last_closing_price 12.28 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.1648\n",
            "ASPN last_closing_price 12.02 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.5595\n",
            "ULCC last_closing_price 12.48 accuracy 0.5 threshold_accuracy 0.0 pred: 0.91\n",
            "RLJ last_closing_price 11.85 accuracy 0.5172 threshold_accuracy 0.0 pred: 1.184\n",
            "SRG last_closing_price 12.66 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4666\n",
            "BAK last_closing_price 12.53 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.9771\n",
            "HPKEW last_closing_price 15.45 accuracy 0.6071 threshold_accuracy 0.3214 pred: 0.3065\n",
            "IPWR last_closing_price 14.0499 accuracy 0.5 threshold_accuracy 0.0714 pred: 0.8783\n",
            "TRDA last_closing_price 13.19 accuracy 0.8929 threshold_accuracy 0.1786 pred: 0.5007\n",
            "CGEM last_closing_price 13.2 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.3953\n",
            "BITO last_closing_price 12.18 accuracy 0.5714 threshold_accuracy 0.0 pred: -1.8118\n",
            "LMNR last_closing_price 11.67 accuracy 0.5 threshold_accuracy 0.0 pred: -3.0228\n",
            "TCDA last_closing_price 12.49 accuracy 0.5172 threshold_accuracy 0.069 pred: 0.3738\n",
            "TPVG last_closing_price 13.01 accuracy 0.6207 threshold_accuracy 0.0 pred: 1.0991\n",
            "SGC last_closing_price 11.4 accuracy 0.75 threshold_accuracy 0.0357 pred: -2.9625\n",
            "AVK last_closing_price 12.94 accuracy 0.5 threshold_accuracy 0.0 pred: 1.2057\n",
            "MARA last_closing_price 11.38 accuracy 0.7143 threshold_accuracy 0.1786 pred: -0.4806\n",
            "FGEN last_closing_price 12.6 accuracy 0.5714 threshold_accuracy 0.0 pred: 0.5718\n",
            "STEM last_closing_price 14.17 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.5412\n",
            "AAN last_closing_price 11.87 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.7878\n",
            "HPP last_closing_price 13.1 accuracy 0.5862 threshold_accuracy 0.0 pred: -1.8874\n",
            "FRSH last_closing_price 13.3 accuracy 0.5714 threshold_accuracy 0.0 pred: -1.2971\n",
            "WMC last_closing_price 13.48 accuracy 0.5 threshold_accuracy 0.0 pred: 0.2134\n",
            "KTOS last_closing_price 12.12 accuracy 0.5 threshold_accuracy 0.0 pred: -1.8644\n",
            "PRMW last_closing_price 13.1 accuracy 0.5862 threshold_accuracy 0.0 pred: 1.0915\n",
            "OSBC last_closing_price 13.5 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.1184\n",
            "HCSG last_closing_price 13.78 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.0991\n",
            "CBAN last_closing_price 14.05 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.0775\n",
            "RTMVY last_closing_price 13.765 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.0217\n",
            "FSTR last_closing_price 12.45 accuracy 0.5714 threshold_accuracy 0.0 pred: -0.796\n",
            "BCRX last_closing_price 14.07 accuracy 0.5 threshold_accuracy 0.0 pred: 0.6349\n",
            "RBCN last_closing_price 3.25 accuracy 0.5357 threshold_accuracy 0.0714 pred: -6.5972\n",
            "KRO last_closing_price 12.77 accuracy 0.8214 threshold_accuracy 0.0 pred: -0.8953\n",
            "KAR last_closing_price 14.28 accuracy 0.5 threshold_accuracy 0.0 pred: -1.4095\n",
            "ASA last_closing_price 13.94 accuracy 0.5 threshold_accuracy 0.0 pred: 1.206\n",
            "OLPX last_closing_price 12.44 accuracy 0.5714 threshold_accuracy 0.0714 pred: 0.9153\n",
            "SLCA last_closing_price 13.72 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.3872\n",
            "LAKE last_closing_price 13.315 accuracy 0.5714 threshold_accuracy 0.0 pred: -1.6725\n",
            "SLM last_closing_price 14.68 accuracy 0.6071 threshold_accuracy 0.0 pred: 1.0223\n",
            "SKYT last_closing_price 12.65 accuracy 0.6429 threshold_accuracy 0.2143 pred: 0.9398\n",
            "PGY last_closing_price 11.17 accuracy 0.6071 threshold_accuracy 0.2857 pred: 1.0358\n",
            "DRIP last_closing_price 15.7 accuracy 0.5 threshold_accuracy 0.1071 pred: 0.4906\n",
            "AERI last_closing_price 15.19 accuracy 0.7857 threshold_accuracy 0.0357 pred: 0.8959\n",
            "DCPH last_closing_price 16.73 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.9733\n",
            "EDIT last_closing_price 14.83 accuracy 0.6786 threshold_accuracy 0.0 pred: 0.8157\n",
            "AU last_closing_price 13.48 accuracy 0.5 threshold_accuracy 0.0 pred: -2.4879\n",
            "ABR last_closing_price 14.54 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.2348\n",
            "BSAC last_closing_price 16.74 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0613\n",
            "NGVC last_closing_price 14.1 accuracy 0.7143 threshold_accuracy 0.0 pred: -0.0254\n",
            "FFNW last_closing_price 14.91 accuracy 0.7857 threshold_accuracy 0.0 pred: 1.114\n",
            "UBCP last_closing_price 15.33 accuracy 0.6552 threshold_accuracy 0.0 pred: 1.0592\n",
            "SEOAY last_closing_price 14.15 accuracy 0.6786 threshold_accuracy 0.0 pred: -2.7088\n",
            "AMPL last_closing_price 14.85 accuracy 0.6207 threshold_accuracy 0.0345 pred: 0.3719\n",
            "WOOF last_closing_price 14.63 accuracy 0.5 threshold_accuracy 0.0 pred: 0.4015\n",
            "GNK last_closing_price 14.01 accuracy 0.6786 threshold_accuracy 0.0 pred: 0.2756\n",
            "NUVL last_closing_price 16.75 accuracy 0.6429 threshold_accuracy 0.1786 pred: 0.4914\n",
            "NXDT last_closing_price 15.58 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.6539\n",
            "IVR last_closing_price 15.76 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.9978\n",
            "CNCR last_closing_price 15.5967 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.3031\n",
            "SKT last_closing_price 15.26 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.2028\n",
            "HOFT last_closing_price 15.18 accuracy 0.5 threshold_accuracy 0.0 pred: 1.096\n",
            "BEKE last_closing_price 16.42 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.4201\n",
            "RCKT last_closing_price 15.54 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.507\n",
            "ACNT last_closing_price 15.44 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.0931\n",
            "BPT last_closing_price 16.76 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.4133\n",
            "OTSKY last_closing_price 16.05 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.0486\n",
            "DKNG last_closing_price 15.49 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.5905\n",
            "KELYA last_closing_price 15.96 accuracy 0.5 threshold_accuracy 0.0 pred: -1.8027\n",
            "FLXS last_closing_price 17.35 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.2945\n",
            "MTR last_closing_price 15.9 accuracy 0.7931 threshold_accuracy 0.069 pred: 1.0688\n",
            "NOAH last_closing_price 15.56 accuracy 0.5862 threshold_accuracy 0.0 pred: -1.1057\n",
            "PAHC last_closing_price 14.66 accuracy 0.75 threshold_accuracy 0.0 pred: -1.3787\n",
            "NXGN last_closing_price 17.04 accuracy 0.5172 threshold_accuracy 0.0 pred: 1.0613\n",
            "CLPXY last_closing_price 15.36 accuracy 0.6071 threshold_accuracy 0.0 pred: 0.6366\n",
            "CIZN last_closing_price 17.2 accuracy 0.5517 threshold_accuracy 0.0 pred: 1.0479\n",
            "CMPS last_closing_price 16.77 accuracy 0.6071 threshold_accuracy 0.0714 pred: 0.4541\n",
            "NJDCY last_closing_price 16.109 accuracy 0.5 threshold_accuracy 0.0 pred: -2.2546\n",
            "CLMT last_closing_price 17.48 accuracy 0.5172 threshold_accuracy 0.0345 pred: 1.0251\n",
            "HHS last_closing_price 14.35 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.5215\n",
            "PHG last_closing_price 16.19 accuracy 0.7241 threshold_accuracy 0.0 pred: -2.4069\n",
            "PBT last_closing_price 17.65 accuracy 0.6207 threshold_accuracy 0.0 pred: 0.3308\n",
            "SSYS last_closing_price 16.81 accuracy 0.5 threshold_accuracy 0.0 pred: -1.1862\n",
            "RCMT last_closing_price 15.82 accuracy 0.7857 threshold_accuracy 0.2143 pred: 0.5302\n",
            "ACLX last_closing_price 18.17 accuracy 0.6552 threshold_accuracy 0.069 pred: 0.3679\n",
            "FMS last_closing_price 16.81 accuracy 0.8929 threshold_accuracy 0.0 pred: -2.0424\n",
            "VTRU last_closing_price 17.8 accuracy 0.6897 threshold_accuracy 0.0 pred: 1.0745\n",
            "T last_closing_price 17.19 accuracy 0.75 threshold_accuracy 0.0 pred: 1.1162\n",
            "WNC last_closing_price 16.11 accuracy 0.5357 threshold_accuracy 0.0 pred: -2.177\n",
            "AINC last_closing_price 16.95 accuracy 0.5357 threshold_accuracy 0.0714 pred: 0.4216\n",
            "AEHR last_closing_price 13.48 accuracy 0.6071 threshold_accuracy 0.0714 pred: -1.9183\n",
            "ICPT last_closing_price 17.82 accuracy 0.5357 threshold_accuracy 0.0357 pred: 0.4926\n",
            "ARIS last_closing_price 16.18 accuracy 0.6071 threshold_accuracy 0.0714 pred: 0.8875\n",
            "CDMO last_closing_price 16.07 accuracy 0.5 threshold_accuracy 0.0357 pred: -0.3921\n",
            "LQDT last_closing_price 17.07 accuracy 0.5 threshold_accuracy 0.0357 pred: 0.48\n",
            "UMH last_closing_price 17.71 accuracy 0.7143 threshold_accuracy 0.0 pred: -1.3014\n",
            "XPOF last_closing_price 18.8 accuracy 0.5357 threshold_accuracy 0.0 pred: 0.8534\n",
            "KEY last_closing_price 17.55 accuracy 0.5714 threshold_accuracy 0.0 pred: 1.1902\n",
            "ALBO last_closing_price 17.19 accuracy 0.75 threshold_accuracy 0.0714 pred: 0.926\n",
            "EXEL last_closing_price 17.91 accuracy 0.7857 threshold_accuracy 0.0 pred: -0.1951\n",
            "QURE last_closing_price 20.66 accuracy 0.5 threshold_accuracy 0.0714 pred: 0.2978\n",
            "AVTE last_closing_price 19.5 accuracy 0.6897 threshold_accuracy 0.1379 pred: 0.6626\n",
            "HTZ last_closing_price 17.89 accuracy 0.5357 threshold_accuracy 0.0 pred: -1.7743\n",
            "UBOH last_closing_price 19.8 accuracy 0.6786 threshold_accuracy 0.0 pred: -1.6224\n",
            "EXFY last_closing_price 16.39 accuracy 0.5 threshold_accuracy 0.0357 pred: -1.2472\n",
            "ASAN last_closing_price 18.16 accuracy 0.5714 threshold_accuracy 0.0357 pred: -2.2543\n",
            "KNYJY last_closing_price 19.44 accuracy 0.5 threshold_accuracy 0.0 pred: -2.1449\n",
            "IGT last_closing_price 17.68 accuracy 0.5 threshold_accuracy 0.0 pred: -2.4036\n",
            "NOTV last_closing_price 20.31 accuracy 0.5 threshold_accuracy 0.0714 pred: 0.4169\n",
            "CRNC last_closing_price 19.52 accuracy 0.5 threshold_accuracy 0.0357 pred: -1.2298\n",
            "NXDT-A last_closing_price 19.75 accuracy 0.7143 threshold_accuracy 0.0 pred: 1.0369\n",
            "DLA last_closing_price 18.09 accuracy 0.7586 threshold_accuracy 0.0 pred: -0.3384\n",
            "BSET last_closing_price 18.85 accuracy 0.5357 threshold_accuracy 0.0 pred: -0.8172\n",
            "RFP last_closing_price 20.19 accuracy 0.7241 threshold_accuracy 0.0 pred: 1.017\n",
            "ARRY last_closing_price 20.27 accuracy 0.7857 threshold_accuracy 0.0714 pred: 0.5661\n",
            "EML last_closing_price 19.39 accuracy 0.5 threshold_accuracy 0.0 pred: 1.233\n",
            "GDOT last_closing_price 20.01 accuracy 0.5517 threshold_accuracy 0.0 pred: -0.5642\n",
            "RDWR last_closing_price 21.73 accuracy 0.5 threshold_accuracy 0.0 pred: 0.3828\n",
            "BZQ last_closing_price 22.68 accuracy 0.6207 threshold_accuracy 0.0 pred: 0.3483\n",
            "WWW last_closing_price 18.98 accuracy 0.5714 threshold_accuracy 0.0 pred: -0.899\n",
            "OFIX last_closing_price 19.53 accuracy 0.5357 threshold_accuracy 0.0 pred: -0.0086\n",
            "MYE last_closing_price 18.71 accuracy 0.5357 threshold_accuracy 0.0 pred: -0.1549\n",
            "PRIM last_closing_price 19.69 accuracy 0.6897 threshold_accuracy 0.0 pred: -2.2121\n",
            "VCYT last_closing_price 19.18 accuracy 0.5517 threshold_accuracy 0.1034 pred: 0.8831\n",
            "PRO last_closing_price 18.52 accuracy 0.6429 threshold_accuracy 0.0 pred: -2.1387\n",
            "BZ last_closing_price 22.71 accuracy 0.6429 threshold_accuracy 0.0 pred: 0.373\n",
            "RDIB last_closing_price 19.0 accuracy 0.5 threshold_accuracy 0.1071 pred: -0.865\n",
            "DLX last_closing_price 18.19 accuracy 0.5714 threshold_accuracy 0.0 pred: -2.2284\n",
            "CBFV last_closing_price 21.998 accuracy 0.6786 threshold_accuracy 0.0 pred: 1.1609\n",
            "RVNC last_closing_price 21.73 accuracy 0.5714 threshold_accuracy 0.0357 pred: 0.7425\n",
            "ISDR last_closing_price 20.87 accuracy 0.7857 threshold_accuracy 0.0 pred: -0.9297\n",
            "RNA last_closing_price 21.26 accuracy 0.5172 threshold_accuracy 0.0345 pred: 0.5332\n",
            "PUK last_closing_price 21.14 accuracy 0.6552 threshold_accuracy 0.0 pred: -1.926\n",
            "RLAY last_closing_price 25.03 accuracy 0.8276 threshold_accuracy 0.069 pred: 1.1152\n",
            "BVH last_closing_price 19.98 accuracy 0.75 threshold_accuracy 0.0 pred: -0.1653\n",
            "NLOK last_closing_price 22.84 accuracy 0.5 threshold_accuracy 0.0 pred: 1.0366\n",
            "TRQ last_closing_price 30.69 accuracy 0.5357 threshold_accuracy 0.0357 pred: 1.1307\n",
            "IIIV last_closing_price 22.45 accuracy 0.5172 threshold_accuracy 0.0 pred: 0.0643\n",
            "UEIC last_closing_price 21.21 accuracy 0.6786 threshold_accuracy 0.0 pred: 0.0506\n",
            "SBGI last_closing_price 22.56 accuracy 0.5517 threshold_accuracy 0.0 pred: 1.2546\n",
            "HPK last_closing_price 26.35 accuracy 0.5 threshold_accuracy 0.0714 pred: 1.1673\n",
            "BBU last_closing_price 21.93 accuracy 0.5 threshold_accuracy 0.0 pred: 1.258\n",
            "FGBI last_closing_price 22.59 accuracy 0.5 threshold_accuracy 0.0 pred: 0.3178\n",
            "SAVE last_closing_price 22.09 accuracy 0.6429 threshold_accuracy 0.0 pred: -2.6092\n",
            "LAND last_closing_price 22.98 accuracy 0.6429 threshold_accuracy 0.0 pred: -1.8136\n",
            "ALRS last_closing_price 23.31 accuracy 0.7143 threshold_accuracy 0.0 pred: 0.3537\n",
            "SNN last_closing_price 24.8 accuracy 0.7143 threshold_accuracy 0.0 pred: 1.1676\n",
            "MATV last_closing_price 24.12 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.0725\n",
            "ACIW last_closing_price 22.65 accuracy 0.6897 threshold_accuracy 0.0 pred: -0.2347\n",
            "FJTSY last_closing_price 22.73 accuracy 0.5357 threshold_accuracy 0.0 pred: -1.1854\n",
            "NOVA last_closing_price 23.8 accuracy 0.6071 threshold_accuracy 0.1071 pred: 0.4867\n",
            "CIHKY last_closing_price 25.29 accuracy 0.5357 threshold_accuracy 0.0 pred: 1.1406\n",
            "ALKS last_closing_price 24.51 accuracy 0.7241 threshold_accuracy 0.0 pred: 0.2404\n",
            "HURC last_closing_price 23.1 accuracy 0.5517 threshold_accuracy 0.0 pred: 0.4846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3gNGkPmSPvu",
        "outputId": "4ce8c81c-8874-4110-ec91-db0f0a42e1bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'api_source': 'benzinga',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/PH1Za-x7lZFcMnfEaU9_ACxtpE4/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9nN1VVTG85NUxRX1l3STNoRVBSUVVtUmU1SEkvYUhSMGNITTZMeTl6TG5scGJXY3VZMjl0TDNWMUwyRndhUzl5WlhNdk1TNHlMMmhYY25aVlVqZFpWR0ZqVTNOb09XOU1hMU5RZFZFdExYNUNMMkZFTURCTlJFRTNaSG93TWsxRVFUZFpXRUozWVZkUk9XVllVbWhaTW1nMVlqSTBMUzlvZEhSd2N6b3ZMMjFsWkdsaExucGxibVp6TG1OdmJTOWxiaTlDWlc1NmFXNW5ZUzgwWWpZME1XUXhNVGRpTURGaE56QmtZelE0Wm1ZMU9EZ3pNbUU1T0RObFpn',\n",
              "  'published_at': '2022-08-23T14:21:28Z',\n",
              "  'relay_url': 'https://news.robinhood.com/f16d014e-13ae-340b-bb62-ac306c07b75f/',\n",
              "  'source': 'Benzinga',\n",
              "  'summary': '',\n",
              "  'title': 'These Investment Wines Have Outperformed Apple, Ford And Tesla Over The Past Year',\n",
              "  'updated_at': '2022-08-23T18:03:20.439469Z',\n",
              "  'url': 'https://finance.yahoo.com/news/investment-wines-outperformed-apple-ford-142128903.html?.tsrc=rss',\n",
              "  'uuid': 'f16d014e-13ae-340b-bb62-ac306c07b75f',\n",
              "  'related_instruments': ['e39ed23a-7bd1-4587-b060-71988d9ef483',\n",
              "   '6df56bd0-0bf2-44ab-8875-f94fd8526942',\n",
              "   '450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': 'As the market volatility extends well into the second half of 2022, alternative investments are increasingly gaining traction against conventional equities and',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': '',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/ylb5JzLwHze4WLVE1IjbdP85BPo/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9jc05aODJFLWZ0YTR0WFhsZFBoZFZpUTJqbXcvYUhSMGNITTZMeTlwYldGblpYTXVZbUZ5Y205dWN5NWpiMjB2YVcwdE5USTRORFkwTDNOdlkybGhiQQ',\n",
              "  'published_at': '2022-08-23T13:40:00Z',\n",
              "  'relay_url': 'https://news.robinhood.com/b59f29a4-1eb4-3957-b7f1-a5c58359a39c/',\n",
              "  'source': \"Barron's\",\n",
              "  'summary': '',\n",
              "  'title': 'Apple Stock Just Had Its Worst 2 Days Since June. Why Investors Are Suddenly Worried.',\n",
              "  'updated_at': '2022-08-23T13:43:14.308771Z',\n",
              "  'url': 'https://www.barrons.com/articles/apple-stock-fed-china-51661261928?mod=bar_RHF',\n",
              "  'uuid': 'b59f29a4-1eb4-3957-b7f1-a5c58359a39c',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': 'Text size\\n\\nApple stock just notched its worst two days since mid-June, with the world’s largest public company getting caught up in a flurry of investor fears r',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'benzinga',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/RAgCQIdtAuc9RtI8RvOV39EU6_c/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9BVEx3WmRqaWRUVlZOZXJsZlZhODVhS2RtS1UvYUhSMGNITTZMeTlqWkc0dVltVnVlbWx1WjJFdVkyOXRMMlpwYkdWekwybHRZV2RsY3k5emRHOXllUzh5TURFeUwybHdiMlF0TWpFNU5EVTJOeTVxY0djX2QybGtkR2c5TVRJd01DWm9aV2xuYUhROU9EQXdKbVpwZEQxamNtOXc',\n",
              "  'published_at': '2022-08-23T13:08:00Z',\n",
              "  'relay_url': 'https://news.robinhood.com/0aaaab97-37f4-365a-883f-ef78fc953cdc/',\n",
              "  'source': 'Benzinga',\n",
              "  'summary': '',\n",
              "  'title': \"Here's How Much You'd Have Right Now If You Invested $1,000 In Apple Shares When Steve Jobs Stepped Down As CEO In 2011\",\n",
              "  'updated_at': '2022-08-23T13:13:06.332059Z',\n",
              "  'url': 'https://www.benzinga.com/trading-ideas/movers/22/03/26009989/if-you-invested-100-in-aapl-steve-jobs-step-down-as-ceo?utm_campaign=partner_feed&utm_source=robinhood.com&utm_medium=partner_feed&utm_content=ticker_page',\n",
              "  'uuid': '0aaaab97-37f4-365a-883f-ef78fc953cdc',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': 'Investors who placed their cash into major U.S. indices have enjoyed respectable returns since summer 2011. The SPDR S&P 500 ETF SPY, Invesco QQQ Trust Series 1',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'benzinga',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/2qMhODtMpBPrCvUdkFVol_j3HX4/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9CZjhabFhEenlES2ZWeWFKWGxYVURpN2JZb1kvYUhSMGNITTZMeTlqWkc0dVltVnVlbWx1WjJFdVkyOXRMMlpwYkdWekwybHRZV2RsY3k5emRHOXllUzh5TURJeUx6QTRMekl6TDNOb2RYUjBaWEp6ZEc5amExODFOVE0yTWpNMk9EVXVhbkJuUDNkcFpIUm9QVEV5TURBbWFHVnBaMmgwUFRnd01DWm1hWFE5WTNKdmNB',\n",
              "  'published_at': '2022-08-23T11:25:15Z',\n",
              "  'relay_url': 'https://news.robinhood.com/8bf2fe39-08a4-3b44-97a6-f3bdaa4cd395/',\n",
              "  'source': 'Benzinga',\n",
              "  'summary': '',\n",
              "  'title': 'Former Apple Engineer Pleads Guilty For Trying To Flee With Self-Driving Car Plans To China',\n",
              "  'updated_at': '2022-08-23T11:32:48.388439Z',\n",
              "  'url': 'https://www.benzinga.com/news/22/08/28594809/former-apple-engineer-caught-while-fleeing-to-china-with-autonomous-car-plans-pleads-guilty?utm_campaign=partner_feed&utm_source=robinhood.com&utm_medium=partner_feed&utm_content=ticker_page',\n",
              "  'uuid': '8bf2fe39-08a4-3b44-97a6-f3bdaa4cd395',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': 'A former Apple Inc AAPL engineer who downloaded plans related to a circuit board for the company’s self-driving vehicle has reportedly pleaded guilty to trade s',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': '',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/H7IxMtPeB9LH_ilMdsD4_Xnxhbg/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9heTJUek91QzFpd3ZCMkpoUHBsTW5TcGVhMzgvYUhSMGNITTZMeTl6TG5scGJXY3VZMjl0TDI1NUwyRndhUzl5WlhNdk1TNHlMMU00UzE5WE4ycHVUa1Z0VG1SclFXVndNMEZFY21jdExTOVpXRUozWVZkUk9XRkhiRzVoUjNob1ltMVNiR05xZEROUVZFVjVUVVJCTjJGRU1ETk5SR2N0TDJoMGRIQnpPaTh2YldWa2FXRXVlbVZ1Wm5NdVkyOXRMMlZ1TDJadmNuUjFibVZmTVRjMUwySTBPV1UyT1dVek5HWTFNMkl4WVdVMll6ZzNOVGMzT0dFNE1EVTVOelZq',\n",
              "  'published_at': '2022-08-23T11:03:57Z',\n",
              "  'relay_url': 'https://news.robinhood.com/e2c6b85f-8964-35f0-ac35-ca94f275f275/',\n",
              "  'source': 'Fortune',\n",
              "  'summary': '',\n",
              "  'title': 'Engineer admits stealing Apple Car trade secrets before trying to flee to work for rivals in China',\n",
              "  'updated_at': '2022-08-23T14:02:55.840736Z',\n",
              "  'url': 'https://finance.yahoo.com/news/engineer-admits-stealing-apple-car-110357974.html?.tsrc=rss',\n",
              "  'uuid': 'e2c6b85f-8964-35f0-ac35-ca94f275f275',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': 'A former Apple employee has pled guilty to stealing trade secrets from the company regarding its highly-secretive self-driving car project.\\n\\nXiaolang Zhang was',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'benzinga',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/lLS3xE32QEDlhc5X7exNue0ACD8/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9ZTXA0VlFkNXZXTkluamJYZ3dLM256QmQ1VWcvYUhSMGNITTZMeTlqWkc0dVltVnVlbWx1WjJFdVkyOXRMMlpwYkdWekwybHRZV2RsY3k5emRHOXllUzh5TURJeUx6QTRMekl6TDNOb2RYUjBaWEp6ZEc5amExOHhOamMyTWpFNE1UQTFYekV1YW5CblAzZHBaSFJvUFRFeU1EQW1hR1ZwWjJoMFBUZ3dNQ1ptYVhROVkzSnZjQQ',\n",
              "  'published_at': '2022-08-23T09:19:16Z',\n",
              "  'relay_url': 'https://news.robinhood.com/89d01eeb-2a11-3306-9671-e6b1ac5db02f/',\n",
              "  'source': 'Benzinga',\n",
              "  'summary': '',\n",
              "  'title': 'New MacBook Pro May Not Feature Chips Utilizing Most-Advanced Process Node Tech',\n",
              "  'updated_at': '2022-08-23T09:22:29.625839Z',\n",
              "  'url': 'https://www.benzinga.com/analyst-ratings/analyst-color/22/08/28595155/apples-new-macbook-pro-may-not-be-powered-by-chips-based-on-the-most-advanced-proce?utm_campaign=partner_feed&utm_source=robinhood.com&utm_medium=partner_feed&utm_content=ticker_page',\n",
              "  'uuid': '89d01eeb-2a11-3306-9671-e6b1ac5db02f',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': 'Apple, Inc. AAPL upcoming MacBook Pro models may not be powered by chips manufactured using the most advanced process node technology.\\n\\nWhat Happened: The new 1',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'benzinga',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/PsfrFj1M5PenX55xKGTE83SDSNo/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9nY0drSHZsaGJiV0xoelZqLS1oZHRwRkZfVW8vYUhSMGNITTZMeTlqWkc0dVltVnVlbWx1WjJFdVkyOXRMMlpwYkdWekwybHRZV2RsY3k5emRHOXllUzh5TURJeUx6QTRMekl6TDNOb2RYUjBaWEp6ZEc5amExOHlNRFUzTkRjeU5EZzFMbXB3Wno5M2FXUjBhRDB4TWpBd0ptaGxhV2RvZEQwNE1EQW1abWwwUFdOeWIzQQ',\n",
              "  'published_at': '2022-08-23T06:32:56Z',\n",
              "  'relay_url': 'https://news.robinhood.com/fc297c64-6935-38d2-9382-dfe42df796c5/',\n",
              "  'source': 'Benzinga',\n",
              "  'summary': '',\n",
              "  'title': 'Apple To Manufacture iPhone 14 In India 2 Months After Initial Release: Report',\n",
              "  'updated_at': '2022-08-23T06:33:22.617817Z',\n",
              "  'url': 'https://www.benzinga.com/news/22/08/28594872/apple-to-manufacture-iphone-14-in-india-two-months-after-initial-release?utm_campaign=partner_feed&utm_source=robinhood.com&utm_medium=partner_feed&utm_content=ticker_page',\n",
              "  'uuid': 'fc297c64-6935-38d2-9382-dfe42df796c5',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': \"Apple Inc AAPL is planning to manufacture the iPhone 14 in India two months after the product's initial release, Bloomberg reported.\\n\\nWhat Happened: Apple, whic\",\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'reuters',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/1YLUzL_afgqug_2A6oOkje3FSw4/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9Jd0hJbU9xZDl1UzVMQXE5ZWR1T2VreV9aZHMvYUhSMGNITTZMeTl6TG5scGJXY3VZMjl0TDJOMkwyRndhWFl5TDNOdlkybGhiQzlwYldGblpYTXZlV0ZvYjI5ZlpHVm1ZWFZzZEY5c2IyZHZMVEV5TURCNE1USXdNQzV3Ym1j',\n",
              "  'published_at': '2022-08-23T05:50:55Z',\n",
              "  'relay_url': 'https://news.robinhood.com/10ad3d10-418a-3c6f-b00f-5464e74a7592/',\n",
              "  'source': 'Reuters',\n",
              "  'summary': '',\n",
              "  'title': 'UPDATE 1-Apple plans to make iPhone 14 in India amid China woes - Bloomberg News',\n",
              "  'updated_at': '2022-08-23T06:03:37.299325Z',\n",
              "  'url': 'https://finance.yahoo.com/news/1-apple-plans-iphone-14-055055139.html?.tsrc=rss',\n",
              "  'uuid': '10ad3d10-418a-3c6f-b00f-5464e74a7592',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': '(Updates with details and background)\\n\\nAug 23 (Reuters) - Apple Inc plans to start manufacturing iPhone 14 in India as the U.S. tech giant seeks alternatives to',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'bloomberg',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/V9YC4YO0rlqLfVQheFjwPxP8-CE/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9ZaUZzOWlwS2FobHEwVFBwVTJ5NElMX3FETVEvYUhSMGNITTZMeTl6TG5scGJXY3VZMjl0TDI1NUwyRndhUzl5WlhNdk1TNHlMemR3VURGTVRrRnFXRTF1VDB0NmFqQk9WekU1U0ZFdExTOVpXRUozWVZkUk9XRkhiRzVoUjNob1ltMVNiR05xZEROUVZFVjVUVVJCTjJGRU1EUk5SRUV0TDJoMGRIQnpPaTh2YldWa2FXRXVlbVZ1Wm5NdVkyOXRMMlZ1TDJKc2IyOXRZbVZ5WjE5MFpXTm9ibTlzYjJkNVh6WTRMMk00TVRRMU9ESmxOVEZrT0RjelkyWTNNekppTXpka056TTFZMk00TkdJdw',\n",
              "  'published_at': '2022-08-23T05:25:27Z',\n",
              "  'relay_url': 'https://news.robinhood.com/453c46b4-0af6-39e2-9b91-cd7f4c64fe30/',\n",
              "  'source': 'Bloomberg',\n",
              "  'summary': '',\n",
              "  'title': 'China-Bound Ex-Apple Engineer Admits to Trade Secrets Theft',\n",
              "  'updated_at': '2022-08-23T09:02:58.798822Z',\n",
              "  'url': 'https://finance.yahoo.com/news/china-bound-ex-apple-engineer-052527700.html?.tsrc=rss',\n",
              "  'uuid': '453c46b4-0af6-39e2-9b91-cd7f4c64fe30',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': '(Bloomberg) -- A former Apple Inc. engineer pleaded guilty to criminal charges that he stole proprietary information from the company while preparing to go work',\n",
              "  'currency_id': 'None'},\n",
              " {'api_source': 'reuters',\n",
              "  'author': '',\n",
              "  'num_clicks': 0,\n",
              "  'preview_image_url': 'https://images.robinhood.com/1YLUzL_afgqug_2A6oOkje3FSw4/aHR0cHM6Ly9pbWFnZXMucm9iaW5ob29kLmNvbS9Jd0hJbU9xZDl1UzVMQXE5ZWR1T2VreV9aZHMvYUhSMGNITTZMeTl6TG5scGJXY3VZMjl0TDJOMkwyRndhWFl5TDNOdlkybGhiQzlwYldGblpYTXZlV0ZvYjI5ZlpHVm1ZWFZzZEY5c2IyZHZMVEV5TURCNE1USXdNQzV3Ym1j',\n",
              "  'published_at': '2022-08-23T05:00:46Z',\n",
              "  'relay_url': 'https://news.robinhood.com/61c61c8a-6ead-3b9b-bded-c52c69801bf4/',\n",
              "  'source': 'Reuters',\n",
              "  'summary': '',\n",
              "  'title': 'UPDATE 1-Former Apple car engineer pleads guilty to trade secret theft',\n",
              "  'updated_at': '2022-08-23T06:03:37.313002Z',\n",
              "  'url': 'https://finance.yahoo.com/news/1-former-apple-car-engineer-050046961.html?.tsrc=rss',\n",
              "  'uuid': '61c61c8a-6ead-3b9b-bded-c52c69801bf4',\n",
              "  'related_instruments': ['450dfc6d-5510-4d40-abfb-f633b7d9be3e'],\n",
              "  'preview_text': \"(Adds Xpeng's response)\\n\\nBy Stephen Nellis\\n\\nAug 22 (Reuters) - A former Apple Inc engineer on Monday pleaded guilty to trade secret theft - one of two people ac\",\n",
              "  'currency_id': 'None'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#rs.robinhood.stocks.get_name_by_symbol(\"FB\")\n",
        "rs.robinhood.get_news(\"AAPL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-31EkGmBb3z"
      },
      "source": [
        "#Let's apply our algorithm on the daily data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAsD4ZOkBihM",
        "outputId": "bf2490ea-f0e3-4ab1-e208-d6c774adbbc3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symbol: AACG 13 12 correct_ratio: 0.92 tomorrow_pred [0.7, 0.29] yesterday_closing: 1.415 latest_price: 1.3208\n",
            "--------\n",
            "Symbol: ABEO 14 14 correct_ratio: 1.0 tomorrow_pred [0.6, 0.46] yesterday_closing: 0.2883 latest_price: 0.2973\n",
            "--------\n",
            "Symbol: ACCD 14 10 correct_ratio: 0.71 tomorrow_pred [0.61, 0.49] yesterday_closing: 15.68 latest_price: 16.76\n",
            "--------\n",
            "Symbol: ACXP 4 4 correct_ratio: 1.0 tomorrow_pred [0.68, 0.31] yesterday_closing: 3.42 latest_price: 3.39\n",
            "--------\n",
            "Symbol: ADCT 10 8 correct_ratio: 0.8 tomorrow_pred [0.66, 0.28] yesterday_closing: 13.28 latest_price: 13.5271\n",
            "--------\n",
            "Symbol: AEHR 17 15 correct_ratio: 0.88 tomorrow_pred [0.65, 0.37] yesterday_closing: 10.22 latest_price: 11.015\n",
            "--------\n",
            "Symbol: AMTX 13 12 correct_ratio: 0.92 tomorrow_pred [0.66, 0.31] yesterday_closing: 10.63 latest_price: 10.93\n",
            "--------\n",
            "Symbol: AERI 11 10 correct_ratio: 0.91 tomorrow_pred [0.62, 0.3] yesterday_closing: 8.34 latest_price: 8.775\n",
            "--------\n",
            "Symbol: AGEN 8 6 correct_ratio: 0.75 tomorrow_pred [0.62, 0.3] yesterday_closing: 2.77 latest_price: 2.7999\n",
            "--------\n",
            "Symbol: AKBA 7 6 correct_ratio: 0.86 tomorrow_pred [0.63, 0.31] yesterday_closing: 2.4 latest_price: 2.52\n",
            "--------\n",
            "Symbol: ALBO 1 1 correct_ratio: 1.0 tomorrow_pred [0.66, 0.28] yesterday_closing: 28.87 latest_price: 29.735\n",
            "--------\n",
            "Symbol: ALNA 12 10 correct_ratio: 0.83 tomorrow_pred [0.72, 0.32] yesterday_closing: 0.275 latest_price: 0.2753\n",
            "--------\n",
            "Symbol: AMBC 1 1 correct_ratio: 1.0 tomorrow_pred [0.71, 0.33] yesterday_closing: 8.97 latest_price: 8.77\n",
            "--------\n",
            "Symbol: AMPE 11 10 correct_ratio: 0.91 tomorrow_pred [0.64, 0.33] yesterday_closing: 0.4282 latest_price: 0.4433\n",
            "--------\n",
            "Symbol: AMRS 16 13 correct_ratio: 0.81 tomorrow_pred [0.62, 0.41] yesterday_closing: 4.35 latest_price: 4.445\n",
            "--------\n",
            "Symbol: ANGI 8 6 correct_ratio: 0.75 tomorrow_pred [0.62, 0.28] yesterday_closing: 5.77 latest_price: 6.155\n",
            "--------\n",
            "Symbol: ARLO 3 3 correct_ratio: 1.0 tomorrow_pred [0.63, 0.26] yesterday_closing: 9.13 latest_price: 9.13\n",
            "--------\n",
            "Symbol: ATAI 13 11 correct_ratio: 0.85 tomorrow_pred [0.6, 0.32] yesterday_closing: 5.28 latest_price: 5.45\n",
            "--------\n",
            "Symbol: AXGN 10 10 correct_ratio: 1.0 tomorrow_pred [0.62, 0.31] yesterday_closing: 8.49 latest_price: 8.53\n",
            "--------\n",
            "Symbol: BLCT 4 4 correct_ratio: 1.0 tomorrow_pred [0.65, 0.35] yesterday_closing: 1.25 latest_price: 1.35\n",
            "--------\n",
            "Symbol: BFI 6 5 correct_ratio: 0.83 tomorrow_pred [0.61, 0.29] yesterday_closing: 4.3 latest_price: 4.1511\n",
            "--------\n",
            "Symbol: CBAT 13 12 correct_ratio: 0.92 tomorrow_pred [0.63, 0.4] yesterday_closing: 1.19 latest_price: 1.265\n",
            "--------\n",
            "Symbol: CERE 7 7 correct_ratio: 1.0 tomorrow_pred [0.62, 0.34] yesterday_closing: 31.01 latest_price: 32.045\n",
            "--------\n",
            "Symbol: CGEM 6 6 correct_ratio: 1.0 tomorrow_pred [0.6, 0.28] yesterday_closing: 12.82 latest_price: 12.865\n",
            "--------\n",
            "Symbol: CLVS 22 17 correct_ratio: 0.77 tomorrow_pred [0.61, 0.39] yesterday_closing: 1.56 latest_price: 1.635\n",
            "--------\n",
            "Symbol: CLWT 5 4 correct_ratio: 0.8 tomorrow_pred [0.6, 0.36] yesterday_closing: 1.38 latest_price: 1.6\n",
            "--------\n",
            "Symbol: CMCM 5 4 correct_ratio: 0.8 tomorrow_pred [0.6, 0.45] yesterday_closing: 0.951 latest_price: 1.06\n",
            "--------\n",
            "Symbol: CMRX 6 5 correct_ratio: 0.83 tomorrow_pred [0.62, 0.28] yesterday_closing: 5.1 latest_price: 5.245\n",
            "--------\n",
            "Symbol: CRTD 15 12 correct_ratio: 0.8 tomorrow_pred [0.65, 0.32] yesterday_closing: 1.11 latest_price: 1.1301\n",
            "--------\n",
            "Symbol: CTIC 4 3 correct_ratio: 0.75 tomorrow_pred [0.65, 0.3] yesterday_closing: 4.08 latest_price: 4.215\n",
            "--------\n",
            "Symbol: CLDS 14 13 correct_ratio: 0.93 tomorrow_pred [0.68, 0.31] yesterday_closing: 26.718 latest_price: 25.131\n",
            "--------\n",
            "Symbol: DDL 18 13 correct_ratio: 0.72 tomorrow_pred [0.71, 0.45] yesterday_closing: 3.42 latest_price: 3.9108\n",
            "--------\n",
            "Symbol: DMTK 15 14 correct_ratio: 0.93 tomorrow_pred [0.6, 0.38] yesterday_closing: 13.14 latest_price: 13.93\n",
            "--------\n",
            "Symbol: DOCS 2 2 correct_ratio: 1.0 tomorrow_pred [0.64, 0.39] yesterday_closing: 48.0 latest_price: 50.01\n",
            "--------\n",
            "Symbol: ELYM 14 14 correct_ratio: 1.0 tomorrow_pred [0.6, 0.36] yesterday_closing: 9.44 latest_price: 9.1398\n",
            "--------\n",
            "Symbol: EVLV 15 12 correct_ratio: 0.8 tomorrow_pred [0.7, 0.33] yesterday_closing: 1.7 latest_price: 1.9257\n",
            "--------\n",
            "Symbol: SNMP 13 13 correct_ratio: 1.0 tomorrow_pred [0.6, 0.42] yesterday_closing: 0.6328 latest_price: 0.605\n",
            "--------\n",
            "Symbol: EXN 14 10 correct_ratio: 0.71 tomorrow_pred [0.73, 0.38] yesterday_closing: 0.905 latest_price: 0.8899\n",
            "--------\n",
            "Symbol: YMM 17 12 correct_ratio: 0.71 tomorrow_pred [0.66, 0.54] yesterday_closing: 6.3 latest_price: 6.775\n",
            "--------\n",
            "Symbol: GAME 15 12 correct_ratio: 0.8 tomorrow_pred [0.65, 0.34] yesterday_closing: 1.8 latest_price: 1.87\n",
            "--------\n",
            "Symbol: GTBP 16 16 correct_ratio: 1.0 tomorrow_pred [0.64, 0.36] yesterday_closing: 2.87 latest_price: 2.84\n",
            "--------\n",
            "Symbol: HGSH 4 4 correct_ratio: 1.0 tomorrow_pred [0.69, 0.37] yesterday_closing: 2.965 latest_price: 3.08\n",
            "--------\n",
            "Symbol: IDEX 15 12 correct_ratio: 0.8 tomorrow_pred [0.63, 0.41] yesterday_closing: 0.9347 latest_price: 0.9624\n",
            "--------\n",
            "Symbol: IMGN 13 12 correct_ratio: 0.92 tomorrow_pred [0.65, 0.26] yesterday_closing: 4.3 latest_price: 4.295\n",
            "--------\n",
            "Symbol: IONM 8 6 correct_ratio: 0.75 tomorrow_pred [0.64, 0.32] yesterday_closing: 6.82 latest_price: 6.94\n",
            "--------\n",
            "Symbol: IREN 10 10 correct_ratio: 1.0 tomorrow_pred [0.65, 0.29] yesterday_closing: 13.07 latest_price: 13.81\n",
            "--------\n",
            "Symbol: KROS 6 6 correct_ratio: 1.0 tomorrow_pred [0.64, 0.31] yesterday_closing: 50.85 latest_price: 52.54\n",
            "--------\n",
            "Symbol: BERZ 14 14 correct_ratio: 1.0 tomorrow_pred [0.69, 0.32] yesterday_closing: 22.47 latest_price: 20.98\n",
            "--------\n",
            "Symbol: BNKD 7 5 correct_ratio: 0.71 tomorrow_pred [0.63, 0.27] yesterday_closing: 9.51 latest_price: 8.96\n",
            "--------\n",
            "Symbol: MACK 5 5 correct_ratio: 1.0 tomorrow_pred [0.6, 0.25] yesterday_closing: 6.19 latest_price: 6.2\n",
            "--------\n",
            "Symbol: GLUE 17 15 correct_ratio: 0.88 tomorrow_pred [0.7, 0.34] yesterday_closing: 12.55 latest_price: 11.945\n",
            "--------\n",
            "Symbol: MODD 6 6 correct_ratio: 1.0 tomorrow_pred [0.69, 0.37] yesterday_closing: 4.0 latest_price: 4.08\n",
            "--------\n",
            "Symbol: MRKR 24 18 correct_ratio: 0.75 tomorrow_pred [0.61, 0.31] yesterday_closing: 0.4173 latest_price: 0.4275\n",
            "--------\n",
            "Symbol: MPLN 3 3 correct_ratio: 1.0 tomorrow_pred [0.61, 0.39] yesterday_closing: 3.98 latest_price: 3.975\n",
            "--------\n",
            "Symbol: NAT 1 1 correct_ratio: 1.0 tomorrow_pred [0.61, 0.24] yesterday_closing: 1.86 latest_price: 1.865\n",
            "--------\n",
            "Symbol: NEWP 9 8 correct_ratio: 0.89 tomorrow_pred [0.61, 0.36] yesterday_closing: 3.27 latest_price: 3.05\n",
            "--------\n",
            "Symbol: NLSP 9 7 correct_ratio: 0.78 tomorrow_pred [0.66, 0.31] yesterday_closing: 1.255 latest_price: 1.271\n",
            "--------\n",
            "Symbol: NM 4 4 correct_ratio: 1.0 tomorrow_pred [0.61, 0.32] yesterday_closing: 3.6 latest_price: 3.7\n",
            "--------\n",
            "Symbol: NYMX 15 15 correct_ratio: 1.0 tomorrow_pred [0.71, 0.34] yesterday_closing: 1.34 latest_price: 1.3\n",
            "--------\n",
            "Symbol: OBLG 10 9 correct_ratio: 0.9 tomorrow_pred [0.69, 0.34] yesterday_closing: 0.5395 latest_price: 0.5605\n",
            "--------\n",
            "Symbol: OIS 6 6 correct_ratio: 1.0 tomorrow_pred [0.6, 0.3] yesterday_closing: 6.71 latest_price: 6.59\n",
            "--------\n",
            "Symbol: OMER 5 5 correct_ratio: 1.0 tomorrow_pred [0.64, 0.35] yesterday_closing: 6.07 latest_price: 6.09\n",
            "--------\n",
            "Symbol: OMIC 14 12 correct_ratio: 0.86 tomorrow_pred [0.68, 0.32] yesterday_closing: 6.34 latest_price: 6.52\n",
            "--------\n",
            "Symbol: OSTK 16 14 correct_ratio: 0.88 tomorrow_pred [0.64, 0.37] yesterday_closing: 48.78 latest_price: 50.0\n",
            "--------\n",
            "Symbol: OSTKO 13 13 correct_ratio: 1.0 tomorrow_pred [0.61, 0.45] yesterday_closing: 45.0 latest_price: 44.85\n",
            "--------\n",
            "Symbol: PACB 14 14 correct_ratio: 1.0 tomorrow_pred [0.67, 0.42] yesterday_closing: 9.94 latest_price: 10.0991\n",
            "--------\n",
            "Symbol: PEI 14 10 correct_ratio: 0.71 tomorrow_pred [0.61, 0.29] yesterday_closing: 0.7599 latest_price: 0.7367\n",
            "--------\n",
            "Symbol: PRTG 20 14 correct_ratio: 0.7 tomorrow_pred [0.62, 0.27] yesterday_closing: 7.55 latest_price: 7.75\n",
            "--------\n",
            "Symbol: RCON 17 15 correct_ratio: 0.88 tomorrow_pred [0.6, 0.39] yesterday_closing: 1.06 latest_price: 1.085\n",
            "--------\n",
            "Symbol: RDIB 15 13 correct_ratio: 0.87 tomorrow_pred [0.71, 0.37] yesterday_closing: 20.65 latest_price: 20.9\n",
            "--------\n",
            "Symbol: REFR 3 3 correct_ratio: 1.0 tomorrow_pred [0.63, 0.28] yesterday_closing: 2.22 latest_price: 2.13\n",
            "--------\n",
            "Symbol: RGLS 12 10 correct_ratio: 0.83 tomorrow_pred [0.64, 0.39] yesterday_closing: 0.275 latest_price: 0.2844\n",
            "--------\n",
            "Symbol: RHE 10 9 correct_ratio: 0.9 tomorrow_pred [0.62, 0.43] yesterday_closing: 3.595 latest_price: 3.68\n",
            "--------\n",
            "Symbol: RXDX 2 2 correct_ratio: 1.0 tomorrow_pred [0.61, 0.24] yesterday_closing: 42.54 latest_price: 41.3\n",
            "--------\n",
            "Symbol: SARK 5 5 correct_ratio: 1.0 tomorrow_pred [0.65, 0.29] yesterday_closing: 48.62 latest_price: 47.0\n",
            "--------\n",
            "Symbol: SISI 16 13 correct_ratio: 0.81 tomorrow_pred [0.63, 0.31] yesterday_closing: 2.64 latest_price: 2.87\n",
            "--------\n",
            "Symbol: STCN 14 12 correct_ratio: 0.86 tomorrow_pred [0.6, 0.25] yesterday_closing: 1.22 latest_price: 1.24\n",
            "--------\n",
            "Symbol: SBFM 10 7 correct_ratio: 0.7 tomorrow_pred [0.63, 0.49] yesterday_closing: 3.0 latest_price: 2.8\n",
            "--------\n",
            "Symbol: SDPI 2 2 correct_ratio: 1.0 tomorrow_pred [0.61, 0.43] yesterday_closing: 1.08 latest_price: 1.0609\n",
            "--------\n",
            "Symbol: SYNA 10 10 correct_ratio: 1.0 tomorrow_pred [0.6, 0.3] yesterday_closing: 217.85 latest_price: 217.6\n",
            "--------\n",
            "Symbol: TBPH 5 5 correct_ratio: 1.0 tomorrow_pred [0.6, 0.27] yesterday_closing: 9.69 latest_price: 9.8\n",
            "--------\n",
            "Symbol: TNYA 9 7 correct_ratio: 0.78 tomorrow_pred [0.6, 0.27] yesterday_closing: 10.91 latest_price: 10.46\n",
            "--------\n",
            "Symbol: TMC 17 17 correct_ratio: 1.0 tomorrow_pred [0.72, 0.36] yesterday_closing: 2.42 latest_price: 2.205\n",
            "--------\n",
            "Symbol: TOPS 6 6 correct_ratio: 1.0 tomorrow_pred [0.61, 0.3] yesterday_closing: 1.06 latest_price: 1.065\n",
            "--------\n",
            "Symbol: TWST 15 11 correct_ratio: 0.73 tomorrow_pred [0.62, 0.44] yesterday_closing: 47.12 latest_price: 49.11\n",
            "--------\n",
            "Symbol: VCEL 4 4 correct_ratio: 1.0 tomorrow_pred [0.62, 0.25] yesterday_closing: 33.01 latest_price: 33.01\n",
            "--------\n",
            "Symbol: VVOS 6 5 correct_ratio: 0.83 tomorrow_pred [0.6, 0.23] yesterday_closing: 2.62 latest_price: 2.67\n",
            "--------\n",
            "Symbol: VLTA 23 18 correct_ratio: 0.78 tomorrow_pred [0.71, 0.38] yesterday_closing: 4.12 latest_price: 4.38\n",
            "--------\n",
            "Symbol: WEBS 15 14 correct_ratio: 0.93 tomorrow_pred [0.69, 0.33] yesterday_closing: 24.99 latest_price: 23.2639\n",
            "--------\n",
            "Symbol: XMTR 14 11 correct_ratio: 0.79 tomorrow_pred [0.69, 0.34] yesterday_closing: 34.9 latest_price: 33.97\n",
            "--------\n",
            "finished evaluating stocks\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from datetime import datetime\n",
        "\n",
        "e0=8\n",
        "model_name=\"exp6-pred2-combined-stocks1-4layer-batches2\"\n",
        "model_dir=os.path.join(cwd,\"models\", model_name) \n",
        "PATH=os.path.join(model_dir, \"model-%s.model\"%e0)\n",
        "#def predict(input_list, model_fpath):\n",
        "\n",
        "daily_dir=\"daily\"\n",
        "date_str=datetime.today().strftime('%Y-%m-%d')\n",
        "daily_dir_path=os.path.join(cwd,daily_dir)\n",
        "if not os.path.exists(daily_dir_path): os.makedirs(daily_dir_path)\n",
        "daily_fpath=os.path.join(daily_dir_path,date_str+\".txt\")\n",
        "# daily_fname='2022-03-21.txt'\n",
        "# daily_fpath=os.path.join(daily_dir_path,daily_fname)\n",
        "daily_fopen=open(daily_fpath)\n",
        "\n",
        "for f_i,f_line in enumerate(daily_fopen):\n",
        "  total_count,correct_count=0,0\n",
        "  sym0,price_json=f_line.strip().split(\"\\t\")\n",
        "  price_obj_list=json.loads(price_json)\n",
        "  close_prices=[float(v[\"close_price\"]) for v in price_obj_list]\n",
        "  #print(sym0,len(close_prices), \"prices:\", close_prices)\n",
        "  percent_pairs=get_prev_next_percent(close_prices)\n",
        "  for prev0,next0 in percent_pairs:\n",
        "\n",
        "    try: prediction=predict(prev0, PATH)\n",
        "    except: continue\n",
        "    actual=extract_labels(next0)\n",
        "    if prediction[0]<0.6: continue\n",
        "    total_count+=1\n",
        "    if actual[0]>0.5: correct_count+=1\n",
        "    # print(\"Symbol:\",sym0)\n",
        "    # print(\"prev0\",prev0)\n",
        "    # print(\"next0\",next0)\n",
        "    # print(\"prediction:\",[round(v,2) for v in prediction])\n",
        "    # print(\"actual:\",actual)\n",
        "    # print(\"----\")\n",
        "  last_close_prices=close_prices[-20:]\n",
        "  \n",
        "  try: last_close_prices_precentages=get_diff_percent(last_close_prices,last_close_prices[-1])\n",
        "  except: continue\n",
        "  try: latest_price=float(rs.robinhood.stocks.get_latest_price(sym0)[0])\n",
        "  except: latest_price=0\n",
        "\n",
        "  tomorrow_pred=predict(last_close_prices_precentages, PATH)\n",
        "  tomorrow_pred=[round(v,2) for v in tomorrow_pred]\n",
        "  #if f_i>100: break\n",
        "  correct_ratio=0\n",
        "  if total_count>0: correct_ratio=round(correct_count/total_count,2)\n",
        "  if correct_ratio==0: continue\n",
        "  if tomorrow_pred[0]<0.6: continue\n",
        "  if correct_ratio<0.7: continue\n",
        "  yesterday_closing=last_close_prices[-1]\n",
        "  print(\"Symbol:\",sym0, total_count,correct_count,\"correct_ratio:\",correct_ratio, \"tomorrow_pred\",tomorrow_pred, \"yesterday_closing:\",yesterday_closing, \"latest_price:\",latest_price)\n",
        "  print(\"--------\")\n",
        "daily_fopen.close()\n",
        "#get_prev_next_percent, get_prev_next_vals\n",
        "print(\"finished evaluating stocks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFkWDqu2yUv9"
      },
      "source": [
        "#Finding daily prices for a certain stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRpi_pWMxiuh",
        "outputId": "80fdc0c4-b6d9-42ad-c66a-d8502aaf798e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "404 Client Error: Not Found for url: https://api.robinhood.com/quotes/historicals/?symbols=FB&interval=day&span=3month&bounds=regular\n",
            "1\n",
            "[None]\n"
          ]
        }
      ],
      "source": [
        "stock_name=\"FB\"\n",
        "test_data = rs.robinhood.stocks.get_stock_historicals([stock_name], interval=\"day\", span=\"3month\")\n",
        "#dropbox_data = rs.stocks.get_stock_historicals()\n",
        "#test_data[:10]\n",
        "print(len(test_data))\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1jhIemFzY6",
        "outputId": "054f5176-7151-4495-9cec-b6d1be069b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3569 ['A', 'AAALY', 'AAC', 'AACAY', 'AAL', 'AAME', 'AAOI', 'AAON', 'AAP', 'AAPL']\n"
          ]
        }
      ],
      "source": [
        "all_symbols=[]\n",
        "stock_groups=[\"sp500\",\"nyse\",\"nasdaq\",\"forbes2000\"]\n",
        "for stg in stock_groups:\n",
        "  cur_root_dir=os.path.join(\"stock_market_data\",stg,\"csv\")\n",
        "  cur_symbols=[v.split(\".\")[0] for v in os.listdir(cur_root_dir)]\n",
        "  all_symbols.extend(cur_symbols)\n",
        "all_symbols=sorted(list(set(all_symbols)))\n",
        "all_stock_info_items=[]\n",
        "# cur_symbols=[v.split(\".\")[0] for v in os.listdir(root_dir)]\n",
        "print(len(all_symbols),all_symbols[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW2NFe-Dx9hc",
        "outputId": "702c51eb-c3fc-41f9-b1ad-d46a63718c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "symbol STWD quantity 14.26504800\n",
            "average_buy_price: 14.0203 current price: 23.58\n",
            "created_at: 2020-10-29T14:12:53.010228Z\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "#check the account and the current stock positions\n",
        "import requests\n",
        "# rs.robinhood.\n",
        "\n",
        "#dir(rs.orders.stocks.urls)\n",
        "#rs.profiles.load_basic_profile()\n",
        "#rs.profiles.load_portfolio_profile()\n",
        "#rs.profiles.load_investment_profile()\n",
        "#rs.profiles.load_user_profile()\n",
        "#rs.account.get_all_positions()\n",
        "positions=[rs.robinhood.account.get_open_stock_positions()]\n",
        "for p0 in positions[0]:\n",
        "  stock_url=p0[\"instrument\"]\n",
        "  average_buy_price=p0[\"average_buy_price\"]\n",
        "  quantity=p0[\"quantity\"]\n",
        "  created_at=p0[\"created_at\"]\n",
        "  res=requests.get(stock_url)\n",
        "  stock_dict=res.json()\n",
        "  symbol=stock_dict[\"symbol\"]\n",
        "  price=float(rs.robinhood.stocks.get_latest_price(symbol)[0])\n",
        "  print(\"symbol\",symbol,\"quantity\",quantity)\n",
        "  print(\"average_buy_price:\", average_buy_price,\"current price:\",price)\n",
        "  print(\"created_at:\",created_at)\n",
        "  #print(p0)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcmjTIua0wuM",
        "outputId": "e69170c7-cb40-4c45-f1ed-5973897b6ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1 results\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'bloomberg_unique': 'EQ0010174300001000',\n",
              "  'country': 'US',\n",
              "  'day_trade_ratio': '0.2500',\n",
              "  'default_collar_fraction': '0.05',\n",
              "  'extended_hours_fractional_tradability': False,\n",
              "  'fractional_tradability': 'tradable',\n",
              "  'fundamentals': 'https://api.robinhood.com/fundamentals/MSFT/',\n",
              "  'id': '50810c35-d215-4866-9758-0ada4ac79ffa',\n",
              "  'ipo_access_cob_deadline': None,\n",
              "  'ipo_access_status': None,\n",
              "  'ipo_access_supports_dsp': False,\n",
              "  'ipo_roadshow_url': None,\n",
              "  'ipo_s1_url': None,\n",
              "  'is_spac': False,\n",
              "  'is_test': False,\n",
              "  'list_date': '1987-09-17',\n",
              "  'maintenance_ratio': '0.2500',\n",
              "  'margin_initial_ratio': '0.5000',\n",
              "  'market': 'https://api.robinhood.com/markets/XNAS/',\n",
              "  'min_tick_size': None,\n",
              "  'name': 'Microsoft Corporation Common Stock',\n",
              "  'quote': 'https://api.robinhood.com/quotes/MSFT/',\n",
              "  'rhs_tradability': 'tradable',\n",
              "  'simple_name': 'Microsoft',\n",
              "  'splits': 'https://api.robinhood.com/instruments/50810c35-d215-4866-9758-0ada4ac79ffa/splits/',\n",
              "  'state': 'active',\n",
              "  'symbol': 'MSFT',\n",
              "  'tradability': 'tradable',\n",
              "  'tradable_chain_id': '1ac71e01-0677-42c6-a490-1457980954f8',\n",
              "  'tradeable': True,\n",
              "  'type': 'stock',\n",
              "  'url': 'https://api.robinhood.com/instruments/50810c35-d215-4866-9758-0ada4ac79ffa/'}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rs.robinhood.stocks.find_instrument_data(\"MSFT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6YdclmF7ZM9"
      },
      "source": [
        "#Dump"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imMJWNGz5MRE"
      },
      "outputs": [],
      "source": [
        "  # for fname in sample_files:\n",
        "  #   t0=time.time()\n",
        "  #   #print(\"epoch0\",epoch0, \"fname\",fname)\n",
        "  #   tmp_path=os.path.join(tmp_model_dir, \"model-%s.model\"%fname)\n",
        "  #   if os.path.exists(tmp_path):\n",
        "  #     checkpoint = torch.load(tmp_path)\n",
        "  #     rnn.load_state_dict(checkpoint['model_state_dict'])\n",
        "  #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  #     print(\"loaded model for this epoch\",tmp_path)\n",
        "  #     continue  \n",
        "  #   #\n",
        "  #   cur_path=os.path.join(root_dir,fname+\".csv\")\n",
        "  #   cur_train0,cur_test0=get_norm_close(cur_path,prev_n,next_n,train_ratio=0.8)\n",
        "  #   if n_train!=None: cur_train0=cur_train0[:n_train]\n",
        "  #   if n_test!=None: cur_test0=cur_test0[:n_test]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybP25SLeH1py"
      },
      "source": [
        "#test filtering NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHcsMXGyHLnk",
        "outputId": "5373565f-24f6-4568-bce7-6fb4fd760a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 ABC.csv\n",
            "[2.96875, 2.8125, 2.9375, 2.953125]\n",
            "[2.96875, 2.84375, 2.90625, 2.84375]\n",
            "[2.9375, 2.84375, 2.84375, 2.890625]\n",
            "[2.90625, 2.84375, 2.90625, 2.84375]\n",
            "[2.90625, 2.84375, 2.84375, 2.875]\n",
            "[2.90625, 2.84375, 2.84375, 2.859375]\n",
            "[2.875, 2.84375, 2.84375, 2.859375]\n",
            "[2.875, 2.84375, 2.84375, 2.875]\n",
            "[2.953125, 2.84375, 2.84375, 2.953125]\n",
            "[2.953125, 2.875, 2.890625, 2.9375]\n",
            "========\n",
            "1 ADP.csv\n",
            "[0.8271409869194031, 0.8085539937019348, 0.0, 0.8178470134735107]\n",
            "[0.8302389979362488, 0.814749002456665, 0.0, 0.8271409869194031]\n",
            "[0.8426309823989868, 0.8271409869194031, 0.0, 0.8426309823989868]\n",
            "[0.8612179756164551, 0.8395329713821411, 0.0, 0.8395329713821411]\n",
            "[0.848825991153717, 0.8364350199699402, 0.0, 0.8364350199699402]\n",
            "[0.8426309823989868, 0.8302389979362488, 0.0, 0.8302389979362488]\n",
            "[0.8426309823989868, 0.8271409869194031, 0.0, 0.8302389979362488]\n",
            "[0.8426309823989868, 0.8271409869194031, 0.0, 0.8426309823989868]\n",
            "[0.8302389979362488, 0.8116520047187805, 0.0, 0.8116520047187805]\n",
            "[0.8426309823989868, 0.8302389979362488, 0.0, 0.8364350199699402]\n",
            "========\n",
            "2 A.csv\n",
            "[35.765380859375, 28.612302780151367, 32.54649353027344, 31.473533630371094]\n",
            "[30.75822639465332, 28.47818374633789, 30.713518142700195, 28.880544662475582]\n",
            "[31.473533630371094, 28.65700912475586, 29.551143646240234, 31.473533630371094]\n",
            "[31.205293655395508, 28.612302780151367, 30.40057182312012, 28.612302780151367]\n",
            "[29.99821281433105, 28.612302780151367, 28.701717376708984, 29.372318267822266]\n",
            "[29.685264587402344, 29.14878463745117, 29.23819732666016, 29.46173095703125]\n",
            "[30.355865478515625, 29.014663696289062, 29.327611923217773, 30.13233184814453]\n",
            "[30.713518142700195, 29.28290367126465, 30.042917251586918, 30.177038192749023]\n",
            "[31.0711727142334, 29.95350456237793, 30.177038192749023, 30.713518142700195]\n",
            "[32.1888427734375, 30.8923454284668, 31.29470634460449, 31.56294631958008]\n",
            "========\n",
            "3 ABT.csv\n",
            "[0.5120279788970947, 0.4979990124702453, 0.0, 0.501505970954895]\n",
            "[0.5120279788970947, 0.4944919943809509, 0.0, 0.5050129890441895]\n",
            "[0.5155349969863892, 0.5032600164413452, 0.0, 0.5120279788970947]\n",
            "[0.5137810111045837, 0.4997529983520508, 0.0, 0.4997529983520508]\n",
            "[0.5067669749259949, 0.4997529983520508, 0.0, 0.5050129890441895]\n",
            "[0.4944919943809509, 0.4839709997177124, 0.0, 0.4892320036888122]\n",
            "[0.4997529983520508, 0.4839709997177124, 0.0, 0.4874779880046844]\n",
            "[0.4962460100650787, 0.4874779880046844, 0.0, 0.4874779880046844]\n",
            "[0.4909850060939789, 0.4787110090255737, 0.0, 0.4909850060939789]\n",
            "[0.5085210204124451, 0.4909850060939789, 0.0, 0.5067669749259949]\n",
            "========\n",
            "4 ABMD.csv\n",
            "[5.5, 5.4375, 0.0, 5.5]\n",
            "[5.5625, 5.4375, 5.5, 5.5625]\n",
            "[5.5625, 5.5, 5.5625, 5.5625]\n",
            "[5.5625, 5.5, 5.5625, 5.5625]\n",
            "[5.5625, 5.5, 5.5625, 5.5625]\n",
            "[5.5625, 5.5, 5.5625, 5.5]\n",
            "[5.5625, 5.5, 5.5, 5.5]\n",
            "[5.5625, 5.5, 5.5, 5.5625]\n",
            "[5.5625, 5.5, 5.5, 5.5625]\n",
            "[5.6875, 5.5625, 5.5625, 5.625]\n",
            "========\n",
            "5 ADI.csv\n",
            "[0.7639999985694885, 0.7279999852180481, 0.0, 0.7279999852180481]\n",
            "[0.7279999852180481, 0.7080000042915344, 0.0, 0.7200000286102295]\n",
            "[0.7360000014305115, 0.7200000286102295, 0.0, 0.7200000286102295]\n",
            "[0.7279999852180481, 0.7200000286102295, 0.0, 0.7200000286102295]\n",
            "[0.7200000286102295, 0.6880000233650208, 0.0, 0.6880000233650208]\n",
            "[0.6919999718666077, 0.6320000290870667, 0.0, 0.6399999856948853]\n",
            "[0.6240000128746033, 0.5920000076293945, 0.0, 0.6159999966621399]\n",
            "[0.6399999856948853, 0.6200000047683716, 0.0, 0.6399999856948853]\n",
            "[0.6240000128746033, 0.5960000157356262, 0.0, 0.6039999723434448]\n",
            "[0.6399999856948853, 0.6159999966621399, 0.0, 0.6399999856948853]\n",
            "========\n",
            "6 ABBV.csv\n",
            "[35.400001525878906, 34.099998474121094, 34.91999816894531, 35.119998931884766]\n",
            "[35.0, 34.15999984741211, 35.0, 34.83000183105469]\n",
            "[34.88999938964844, 34.25, 34.619998931884766, 34.38999938964844]\n",
            "[35.45000076293945, 34.150001525878906, 34.150001525878906, 34.459999084472656]\n",
            "[34.63999938964844, 33.36000061035156, 34.290000915527344, 33.709999084472656]\n",
            "[33.95000076293945, 33.61000061035156, 33.61000061035156, 33.900001525878906]\n",
            "[34.0, 33.33000183105469, 33.65999984741211, 34.0]\n",
            "[33.900001525878906, 33.349998474121094, 33.59000015258789, 33.849998474121094]\n",
            "[34.25, 33.79999923706055, 34.0099983215332, 34.09000015258789]\n",
            "[34.720001220703125, 33.709999084472656, 33.709999084472656, 34.599998474121094]\n",
            "========\n",
            "7 AAPL.csv\n",
            "[0.1289059966802597, 0.1283479928970337, 0.1283479928970337, 0.1283479928970337]\n",
            "[0.1222100034356117, 0.1216519996523857, 0.1222100034356117, 0.1216519996523857]\n",
            "[0.1132809966802597, 0.1127230003476142, 0.1132809966802597, 0.1127230003476142]\n",
            "[0.1160710006952285, 0.1155129969120025, 0.1155129969120025, 0.1155129969120025]\n",
            "[0.1194199994206428, 0.1188620030879974, 0.1188620030879974, 0.1188620030879974]\n",
            "[0.1266739964485168, 0.1261159926652908, 0.1261159926652908, 0.1261159926652908]\n",
            "[0.1328130066394806, 0.1322540044784546, 0.1322540044784546, 0.1322540044784546]\n",
            "[0.1383929997682571, 0.1378349959850311, 0.1378349959850311, 0.1378349959850311]\n",
            "[0.1456470042467117, 0.1450890004634857, 0.1450890004634857, 0.1450890004634857]\n",
            "[0.1590400040149688, 0.1584820002317428, 0.1584820002317428, 0.1584820002317428]\n",
            "========\n",
            "8 ADSK.csv\n",
            "[0.5104169845581055, 0.5, 0.0, 0.5]\n",
            "[0.5104169845581055, 0.5, 0.5, 0.5]\n",
            "[0.53125, 0.5208330154418945, 0.5208330154418945, 0.5208330154418945]\n",
            "[0.53125, 0.5208330154418945, 0.5208330154418945, 0.5208330154418945]\n",
            "[0.5416669845581055, 0.53125, 0.53125, 0.53125]\n",
            "[0.5208330154418945, 0.5104169845581055, 0.5208330154418945, 0.5104169845581055]\n",
            "[0.5104169845581055, 0.5, 0.5104169845581055, 0.5]\n",
            "[0.5208330154418945, 0.5104169845581055, 0.5104169845581055, 0.5104169845581055]\n",
            "[0.53125, 0.5208330154418945, 0.5208330154418945, 0.5208330154418945]\n",
            "[0.53125, 0.5208330154418945, 0.5208330154418945, 0.5208330154418945]\n",
            "========\n",
            "9 ADM.csv\n",
            "[2.000454902648926, 1.8485219478607176, 0.0, 1.8485219478607176]\n",
            "[1.916046977043152, 1.8316400051116943, 0.0, 1.9076069593429563]\n",
            "[2.034218072891236, 1.9329290390014648, 0.0, 1.9329290390014648]\n",
            "[2.017335891723633, 1.958250999450684, 0.0, 1.9835729598999023]\n",
            "[1.992014050483704, 1.9329290390014648, 0.0, 1.9329290390014648]\n",
            "[1.949810028076172, 1.8485219478607176, 0.0, 1.8822840452194212]\n",
            "[1.890725016593933, 1.7894359827041626, 0.0, 1.806318044662476]\n",
            "[1.8231990337371824, 1.7303509712219238, 0.0, 1.7303509712219238]\n",
            "[1.7387919425964355, 1.519333004951477, 0.0, 1.688148021697998]\n",
            "[1.8485219478607176, 1.671265959739685, 0.0, 1.8485219478607176]\n",
            "========\n",
            "10 ACN.csv\n",
            "[15.289999961853027, 15.0, 15.100000381469728, 15.170000076293944]\n",
            "[15.050000190734863, 14.800000190734863, 15.050000190734863, 15.010000228881836]\n",
            "[15.010000228881836, 14.550000190734863, 15.0, 15.0]\n",
            "[14.970000267028809, 14.699999809265137, 14.949999809265137, 14.859999656677246]\n",
            "[14.949999809265137, 14.649999618530272, 14.699999809265137, 14.949999809265137]\n",
            "[14.989999771118164, 14.5, 14.949999809265137, 14.5]\n",
            "[14.59000015258789, 14.5, 14.510000228881836, 14.510000228881836]\n",
            "[14.779999732971191, 14.5, 14.5, 14.699999809265137]\n",
            "[15.010000228881836, 14.600000381469728, 14.710000038146973, 14.960000038146973]\n",
            "[15.5, 14.899999618530272, 15.0, 15.5]\n",
            "========\n",
            "11 AAP.csv\n",
            "[14.466667175292969, 13.359999656677246, 13.386667251586914, 13.880000114440918]\n",
            "[14.266667366027832, 13.880000114440918, 13.880000114440918, 14.266667366027832]\n",
            "[14.233332633972168, 13.783332824707031, 14.233332633972168, 13.783332824707031]\n",
            "[13.783332824707031, 13.233332633972168, 13.783332824707031, 13.233332633972168]\n",
            "[14.783332824707031, 13.43333339691162, 13.43333339691162, 14.666666984558104]\n",
            "[14.733332633972168, 14.56999969482422, 14.733332633972168, 14.57333278656006]\n",
            "[14.666666984558104, 14.466667175292969, 14.583333015441896, 14.666666984558104]\n",
            "[14.640000343322754, 14.58666706085205, 14.633333206176758, 14.616666793823242]\n",
            "[14.710000038146973, 14.58666706085205, 14.600000381469728, 14.58666706085205]\n",
            "[14.583333015441896, 14.036666870117188, 14.583333015441896, 14.100000381469728]\n",
            "========\n",
            "12 AAL.csv\n",
            "[21.399999618530277, 19.100000381469727, 21.049999237060547, 19.299999237060547]\n",
            "[20.530000686645508, 19.200000762939453, 19.299999237060547, 20.5]\n",
            "[20.57999992370605, 20.100000381469727, 20.399999618530277, 20.209999084472656]\n",
            "[21.049999237060547, 20.18000030517578, 20.26000022888184, 21.01000022888184]\n",
            "[21.75, 20.899999618530277, 20.899999618530277, 21.5]\n",
            "[22.5, 21.440000534057617, 21.440000534057617, 22.15999984741211]\n",
            "[22.309999465942383, 21.75, 22.100000381469727, 22.200000762939453]\n",
            "[23.0, 22.399999618530277, 22.600000381469727, 22.57999992370605]\n",
            "[22.600000381469727, 21.799999237060547, 22.25, 22.149999618530277]\n",
            "[22.290000915527344, 22.100000381469727, 22.280000686645508, 22.209999084472656]\n",
            "========\n",
            "13 ALGN.csv\n",
            "[18.1875, 16.25, 18.125, 16.875]\n",
            "[17.1875, 13.4375, 17.125, 13.5]\n",
            "[14.375, 13.0, 13.125, 14.1875]\n",
            "[14.5625, 13.75, 14.0, 14.125]\n",
            "[14.5, 13.875, 14.4375, 14.125]\n",
            "[14.0625, 13.875, 14.015625, 14.0]\n",
            "[14.0625, 13.0, 14.0, 13.0625]\n",
            "[13.1875, 13.0, 13.125, 13.0625]\n",
            "[13.0625, 11.875, 13.046875, 12.0]\n",
            "[12.125, 10.9375, 12.046875, 11.0625]\n",
            "========\n",
            "14 APH.csv\n",
            "[0.2890630066394806, 0.2890630066394806, 0.2890630066394806, 0.2890630066394806]\n",
            "[0.2890630066394806, 0.2890630066394806, 0.2890630066394806, 0.2890630066394806]\n",
            "[0.2890630066394806, 0.2890630066394806, 0.2890630066394806, 0.2890630066394806]\n",
            "[0.2890630066394806, 0.28125, 0.2890630066394806, 0.28125]\n",
            "[0.2734380066394806, 0.2617189884185791, 0.2617189884185791, 0.2734380066394806]\n",
            "[0.2734380066394806, 0.2578130066394806, 0.2734380066394806, 0.2578130066394806]\n",
            "[0.2578130066394806, 0.2382809966802597, 0.2539060115814209, 0.2421880066394806]\n",
            "[0.25, 0.234375, 0.2421880066394806, 0.2421880066394806]\n",
            "[0.2460940033197403, 0.2421880066394806, 0.2460940033197403, 0.2460940033197403]\n",
            "[0.2421880066394806, 0.234375, 0.2421880066394806, 0.234375]\n",
            "========\n",
            "15 AOS.csv\n",
            "[1.125, 1.0833330154418943, 0.0, 1.0833330154418943]\n",
            "[1.0902780294418335, 1.076388955116272, 0.0, 1.0833330154418943]\n",
            "[1.076388955116272, 1.069443941116333, 0.0, 1.069443941116333]\n",
            "[1.0625, 0.9444440007209778, 0.0, 0.9444440007209778]\n",
            "[0.9652780294418336, 0.9375, 0.0, 0.9652780294418336]\n",
            "[0.9583330154418944, 0.9513890147209167, 0.0, 0.9513890147209167]\n",
            "[1.0277780294418335, 0.9444440007209778, 0.0, 1.0277780294418335]\n",
            "[1.048611044883728, 1.0, 0.0, 1.0]\n",
            "[0.9722219705581664, 0.9444440007209778, 0.0, 0.9444440007209778]\n",
            "[0.9513890147209167, 0.9513890147209167, 0.0, 0.9513890147209167]\n",
            "========\n",
            "16 AWK.csv\n",
            "[21.450000762939453, 20.21999931335449, 20.600000381469727, 20.600000381469727]\n",
            "[21.0, 20.309999465942383, 20.71999931335449, 20.809999465942383]\n",
            "[21.25, 20.700000762939453, 21.0, 21.170000076293945]\n",
            "[21.489999771118164, 20.809999465942383, 21.489999771118164, 21.0]\n",
            "[21.38999938964844, 20.81999969482422, 21.38999938964844, 21.06999969482422]\n",
            "[21.200000762939453, 20.920000076293945, 21.07999992370605, 21.149999618530277]\n",
            "[22.0, 21.0, 21.21999931335449, 21.90999984741211]\n",
            "[22.15999984741211, 21.489999771118164, 22.15999984741211, 22.01000022888184]\n",
            "[22.450000762939453, 21.559999465942383, 22.450000762939453, 21.90999984741211]\n",
            "[21.86000061035156, 21.3700008392334, 21.799999237060547, 21.549999237060547]\n",
            "========\n",
            "17 ALLE.csv\n",
            "[50.0, 47.4900016784668, 49.75, 48.47999954223633]\n",
            "[49.0, 45.29999923706055, 48.9900016784668, 45.79999923706055]\n",
            "[46.0, 43.9900016784668, 45.97999954223633, 44.650001525878906]\n",
            "[45.150001525878906, 43.040000915527344, 44.5, 43.790000915527344]\n",
            "[44.09999847412109, 43.25, 43.900001525878906, 43.9900016784668]\n",
            "[45.619998931884766, 43.70000076293945, 44.27999877929688, 44.18000030517578]\n",
            "[45.0, 43.040000915527344, 45.0, 43.20000076293945]\n",
            "[43.54999923706055, 42.5, 43.54999923706055, 43.2400016784668]\n",
            "[43.84999847412109, 42.75, 42.75, 43.2400016784668]\n",
            "[43.66999816894531, 41.52000045776367, 43.08000183105469, 43.13999938964844]\n",
            "========\n",
            "18 AME.csv\n",
            "[1.805556058883667, 1.7777780294418335, 1.796296000480652, 1.796296000480652]\n",
            "[1.805556058883667, 1.7777780294418335, 1.787037014961243, 1.7777780294418335]\n",
            "[1.7777780294418335, 1.7314809560775757, 1.7685190439224243, 1.7314809560775757]\n",
            "[1.75, 1.7222219705581665, 1.7407410144805908, 1.7314809560775757]\n",
            "[1.7314809560775757, 1.694443941116333, 1.703703999519348, 1.7314809560775757]\n",
            "[1.75, 1.7407410144805908, 1.75, 1.75]\n",
            "[1.7592589855194092, 1.7314809560775757, 1.75, 1.75]\n",
            "[1.7685190439224243, 1.7407410144805908, 1.7685190439224243, 1.7407410144805908]\n",
            "[1.7777780294418335, 1.7314809560775757, 1.7407410144805908, 1.7777780294418335]\n",
            "[1.9166669845581052, 1.7777780294418335, 1.7777780294418335, 1.888888955116272]\n",
            "========\n",
            "19 APD.csv\n",
            "[4.032723903656006, 3.9459989070892334, 0.0, 3.9893620014190674]\n",
            "[4.0038161277771, 3.9459989070892334, 0.0, 3.974906921386719]\n",
            "[4.104994773864746, 4.018270015716553, 0.0, 4.047179222106934]\n",
            "[4.047179222106934, 4.018270015716553, 0.0, 4.032723903656006]\n",
            "[4.032723903656006, 3.9893620014190674, 0.0, 4.032723903656006]\n",
            "[4.0038161277771, 3.8881819248199454, 0.0, 3.8881819248199454]\n",
            "[3.9026360511779785, 3.830364942550659, 0.0, 3.844820022583008]\n",
            "[3.9026360511779785, 3.830364942550659, 0.0, 3.859273910522461]\n",
            "[3.844820022583008, 3.815911054611206, 0.0, 3.830364942550659]\n",
            "[4.047179222106934, 3.859273910522461, 0.0, 4.0038161277771]\n",
            "========\n",
            "20 ARE.csv\n",
            "[21.375, 20.625, 20.625, 21.0]\n",
            "[21.375, 21.0, 21.0, 21.375]\n",
            "[22.125, 21.125, 21.125, 22.0]\n",
            "[22.25, 21.5, 22.125, 22.0]\n",
            "[22.125, 21.625, 21.75, 22.125]\n",
            "[22.25, 21.75, 22.25, 21.875]\n",
            "[21.875, 21.25, 21.5, 21.875]\n",
            "[21.5, 21.5, 21.5, 21.5]\n",
            "[21.5, 21.5, 21.5, 21.5]\n",
            "[21.75, 21.5, 21.5, 21.75]\n",
            "========\n",
            "21 AIZ.csv\n",
            "[24.940000534057617, 23.25, 23.75, 24.700000762939453]\n",
            "[24.940000534057617, 24.149999618530277, 24.709999084472656, 24.850000381469727]\n",
            "[24.700000762939453, 24.0, 24.600000381469727, 24.46999931335449]\n",
            "[24.46999931335449, 24.049999237060547, 24.149999618530277, 24.350000381469727]\n",
            "[24.5, 24.149999618530277, 24.280000686645508, 24.25]\n",
            "[25.0, 24.170000076293945, 24.190000534057617, 24.68000030517578]\n",
            "[24.84000015258789, 24.280000686645508, 24.790000915527344, 24.700000762939453]\n",
            "[24.899999618530277, 24.5, 24.75, 24.600000381469727]\n",
            "[25.399999618530277, 24.350000381469727, 24.600000381469727, 25.399999618530277]\n",
            "[25.979999542236328, 25.25, 25.25, 25.68000030517578]\n",
            "========\n",
            "22 ALB.csv\n",
            "[7.375, 7.25, 7.375, 7.25]\n",
            "[7.125, 6.75, 7.125, 6.9375]\n",
            "[6.9375, 6.8125, 6.9375, 6.8125]\n",
            "[6.6875, 6.25, 6.6875, 6.4375]\n",
            "[6.75, 6.4375, 6.5, 6.625]\n",
            "[6.625, 6.5, 6.625, 6.5625]\n",
            "[6.5625, 6.4375, 6.4375, 6.5625]\n",
            "[6.625, 6.5, 6.625, 6.5]\n",
            "[6.75, 6.4375, 6.5, 6.75]\n",
            "[6.8125, 6.4375, 6.4375, 6.75]\n",
            "========\n",
            "23 APA.csv\n",
            "[3.607503890991211, 3.5113039016723637, 3.559403896331787, 3.559403896331787]\n",
            "[3.823954105377197, 3.583453893661499, 3.583453893661499, 3.799904108047485]\n",
            "[3.848004102706909, 3.799904108047485, 3.799904108047485, 3.848004102706909]\n",
            "[3.9923040866851807, 3.848004102706909, 3.848004102706909, 3.9682540893554688]\n",
            "[4.064454078674316, 3.920154094696045, 3.9682540893554688, 4.040403842926025]\n",
            "[4.136603832244873, 4.040403842926025, 4.040403842926025, 4.088503837585449]\n",
            "[4.088503837585449, 3.9682540893554688, 4.088503837585449, 3.9682540893554688]\n",
            "[3.9923040866851807, 3.872054100036621, 3.9682540893554688, 3.920154094696045]\n",
            "[3.9923040866851807, 3.872054100036621, 3.920154094696045, 3.9682540893554688]\n",
            "[4.184703826904297, 3.9682540893554688, 3.9682540893554688, 4.11255407333374]\n",
            "========\n",
            "24 ALK.csv\n",
            "[0.8786849975585938, 0.8503400087356567, 0.0, 0.8786849975585938]\n",
            "[0.8786849975585938, 0.8503400087356567, 0.0, 0.8503400087356567]\n",
            "[0.9353740215301514, 0.8786849975585938, 0.0, 0.8786849975585938]\n",
            "[0.9070289731025696, 0.8786849975585938, 0.0, 0.8786849975585938]\n",
            "[0.8786849975585938, 0.8503400087356567, 0.0, 0.8503400087356567]\n",
            "[0.8786849975585938, 0.8503400087356567, 0.0, 0.8786849975585938]\n",
            "[0.8786849975585938, 0.8219950199127197, 0.0, 0.8219950199127197]\n",
            "[0.8786849975585938, 0.8219950199127197, 0.0, 0.8219950199127197]\n",
            "[0.7936509847640991, 0.7936509847640991, 0.0, 0.7936509847640991]\n",
            "[0.8503400087356567, 0.7936509847640991, 0.0, 0.8503400087356567]\n",
            "========\n",
            "25 AEE.csv\n",
            "[43.0625, 42.375, 43.0, 42.8125]\n",
            "[43.125, 42.0625, 43.0625, 42.5]\n",
            "[42.1875, 41.75, 42.125, 41.875]\n",
            "[41.875, 40.875, 41.875, 41.5]\n",
            "[41.5625, 40.75, 41.5625, 41.0]\n",
            "[41.8125, 40.9375, 40.9375, 41.375]\n",
            "[41.8125, 41.0, 41.125, 41.75]\n",
            "[41.75, 41.1875, 41.75, 41.4375]\n",
            "[41.4375, 41.125, 41.4375, 41.25]\n",
            "[41.1875, 40.25, 41.1875, 40.3125]\n",
            "========\n",
            "26 AMGN.csv\n",
            "[0.3489579856395721, 0.34375, 0.0, 0.34375]\n",
            "[0.3072920143604278, 0.3020829856395721, 0.0, 0.3020829856395721]\n",
            "[0.3125, 0.3020829856395721, 0.0, 0.3020829856395721]\n",
            "[0.3177079856395721, 0.3125, 0.0, 0.3125]\n",
            "[0.3177079856395721, 0.3125, 0.0, 0.3125]\n",
            "[0.3125, 0.3072920143604278, 0.0, 0.3072920143604278]\n",
            "[0.28125, 0.2708329856395721, 0.0, 0.2708329856395721]\n",
            "[0.2760420143604278, 0.2708329856395721, 0.0, 0.2708329856395721]\n",
            "[0.2708329856395721, 0.265625, 0.0, 0.265625]\n",
            "[0.2786459922790527, 0.2708329856395721, 0.0, 0.2708329856395721]\n",
            "========\n",
            "27 ANTM.csv\n",
            "[20.450000762939453, 20.174999237060547, 20.25, 20.450000762939453]\n",
            "[21.25, 20.5049991607666, 20.549999237060547, 20.940000534057617]\n",
            "[21.299999237060547, 20.899999618530277, 21.125, 21.084999084472656]\n",
            "[21.549999237060547, 21.01000022888184, 21.024999618530277, 21.450000762939453]\n",
            "[21.75, 21.375, 21.75, 21.44499969482422]\n",
            "[22.149999618530277, 21.075000762939453, 21.475000381469727, 22.024999618530277]\n",
            "[21.875, 21.2450008392334, 21.875, 21.625]\n",
            "[21.549999237060547, 21.0, 21.5, 21.350000381469727]\n",
            "[21.25, 20.5, 21.25, 21.0]\n",
            "[21.424999237060547, 20.850000381469727, 21.0, 21.424999237060547]\n",
            "========\n",
            "28 AEP.csv\n",
            "[31.0, 30.125, 0.0, 30.625]\n",
            "[31.0, 30.25, 30.625, 31.0]\n",
            "[31.25, 30.75, 31.0, 30.75]\n",
            "[30.875, 30.5, 30.75, 30.875]\n",
            "[30.75, 30.5, 30.75, 30.5]\n",
            "[30.875, 30.375, 30.5, 30.75]\n",
            "[30.625, 30.25, 30.625, 30.5]\n",
            "[30.5, 29.625, 30.5, 29.625]\n",
            "[29.75, 29.125, 29.625, 29.625]\n",
            "[29.625, 29.25, 29.625, 29.5]\n",
            "========\n",
            "29 AON.csv\n",
            "[2.7037041187286377, 2.685184955596924, 0.0, 2.685184955596924]\n",
            "[2.722222089767456, 2.685184955596924, 0.0, 2.722222089767456]\n",
            "[2.722222089767456, 2.7037041187286377, 0.0, 2.722222089767456]\n",
            "[2.740741014480591, 2.7037041187286377, 0.0, 2.740741014480591]\n",
            "[2.759258985519409, 2.7037041187286377, 0.0, 2.7037041187286377]\n",
            "[2.740741014480591, 2.7037041187286377, 0.0, 2.722222089767456]\n",
            "[2.7962958812713623, 2.7037041187286377, 0.0, 2.7962958812713623]\n",
            "[2.90740704536438, 2.777777910232544, 0.0, 2.90740704536438]\n",
            "[3.0, 2.90740704536438, 0.0, 2.962963104248047]\n",
            "[3.055556058883667, 2.981481075286865, 0.0, 3.0]\n",
            "========\n",
            "30 AKAM.csv\n",
            "[165.0, 110.0, 110.0, 145.1875]\n",
            "[179.0, 143.0, 144.0, 174.3125]\n",
            "[199.0, 175.0, 178.0, 192.75]\n",
            "[204.0, 185.125, 200.5, 190.875]\n",
            "[191.9375, 171.0, 191.875, 173.25]\n",
            "[183.0, 166.4375, 178.0, 174.25]\n",
            "[190.5, 172.875, 173.0, 187.5625]\n",
            "[197.0, 178.0, 189.1875, 189.75]\n",
            "[195.5, 181.5, 190.375, 190.0]\n",
            "[194.25, 178.5, 190.0, 182.1875]\n",
            "========\n",
            "31 AXP.csv\n",
            "[3.693286895751953, 3.693286895751953, 3.693286895751953, 3.693286895751953]\n",
            "[3.677195072174072, 3.677195072174072, 3.677195072174072, 3.677195072174072]\n",
            "[3.6047770977020255, 3.6047770977020255, 3.6047770977020255, 3.6047770977020255]\n",
            "[3.4116640090942383, 3.4116640090942383, 3.4116640090942383, 3.4116640090942383]\n",
            "[3.371432065963745, 3.371432065963745, 3.371432065963745, 3.371432065963745]\n",
            "[3.4518959522247314, 3.4518959522247314, 3.4518959522247314, 3.4518959522247314]\n",
            "[3.427757024765014, 3.427757024765014, 3.427757024765014, 3.427757024765014]\n",
            "[3.484081983566284, 3.484081983566284, 3.484081983566284, 3.484081983566284]\n",
            "[3.484081983566284, 3.484081983566284, 3.484081983566284, 3.484081983566284]\n",
            "[3.443850040435791, 3.443850040435791, 3.443850040435791, 3.443850040435791]\n",
            "========\n",
            "32 AMD.csv\n",
            "[3.3020830154418945, 3.125, 0.0, 3.1458330154418945]\n",
            "[3.125, 2.9375, 0.0, 3.03125]\n",
            "[3.0833330154418945, 3.0208330154418945, 0.0, 3.041666984558105]\n",
            "[3.0625, 3.010416984558105, 0.0, 3.010416984558105]\n",
            "[3.0208330154418945, 2.90625, 0.0, 2.916666984558105]\n",
            "[2.916666984558105, 2.635416984558105, 0.0, 2.666666984558105]\n",
            "[2.75, 2.5520830154418945, 0.0, 2.604166984558105]\n",
            "[2.604166984558105, 2.4270830154418945, 0.0, 2.447916984558105]\n",
            "[2.375, 2.2395830154418945, 0.0, 2.375]\n",
            "[2.5833330154418945, 2.4583330154418945, 0.0, 2.541666984558105]\n",
            "========\n",
            "33 AMAT.csv\n",
            "[0.0980900004506111, 0.0954860001802444, 0.0, 0.0954860001802444]\n",
            "[0.0963540002703666, 0.09375, 0.0, 0.09375]\n",
            "[0.0980900004506111, 0.0954860001802444, 0.0, 0.0954860001802444]\n",
            "[0.0998260006308555, 0.0972220003604888, 0.0, 0.0972220003604888]\n",
            "[0.1032989993691444, 0.1006940007209777, 0.0, 0.1006940007209777]\n",
            "[0.0980900004506111, 0.0954860001802444, 0.0, 0.0954860001802444]\n",
            "[0.0954860001802444, 0.0928819999098777, 0.0, 0.0928819999098777]\n",
            "[0.0859379991889, 0.0833330005407333, 0.0, 0.0833330005407333]\n",
            "[0.0798610001802444, 0.0763889998197555, 0.0, 0.0763889998197555]\n",
            "[0.0885419994592666, 0.0850690007209777, 0.0, 0.0850690007209777]\n",
            "========\n",
            "34 AMP.csv\n",
            "[38.09999847412109, 37.0, 38.02000045776367, 37.0099983215332]\n",
            "[37.0, 36.59999847412109, 37.0, 36.59999847412109]\n",
            "[36.70000076293945, 36.45000076293945, 36.59999847412109, 36.70000076293945]\n",
            "[37.0, 36.59999847412109, 36.59999847412109, 36.70000076293945]\n",
            "[36.59999847412109, 36.0, 36.59999847412109, 36.400001525878906]\n",
            "[37.20000076293945, 36.400001525878906, 36.400001525878906, 37.20000076293945]\n",
            "[38.09999847412109, 37.20000076293945, 37.20000076293945, 37.68000030517578]\n",
            "[38.150001525878906, 37.75, 37.75, 37.75]\n",
            "[37.900001525878906, 37.13999938964844, 37.75, 37.25]\n",
            "[37.400001525878906, 36.79999923706055, 37.2400016784668, 37.0]\n",
            "========\n",
            "35 ANET.csv\n",
            "[15.0, 13.75, 13.8125, 13.75]\n",
            "[14.25, 13.75, 14.0625, 13.800000190734863]\n",
            "[15.0, 13.8125, 13.8125, 15.0]\n",
            "[15.875, 14.664999961853027, 15.0, 15.875]\n",
            "[16.957500457763672, 15.767499923706056, 15.912500381469728, 16.5]\n",
            "[18.0, 16.75, 17.11750030517578, 17.395000457763672]\n",
            "[17.875, 16.908750534057617, 17.80500030517578, 17.875]\n",
            "[18.15749931335449, 17.25, 17.97249984741211, 17.427499771118164]\n",
            "[17.75, 16.969999313354492, 17.517499923706055, 17.239999771118164]\n",
            "[17.202499389648438, 16.127500534057617, 17.00749969482422, 16.417499542236328]\n",
            "========\n",
            "36 AJG.csv\n",
            "[1.875, 1.84375, 0.0, 1.84375]\n",
            "[1.84375, 1.78125, 0.0, 1.78125]\n",
            "[1.8125, 1.78125, 0.0, 1.78125]\n",
            "[1.8125, 1.75, 0.0, 1.75]\n",
            "[1.75, 1.71875, 0.0, 1.71875]\n",
            "[1.78125, 1.75, 0.0, 1.75]\n",
            "[1.78125, 1.75, 0.0, 1.75]\n",
            "[1.875, 1.8125, 0.0, 1.8125]\n",
            "[1.875, 1.8125, 0.0, 1.8125]\n",
            "[1.875, 1.8125, 0.0, 1.8125]\n",
            "========\n",
            "37 AZO.csv\n",
            "[7.375, 6.5625, 6.875, 7.34375]\n",
            "[7.34375, 6.875, 7.28125, 7.3125]\n",
            "[7.28125, 7.09375, 7.28125, 7.21875]\n",
            "[7.1875, 7.09375, 7.125, 7.125]\n",
            "[7.1875, 6.875, 7.0625, 7.125]\n",
            "[7.25, 7.125, 7.125, 7.1875]\n",
            "[7.5, 7.125, 7.125, 7.5]\n",
            "[8.28125, 7.75, 7.75, 8.0625]\n",
            "[8.25, 8.0625, 8.125, 8.15625]\n",
            "[8.21875, 7.9375, 8.21875, 8.0]\n",
            "========\n",
            "38 ATVI.csv\n",
            "[1.25, 0.9375, 1.25, 0.9375]\n",
            "[1.2291669845581057, 0.8541669845581055, 1.2291669845581057, 0.8645830154418945]\n",
            "[0.8645830154418945, 0.8645830154418945, 0.8645830154418945, 0.8645830154418945]\n",
            "[0.8333330154418945, 0.5833330154418945, 0.7083330154418945, 0.8229169845581055]\n",
            "[0.875, 0.8333330154418945, 0.875, 0.8333330154418945]\n",
            "[0.875, 0.8333330154418945, 0.8333330154418945, 0.875]\n",
            "[0.875, 0.8333330154418945, 0.8333330154418945, 0.875]\n",
            "[0.9791669845581056, 0.8541669845581055, 0.8541669845581055, 0.9583330154418944]\n",
            "[1.0, 0.9583330154418944, 1.0, 0.9583330154418944]\n",
            "[1.0, 0.8541669845581055, 0.8541669845581055, 1.0]\n",
            "========\n",
            "39 AMZN.csv\n",
            "[2.5, 1.9270830154418943, 2.4375, 1.9583330154418943]\n",
            "[1.9791669845581052, 1.7083330154418943, 1.96875, 1.7291669845581057]\n",
            "[1.7708330154418943, 1.625, 1.7604169845581057, 1.7083330154418943]\n",
            "[1.75, 1.6354169845581057, 1.7291669845581057, 1.6354169845581057]\n",
            "[1.6458330154418943, 1.375, 1.6354169845581057, 1.4270830154418943]\n",
            "[1.4479169845581057, 1.3125, 1.4375, 1.3958330154418943]\n",
            "[1.5208330154418943, 1.3333330154418943, 1.40625, 1.5]\n",
            "[1.6458330154418943, 1.4583330154418943, 1.5104169845581057, 1.5833330154418943]\n",
            "[1.6354169845581057, 1.53125, 1.625, 1.53125]\n",
            "[1.5416669845581057, 1.4791669845581057, 1.5416669845581057, 1.5052080154418943]\n",
            "========\n",
            "40 AMT.csv\n",
            "[18.0, 17.375, 17.375, 17.375]\n",
            "[17.25, 17.0, 17.25, 17.25]\n",
            "[17.125, 16.75, 17.125, 17.125]\n",
            "[15.75, 15.5, 15.75, 15.75]\n",
            "[16.125, 16.125, 16.125, 16.125]\n",
            "[16.375, 16.125, 16.125, 16.125]\n",
            "[16.5, 15.875, 16.125, 15.875]\n",
            "[16.875, 15.875, 15.875, 16.25]\n",
            "[16.5, 16.125, 16.25, 16.25]\n",
            "[16.375, 16.125, 16.25, 16.375]\n",
            "========\n",
            "41 AVB.csv\n",
            "[20.875, 20.625, 20.625, 20.875]\n",
            "[20.75, 20.625, 20.75, 20.625]\n",
            "[20.75, 20.625, 20.625, 20.625]\n",
            "[21.375, 20.625, 20.625, 21.375]\n",
            "[21.375, 20.875, 21.375, 21.125]\n",
            "[21.125, 20.75, 21.125, 21.0]\n",
            "[21.0, 20.875, 21.0, 20.875]\n",
            "[20.875, 20.75, 20.875, 20.75]\n",
            "[21.5, 20.75, 20.75, 21.25]\n",
            "[21.125, 21.0, 21.125, 21.0]\n",
            "========\n",
            "42 ALTR.csv\n",
            "[18.979999542236328, 16.549999237060547, 17.100000381469727, 18.309999465942383]\n",
            "[18.200000762939453, 17.329999923706055, 18.049999237060547, 17.40999984741211]\n",
            "[19.399999618530277, 17.5, 17.5, 19.3700008392334]\n",
            "[19.600000381469727, 18.10099983215332, 19.059999465942383, 18.96999931335449]\n",
            "[19.94499969482422, 18.850000381469727, 18.989999771118164, 19.51000022888184]\n",
            "[20.3799991607666, 18.6200008392334, 19.549999237060547, 20.36000061035156]\n",
            "[20.6200008392334, 20.0, 20.1200008392334, 20.399999618530277]\n",
            "[20.850000381469727, 20.229999542236328, 20.399999618530277, 20.76000022888184]\n",
            "[21.780000686645508, 20.500999450683597, 20.6200008392334, 21.71999931335449]\n",
            "[22.979999542236328, 21.520000457763672, 21.540000915527344, 22.36000061035156]\n",
            "========\n",
            "43 AVY.csv\n",
            "[5.1875, 5.15625, 0.0, 5.15625]\n",
            "[5.203125, 5.171875, 0.0, 5.171875]\n",
            "[5.171875, 5.125, 0.0, 5.140625]\n",
            "[5.125, 5.125, 0.0, 5.125]\n",
            "[5.140625, 5.125, 0.0, 5.125]\n",
            "[5.140625, 5.125, 0.0, 5.125]\n",
            "[5.125, 5.0625, 0.0, 5.0625]\n",
            "[5.0625, 5.03125, 0.0, 5.03125]\n",
            "[5.046875, 5.0, 0.0, 5.015625]\n",
            "[5.078125, 5.015625, 0.0, 5.078125]\n",
            "========\n",
            "44 CAH.csv\n",
            "[1.4041999578475952, 1.3791249990463257, 0.0, 1.3791249990463257]\n",
            "[1.4041999578475952, 1.3791249990463257, 0.0, 1.3791249990463257]\n",
            "[1.3791249990463257, 1.3540500402450562, 0.0, 1.3540500402450562]\n",
            "[1.4041999578475952, 1.3540500402450562, 0.0, 1.3540500402450562]\n",
            "[1.4041999578475952, 1.3540500402450562, 0.0, 1.3540500402450562]\n",
            "[1.4292750358581543, 1.3791249990463257, 0.0, 1.3791249990463257]\n",
            "[1.4292750358581543, 1.3791249990463257, 0.0, 1.3791249990463257]\n",
            "[1.529574990272522, 1.4794249534606934, 0.0, 1.4794249534606934]\n",
            "[1.7050989866256714, 1.6298749446868896, 0.0, 1.6298749446868896]\n",
            "[1.680024027824402, 1.6298749446868896, 0.0, 1.6298749446868896]\n",
            "========\n",
            "45 CDNS.csv\n",
            "[2.1111109256744385, 1.944443941116333, 0.0, 2.0]\n",
            "[2.027777910232544, 1.888888955116272, 0.0, 1.888888955116272]\n",
            "[1.944443941116333, 1.888888955116272, 0.0, 1.888888955116272]\n",
            "[1.944443941116333, 1.888888955116272, 0.0, 1.888888955116272]\n",
            "[1.9166669845581052, 1.8333330154418943, 0.0, 1.8333330154418943]\n",
            "[1.888888955116272, 1.805556058883667, 0.0, 1.888888955116272]\n",
            "[1.888888955116272, 1.8333330154418943, 0.0, 1.8333330154418943]\n",
            "[1.861111044883728, 1.8333330154418943, 0.0, 1.861111044883728]\n",
            "[1.861111044883728, 1.8333330154418943, 0.0, 1.8333330154418943]\n",
            "[1.861111044883728, 1.7777780294418335, 0.0, 1.805556058883667]\n",
            "========\n",
            "46 BIO.csv\n",
            "[2.5462958812713623, 2.5462958812713623, 2.5462958812713623, 2.5462958812713623]\n",
            "[2.5462958812713623, 2.5462958812713623, 2.5462958812713623, 2.5462958812713623]\n",
            "[2.65740704536438, 2.6481480598449707, 2.6481480598449707, 2.65740704536438]\n",
            "[2.65740704536438, 2.65740704536438, 2.65740704536438, 2.65740704536438]\n",
            "[2.6018519401550293, 2.5462958812713623, 2.6018519401550293, 2.564815044403076]\n",
            "[2.564815044403076, 2.564815044403076, 2.564815044403076, 2.564815044403076]\n",
            "[2.564815044403076, 2.564815044403076, 2.564815044403076, 2.564815044403076]\n",
            "[2.1296300888061523, 2.1296300888061523, 2.1296300888061523, 2.1296300888061523]\n",
            "[2.138889074325561, 2.138889074325561, 2.138889074325561, 2.138889074325561]\n",
            "[2.15740704536438, 2.1296300888061523, 2.138889074325561, 2.1296300888061523]\n",
            "========\n",
            "47 CDE.csv\n",
            "[148.0, 136.0, 0.0, 136.0]\n",
            "[152.0, 140.0, 0.0, 140.0]\n",
            "[164.0, 152.0, 0.0, 152.0]\n",
            "[168.0, 156.0, 0.0, 156.0]\n",
            "[168.0, 156.0, 0.0, 156.0]\n",
            "[140.0, 128.0, 0.0, 128.0]\n",
            "[132.0, 120.0, 0.0, 120.0]\n",
            "[128.0, 116.0, 0.0, 116.0]\n",
            "[104.0, 92.0, 0.0, 92.0]\n",
            "[140.0, 128.0, 0.0, 128.0]\n",
            "========\n",
            "48 BXP.csv\n",
            "[26.875, 26.125, 26.125, 26.75]\n",
            "[27.0, 26.5, 26.875, 26.625]\n",
            "[26.75, 26.25, 26.75, 26.375]\n",
            "[26.375, 26.125, 26.375, 26.125]\n",
            "[26.25, 26.0625, 26.1875, 26.0625]\n",
            "[26.5625, 26.25, 26.25, 26.4375]\n",
            "[27.0, 26.5625, 26.75, 27.0]\n",
            "[27.125, 26.75, 27.0, 26.875]\n",
            "[27.5, 26.25, 26.8125, 27.25]\n",
            "[27.125, 26.625, 27.125, 26.625]\n",
            "========\n",
            "49 BK.csv\n",
            "[1.4906189441680908, 1.4795769453048706, 1.4795769453048706, 1.4906189441680908]\n",
            "[1.4906189441680908, 1.4795769453048706, 1.4906189441680908, 1.4906189441680908]\n",
            "[1.4906189441680908, 1.4685360193252563, 1.4906189441680908, 1.4906189441680908]\n",
            "[1.4795769453048706, 1.4795769453048706, 1.4795769453048706, 1.4795769453048706]\n",
            "[1.501660943031311, 1.4685360193252563, 1.4795769453048706, 1.501660943031311]\n",
            "[1.501660943031311, 1.501660943031311, 1.501660943031311, 1.501660943031311]\n",
            "[1.501660943031311, 1.4906189441680908, 1.501660943031311, 1.501660943031311]\n",
            "[1.4906189441680908, 1.4795769453048706, 1.4906189441680908, 1.4906189441680908]\n",
            "[1.4906189441680908, 1.4685360193252563, 1.4906189441680908, 1.4795769453048706]\n",
            "[1.501660943031311, 1.4795769453048706, 1.4795769453048706, 1.4795769453048706]\n",
            "========\n",
            "50 BEN.csv\n",
            "[0.0671600028872489, 0.0632100030779838, 0.0, 0.0632100030779838]\n",
            "[0.0661730021238327, 0.0632100030779838, 0.0, 0.0632100030779838]\n",
            "[0.0720990002155304, 0.0691360011696815, 0.0, 0.0691360011696815]\n",
            "[0.0711110010743141, 0.0691360011696815, 0.0, 0.0691360011696815]\n",
            "[0.0681480020284652, 0.0651850029826164, 0.0, 0.0651850029826164]\n",
            "[0.0641980022192001, 0.0612349994480609, 0.0, 0.0612349994480609]\n",
            "[0.0641980022192001, 0.0612349994480609, 0.0, 0.0612349994480609]\n",
            "[0.0681480020284652, 0.0641980022192001, 0.0, 0.0641980022192001]\n",
            "[0.0691360011696815, 0.0651850029826164, 0.0, 0.0651850029826164]\n",
            "[0.0701230019330978, 0.0671600028872489, 0.0, 0.0671600028872489]\n",
            "========\n",
            "51 C.csv\n",
            "[16.236875534057617, 16.13312530517578, 16.13312530517578, 16.236875534057617]\n",
            "[16.28874969482422, 16.184999465942383, 16.236875534057617, 16.28874969482422]\n",
            "[16.28874969482422, 16.13312530517578, 16.28874969482422, 16.184999465942383]\n",
            "[16.13312530517578, 15.92562484741211, 16.13312530517578, 16.081249237060547]\n",
            "[16.13312530517578, 15.92562484741211, 16.081249237060547, 16.13312530517578]\n",
            "[16.340625762939453, 16.13312530517578, 16.13312530517578, 16.28874969482422]\n",
            "[16.392499923706055, 16.28874969482422, 16.28874969482422, 16.340625762939453]\n",
            "[16.340625762939453, 16.13312530517578, 16.340625762939453, 16.340625762939453]\n",
            "[16.548124313354492, 16.340625762939453, 16.340625762939453, 16.444374084472656]\n",
            "[16.548124313354492, 16.392499923706055, 16.444374084472656, 16.49625015258789]\n",
            "========\n",
            "52 BMRA.csv\n",
            "[5.5, 5.25, 5.5, 5.25]\n",
            "[5.5, 5.25, 5.25, 5.25]\n",
            "[5.5, 5.25, 5.25, 5.25]\n",
            "[5.5, 5.5, 5.5, 5.5]\n",
            "[5.5, 5.0, 5.5, 5.0]\n",
            "[5.5, 5.0, 5.0, 5.25]\n",
            "[5.25, 4.75, 5.25, 4.75]\n",
            "[5.0, 4.75, 4.75, 4.75]\n",
            "[5.0, 5.0, 5.0, 5.0]\n",
            "[5.25, 5.25, 5.25, 5.25]\n",
            "========\n",
            "53 BAX.csv\n",
            "[3.885014057159424, 3.8079309463500977, 0.0, 3.854181051254273]\n",
            "[3.885014057159424, 3.838763952255249, 0.0, 3.854181051254273]\n",
            "[3.854181051254273, 3.8233470916748047, 0.0, 3.8233470916748047]\n",
            "[3.8773059844970703, 3.8079309463500977, 0.0, 3.8773059844970703]\n",
            "[4.039181232452393, 3.992930889129639, 0.0, 4.0237650871276855]\n",
            "[4.062306880950928, 4.008347988128662, 0.0, 4.046889781951904]\n",
            "[4.062306880950928, 3.992930889129639, 0.0, 4.000639915466309]\n",
            "[4.008347988128662, 3.946681022644043, 0.0, 3.946681022644043]\n",
            "[3.915848016738892, 3.854181051254273, 0.0, 3.869597911834717]\n",
            "[3.9620978832244873, 3.869597911834717, 0.0, 3.93126392364502]\n",
            "========\n",
            "54 BLK.csv\n",
            "[14.25, 14.0, 14.0, 14.125]\n",
            "[14.125, 14.0, 14.125, 14.0]\n",
            "[14.125, 14.0, 14.0, 14.125]\n",
            "[14.875, 14.125, 14.125, 14.875]\n",
            "[14.875, 14.0625, 14.875, 14.125]\n",
            "[14.1875, 14.0625, 14.1875, 14.0625]\n",
            "[14.125, 14.0, 14.0625, 14.0625]\n",
            "[14.125, 14.0, 14.0625, 14.125]\n",
            "[14.375, 14.0, 14.125, 14.0625]\n",
            "[14.0625, 13.25, 14.0, 13.625]\n",
            "========\n",
            "55 BF-A.csv\n",
            "[0.211324006319046, 0.2091110050678253, 0.2091110050678253, 0.2091110050678253]\n",
            "[0.2091110050678253, 0.2091110050678253, 0.2091110050678253, 0.2091110050678253]\n",
            "[0.211324006319046, 0.211324006319046, 0.211324006319046, 0.211324006319046]\n",
            "[0.211324006319046, 0.211324006319046, 0.211324006319046, 0.211324006319046]\n",
            "[0.211324006319046, 0.211324006319046, 0.211324006319046, 0.211324006319046]\n",
            "[0.211324006319046, 0.211324006319046, 0.211324006319046, 0.211324006319046]\n",
            "[0.211324006319046, 0.2091110050678253, 0.211324006319046, 0.2091110050678253]\n",
            "[0.211324006319046, 0.2091110050678253, 0.2091110050678253, 0.211324006319046]\n",
            "[0.211324006319046, 0.211324006319046, 0.211324006319046, 0.211324006319046]\n",
            "[0.2091110050678253, 0.2024729996919632, 0.2091110050678253, 0.2024729996919632]\n",
            "========\n",
            "56 BDX.csv\n",
            "[2.375, 2.3125, 0.0, 2.320312976837158]\n",
            "[2.398437976837158, 2.328125, 0.0, 2.34375]\n",
            "[2.390625, 2.351562976837158, 0.0, 2.367187976837158]\n",
            "[2.40625, 2.367187976837158, 0.0, 2.398437976837158]\n",
            "[2.414062976837158, 2.304687976837158, 0.0, 2.304687976837158]\n",
            "[2.367187976837158, 2.289062976837158, 0.0, 2.351562976837158]\n",
            "[2.367187976837158, 2.320312976837158, 0.0, 2.320312976837158]\n",
            "[2.382812976837158, 2.265625, 0.0, 2.382812976837158]\n",
            "[2.398437976837158, 2.375, 0.0, 2.382812976837158]\n",
            "[2.421875, 2.390625, 0.0, 2.40625]\n",
            "========\n",
            "57 BR.csv\n",
            "[20.25, 18.0, 18.0, 20.100000381469727]\n",
            "[22.899999618530277, 20.049999237060547, 20.100000381469727, 20.32999992370605]\n",
            "[20.75, 20.149999618530277, 20.32999992370605, 20.670000076293945]\n",
            "[20.82999992370605, 20.299999237060547, 20.75, 20.350000381469727]\n",
            "[20.299999237060547, 19.700000762939453, 20.299999237060547, 19.899999618530277]\n",
            "[20.1299991607666, 19.5, 19.899999618530277, 19.700000762939453]\n",
            "[19.950000762939453, 19.63999938964844, 19.799999237060547, 19.700000762939453]\n",
            "[19.700000762939453, 18.81999969482422, 19.700000762939453, 19.51000022888184]\n",
            "[19.5, 18.489999771118164, 19.479999542236328, 19.06999969482422]\n",
            "[19.15999984741211, 18.799999237060547, 19.020000457763672, 18.90999984741211]\n",
            "========\n",
            "58 BSHI.csv\n",
            "[2.25, 2.25, 2.25, 2.25]\n",
            "[2.25, 2.25, 2.25, 2.25]\n",
            "[2.25, 2.25, 2.25, 2.25]\n",
            "[2.375, 2.375, 2.375, 2.375]\n",
            "[2.375, 2.375, 2.375, 2.375]\n",
            "[2.375, 2.375, 2.375, 2.375]\n",
            "[2.375, 2.375, 2.375, 2.375]\n",
            "[2.5, 2.375, 2.375, 2.5]\n",
            "[2.5, 2.5, 2.5, 2.5]\n",
            "[2.5, 2.5, 2.5, 2.5]\n",
            "========\n",
            "59 CB.csv\n",
            "[9.708333015441896, 9.458333015441896, 9.666666984558104, 9.541666984558104]\n",
            "[9.708333015441896, 9.5, 9.541666984558104, 9.583333015441896]\n",
            "[9.583333015441896, 9.416666984558104, 9.5, 9.458333015441896]\n",
            "[9.458333015441896, 9.333333015441896, 9.416666984558104, 9.458333015441896]\n",
            "[9.708333015441896, 9.416666984558104, 9.416666984558104, 9.666666984558104]\n",
            "[9.666666984558104, 9.541666984558104, 9.625, 9.541666984558104]\n",
            "[9.541666984558104, 9.5, 9.541666984558104, 9.5]\n",
            "[9.458333015441896, 9.375, 9.458333015441896, 9.375]\n",
            "[9.5, 9.416666984558104, 9.416666984558104, 9.5]\n",
            "[9.875, 9.5, 9.5, 9.708333015441896]\n",
            "========\n",
            "60 CAG.csv\n",
            "[0.4935869872570038, 0.4863809943199157, 0.0, 0.4863809943199157]\n",
            "[0.4899840056896209, 0.4827780127525329, 0.0, 0.4827780127525329]\n",
            "[0.4827780127525329, 0.4827780127525329, 0.0, 0.4827780127525329]\n",
            "[0.4827780127525329, 0.4827780127525329, 0.0, 0.4827780127525329]\n",
            "[0.4755730032920837, 0.4611620008945465, 0.0, 0.4611620008945465]\n",
            "[0.4683670103549957, 0.4611620008945465, 0.0, 0.4611620008945465]\n",
            "[0.4611620008945465, 0.4611620008945465, 0.0, 0.4611620008945465]\n",
            "[0.4611620008945465, 0.4539560079574585, 0.0, 0.4539560079574585]\n",
            "[0.4539560079574585, 0.4395450055599212, 0.0, 0.4395450055599212]\n",
            "[0.4575589895248413, 0.4467499852180481, 0.0, 0.4467499852180481]\n",
            "========\n",
            "61 BIIB.csv\n",
            "[3.25, 3.0, 3.0, 3.125]\n",
            "[3.2083330154418945, 3.041666984558105, 3.041666984558105, 3.125]\n",
            "[3.166666984558105, 3.041666984558105, 3.041666984558105, 3.1458330154418945]\n",
            "[3.166666984558105, 3.0833330154418945, 3.166666984558105, 3.0833330154418945]\n",
            "[3.166666984558105, 2.9583330154418945, 3.166666984558105, 2.9583330154418945]\n",
            "[3.0833330154418945, 2.9583330154418945, 2.9583330154418945, 3.0]\n",
            "[3.0833330154418945, 2.9583330154418945, 2.9583330154418945, 3.041666984558105]\n",
            "[3.25, 3.041666984558105, 3.041666984558105, 3.166666984558105]\n",
            "[3.375, 3.166666984558105, 3.25, 3.3333330154418945]\n",
            "[3.3333330154418945, 3.041666984558105, 3.3333330154418945, 3.125]\n",
            "========\n",
            "62 BAC.csv\n",
            "[4.625, 4.625, 4.625, 4.625]\n",
            "[4.640625, 4.640625, 4.640625, 4.640625]\n",
            "[4.625, 4.625, 4.625, 4.625]\n",
            "[4.625, 4.625, 4.625, 4.625]\n",
            "[4.625, 4.625, 4.625, 4.625]\n",
            "[4.625, 4.625, 4.625, 4.625]\n",
            "[4.65625, 4.65625, 4.65625, 4.65625]\n",
            "[4.671875, 4.671875, 4.671875, 4.671875]\n",
            "[4.734375, 4.734375, 4.734375, 4.734375]\n",
            "[4.875, 4.875, 4.875, 4.875]\n",
            "========\n",
            "63 BMY.csv\n",
            "[1.87752902507782, 1.8589390516281128, 0.0, 1.8663749694824217]\n",
            "[1.8924000263214111, 1.8663749694824217, 1.8663749694824217, 1.884964942932129]\n",
            "[1.8961180448532104, 1.8812470436096191, 1.884964942932129, 1.888682007789612]\n",
            "[1.89983594417572, 1.87752902507782, 1.888682007789612, 1.87752902507782]\n",
            "[1.884964942932129, 1.8663749694824217, 1.87752902507782, 1.8738110065460205]\n",
            "[1.884964942932129, 1.8626569509506223, 1.8738110065460205, 1.884964942932129]\n",
            "[1.8738110065460205, 1.8626569509506223, 1.8738110065460205, 1.8626569509506223]\n",
            "[1.8812470436096191, 1.8552219867706297, 1.8626569509506223, 1.8663749694824217]\n",
            "[1.8812470436096191, 1.8552219867706297, 1.8663749694824217, 1.8700929880142207]\n",
            "[1.8663749694824217, 1.829195976257324, 1.8663749694824217, 1.8440680503845213]\n",
            "========\n",
            "64 CCI.csv\n",
            "[13.125, 12.875, 12.875, 13.0]\n",
            "[13.25, 13.0, 13.0, 13.0]\n",
            "[13.25, 13.0, 13.125, 13.0]\n",
            "[13.0, 13.0, 13.0, 13.0]\n",
            "[13.0, 13.0, 13.0, 13.0]\n",
            "[13.0, 12.875, 13.0, 13.0]\n",
            "[13.0, 12.125, 13.0, 12.25]\n",
            "[12.375, 10.75, 12.25, 11.125]\n",
            "[11.375, 10.0, 11.375, 10.0]\n",
            "[10.125, 7.625, 10.0, 8.125]\n",
            "========\n",
            "65 BSX.csv\n",
            "[4.34375, 4.21875, 4.25, 4.28125]\n",
            "[4.375, 4.3125, 4.34375, 4.375]\n",
            "[4.375, 4.3125, 4.375, 4.375]\n",
            "[4.59375, 4.375, 4.40625, 4.53125]\n",
            "[4.53125, 4.375, 4.5, 4.375]\n",
            "[4.46875, 4.40625, 4.40625, 4.4375]\n",
            "[4.5, 4.4375, 4.46875, 4.46875]\n",
            "[4.59375, 4.46875, 4.5, 4.5625]\n",
            "[4.65625, 4.5625, 4.5625, 4.625]\n",
            "[4.6875, 4.5625, 4.6875, 4.65625]\n",
            "========\n",
            "66 CAT.csv\n",
            "[3.541666984558105, 3.447916984558105, 3.541666984558105, 3.4583330154418945]\n",
            "[3.4895830154418945, 3.4375, 3.4583330154418945, 3.447916984558105]\n",
            "[3.447916984558105, 3.34375, 3.447916984558105, 3.3645830154418945]\n",
            "[3.3333330154418945, 3.2395830154418945, 3.3333330154418945, 3.260416984558105]\n",
            "[3.3333330154418945, 3.25, 3.260416984558105, 3.2708330154418945]\n",
            "[3.2708330154418945, 3.2083330154418945, 3.2708330154418945, 3.2083330154418945]\n",
            "[3.2395830154418945, 3.1458330154418945, 3.2083330154418945, 3.1875]\n",
            "[3.2395830154418945, 3.197916984558105, 3.197916984558105, 3.229166984558105]\n",
            "[3.260416984558105, 3.1770830154418945, 3.229166984558105, 3.260416984558105]\n",
            "[3.260416984558105, 3.21875, 3.260416984558105, 3.25]\n",
            "========\n",
            "67 BRK-A.csv\n",
            "[310.0, 290.0, 290.0, 290.0]\n",
            "[290.0, 290.0, 290.0, 290.0]\n",
            "[310.0, 290.0, 290.0, 290.0]\n",
            "[290.0, 290.0, 290.0, 290.0]\n",
            "[290.0, 290.0, 290.0, 290.0]\n",
            "[290.0, 270.0, 290.0, 270.0]\n",
            "[270.0, 270.0, 270.0, 270.0]\n",
            "[270.0, 270.0, 270.0, 270.0]\n",
            "[270.0, 270.0, 270.0, 270.0]\n",
            "[270.0, 270.0, 270.0, 270.0]\n",
            "========\n",
            "68 BBY.csv\n",
            "[0.1697530001401901, 0.1666669994592666, 0.0, 0.1666669994592666]\n",
            "[0.1697530001401901, 0.1682099997997284, 0.1682099997997284, 0.1682099997997284]\n",
            "[0.1666669994592666, 0.1666669994592666, 0.1666669994592666, 0.1666669994592666]\n",
            "[0.1666669994592666, 0.1635800004005432, 0.1666669994592666, 0.1635800004005432]\n",
            "[0.1651230007410049, 0.1635800004005432, 0.1635800004005432, 0.1635800004005432]\n",
            "[0.1574070006608963, 0.1558640003204345, 0.1574070006608963, 0.1558640003204345]\n",
            "[0.1558640003204345, 0.1527779996395111, 0.1558640003204345, 0.1527779996395111]\n",
            "[0.1512349992990493, 0.1481480002403259, 0.1512349992990493, 0.1481480002403259]\n",
            "[0.1527779996395111, 0.1481480002403259, 0.1481480002403259, 0.1481480002403259]\n",
            "[0.1435189992189407, 0.1404320001602172, 0.1435189992189407, 0.1404320001602172]\n",
            "========\n",
            "69 BA.csv\n",
            "[0.979423999786377, 0.9259260296821594, 0.9259260296821594, 0.979423999786377]\n",
            "[1.0246909856796265, 1.0082299709320068, 1.0082299709320068, 1.0164610147476196]\n",
            "[1.0493830442428589, 1.0246909856796265, 1.0246909856796265, 1.0288070440292358]\n",
            "[1.0452669858932495, 1.020576000213623, 1.0288070440292358, 1.0329220294952393]\n",
            "[1.037037014961243, 1.0082299709320068, 1.0329220294952393, 1.020576000213623]\n",
            "[1.0329220294952393, 0.9958850145339966, 1.020576000213623, 0.9958850145339966]\n",
            "[1.0, 0.979423999786377, 0.9958850145339966, 0.9876539707183838]\n",
            "[0.9835389852523804, 0.9341560006141664, 0.9835389852523804, 0.9382719993591307]\n",
            "[0.9341560006141664, 0.9135800004005432, 0.9341560006141664, 0.9300410151481628]\n",
            "[0.9259260296821594, 0.9053500294685364, 0.9259260296821594, 0.9094650149345398]\n",
            "========\n",
            "70 BWA.csv\n",
            "[3.1875, 3.125, 3.171875, 3.125]\n",
            "[3.140625, 3.03125, 3.140625, 3.09375]\n",
            "[3.09375, 3.046875, 3.0625, 3.046875]\n",
            "[3.125, 3.046875, 3.046875, 3.09375]\n",
            "[3.15625, 3.0625, 3.078125, 3.15625]\n",
            "[3.203125, 3.140625, 3.15625, 3.1875]\n",
            "[3.171875, 3.140625, 3.171875, 3.140625]\n",
            "[3.15625, 3.125, 3.15625, 3.125]\n",
            "[3.15625, 3.125, 3.125, 3.15625]\n",
            "[3.1875, 3.15625, 3.15625, 3.15625]\n",
            "========\n",
            "71 CME.csv\n",
            "[8.640000343322754, 7.78000020980835, 7.800000190734863, 8.579999923706055]\n",
            "[8.5600004196167, 8.227999687194824, 8.539999961853027, 8.404000282287598]\n",
            "[8.65999984741211, 8.329999923706055, 8.40999984741211, 8.550000190734863]\n",
            "[9.100000381469728, 8.460000038146973, 8.539999961853027, 9.01200008392334]\n",
            "[9.01200008392334, 8.720000267028809, 9.01200008392334, 8.781999588012695]\n",
            "[8.793999671936035, 8.649999618530273, 8.781999588012695, 8.729999542236328]\n",
            "[8.75, 8.59000015258789, 8.699999809265137, 8.619999885559082]\n",
            "[8.800000190734863, 8.5600004196167, 8.600000381469727, 8.654000282287598]\n",
            "[8.640000343322754, 8.510000228881836, 8.640000343322754, 8.534000396728516]\n",
            "[8.539999961853027, 8.434000015258789, 8.510000228881836, 8.460000038146973]\n",
            "========\n",
            "72 CNWT.csv\n",
            "[6.25, 3.75, 5.630000114440918, 5.630000114440918]\n",
            "[3.5, 2.809999942779541, 3.130000114440918, 3.130000114440918]\n",
            "[9.380000114440918, 8.130000114440918, 8.130000114440918, 8.130000114440918]\n",
            "[8.100000381469727, 8.100000381469727, 8.100000381469727, 8.100000381469727]\n",
            "[8.100000381469727, 8.100000381469727, 8.100000381469727, 8.100000381469727]\n",
            "[8.100000381469727, 8.100000381469727, 8.100000381469727, 8.100000381469727]\n",
            "[5.0, 4.380000114440918, 5.0, 5.0]\n",
            "[5.0, 5.0, 5.0, 5.0]\n",
            "[8.75, 4.5, 8.75, 8.75]\n",
            "[8.75, 8.75, 8.75, 8.75]\n",
            "========\n",
            "73 CF.csv\n",
            "[3.309999942779541, 3.200000047683716, 3.25, 3.240000009536743]\n",
            "[3.5999999046325684, 3.25, 3.25, 3.5859999656677246]\n",
            "[3.569999933242798, 3.45199990272522, 3.549999952316284, 3.4600000381469727]\n",
            "[3.4600000381469727, 3.380000114440918, 3.380000114440918, 3.430000066757202]\n",
            "[3.440000057220459, 3.359999895095825, 3.430000066757202, 3.369999885559082]\n",
            "[3.3959999084472656, 3.25, 3.390000104904175, 3.2699999809265137]\n",
            "[3.338000059127808, 3.25, 3.2980000972747803, 3.319999933242798]\n",
            "[3.4040000438690186, 3.282000064849853, 3.3299999237060547, 3.400000095367432]\n",
            "[3.588000059127808, 3.4600000381469727, 3.5, 3.51200008392334]\n",
            "[3.549999952316284, 3.4660000801086426, 3.5, 3.5]\n",
            "========\n",
            "74 CTXS.csv\n",
            "[2.023941993713379, 1.692147970199585, 1.725327968597412, 1.990762948989868]\n",
            "[2.322556972503662, 2.057121992111206, 2.065416097640991, 2.1898388862609863]\n",
            "[2.305967092514038, 2.1234800815582275, 2.1732490062713623, 2.1234800815582275]\n",
            "[2.140069961547852, 2.0737109184265137, 2.090301036834717, 2.0820059776306152]\n",
            "[2.106890916824341, 1.7750970125198364, 2.057121992111206, 1.9409940242767332]\n",
            "[1.9409940242767332, 1.8746349811553955, 1.9078140258789065, 1.8746349811553955]\n",
            "[1.8746349811553955, 1.6423790454864502, 1.858044981956482, 1.6755590438842771]\n",
            "[1.8248660564422607, 1.6589690446853638, 1.6589690446853638, 1.8082760572433472]\n",
            "[1.8248660564422607, 1.7750970125198364, 1.8248660564422607, 1.79168701171875]\n",
            "[1.8248660564422607, 1.7750970125198364, 1.7750970125198364, 1.8248660564422607]\n",
            "========\n",
            "75 D.csv\n",
            "[3.291666984558105, 3.166666984558105, 0.0, 3.166666984558105]\n",
            "[3.291666984558105, 3.166666984558105, 0.0, 3.25]\n",
            "[3.291666984558105, 3.25, 0.0, 3.25]\n",
            "[3.3333330154418945, 3.25, 0.0, 3.291666984558105]\n",
            "[3.3333330154418945, 3.25, 0.0, 3.291666984558105]\n",
            "[3.291666984558105, 3.2083330154418945, 0.0, 3.2083330154418945]\n",
            "[3.25, 3.166666984558105, 0.0, 3.2083330154418945]\n",
            "[3.291666984558105, 3.2083330154418945, 0.0, 3.25]\n",
            "[3.2083330154418945, 3.0833330154418945, 0.0, 3.125]\n",
            "[3.2083330154418945, 3.125, 0.0, 3.166666984558105]\n",
            "========\n",
            "76 CTSH.csv\n",
            "[0.21875, 0.1979169994592666, 0.2083330005407333, 0.2083330005407333]\n",
            "[0.2109380066394806, 0.205728992819786, 0.2070309966802597, 0.2083330005407333]\n",
            "[0.2109380066394806, 0.2005210071802139, 0.2076819986104965, 0.2083330005407333]\n",
            "[0.2135419994592666, 0.205728992819786, 0.2083330005407333, 0.2135419994592666]\n",
            "[0.2447919994592666, 0.2174479961395263, 0.21875, 0.234375]\n",
            "[0.265625, 0.2330729961395263, 0.236978992819786, 0.2526040077209472]\n",
            "[0.2682290077209472, 0.2526040077209472, 0.2526040077209472, 0.2591150104999542]\n",
            "[0.2643229961395263, 0.2473960071802139, 0.2643229961395263, 0.2513020038604736]\n",
            "[0.2526040077209472, 0.2473960071802139, 0.25, 0.2473960071802139]\n",
            "[0.2526040077209472, 0.2473960071802139, 0.25, 0.25]\n",
            "========\n",
            "77 CHD.csv\n",
            "[0.1913580000400543, 0.1728399991989135, 0.0, 0.1728399991989135]\n",
            "[0.1882719993591308, 0.1697530001401901, 0.0, 0.1697530001401901]\n",
            "[0.1697530001401901, 0.1697530001401901, 0.1697530001401901, 0.1697530001401901]\n",
            "[0.1697530001401901, 0.1697530001401901, 0.1697530001401901, 0.1697530001401901]\n",
            "[0.1882719993591308, 0.1697530001401901, 0.0, 0.1697530001401901]\n",
            "[0.1882719993591308, 0.1697530001401901, 0.0, 0.1697530001401901]\n",
            "[0.1697530001401901, 0.1697530001401901, 0.1697530001401901, 0.1697530001401901]\n",
            "[0.1882719993591308, 0.1697530001401901, 0.0, 0.1697530001401901]\n",
            "[0.1851850003004074, 0.1666669994592666, 0.0, 0.1666669994592666]\n",
            "[0.1882719993591308, 0.1697530001401901, 0.0, 0.1697530001401901]\n",
            "========\n",
            "78 CFG.csv\n",
            "[23.21999931335449, 21.350000381469727, 21.5, 23.07999992370605]\n",
            "[23.40999984741211, 22.450000762939453, 22.559999465942383, 23.049999237060547]\n",
            "[23.3700008392334, 22.90999984741211, 23.18000030517578, 23.25]\n",
            "[23.450000762939453, 22.90999984741211, 23.13999938964844, 23.229999542236328]\n",
            "[23.56999969482422, 23.049999237060547, 23.11000061035156, 23.420000076293945]\n",
            "[23.61000061035156, 23.11000061035156, 23.21999931335449, 23.440000534057617]\n",
            "[23.59000015258789, 23.30299949645996, 23.34000015258789, 23.34000015258789]\n",
            "[23.46999931335449, 23.01000022888184, 23.3799991607666, 23.229999542236328]\n",
            "[23.3700008392334, 22.61000061035156, 23.280000686645508, 22.63999938964844]\n",
            "[22.739999771118164, 22.26000022888184, 22.559999465942383, 22.520000457763672]\n",
            "========\n",
            "79 DFS.csv\n",
            "[30.5, 28.0, 28.0, 28.75]\n",
            "[29.799999237060547, 28.8799991607666, 29.5, 29.07999992370605]\n",
            "[30.200000762939453, 29.299999237060547, 29.31999969482422, 29.93000030517578]\n",
            "[30.299999237060547, 29.600000381469727, 30.0, 29.700000762939453]\n",
            "[30.100000381469727, 29.56999969482422, 30.040000915527344, 29.809999465942383]\n",
            "[31.950000762939453, 29.61000061035156, 30.059999465942383, 31.209999084472656]\n",
            "[32.16999816894531, 31.0, 31.5, 31.700000762939453]\n",
            "[32.0, 31.01000022888184, 31.5, 31.299999237060547]\n",
            "[31.649999618530277, 30.25, 31.299999237060547, 30.40999984741211]\n",
            "[30.799999237060547, 29.56999969482422, 30.600000381469727, 29.57999992370605]\n",
            "========\n",
            "80 CPICQ.csv\n",
            "[3.5, 3.4375, 3.4375, 3.4375]\n",
            "[3.5, 3.375, 3.375, 3.375]\n",
            "[3.5625, 3.5, 3.5, 3.5]\n",
            "[3.5625, 3.5, 3.5, 3.5]\n",
            "[3.5625, 3.4375, 3.4375, 3.4375]\n",
            "[3.5, 3.4375, 3.4375, 3.4375]\n",
            "[3.5, 3.4375, 3.4375, 3.4375]\n",
            "[3.5, 3.4375, 3.4375, 3.4375]\n",
            "[3.625, 3.4375, 3.4375, 3.4375]\n",
            "[3.625, 3.5, 3.5, 3.5]\n",
            "========\n",
            "81 DG.csv\n",
            "[23.100000381469727, 21.75, 22.0, 22.729999542236328]\n",
            "[23.100000381469727, 22.1299991607666, 22.899999618530277, 23.100000381469727]\n",
            "[23.170000076293945, 22.5, 23.030000686645508, 23.040000915527344]\n",
            "[23.020000457763672, 22.5, 23.0, 22.549999237060547]\n",
            "[23.13999938964844, 22.270000457763672, 22.270000457763672, 22.8700008392334]\n",
            "[23.32999992370605, 22.5, 22.5, 23.209999084472656]\n",
            "[23.979999542236328, 23.18000030517578, 23.700000762939453, 23.399999618530277]\n",
            "[23.649999618530277, 23.25, 23.600000381469727, 23.5]\n",
            "[23.799999237060547, 22.989999771118164, 23.0, 23.739999771118164]\n",
            "[23.299999237060547, 22.739999771118164, 22.739999771118164, 23.149999618530277]\n",
            "========\n",
            "82 CRM.csv\n",
            "[4.324999809265137, 3.6875, 3.75, 4.300000190734863]\n",
            "[4.422500133514404, 4.125, 4.387499809265137, 4.190000057220459]\n",
            "[4.1875, 3.947499990463257, 4.127500057220459, 3.950000047683716]\n",
            "[4.052499771118164, 3.859999895095825, 4.0, 4.0]\n",
            "[4.175000190734863, 3.9574999809265137, 4.0, 4.099999904632568]\n",
            "[4.235000133514404, 4.007500171661377, 4.099999904632568, 4.017499923706055]\n",
            "[4.050000190734863, 3.974999904632568, 4.019999980926514, 4.007500171661377]\n",
            "[4.244999885559082, 4.012499809265137, 4.025000095367432, 4.244999885559082]\n",
            "[4.25, 4.119999885559082, 4.232500076293945, 4.25]\n",
            "[4.224999904632568, 4.012499809265137, 4.224999904632568, 4.077499866485596]\n",
            "========\n",
            "83 CHRW.csv\n",
            "[5.8125, 5.5625, 5.8125, 5.6875]\n",
            "[5.75, 5.40625, 5.65625, 5.59375]\n",
            "[5.625, 5.46875, 5.5, 5.53125]\n",
            "[5.6875, 5.5, 5.5625, 5.59375]\n",
            "[5.75, 5.59375, 5.6875, 5.71875]\n",
            "[5.6875, 5.53125, 5.6875, 5.59375]\n",
            "[5.71875, 5.5625, 5.65625, 5.71875]\n",
            "[5.75, 5.375, 5.65625, 5.421875]\n",
            "[5.6875, 4.9375, 5.25, 5.59375]\n",
            "[5.71875, 5.59375, 5.6875, 5.65625]\n",
            "========\n",
            "84 CLX.csv\n",
            "[5.65625, 5.453125, 0.0, 5.53125]\n",
            "[5.515625, 5.4375, 0.0, 5.4375]\n",
            "[5.5, 5.40625, 0.0, 5.46875]\n",
            "[5.421875, 5.265625, 0.0, 5.375]\n",
            "[5.40625, 4.84375, 0.0, 4.859375]\n",
            "[5.03125, 4.75, 0.0, 5.0]\n",
            "[5.1875, 4.8125, 0.0, 4.8125]\n",
            "[4.90625, 4.5625, 0.0, 4.90625]\n",
            "[5.09375, 4.90625, 0.0, 5.078125]\n",
            "[5.21875, 5.09375, 0.0, 5.21875]\n",
            "========\n",
            "85 DGX.csv\n",
            "[3.75, 3.5, 3.75, 3.5]\n",
            "[3.59375, 3.3125, 3.5, 3.5625]\n",
            "[3.71875, 3.59375, 3.59375, 3.65625]\n",
            "[3.71875, 3.625, 3.625, 3.71875]\n",
            "[3.84375, 3.71875, 3.75, 3.8125]\n",
            "[3.90625, 3.84375, 3.84375, 3.90625]\n",
            "[3.9375, 3.8125, 3.90625, 3.8125]\n",
            "[3.8125, 3.8125, 3.8125, 3.8125]\n",
            "[3.84375, 3.78125, 3.84375, 3.78125]\n",
            "[3.78125, 3.75, 3.75, 3.78125]\n",
            "========\n",
            "86 CPB.csv\n",
            "[2.0625, 2.007812976837158, 0.0, 2.0625]\n",
            "[2.101562976837158, 2.0625, 0.0, 2.09375]\n",
            "[2.09375, 2.078125, 0.0, 2.09375]\n",
            "[2.109375, 2.078125, 0.0, 2.078125]\n",
            "[2.09375, 2.078125, 0.0, 2.078125]\n",
            "[2.085937976837158, 2.078125, 0.0, 2.078125]\n",
            "[2.09375, 2.03125, 0.0, 2.03125]\n",
            "[2.015625, 1.96875, 0.0, 2.0]\n",
            "[2.0, 1.921875, 0.0, 1.9375]\n",
            "[1.9765629768371584, 1.9375, 0.0, 1.9765629768371584]\n",
            "========\n",
            "87 COTY.csv\n",
            "[17.520000457763672, 16.899999618530273, 17.5, 17.360000610351562]\n",
            "[17.549999237060547, 17.200000762939453, 17.450000762939453, 17.520000457763672]\n",
            "[17.649999618530273, 17.309999465942383, 17.610000610351562, 17.389999389648438]\n",
            "[17.420000076293945, 17.15999984741211, 17.389999389648438, 17.299999237060547]\n",
            "[17.350000381469727, 16.399999618530273, 17.25, 17.190000534057617]\n",
            "[16.850000381469727, 16.309999465942383, 16.799999237060547, 16.75]\n",
            "[17.06999969482422, 16.5, 16.75, 16.989999771118164]\n",
            "[16.81999969482422, 16.459999084472656, 16.799999237060547, 16.610000610351562]\n",
            "[16.8799991607666, 16.549999237060547, 16.700000762939453, 16.75]\n",
            "[17.139999389648438, 16.799999237060547, 16.799999237060547, 17.020000457763672]\n",
            "========\n",
            "88 CHTR.csv\n",
            "[35.0, 35.0, 35.0, 35.0]\n",
            "[35.0, 35.0, 35.0, 35.0]\n",
            "[35.5, 35.0, 35.25, 35.0]\n",
            "[34.54999923706055, 33.5, 34.54999923706055, 33.5]\n",
            "[33.5, 33.5, 33.5, 33.5]\n",
            "[33.54999923706055, 33.5, 33.5, 33.5]\n",
            "[33.0099983215332, 32.5, 33.0099983215332, 32.75]\n",
            "[32.75, 32.75, 32.75, 32.75]\n",
            "[32.5, 32.25, 32.5, 32.25]\n",
            "[32.25, 31.75, 32.25, 32.0]\n",
            "========\n",
            "89 COP.csv\n",
            "[5.256807804107666, 5.129755020141602, 5.161518096923828, 5.145637035369873]\n",
            "[5.129755020141602, 5.050346851348877, 5.097991943359375, 5.066228866577148]\n",
            "[5.034465789794922, 4.843885898590088, 4.939176082611084, 4.875649929046631]\n",
            "[5.002702236175537, 4.685070991516113, 4.843885898590088, 4.780360221862793]\n",
            "[4.780360221862793, 4.685070991516113, 4.732715129852295, 4.732715129852295]\n",
            "[4.780360221862793, 4.637425899505615, 4.71683406829834, 4.637425899505615]\n",
            "[4.65330696105957, 4.415082931518555, 4.542136192321777, 4.446846961975098]\n",
            "[4.494491100311279, 4.303912162780762, 4.3992018699646, 4.367438793182373]\n",
            "[4.478610038757324, 4.33567476272583, 4.415082931518555, 4.351556777954102]\n",
            "[4.478610038757324, 4.288031101226807, 4.383319854736328, 4.478610038757324]\n",
            "========\n",
            "90 CNC.csv\n",
            "[1.475000023841858, 1.189167022705078, 1.189167022705078, 1.4333330392837524]\n",
            "[1.4408329725265503, 1.3708330392837524, 1.4341670274734497, 1.3916670083999634]\n",
            "[1.4249999523162842, 1.3833329677581787, 1.3916670083999634, 1.4149999618530271]\n",
            "[1.4474999904632568, 1.3916670083999634, 1.4083329439163208, 1.4383330345153809]\n",
            "[1.774999976158142, 1.4333330392837524, 1.4375, 1.7258330583572388]\n",
            "[1.899999976158142, 1.6458330154418943, 1.7000000476837158, 1.6541670560836792]\n",
            "[1.6749999523162842, 1.6541670560836792, 1.6741670370101929, 1.6583329439163208]\n",
            "[1.6875, 1.634166955947876, 1.661666989326477, 1.662500023841858]\n",
            "[1.7166670560836792, 1.6516669988632202, 1.6583329439163208, 1.6933330297470093]\n",
            "[1.740833044052124, 1.690000057220459, 1.7083330154418943, 1.7333329916000366]\n",
            "========\n",
            "91 CNP.csv\n",
            "[11.204874992370604, 10.994125366210938, 11.099499702453612, 11.169750213623049]\n",
            "[11.345375061035156, 11.169750213623049, 11.169750213623049, 11.345375061035156]\n",
            "[11.310250282287598, 11.029250144958496, 11.310250282287598, 11.134625434875488]\n",
            "[11.099499702453612, 10.713125228881836, 11.099499702453612, 10.958999633789062]\n",
            "[10.958999633789062, 10.853625297546388, 10.958999633789062, 10.958999633789062]\n",
            "[10.958999633789062, 10.888750076293944, 10.958999633789062, 10.888750076293944]\n",
            "[11.134625434875488, 11.029250144958496, 11.029250144958496, 11.134625434875488]\n",
            "[11.169750213623049, 11.064374923706056, 11.134625434875488, 11.099499702453612]\n",
            "[11.099499702453612, 10.958999633789062, 11.099499702453612, 10.958999633789062]\n",
            "[11.099499702453612, 10.818499565124512, 10.958999633789062, 10.818499565124512]\n",
            "========\n",
            "92 DE.csv\n",
            "[2.682291984558105, 2.6458330154418945, 0.0, 2.6770830154418945]\n",
            "[2.734375, 2.6770830154418945, 2.6770830154418945, 2.729166984558105]\n",
            "[2.7239580154418945, 2.6875, 2.7239580154418945, 2.71875]\n",
            "[2.729166984558105, 2.6875, 2.71875, 2.6927080154418945]\n",
            "[2.6770830154418945, 2.65625, 2.6770830154418945, 2.6614580154418945]\n",
            "[2.697916984558105, 2.671875, 2.671875, 2.6875]\n",
            "[2.682291984558105, 2.666666984558105, 2.682291984558105, 2.671875]\n",
            "[2.682291984558105, 2.625, 2.671875, 2.625]\n",
            "[2.59375, 2.5208330154418945, 2.59375, 2.59375]\n",
            "[2.666666984558105, 2.609375, 2.609375, 2.635416984558105]\n",
            "========\n",
            "93 COO.csv\n",
            "[32.25, 31.5, 0.0, 31.5]\n",
            "[33.75, 33.5625, 0.0, 33.5625]\n",
            "[35.625, 35.4375, 0.0, 35.4375]\n",
            "[35.625, 35.4375, 0.0, 35.4375]\n",
            "[37.5, 37.3125, 0.0, 37.3125]\n",
            "[36.9375, 36.75, 0.0, 36.75]\n",
            "[36.5625, 36.1875, 0.0, 36.1875]\n",
            "[35.25, 35.0625, 0.0, 35.0625]\n",
            "[35.0625, 34.875, 0.0, 34.875]\n",
            "[35.4375, 35.25, 0.0, 35.25]\n",
            "========\n",
            "94 CUK.csv\n",
            "[14.987509727478027, 14.467110633850098, 14.467110633850098, 14.987509727478027]\n",
            "[14.363031387329102, 13.738551139831545, 14.363031387329102, 13.842631340026855]\n",
            "[13.582430839538574, 13.32223129272461, 13.32223129272461, 13.582430839538574]\n",
            "[13.166111946105955, 13.062031745910645, 13.1140718460083, 13.1140718460083]\n",
            "[13.009991645812988, 12.853872299194336, 12.853872299194336, 13.009991645812988]\n",
            "[13.062031745910645, 12.905912399291992, 13.009991645812988, 12.957951545715332]\n",
            "[13.166111946105955, 13.009991645812988, 13.009991645812988, 13.062031745910645]\n",
            "[13.009991645812988, 12.853872299194336, 12.957951545715332, 13.009991645812988]\n",
            "[13.009991645812988, 12.593671798706056, 12.905912399291992, 12.593671798706056]\n",
            "[12.905912399291992, 12.749792098999023, 12.80183219909668, 12.80183219909668]\n",
            "========\n",
            "95 CPRT.csv\n",
            "[0.6197919845581055, 0.5416669845581055, 0.5625, 0.609375]\n",
            "[0.6354169845581055, 0.5989580154418945, 0.625, 0.6354169845581055]\n",
            "[0.65625, 0.6197919845581055, 0.6354169845581055, 0.6354169845581055]\n",
            "[0.6666669845581055, 0.6354169845581055, 0.65625, 0.6354169845581055]\n",
            "[0.6458330154418945, 0.625, 0.6458330154418945, 0.6354169845581055]\n",
            "[0.640625, 0.625, 0.640625, 0.640625]\n",
            "[0.6666669845581055, 0.625, 0.640625, 0.6666669845581055]\n",
            "[0.6666669845581055, 0.625, 0.6666669845581055, 0.6302080154418945]\n",
            "[0.6354169845581055, 0.59375, 0.6354169845581055, 0.59375]\n",
            "[0.6145830154418945, 0.59375, 0.59375, 0.6041669845581055]\n",
            "========\n",
            "96 COST.csv\n",
            "[11.0625, 10.5625, 10.6875, 11.0]\n",
            "[11.125, 10.6875, 11.0625, 10.9375]\n",
            "[10.9375, 10.5625, 10.875, 10.5625]\n",
            "[10.625, 10.125, 10.625, 10.25]\n",
            "[10.1875, 9.3125, 10.1875, 9.9375]\n",
            "[10.5, 9.9375, 9.9375, 10.125]\n",
            "[10.25, 10.0, 10.125, 10.1875]\n",
            "[10.34375, 10.0, 10.1875, 10.25]\n",
            "[10.3125, 10.1875, 10.25, 10.25]\n",
            "[10.28125, 10.1875, 10.25, 10.1875]\n",
            "========\n",
            "97 CINF.csv\n",
            "[1.371170997619629, 1.325464963912964, 0.0, 1.325464963912964]\n",
            "[1.371170997619629, 1.325464963912964, 0.0, 1.325464963912964]\n",
            "[1.371170997619629, 1.325464963912964, 0.0, 1.325464963912964]\n",
            "[1.371170997619629, 1.325464963912964, 0.0, 1.325464963912964]\n",
            "[1.371170997619629, 1.325464963912964, 0.0, 1.325464963912964]\n",
            "[1.3026119470596311, 1.2569069862365725, 0.0, 1.2569069862365725]\n",
            "[1.2797590494155884, 1.2340539693832395, 0.0, 1.2340539693832395]\n",
            "[1.2797590494155884, 1.2340539693832395, 0.0, 1.2340539693832395]\n",
            "[1.2340539693832395, 1.1883480548858645, 0.0, 1.1883480548858645]\n",
            "[1.1197890043258667, 1.0740840435028076, 0.0, 1.0740840435028076]\n",
            "========\n",
            "98 CMG.csv\n",
            "[48.27999877929688, 39.5099983215332, 45.0, 44.0]\n",
            "[44.900001525878906, 41.75, 44.880001068115234, 42.20000076293945]\n",
            "[45.5099983215332, 41.25, 42.20000076293945, 44.79999923706055]\n",
            "[49.29999923706055, 45.380001068115234, 45.5, 47.52000045776367]\n",
            "[49.2400016784668, 45.25, 48.900001525878906, 46.560001373291016]\n",
            "[47.90999984741211, 44.2599983215332, 47.900001525878906, 44.75]\n",
            "[46.560001373291016, 44.25, 44.84999847412109, 46.54999923706055]\n",
            "[48.0, 46.0, 47.47999954223633, 47.709999084472656]\n",
            "[47.75, 45.25, 47.709999084472656, 46.27999877929688]\n",
            "[46.59999847412109, 45.040000915527344, 45.84999847412109, 45.7400016784668]\n",
            "========\n",
            "99 CL.csv\n",
            "[2.0, 1.9661459922790527, 0.0, 2.0]\n",
            "[2.0130209922790527, 1.953125, 0.0, 2.0052080154418945]\n",
            "[2.0286459922790527, 1.9895830154418943, 0.0, 2.0]\n",
            "[2.015625, 1.984375, 0.0, 1.9895830154418943]\n",
            "[1.9895830154418943, 1.9479169845581052, 0.0, 1.9869790077209477]\n",
            "[2.0, 1.9791669845581052, 0.0, 1.9791669845581052]\n",
            "[1.9895830154418943, 1.9713540077209477, 0.0, 1.9791669845581052]\n",
            "[1.9895830154418943, 1.9739580154418943, 0.0, 1.9791669845581052]\n",
            "[1.984375, 1.9505209922790527, 0.0, 1.9505209922790527]\n",
            "[1.96875, 1.9427080154418943, 0.0, 1.9583330154418943]\n",
            "========\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def get_pd_col_data(pd_frame0,col_names0):\n",
        "  all_col_data=[]\n",
        "  for index0,row_dict0 in pd_frame0.iterrows():\n",
        "    cur_list=[row_dict0.get(v,0.) for v in col_names0]\n",
        "    valid_row=True\n",
        "    for cl in cur_list: \n",
        "      if not type(cl) is float: valid_row=False\n",
        "    if not valid_row: continue\n",
        "    all_col_data.append(cur_list)\n",
        "  return all_col_data\n",
        "\n",
        "root_dir='stock_market_data/sp500/csv'\n",
        "files=['ABC', 'ADP', 'A', 'ABT', 'ABMD', 'ADI', 'ABBV', 'AAPL', 'ADSK', 'ADM', 'ACN', 'AAP', 'AAL', 'ALGN', 'APH', 'AOS', 'AWK', 'ALLE', 'AME', 'APD', 'ARE', 'AIZ', 'ALB', 'APA', 'ALK', 'AEE', 'AMGN', 'ANTM', 'AEP', 'AON', 'AKAM', 'AXP', 'AMD', 'AMAT', 'AMP', 'ANET', 'AJG', 'AZO', 'ATVI', 'AMZN', 'AMT', 'AVB', 'ALTR', 'AVY', 'CAH', 'CDNS', 'BIO', 'CDE', 'BXP', 'BK', 'BEN', 'C', 'BMRA', 'BAX', 'BLK', 'BF-A', 'BDX', 'BR', 'BSHI', 'CB', 'CAG', 'BIIB', 'BAC', 'BMY', 'CCI', 'BSX', 'CAT', 'BRK-A', 'BBY', 'BA', 'BWA', 'CME', 'CNWT', 'CF', 'CTXS', 'D', 'CTSH', 'CHD', 'CFG', 'DFS', 'CPICQ', 'DG', 'CRM', 'CHRW', 'CLX', 'DGX', 'CPB', 'COTY', 'CHTR', 'COP', 'CNC', 'CNP', 'DE', 'COO', 'CUK', 'CPRT', 'COST', 'CINF', 'CMG', 'CL', 'CTQ', 'CTAS', 'CMI', 'CSCO', 'COWN', 'DAL', 'DTE', 'ENS', 'EQIX', 'DRE', 'DOV', 'DHI', 'EW', 'ES', 'EQR', 'DIS', 'DPZ', 'FANG', 'EXR', 'EMR', 'DLTR', 'EMN', 'FAST', 'DVA', 'EBAY', 'EA', 'DRI', 'EOG', 'EL', 'ESS', 'EIX', 'DXCM', 'EFX', 'F', 'ECL', 'ED', 'GS-PJ', 'GILD', 'GIS', 'FMBM', 'FPLPF', 'GM', 'FBHS', 'HBAN', 'FLS', 'FIS', 'FE', 'FRT', 'FRMC', 'FFIV', 'GWW', 'GRMN', 'GGG', 'FN', 'GOOG', 'GPC', 'FLT', 'FITB', 'FCX', 'FISV', 'GPN', 'FMC', 'FRC', 'HAL', 'FDX', 'FCGN', 'FB', 'GE', 'FTI', 'GD', 'HAS', 'HD', 'INTU', 'IFF', 'IRM', 'ICE', 'HLT', 'IDXX', 'HII', 'ILMN', 'HTLF', 'HPQ', 'HON', 'IBM', 'IPGP', 'HCA', 'HRL', 'IR', 'HSY', 'HOLX', 'ISRG', 'HPE', 'HRB', 'HSIC', 'INTH', 'HFC', 'HBI', 'HUM', 'IP', 'HST', 'IEX', 'HES', 'KSU', 'LNT', 'KRA', 'KHC', 'KR', 'KMB', 'JKHY', 'JNJ', 'IT', 'LEG', 'ITW', 'KSS', 'KEY', 'JNPR', 'LKQ', 'KIM', 'IVZ', 'KO', 'LNC', 'JBHT', 'LDOS', 'KMX', 'LMT', 'K', 'JPM', 'KGNR', 'KACPF', 'JCI', 'LH', 'KEYS', 'LBTYA', 'MSFT', 'MKTX', 'LYB', 'MCO', 'MRO', 'MDLZ', 'MLM', 'LVS', 'LRCX', 'MSCI', 'MOS', 'MRK', 'MET', 'MGM', 'MNST', 'MMC', 'MO', 'MCHP', 'LYV', 'MHK', 'MDT', 'LUV', 'MCK', 'MS-PF', 'MMM', 'MAA', 'MCD', 'MPC', 'MAR', 'LOW', 'MRCR', 'NOV', 'PEG', 'NVRO', 'NTRR', 'MU', 'NTRA', 'PAYX', 'NSC', 'NRG', 'ODFL', 'NTAP', 'PBCT', 'NFLX', 'ORLY', 'OMC', 'NTRS', 'NCTKF', 'NOXL', 'OKE', 'NI', 'NVR', 'NOC', 'O', 'NOW', 'PCAR', 'NEE', 'NLSN', 'NWL', 'MSI', 'NDAQ', 'NMHLY', 'OXY', 'NOK', 'NEOG', 'NCLH', 'RF', 'PSX', 'RE', 'PPG', 'ROK', 'PXD', 'RIBT', 'RCL', 'REGN', 'RMD', 'PKI', 'RL', 'RJF', 'PG', 'QRVO', 'REG', 'PHM', 'PNWRF', 'PKG', 'PNW', 'PLD', 'PVH', 'PM', 'PNR', 'PWR', 'PH', 'RLI', 'PEP', 'PRU', 'PFE', 'RHI', 'ROST', 'TAP', 'TEL', 'SRG', 'SLG', 'RSG', 'SYK', 'SNPS', 'SCHW', 'SHW', 'RXMD', 'SEGXF', 'SWKS', 'SBUX', 'RSNHF', 'SWK', 'SONC', 'ROP', 'STZ-B', 'TCYSF', 'STT', 'SPG', 'SYF', 'T', 'STX', 'SIVB', 'SO', 'ROL', 'TJX', 'SEE', 'SLB', 'SRE', 'VZ', 'UNP', 'TMUS', 'TRAUF', 'V', 'TW', 'VRSK', 'TWTR', 'URI', 'ULTA', 'UPS', 'UDR', 'TSN', 'UAL', 'TSCO', 'TTWO', 'VRSN', 'UA', 'TMO', 'WBA', 'TXN', 'UNM', 'USB', 'TXT', 'VMC', 'WAT', 'UHS', 'UEEC', 'VTR', 'TYL', 'TROW', 'TRV', 'VFC', 'WYNN', 'WSPOF', 'WU', 'YUM', 'XYL', 'WST', 'WRK', 'WEC', 'WM', 'ZTS', 'ZBH', 'XOM', 'XEL', 'WDC', 'WRB', 'WY', 'ZION', 'WHR', 'XLEFF', 'WMB', 'XLNX']\n",
        "\n",
        "file_i=55\n",
        "cols=[\"High\",\"Low\",\"Open\",\"Close\"]\n",
        "for file_i in range(100):\n",
        "  fname=\"%s.csv\"%files[file_i]\n",
        "  print(file_i,fname)\n",
        "  cur_fpath=os.path.join(root_dir,fname)\n",
        "  pd_df=pd.read_csv(cur_fpath)\n",
        "  content_2d=get_pd_col_data(pd_df,cols)\n",
        "\n",
        "  for a in content_2d[:10]:\n",
        "    print(a)\n",
        "  print(\"========\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc97iWt2HiqB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}